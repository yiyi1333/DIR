import torch
import torch.nn.functional as F


def norm(x):
    # 归一化处理
    x = F.normalize(x, p=4, dim=-1)
    # 过滤负值，负值为0
    x = F.relu(x)
    # 0值用1e-10替换
    x = torch.where(x == 0, torch.tensor(1e-10), x)
    # x = F.log_softmax(x, dim=-1)
    # print(x)
    return x


# 创建一个示例张量
x1 = torch.tensor(
    [
        1.4624,
        -1.7762,
        -1.6897,
        -1.1976,
        0.2152,
        -1.6692,
        1.8654,
        2.5206,
        -0.8357,
        -1.1684,
        -1.6990,
        -1.3915,
        0.5964,
        -0.4644,
        0.0597,
        -0.1395,
        -0.5249,
        -2.1059,
        -1.5179,
        -0.8948,
        -2.1592,
        1.6795,
        -1.5390,
        -0.5688,
        -0.0923,
        -1.2822,
        1.6955,
        0.7988,
        2.0008,
        -1.1474,
        0.0211,
        -1.4768,
        0.3605,
        0.3202,
        0.3274,
        0.3639,
        0.3882,
        0.3983,
        0.4099,
        0.4344,
        0.4359,
        0.3981,
        0.4689,
        0.3582,
        0.4020,
        0.3332,
        0.3566,
        0.4792,
        0.3997,
        0.3980,
        0.4547,
        0.4016,
        0.3873,
        0.3514,
        0.4353,
        0.3938,
        0.3995,
        0.3494,
        0.3465,
        0.3780,
        0.3540,
        0.4352,
        0.3964,
        0.3865,
    ],
    dtype=torch.float32,
)
x2 = torch.tensor(
    [
        3.0992,
        -1.2010,
        -0.3993,
        -1.8783,
        2.3356,
        0.3463,
        1.5282,
        -1.2026,
        1.0468,
        -2.5510,
        -0.6059,
        -0.7168,
        0.2147,
        -0.9711,
        -0.8342,
        0.3905,
        0.9457,
        -0.3601,
        -2.4153,
        -1.1606,
        -2.8179,
        -0.0435,
        1.4487,
        -1.5259,
        -0.3240,
        -0.9447,
        1.1651,
        1.7998,
        0.1225,
        -0.9318,
        0.6238,
        -1.1439,
        0.1934,
        0.1599,
        0.1609,
        0.1988,
        0.2175,
        0.2210,
        0.2233,
        0.2797,
        0.2513,
        0.2383,
        0.2850,
        0.1904,
        0.2293,
        0.1613,
        0.1970,
        0.2946,
        0.2182,
        0.2417,
        0.2605,
        0.2193,
        0.2294,
        0.1811,
        0.2622,
        0.2373,
        0.2138,
        0.1862,
        0.1833,
        0.2009,
        0.2037,
        0.2778,
        0.2050,
        0.2009,
    ],
    dtype=torch.float32,
)


p = norm(x1)
q = norm(x2)

# 计算两个概率分布的KL散度
kl = F.kl_div(p.log(), q, reduction="sum")
print(kl)

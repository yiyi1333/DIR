Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9536, Train: 0.2429, Val: 0.1460, Test: 0.1650
Epoch: 2, Loss: 1.9775, Train: 0.3714, Val: 0.2620, Test: 0.2780
Epoch: 3, Loss: 1.9203, Train: 0.1500, Val: 0.0720, Test: 0.0920
Epoch: 4, Loss: 1.9749, Train: 0.1786, Val: 0.0720, Test: 0.0940
Epoch: 5, Loss: 1.9590, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 6, Loss: 1.9238, Train: 0.1500, Val: 0.0840, Test: 0.0990
Epoch: 7, Loss: 1.9626, Train: 0.2714, Val: 0.1640, Test: 0.1800
Epoch: 8, Loss: 1.9547, Train: 0.3786, Val: 0.2320, Test: 0.2440
Epoch: 9, Loss: 1.9297, Train: 0.3857, Val: 0.2900, Test: 0.2950
Epoch: 10, Loss: 1.9121, Train: 0.4000, Val: 0.2880, Test: 0.2960
Epoch: 11, Loss: 1.9392, Train: 0.3071, Val: 0.2180, Test: 0.2250
Epoch: 12, Loss: 1.9438, Train: 0.3857, Val: 0.2200, Test: 0.2360
Epoch: 13, Loss: 1.8969, Train: 0.3429, Val: 0.2000, Test: 0.2250
Epoch: 14, Loss: 1.8651, Train: 0.4429, Val: 0.2340, Test: 0.2660
Epoch: 15, Loss: 1.8454, Train: 0.5286, Val: 0.3220, Test: 0.3640
Epoch: 16, Loss: 1.8124, Train: 0.4857, Val: 0.2580, Test: 0.2950
Epoch: 17, Loss: 1.6985, Train: 0.3500, Val: 0.1860, Test: 0.2100
Epoch: 18, Loss: 1.6362, Train: 0.3429, Val: 0.1760, Test: 0.1880
Epoch: 19, Loss: 1.5421, Train: 0.3643, Val: 0.2260, Test: 0.2360
Epoch: 20, Loss: 1.4392, Train: 0.5429, Val: 0.3620, Test: 0.3730
Epoch: 21, Loss: 1.3394, Train: 0.6429, Val: 0.5140, Test: 0.4990
Epoch: 22, Loss: 1.2726, Train: 0.5571, Val: 0.4900, Test: 0.4580
Epoch: 23, Loss: 1.1718, Train: 0.7143, Val: 0.5640, Test: 0.5650
Epoch: 24, Loss: 1.1411, Train: 0.6429, Val: 0.5080, Test: 0.5110
Epoch: 25, Loss: 0.9881, Train: 0.7000, Val: 0.5460, Test: 0.5590
Epoch: 26, Loss: 0.9588, Train: 0.6929, Val: 0.6060, Test: 0.6080
Epoch: 27, Loss: 0.8979, Train: 0.8429, Val: 0.6880, Test: 0.6880
Epoch: 28, Loss: 0.7357, Train: 0.8929, Val: 0.7500, Test: 0.7530
Epoch: 29, Loss: 0.7367, Train: 0.9429, Val: 0.7440, Test: 0.7560
Epoch: 30, Loss: 0.6107, Train: 0.9214, Val: 0.6920, Test: 0.7100
Epoch: 31, Loss: 0.5995, Train: 0.9429, Val: 0.7560, Test: 0.7670
Epoch: 32, Loss: 0.4838, Train: 0.9214, Val: 0.7740, Test: 0.7690
Epoch: 33, Loss: 0.4632, Train: 0.9143, Val: 0.7360, Test: 0.7320
Epoch: 34, Loss: 0.4350, Train: 0.9643, Val: 0.7900, Test: 0.7810
Epoch: 35, Loss: 0.3529, Train: 0.9714, Val: 0.7520, Test: 0.7570
Epoch: 36, Loss: 0.2531, Train: 0.9571, Val: 0.7140, Test: 0.7220
Epoch: 37, Loss: 0.2402, Train: 0.9786, Val: 0.7340, Test: 0.7180
Epoch: 38, Loss: 0.2254, Train: 0.9929, Val: 0.7460, Test: 0.7530
Epoch: 39, Loss: 0.1580, Train: 0.9929, Val: 0.7380, Test: 0.7450
Epoch: 40, Loss: 0.1464, Train: 0.9929, Val: 0.7660, Test: 0.7530
Epoch: 41, Loss: 0.1046, Train: 0.9857, Val: 0.7800, Test: 0.7800
Epoch: 42, Loss: 0.1037, Train: 0.9929, Val: 0.7700, Test: 0.7820
Epoch: 43, Loss: 0.1131, Train: 0.9714, Val: 0.7600, Test: 0.7690
Epoch: 44, Loss: 0.0860, Train: 0.9857, Val: 0.7500, Test: 0.7640
Epoch: 45, Loss: 0.0658, Train: 0.9714, Val: 0.7460, Test: 0.7600
Epoch: 46, Loss: 0.1122, Train: 0.9786, Val: 0.7480, Test: 0.7560
Epoch: 47, Loss: 0.1162, Train: 0.9929, Val: 0.7620, Test: 0.7760
Epoch: 48, Loss: 0.0501, Train: 0.9857, Val: 0.7520, Test: 0.7780
Epoch: 49, Loss: 0.0717, Train: 0.9786, Val: 0.7500, Test: 0.7630
Epoch: 50, Loss: 0.1623, Train: 0.9857, Val: 0.7700, Test: 0.7780
MAD:  0.5478
Best Test Accuracy: 0.7820, Val Accuracy: 0.7700, Train Accuracy: 0.9929
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9736, Train: 0.1429, Val: 0.0800, Test: 0.0960
Epoch: 2, Loss: 2.0050, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 3, Loss: 1.9382, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 4, Loss: 1.9323, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 5, Loss: 1.9470, Train: 0.1571, Val: 0.0860, Test: 0.1000
Epoch: 6, Loss: 1.9321, Train: 0.2714, Val: 0.2200, Test: 0.2200
Epoch: 7, Loss: 1.9746, Train: 0.2357, Val: 0.1960, Test: 0.1790
Epoch: 8, Loss: 1.9382, Train: 0.2071, Val: 0.1960, Test: 0.1920
Epoch: 9, Loss: 1.9086, Train: 0.3286, Val: 0.2660, Test: 0.2650
Epoch: 10, Loss: 1.9082, Train: 0.4000, Val: 0.3320, Test: 0.3310
Epoch: 11, Loss: 1.9331, Train: 0.4500, Val: 0.3460, Test: 0.3460
Epoch: 12, Loss: 1.8903, Train: 0.5643, Val: 0.4480, Test: 0.4670
Epoch: 13, Loss: 1.9306, Train: 0.5071, Val: 0.4740, Test: 0.4860
Epoch: 14, Loss: 1.8226, Train: 0.5643, Val: 0.5380, Test: 0.5430
Epoch: 15, Loss: 1.8290, Train: 0.6357, Val: 0.5780, Test: 0.5650
Epoch: 16, Loss: 1.6931, Train: 0.6571, Val: 0.5680, Test: 0.5480
Epoch: 17, Loss: 1.6671, Train: 0.6000, Val: 0.4080, Test: 0.4050
Epoch: 18, Loss: 1.5556, Train: 0.5857, Val: 0.3800, Test: 0.3860
Epoch: 19, Loss: 1.3832, Train: 0.6286, Val: 0.4140, Test: 0.4180
Epoch: 20, Loss: 1.2254, Train: 0.6857, Val: 0.4660, Test: 0.4690
Epoch: 21, Loss: 1.1008, Train: 0.7643, Val: 0.5240, Test: 0.5340
Epoch: 22, Loss: 0.9810, Train: 0.7714, Val: 0.6040, Test: 0.5980
Epoch: 23, Loss: 0.9190, Train: 0.8000, Val: 0.5260, Test: 0.5480
Epoch: 24, Loss: 0.8250, Train: 0.8000, Val: 0.5460, Test: 0.5440
Epoch: 25, Loss: 0.7537, Train: 0.8643, Val: 0.6960, Test: 0.7080
Epoch: 26, Loss: 0.6431, Train: 0.8286, Val: 0.6640, Test: 0.6960
Epoch: 27, Loss: 0.5793, Train: 0.8857, Val: 0.6820, Test: 0.7090
Epoch: 28, Loss: 0.4866, Train: 0.9286, Val: 0.6960, Test: 0.7020
Epoch: 29, Loss: 0.5079, Train: 0.9714, Val: 0.6800, Test: 0.6950
Epoch: 30, Loss: 0.3801, Train: 0.9643, Val: 0.6860, Test: 0.7220
Epoch: 31, Loss: 0.3804, Train: 0.9643, Val: 0.6860, Test: 0.6980
Epoch: 32, Loss: 0.3210, Train: 0.9714, Val: 0.6640, Test: 0.6860
Epoch: 33, Loss: 0.3055, Train: 0.9786, Val: 0.6980, Test: 0.7280
Epoch: 34, Loss: 0.2422, Train: 0.9357, Val: 0.6840, Test: 0.7150
Epoch: 35, Loss: 0.2693, Train: 0.9929, Val: 0.7280, Test: 0.7570
Epoch: 36, Loss: 0.2198, Train: 0.9929, Val: 0.6820, Test: 0.7130
Epoch: 37, Loss: 0.1727, Train: 0.9786, Val: 0.7140, Test: 0.7350
Epoch: 38, Loss: 0.1562, Train: 0.9857, Val: 0.7000, Test: 0.7440
Epoch: 39, Loss: 0.1490, Train: 0.9857, Val: 0.7140, Test: 0.7570
Epoch: 40, Loss: 0.1009, Train: 0.9929, Val: 0.6940, Test: 0.7240
Epoch: 41, Loss: 0.0800, Train: 0.9786, Val: 0.6640, Test: 0.6850
Epoch: 42, Loss: 0.1271, Train: 0.9929, Val: 0.6940, Test: 0.7160
Epoch: 43, Loss: 0.0701, Train: 0.9714, Val: 0.7140, Test: 0.7450
Epoch: 44, Loss: 0.1361, Train: 0.9857, Val: 0.7220, Test: 0.7580
Epoch: 45, Loss: 0.0686, Train: 1.0000, Val: 0.7400, Test: 0.7530
Epoch: 46, Loss: 0.1048, Train: 1.0000, Val: 0.7220, Test: 0.7470
Epoch: 47, Loss: 0.0586, Train: 1.0000, Val: 0.7060, Test: 0.7150
Epoch: 48, Loss: 0.0461, Train: 0.9929, Val: 0.6980, Test: 0.7050
Epoch: 49, Loss: 0.0844, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 50, Loss: 0.0304, Train: 0.9857, Val: 0.7520, Test: 0.7800
MAD:  0.5709
Best Test Accuracy: 0.7800, Val Accuracy: 0.7520, Train Accuracy: 0.9857
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9642, Train: 0.1429, Val: 0.1580, Test: 0.1470
Epoch: 2, Loss: 1.9454, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 3, Loss: 1.9927, Train: 0.2000, Val: 0.2120, Test: 0.1980
Epoch: 4, Loss: 1.9457, Train: 0.1571, Val: 0.1160, Test: 0.1060
Epoch: 5, Loss: 1.9167, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 6, Loss: 1.9288, Train: 0.1500, Val: 0.1140, Test: 0.1040
Epoch: 7, Loss: 1.9312, Train: 0.3500, Val: 0.1640, Test: 0.1880
Epoch: 8, Loss: 1.9509, Train: 0.2571, Val: 0.0960, Test: 0.1170
Epoch: 9, Loss: 1.9417, Train: 0.1857, Val: 0.0720, Test: 0.0950
Epoch: 10, Loss: 1.9137, Train: 0.1929, Val: 0.0720, Test: 0.0960
Epoch: 11, Loss: 1.9322, Train: 0.2286, Val: 0.0860, Test: 0.1060
Epoch: 12, Loss: 1.9112, Train: 0.4143, Val: 0.2060, Test: 0.2150
Epoch: 13, Loss: 1.9207, Train: 0.5929, Val: 0.3440, Test: 0.3730
Epoch: 14, Loss: 1.8749, Train: 0.6571, Val: 0.5100, Test: 0.5150
Epoch: 15, Loss: 1.8135, Train: 0.6143, Val: 0.5420, Test: 0.5250
Epoch: 16, Loss: 1.7460, Train: 0.6500, Val: 0.5660, Test: 0.5440
Epoch: 17, Loss: 1.6574, Train: 0.6429, Val: 0.6440, Test: 0.6300
Epoch: 18, Loss: 1.5182, Train: 0.5786, Val: 0.6140, Test: 0.5810
Epoch: 19, Loss: 1.3761, Train: 0.5857, Val: 0.6080, Test: 0.5760
Epoch: 20, Loss: 1.3365, Train: 0.5929, Val: 0.6200, Test: 0.5800
Epoch: 21, Loss: 1.2196, Train: 0.5929, Val: 0.6320, Test: 0.5990
Epoch: 22, Loss: 1.0945, Train: 0.7643, Val: 0.7040, Test: 0.6790
Epoch: 23, Loss: 1.0074, Train: 0.7571, Val: 0.7040, Test: 0.6880
Epoch: 24, Loss: 0.9044, Train: 0.8143, Val: 0.7120, Test: 0.7110
Epoch: 25, Loss: 0.8623, Train: 0.8714, Val: 0.7320, Test: 0.7280
Epoch: 26, Loss: 0.8229, Train: 0.8786, Val: 0.7020, Test: 0.7020
Epoch: 27, Loss: 0.7157, Train: 0.8857, Val: 0.7060, Test: 0.7220
Epoch: 28, Loss: 0.7225, Train: 0.8929, Val: 0.6840, Test: 0.6570
Epoch: 29, Loss: 0.6519, Train: 0.9071, Val: 0.7060, Test: 0.6770
Epoch: 30, Loss: 0.6158, Train: 0.9643, Val: 0.7240, Test: 0.7440
Epoch: 31, Loss: 0.4938, Train: 0.9571, Val: 0.7360, Test: 0.7240
Epoch: 32, Loss: 0.4593, Train: 0.9286, Val: 0.7040, Test: 0.7140
Epoch: 33, Loss: 0.4676, Train: 0.9286, Val: 0.6880, Test: 0.7340
Epoch: 34, Loss: 0.4446, Train: 0.9571, Val: 0.7180, Test: 0.7580
Epoch: 35, Loss: 0.3277, Train: 0.9929, Val: 0.7340, Test: 0.7590
Epoch: 36, Loss: 0.2981, Train: 0.9643, Val: 0.7200, Test: 0.7180
Epoch: 37, Loss: 0.2532, Train: 0.9643, Val: 0.7340, Test: 0.7420
Epoch: 38, Loss: 0.2211, Train: 0.9786, Val: 0.7560, Test: 0.7760
Epoch: 39, Loss: 0.2004, Train: 0.9857, Val: 0.7540, Test: 0.7700
Epoch: 40, Loss: 0.1624, Train: 0.9929, Val: 0.7440, Test: 0.7710
Epoch: 41, Loss: 0.1662, Train: 1.0000, Val: 0.7560, Test: 0.7730
Epoch: 42, Loss: 0.1140, Train: 0.9929, Val: 0.7520, Test: 0.7750
Epoch: 43, Loss: 0.1069, Train: 0.9929, Val: 0.7480, Test: 0.7650
Epoch: 44, Loss: 0.0721, Train: 1.0000, Val: 0.7460, Test: 0.7540
Epoch: 45, Loss: 0.0701, Train: 1.0000, Val: 0.7240, Test: 0.7500
Epoch: 46, Loss: 0.0868, Train: 0.9929, Val: 0.7280, Test: 0.7610
Epoch: 47, Loss: 0.0635, Train: 1.0000, Val: 0.7280, Test: 0.7690
Epoch: 48, Loss: 0.0484, Train: 0.9929, Val: 0.7500, Test: 0.7770
Epoch: 49, Loss: 0.0798, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 50, Loss: 0.0603, Train: 1.0000, Val: 0.7420, Test: 0.7730
MAD:  0.5551
Best Test Accuracy: 0.7770, Val Accuracy: 0.7500, Train Accuracy: 0.9929
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9906, Train: 0.1643, Val: 0.0780, Test: 0.1000
Epoch: 2, Loss: 1.9991, Train: 0.1571, Val: 0.1200, Test: 0.1300
Epoch: 3, Loss: 1.9230, Train: 0.1714, Val: 0.1240, Test: 0.1430
Epoch: 4, Loss: 1.9238, Train: 0.2929, Val: 0.1200, Test: 0.1520
Epoch: 5, Loss: 1.9339, Train: 0.4214, Val: 0.2300, Test: 0.2450
Epoch: 6, Loss: 1.9191, Train: 0.3143, Val: 0.1960, Test: 0.2020
Epoch: 7, Loss: 1.9423, Train: 0.4500, Val: 0.3120, Test: 0.2930
Epoch: 8, Loss: 1.9751, Train: 0.2071, Val: 0.1280, Test: 0.1160
Epoch: 9, Loss: 1.9506, Train: 0.1429, Val: 0.1200, Test: 0.1050
Epoch: 10, Loss: 1.9300, Train: 0.2429, Val: 0.1580, Test: 0.1480
Epoch: 11, Loss: 1.9577, Train: 0.2214, Val: 0.1080, Test: 0.1290
Epoch: 12, Loss: 1.9154, Train: 0.2143, Val: 0.0820, Test: 0.1010
Epoch: 13, Loss: 1.8831, Train: 0.3500, Val: 0.1720, Test: 0.1940
Epoch: 14, Loss: 1.8990, Train: 0.5643, Val: 0.4020, Test: 0.4560
Epoch: 15, Loss: 1.8790, Train: 0.3929, Val: 0.2740, Test: 0.2830
Epoch: 16, Loss: 1.8115, Train: 0.3786, Val: 0.2740, Test: 0.2680
Epoch: 17, Loss: 1.7931, Train: 0.5429, Val: 0.3880, Test: 0.3940
Epoch: 18, Loss: 1.7280, Train: 0.6714, Val: 0.5620, Test: 0.5900
Epoch: 19, Loss: 1.6258, Train: 0.6929, Val: 0.5940, Test: 0.6190
Epoch: 20, Loss: 1.4991, Train: 0.6786, Val: 0.5920, Test: 0.6390
Epoch: 21, Loss: 1.3893, Train: 0.6643, Val: 0.5760, Test: 0.6180
Epoch: 22, Loss: 1.2194, Train: 0.7500, Val: 0.6440, Test: 0.6910
Epoch: 23, Loss: 1.0917, Train: 0.8214, Val: 0.6520, Test: 0.6760
Epoch: 24, Loss: 0.9738, Train: 0.8714, Val: 0.6580, Test: 0.6440
Epoch: 25, Loss: 0.9044, Train: 0.8571, Val: 0.6660, Test: 0.6520
Epoch: 26, Loss: 0.7586, Train: 0.9000, Val: 0.6980, Test: 0.7140
Epoch: 27, Loss: 0.7373, Train: 0.9071, Val: 0.6880, Test: 0.7100
Epoch: 28, Loss: 0.6557, Train: 0.8429, Val: 0.6880, Test: 0.6740
Epoch: 29, Loss: 0.5841, Train: 0.9214, Val: 0.7120, Test: 0.7260
Epoch: 30, Loss: 0.4661, Train: 0.9643, Val: 0.7440, Test: 0.7570
Epoch: 31, Loss: 0.3547, Train: 0.9357, Val: 0.7080, Test: 0.6940
Epoch: 32, Loss: 0.3947, Train: 0.9429, Val: 0.7360, Test: 0.7610
Epoch: 33, Loss: 0.3177, Train: 0.9571, Val: 0.7540, Test: 0.7600
Epoch: 34, Loss: 0.2584, Train: 0.9786, Val: 0.7360, Test: 0.7470
Epoch: 35, Loss: 0.2305, Train: 0.9286, Val: 0.6940, Test: 0.7120
Epoch: 36, Loss: 0.2629, Train: 0.9857, Val: 0.7500, Test: 0.7530
Epoch: 37, Loss: 0.1498, Train: 0.9714, Val: 0.7440, Test: 0.7520
Epoch: 38, Loss: 0.1709, Train: 0.9786, Val: 0.7420, Test: 0.7460
Epoch: 39, Loss: 0.1199, Train: 0.9857, Val: 0.7400, Test: 0.7380
Epoch: 40, Loss: 0.0950, Train: 0.9857, Val: 0.7440, Test: 0.7440
Epoch: 41, Loss: 0.1244, Train: 0.9857, Val: 0.7480, Test: 0.7400
Epoch: 42, Loss: 0.1146, Train: 0.9857, Val: 0.7360, Test: 0.7310
Epoch: 43, Loss: 0.0722, Train: 0.9929, Val: 0.7440, Test: 0.7420
Epoch: 44, Loss: 0.0547, Train: 0.9857, Val: 0.7460, Test: 0.7450
Epoch: 45, Loss: 0.0720, Train: 0.9857, Val: 0.7360, Test: 0.7390
Epoch: 46, Loss: 0.0867, Train: 0.9929, Val: 0.7380, Test: 0.7510
Epoch: 47, Loss: 0.0486, Train: 1.0000, Val: 0.7380, Test: 0.7550
Epoch: 48, Loss: 0.0534, Train: 1.0000, Val: 0.7340, Test: 0.7490
Epoch: 49, Loss: 0.0349, Train: 0.9929, Val: 0.7380, Test: 0.7490
Epoch: 50, Loss: 0.0625, Train: 1.0000, Val: 0.7340, Test: 0.7500
MAD:  0.4578
Best Test Accuracy: 0.7610, Val Accuracy: 0.7360, Train Accuracy: 0.9429
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0128, Train: 0.1857, Val: 0.2400, Test: 0.2370
Epoch: 2, Loss: 1.9418, Train: 0.1857, Val: 0.3240, Test: 0.3240
Epoch: 3, Loss: 1.9762, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 4, Loss: 1.9664, Train: 0.1857, Val: 0.3220, Test: 0.3230
Epoch: 5, Loss: 1.9420, Train: 0.2071, Val: 0.1340, Test: 0.1330
Epoch: 6, Loss: 1.9460, Train: 0.2429, Val: 0.1260, Test: 0.1370
Epoch: 7, Loss: 1.9332, Train: 0.1643, Val: 0.0820, Test: 0.0950
Epoch: 8, Loss: 1.9500, Train: 0.1500, Val: 0.0720, Test: 0.0920
Epoch: 9, Loss: 1.9287, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 10, Loss: 1.9424, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 11, Loss: 1.9579, Train: 0.2500, Val: 0.1260, Test: 0.1290
Epoch: 12, Loss: 1.9536, Train: 0.4143, Val: 0.2680, Test: 0.2620
Epoch: 13, Loss: 1.9224, Train: 0.3143, Val: 0.2040, Test: 0.1930
Epoch: 14, Loss: 1.9472, Train: 0.2071, Val: 0.1480, Test: 0.1610
Epoch: 15, Loss: 1.8939, Train: 0.3000, Val: 0.3140, Test: 0.2760
Epoch: 16, Loss: 1.9202, Train: 0.3143, Val: 0.3220, Test: 0.2770
Epoch: 17, Loss: 1.9029, Train: 0.3143, Val: 0.3000, Test: 0.2670
Epoch: 18, Loss: 1.8965, Train: 0.3786, Val: 0.3220, Test: 0.2870
Epoch: 19, Loss: 1.8563, Train: 0.4357, Val: 0.3640, Test: 0.3570
Epoch: 20, Loss: 1.8376, Train: 0.5286, Val: 0.4180, Test: 0.4410
Epoch: 21, Loss: 1.7282, Train: 0.5357, Val: 0.4120, Test: 0.4140
Epoch: 22, Loss: 1.7058, Train: 0.4929, Val: 0.4000, Test: 0.3690
Epoch: 23, Loss: 1.5690, Train: 0.5000, Val: 0.4040, Test: 0.3830
Epoch: 24, Loss: 1.4666, Train: 0.6071, Val: 0.4720, Test: 0.4380
Epoch: 25, Loss: 1.3935, Train: 0.6286, Val: 0.4640, Test: 0.4410
Epoch: 26, Loss: 1.2617, Train: 0.6000, Val: 0.4620, Test: 0.4310
Epoch: 27, Loss: 1.1815, Train: 0.6643, Val: 0.4840, Test: 0.4650
Epoch: 28, Loss: 1.0836, Train: 0.6643, Val: 0.5060, Test: 0.4750
Epoch: 29, Loss: 1.0508, Train: 0.7786, Val: 0.5520, Test: 0.5320
Epoch: 30, Loss: 0.8558, Train: 0.7786, Val: 0.5180, Test: 0.5070
Epoch: 31, Loss: 0.8048, Train: 0.8429, Val: 0.5620, Test: 0.5630
Epoch: 32, Loss: 0.7239, Train: 0.9071, Val: 0.7080, Test: 0.6880
Epoch: 33, Loss: 0.6808, Train: 0.8286, Val: 0.6800, Test: 0.6630
Epoch: 34, Loss: 0.5755, Train: 0.8714, Val: 0.6760, Test: 0.6820
Epoch: 35, Loss: 0.5247, Train: 0.9571, Val: 0.7300, Test: 0.7290
Epoch: 36, Loss: 0.4375, Train: 0.9286, Val: 0.7000, Test: 0.6910
Epoch: 37, Loss: 0.4504, Train: 0.9357, Val: 0.7040, Test: 0.6870
Epoch: 38, Loss: 0.3562, Train: 0.9500, Val: 0.7060, Test: 0.7220
Epoch: 39, Loss: 0.2966, Train: 0.9786, Val: 0.7300, Test: 0.7580
Epoch: 40, Loss: 0.3318, Train: 0.9786, Val: 0.7380, Test: 0.7510
Epoch: 41, Loss: 0.2219, Train: 0.9714, Val: 0.6880, Test: 0.7150
Epoch: 42, Loss: 0.2492, Train: 0.9786, Val: 0.7280, Test: 0.7190
Epoch: 43, Loss: 0.1317, Train: 0.9714, Val: 0.7260, Test: 0.7210
Epoch: 44, Loss: 0.1492, Train: 0.9714, Val: 0.7340, Test: 0.7420
Epoch: 45, Loss: 0.1522, Train: 0.9786, Val: 0.7240, Test: 0.7510
Epoch: 46, Loss: 0.1040, Train: 0.9786, Val: 0.7120, Test: 0.7400
Epoch: 47, Loss: 0.1238, Train: 0.9857, Val: 0.7140, Test: 0.7330
Epoch: 48, Loss: 0.0665, Train: 0.9929, Val: 0.7240, Test: 0.7180
Epoch: 49, Loss: 0.0748, Train: 0.9929, Val: 0.7260, Test: 0.7190
Epoch: 50, Loss: 0.0770, Train: 1.0000, Val: 0.7440, Test: 0.7460
MAD:  0.439
Best Test Accuracy: 0.7580, Val Accuracy: 0.7300, Train Accuracy: 0.9786
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0002, Train: 0.1429, Val: 0.1180, Test: 0.1430
Epoch: 2, Loss: 1.9467, Train: 0.2071, Val: 0.2820, Test: 0.2910
Epoch: 3, Loss: 1.9624, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 4, Loss: 1.9339, Train: 0.2000, Val: 0.1640, Test: 0.1620
Epoch: 5, Loss: 1.9547, Train: 0.2000, Val: 0.1160, Test: 0.1330
Epoch: 6, Loss: 1.9775, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 7, Loss: 1.9635, Train: 0.3643, Val: 0.1920, Test: 0.1820
Epoch: 8, Loss: 1.9499, Train: 0.1786, Val: 0.0720, Test: 0.0940
Epoch: 9, Loss: 1.9454, Train: 0.1786, Val: 0.0720, Test: 0.0950
Epoch: 10, Loss: 1.9801, Train: 0.1571, Val: 0.0720, Test: 0.0930
Epoch: 11, Loss: 1.9421, Train: 0.1857, Val: 0.0760, Test: 0.0990
Epoch: 12, Loss: 1.9497, Train: 0.3143, Val: 0.1840, Test: 0.1790
Epoch: 13, Loss: 1.9208, Train: 0.1857, Val: 0.1220, Test: 0.1380
Epoch: 14, Loss: 1.9180, Train: 0.1786, Val: 0.1220, Test: 0.1340
Epoch: 15, Loss: 1.9149, Train: 0.3500, Val: 0.2420, Test: 0.2540
Epoch: 16, Loss: 1.9366, Train: 0.5000, Val: 0.4740, Test: 0.4690
Epoch: 17, Loss: 1.9117, Train: 0.6071, Val: 0.5720, Test: 0.5800
Epoch: 18, Loss: 1.8932, Train: 0.4714, Val: 0.4300, Test: 0.4180
Epoch: 19, Loss: 1.8603, Train: 0.5000, Val: 0.4080, Test: 0.3970
Epoch: 20, Loss: 1.8267, Train: 0.6714, Val: 0.5880, Test: 0.5880
Epoch: 21, Loss: 1.7421, Train: 0.7357, Val: 0.6680, Test: 0.6460
Epoch: 22, Loss: 1.6259, Train: 0.7357, Val: 0.6880, Test: 0.6840
Epoch: 23, Loss: 1.5405, Train: 0.6786, Val: 0.6240, Test: 0.5970
Epoch: 24, Loss: 1.3948, Train: 0.6500, Val: 0.5500, Test: 0.5350
Epoch: 25, Loss: 1.3335, Train: 0.6143, Val: 0.4780, Test: 0.4650
Epoch: 26, Loss: 1.2195, Train: 0.7357, Val: 0.5600, Test: 0.5350
Epoch: 27, Loss: 1.1298, Train: 0.7643, Val: 0.5780, Test: 0.5780
Epoch: 28, Loss: 0.9853, Train: 0.8071, Val: 0.6040, Test: 0.6120
Epoch: 29, Loss: 0.8740, Train: 0.9071, Val: 0.6900, Test: 0.7130
Epoch: 30, Loss: 0.7727, Train: 0.9143, Val: 0.7000, Test: 0.7220
Epoch: 31, Loss: 0.6781, Train: 0.9786, Val: 0.7380, Test: 0.7610
Epoch: 32, Loss: 0.5440, Train: 0.9857, Val: 0.7520, Test: 0.7740
Epoch: 33, Loss: 0.4759, Train: 0.9643, Val: 0.7260, Test: 0.7600
Epoch: 34, Loss: 0.3846, Train: 0.9857, Val: 0.7400, Test: 0.7480
Epoch: 35, Loss: 0.3181, Train: 0.9714, Val: 0.7540, Test: 0.7870
Epoch: 36, Loss: 0.2768, Train: 0.9714, Val: 0.7620, Test: 0.7810
Epoch: 37, Loss: 0.2544, Train: 0.9714, Val: 0.7660, Test: 0.7730
Epoch: 38, Loss: 0.2097, Train: 0.9929, Val: 0.7640, Test: 0.7790
Epoch: 39, Loss: 0.1698, Train: 0.9857, Val: 0.7680, Test: 0.7740
Epoch: 40, Loss: 0.1305, Train: 0.9929, Val: 0.7520, Test: 0.7720
Epoch: 41, Loss: 0.1155, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 42, Loss: 0.0963, Train: 0.9857, Val: 0.7380, Test: 0.7710
Epoch: 43, Loss: 0.0952, Train: 0.9857, Val: 0.7480, Test: 0.7680
Epoch: 44, Loss: 0.0783, Train: 0.9857, Val: 0.7480, Test: 0.7720
Epoch: 45, Loss: 0.0981, Train: 0.9929, Val: 0.7500, Test: 0.7820
Epoch: 46, Loss: 0.0503, Train: 0.9857, Val: 0.7420, Test: 0.7670
Epoch: 47, Loss: 0.0905, Train: 0.9857, Val: 0.7760, Test: 0.7670
Epoch: 48, Loss: 0.0959, Train: 1.0000, Val: 0.7620, Test: 0.7680
Epoch: 49, Loss: 0.0523, Train: 0.9786, Val: 0.7640, Test: 0.7720
Epoch: 50, Loss: 0.0866, Train: 0.9714, Val: 0.7540, Test: 0.7770
MAD:  0.4979
Best Test Accuracy: 0.7870, Val Accuracy: 0.7540, Train Accuracy: 0.9714
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9935, Train: 0.1429, Val: 0.0760, Test: 0.0910
Epoch: 2, Loss: 1.9563, Train: 0.1357, Val: 0.0820, Test: 0.0980
Epoch: 3, Loss: 1.9681, Train: 0.2071, Val: 0.0700, Test: 0.0830
Epoch: 4, Loss: 1.9625, Train: 0.1571, Val: 0.0740, Test: 0.0950
Epoch: 5, Loss: 1.9713, Train: 0.1571, Val: 0.1600, Test: 0.1500
Epoch: 6, Loss: 1.9312, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 7, Loss: 1.9506, Train: 0.1429, Val: 0.1280, Test: 0.1300
Epoch: 8, Loss: 1.9468, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 9, Loss: 1.9769, Train: 0.2571, Val: 0.1640, Test: 0.1640
Epoch: 10, Loss: 1.9605, Train: 0.3143, Val: 0.1660, Test: 0.1750
Epoch: 11, Loss: 1.9106, Train: 0.3000, Val: 0.1400, Test: 0.1660
Epoch: 12, Loss: 1.9250, Train: 0.2857, Val: 0.1260, Test: 0.1530
Epoch: 13, Loss: 1.9155, Train: 0.2786, Val: 0.1240, Test: 0.1490
Epoch: 14, Loss: 1.9239, Train: 0.2714, Val: 0.1140, Test: 0.1350
Epoch: 15, Loss: 1.9074, Train: 0.4929, Val: 0.2820, Test: 0.2860
Epoch: 16, Loss: 1.8888, Train: 0.5929, Val: 0.3920, Test: 0.3930
Epoch: 17, Loss: 1.8620, Train: 0.5214, Val: 0.3840, Test: 0.3940
Epoch: 18, Loss: 1.7721, Train: 0.4714, Val: 0.3300, Test: 0.3330
Epoch: 19, Loss: 1.6893, Train: 0.3500, Val: 0.2600, Test: 0.2600
Epoch: 20, Loss: 1.6079, Train: 0.3429, Val: 0.2440, Test: 0.2500
Epoch: 21, Loss: 1.5154, Train: 0.3357, Val: 0.2660, Test: 0.2690
Epoch: 22, Loss: 1.4512, Train: 0.4000, Val: 0.2840, Test: 0.3060
Epoch: 23, Loss: 1.3867, Train: 0.5571, Val: 0.3860, Test: 0.3970
Epoch: 24, Loss: 1.3528, Train: 0.7571, Val: 0.5220, Test: 0.5350
Epoch: 25, Loss: 1.2405, Train: 0.7929, Val: 0.5900, Test: 0.5960
Epoch: 26, Loss: 1.1893, Train: 0.8071, Val: 0.5960, Test: 0.6170
Epoch: 27, Loss: 1.0529, Train: 0.8000, Val: 0.6220, Test: 0.6210
Epoch: 28, Loss: 1.0071, Train: 0.8857, Val: 0.6440, Test: 0.6540
Epoch: 29, Loss: 0.8969, Train: 0.7929, Val: 0.5700, Test: 0.6000
Epoch: 30, Loss: 0.9357, Train: 0.8929, Val: 0.6620, Test: 0.6710
Epoch: 31, Loss: 0.7871, Train: 0.9286, Val: 0.7080, Test: 0.7230
Epoch: 32, Loss: 0.6471, Train: 0.9071, Val: 0.7120, Test: 0.7190
Epoch: 33, Loss: 0.6211, Train: 0.9429, Val: 0.7080, Test: 0.7200
Epoch: 34, Loss: 0.5701, Train: 0.9571, Val: 0.6860, Test: 0.6950
Epoch: 35, Loss: 0.4771, Train: 0.9500, Val: 0.7300, Test: 0.7210
Epoch: 36, Loss: 0.3563, Train: 0.9571, Val: 0.7160, Test: 0.7360
Epoch: 37, Loss: 0.3589, Train: 0.9786, Val: 0.7320, Test: 0.7740
Epoch: 38, Loss: 0.2835, Train: 0.9929, Val: 0.7540, Test: 0.7630
Epoch: 39, Loss: 0.2509, Train: 0.9786, Val: 0.6880, Test: 0.7090
Epoch: 40, Loss: 0.2247, Train: 0.9714, Val: 0.7320, Test: 0.7340
Epoch: 41, Loss: 0.2045, Train: 1.0000, Val: 0.7640, Test: 0.7740
Epoch: 42, Loss: 0.1572, Train: 0.9857, Val: 0.7500, Test: 0.7760
Epoch: 43, Loss: 0.1467, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 44, Loss: 0.1303, Train: 0.9857, Val: 0.7640, Test: 0.7580
Epoch: 45, Loss: 0.0931, Train: 0.9929, Val: 0.7580, Test: 0.7580
Epoch: 46, Loss: 0.0993, Train: 1.0000, Val: 0.7440, Test: 0.7550
Epoch: 47, Loss: 0.0612, Train: 1.0000, Val: 0.7420, Test: 0.7570
Epoch: 48, Loss: 0.0685, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 49, Loss: 0.0746, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 50, Loss: 0.0665, Train: 0.9929, Val: 0.7400, Test: 0.7620
MAD:  0.5106
Best Test Accuracy: 0.7760, Val Accuracy: 0.7500, Train Accuracy: 0.9857
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9861, Train: 0.1429, Val: 0.1580, Test: 0.1480
Epoch: 2, Loss: 1.9585, Train: 0.2071, Val: 0.1440, Test: 0.1470
Epoch: 3, Loss: 1.9863, Train: 0.1786, Val: 0.1360, Test: 0.1500
Epoch: 4, Loss: 1.9420, Train: 0.1500, Val: 0.0860, Test: 0.1090
Epoch: 5, Loss: 1.9491, Train: 0.3071, Val: 0.1900, Test: 0.1950
Epoch: 6, Loss: 1.9396, Train: 0.3143, Val: 0.2000, Test: 0.2050
Epoch: 7, Loss: 1.9101, Train: 0.2857, Val: 0.2080, Test: 0.1960
Epoch: 8, Loss: 1.9414, Train: 0.2143, Val: 0.1660, Test: 0.1590
Epoch: 9, Loss: 1.9409, Train: 0.2857, Val: 0.1880, Test: 0.1730
Epoch: 10, Loss: 1.9553, Train: 0.2786, Val: 0.1860, Test: 0.1760
Epoch: 11, Loss: 1.9263, Train: 0.3643, Val: 0.2540, Test: 0.2580
Epoch: 12, Loss: 1.9200, Train: 0.3500, Val: 0.2320, Test: 0.2450
Epoch: 13, Loss: 1.9063, Train: 0.3714, Val: 0.2600, Test: 0.2630
Epoch: 14, Loss: 1.8659, Train: 0.4571, Val: 0.2960, Test: 0.2940
Epoch: 15, Loss: 1.8549, Train: 0.6000, Val: 0.3960, Test: 0.4220
Epoch: 16, Loss: 1.7993, Train: 0.5786, Val: 0.3860, Test: 0.4120
Epoch: 17, Loss: 1.7605, Train: 0.3857, Val: 0.2920, Test: 0.2880
Epoch: 18, Loss: 1.6315, Train: 0.3714, Val: 0.2760, Test: 0.2930
Epoch: 19, Loss: 1.5273, Train: 0.4071, Val: 0.2960, Test: 0.3240
Epoch: 20, Loss: 1.4279, Train: 0.6000, Val: 0.4020, Test: 0.3860
Epoch: 21, Loss: 1.4093, Train: 0.6714, Val: 0.4380, Test: 0.4360
Epoch: 22, Loss: 1.3646, Train: 0.6857, Val: 0.4640, Test: 0.4560
Epoch: 23, Loss: 1.2656, Train: 0.6929, Val: 0.4760, Test: 0.4690
Epoch: 24, Loss: 1.1727, Train: 0.6857, Val: 0.5200, Test: 0.4990
Epoch: 25, Loss: 1.0735, Train: 0.7071, Val: 0.5860, Test: 0.5590
Epoch: 26, Loss: 1.0375, Train: 0.7714, Val: 0.5380, Test: 0.5480
Epoch: 27, Loss: 0.9598, Train: 0.8714, Val: 0.6340, Test: 0.6310
Epoch: 28, Loss: 0.8250, Train: 0.8286, Val: 0.6720, Test: 0.6620
Epoch: 29, Loss: 0.7622, Train: 0.8929, Val: 0.7200, Test: 0.7150
Epoch: 30, Loss: 0.7284, Train: 0.9214, Val: 0.7320, Test: 0.7220
Epoch: 31, Loss: 0.6001, Train: 0.9357, Val: 0.7500, Test: 0.7470
Epoch: 32, Loss: 0.6020, Train: 0.9571, Val: 0.7640, Test: 0.7710
Epoch: 33, Loss: 0.4153, Train: 0.9357, Val: 0.7560, Test: 0.7650
Epoch: 34, Loss: 0.4224, Train: 0.9500, Val: 0.7600, Test: 0.7540
Epoch: 35, Loss: 0.3306, Train: 0.9571, Val: 0.7580, Test: 0.7660
Epoch: 36, Loss: 0.2567, Train: 0.9714, Val: 0.7740, Test: 0.7810
Epoch: 37, Loss: 0.2326, Train: 0.9786, Val: 0.7680, Test: 0.7790
Epoch: 38, Loss: 0.1801, Train: 0.9929, Val: 0.7620, Test: 0.7740
Epoch: 39, Loss: 0.1065, Train: 0.9857, Val: 0.7660, Test: 0.7730
Epoch: 40, Loss: 0.1185, Train: 0.9929, Val: 0.7540, Test: 0.7760
Epoch: 41, Loss: 0.1238, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 42, Loss: 0.0673, Train: 0.9929, Val: 0.7420, Test: 0.7610
Epoch: 43, Loss: 0.0860, Train: 1.0000, Val: 0.7560, Test: 0.7680
Epoch: 44, Loss: 0.0830, Train: 0.9929, Val: 0.7480, Test: 0.7580
Epoch: 45, Loss: 0.0475, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 46, Loss: 0.0471, Train: 1.0000, Val: 0.7540, Test: 0.7660
Epoch: 47, Loss: 0.0418, Train: 1.0000, Val: 0.7540, Test: 0.7750
Epoch: 48, Loss: 0.0330, Train: 1.0000, Val: 0.7560, Test: 0.7820
Epoch: 49, Loss: 0.0362, Train: 1.0000, Val: 0.7620, Test: 0.7860
Epoch: 50, Loss: 0.0415, Train: 0.9857, Val: 0.7480, Test: 0.7780
MAD:  0.5968
Best Test Accuracy: 0.7860, Val Accuracy: 0.7620, Train Accuracy: 1.0000
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9735, Train: 0.1429, Val: 0.1560, Test: 0.1430
Epoch: 2, Loss: 1.9729, Train: 0.1571, Val: 0.1620, Test: 0.1450
Epoch: 3, Loss: 1.9732, Train: 0.3286, Val: 0.2320, Test: 0.2310
Epoch: 4, Loss: 1.9551, Train: 0.2857, Val: 0.1980, Test: 0.2130
Epoch: 5, Loss: 1.9553, Train: 0.1714, Val: 0.1380, Test: 0.1560
Epoch: 6, Loss: 1.9730, Train: 0.1786, Val: 0.1720, Test: 0.1580
Epoch: 7, Loss: 1.9541, Train: 0.1857, Val: 0.1560, Test: 0.1490
Epoch: 8, Loss: 1.9455, Train: 0.3143, Val: 0.2380, Test: 0.2220
Epoch: 9, Loss: 1.9228, Train: 0.4214, Val: 0.2600, Test: 0.2490
Epoch: 10, Loss: 1.9599, Train: 0.4714, Val: 0.3380, Test: 0.3500
Epoch: 11, Loss: 1.9450, Train: 0.1929, Val: 0.1680, Test: 0.1580
Epoch: 12, Loss: 1.9271, Train: 0.1857, Val: 0.1620, Test: 0.1530
Epoch: 13, Loss: 1.9153, Train: 0.1857, Val: 0.1620, Test: 0.1540
Epoch: 14, Loss: 1.9180, Train: 0.2071, Val: 0.1620, Test: 0.1610
Epoch: 15, Loss: 1.8806, Train: 0.3857, Val: 0.2380, Test: 0.2440
Epoch: 16, Loss: 1.8920, Train: 0.5929, Val: 0.4500, Test: 0.4510
Epoch: 17, Loss: 1.8293, Train: 0.6214, Val: 0.4720, Test: 0.4770
Epoch: 18, Loss: 1.7876, Train: 0.5214, Val: 0.3740, Test: 0.3710
Epoch: 19, Loss: 1.6832, Train: 0.5571, Val: 0.4340, Test: 0.4330
Epoch: 20, Loss: 1.5880, Train: 0.4929, Val: 0.4220, Test: 0.4010
Epoch: 21, Loss: 1.4755, Train: 0.5071, Val: 0.4360, Test: 0.4140
Epoch: 22, Loss: 1.3813, Train: 0.6214, Val: 0.5580, Test: 0.5210
Epoch: 23, Loss: 1.3207, Train: 0.6214, Val: 0.5180, Test: 0.4990
Epoch: 24, Loss: 1.2382, Train: 0.6786, Val: 0.4960, Test: 0.4830
Epoch: 25, Loss: 1.1631, Train: 0.7786, Val: 0.5520, Test: 0.5290
Epoch: 26, Loss: 1.0800, Train: 0.8143, Val: 0.5640, Test: 0.5700
Epoch: 27, Loss: 1.0291, Train: 0.8071, Val: 0.5680, Test: 0.5590
Epoch: 28, Loss: 0.8896, Train: 0.8929, Val: 0.6220, Test: 0.6200
Epoch: 29, Loss: 0.8218, Train: 0.9143, Val: 0.6660, Test: 0.6970
Epoch: 30, Loss: 0.7492, Train: 0.8143, Val: 0.6660, Test: 0.6830
Epoch: 31, Loss: 0.6896, Train: 0.9214, Val: 0.7000, Test: 0.7220
Epoch: 32, Loss: 0.5862, Train: 0.9214, Val: 0.7040, Test: 0.7500
Epoch: 33, Loss: 0.5954, Train: 0.9571, Val: 0.6920, Test: 0.7280
Epoch: 34, Loss: 0.4529, Train: 0.9500, Val: 0.6500, Test: 0.6860
Epoch: 35, Loss: 0.5123, Train: 1.0000, Val: 0.6900, Test: 0.7340
Epoch: 36, Loss: 0.3459, Train: 0.9857, Val: 0.7240, Test: 0.7530
Epoch: 37, Loss: 0.3223, Train: 0.9929, Val: 0.7220, Test: 0.7510
Epoch: 38, Loss: 0.2424, Train: 0.9857, Val: 0.7120, Test: 0.7190
Epoch: 39, Loss: 0.1997, Train: 0.9714, Val: 0.7000, Test: 0.6850
Epoch: 40, Loss: 0.1914, Train: 0.9929, Val: 0.7140, Test: 0.7150
Epoch: 41, Loss: 0.1428, Train: 0.9786, Val: 0.7040, Test: 0.7270
Epoch: 42, Loss: 0.1290, Train: 0.9857, Val: 0.7140, Test: 0.7450
Epoch: 43, Loss: 0.1875, Train: 0.9929, Val: 0.7460, Test: 0.7670
Epoch: 44, Loss: 0.1193, Train: 0.9786, Val: 0.7460, Test: 0.7400
Epoch: 45, Loss: 0.1203, Train: 1.0000, Val: 0.7240, Test: 0.7240
Epoch: 46, Loss: 0.0923, Train: 0.9643, Val: 0.7020, Test: 0.7090
Epoch: 47, Loss: 0.1632, Train: 0.9643, Val: 0.7200, Test: 0.7400
Epoch: 48, Loss: 0.1060, Train: 0.9857, Val: 0.7280, Test: 0.7480
Epoch: 49, Loss: 0.0861, Train: 0.9857, Val: 0.7160, Test: 0.7250
Epoch: 50, Loss: 0.0848, Train: 0.9714, Val: 0.7080, Test: 0.7070
MAD:  0.5485
Best Test Accuracy: 0.7670, Val Accuracy: 0.7460, Train Accuracy: 0.9929
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-7): 7 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (8): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9739, Train: 0.1571, Val: 0.1460, Test: 0.1460
Epoch: 2, Loss: 1.9766, Train: 0.2143, Val: 0.2060, Test: 0.2020
Epoch: 3, Loss: 1.9718, Train: 0.2500, Val: 0.2720, Test: 0.2390
Epoch: 4, Loss: 1.9706, Train: 0.1429, Val: 0.1580, Test: 0.1460
Epoch: 5, Loss: 1.9414, Train: 0.1929, Val: 0.1060, Test: 0.1200
Epoch: 6, Loss: 1.9440, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 7, Loss: 1.9426, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 8, Loss: 1.9596, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 9, Loss: 1.9366, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 10, Loss: 1.9449, Train: 0.1643, Val: 0.0720, Test: 0.0920
Epoch: 11, Loss: 1.9230, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 12, Loss: 1.9540, Train: 0.2571, Val: 0.2360, Test: 0.2360
Epoch: 13, Loss: 1.9336, Train: 0.2357, Val: 0.2120, Test: 0.2070
Epoch: 14, Loss: 1.9419, Train: 0.2071, Val: 0.1820, Test: 0.1770
Epoch: 15, Loss: 1.9132, Train: 0.2714, Val: 0.2880, Test: 0.2590
Epoch: 16, Loss: 1.9316, Train: 0.2714, Val: 0.2920, Test: 0.2640
Epoch: 17, Loss: 1.9415, Train: 0.2786, Val: 0.2920, Test: 0.2700
Epoch: 18, Loss: 1.9108, Train: 0.2857, Val: 0.2940, Test: 0.2730
Epoch: 19, Loss: 1.9487, Train: 0.3214, Val: 0.2920, Test: 0.2770
Epoch: 20, Loss: 1.8922, Train: 0.4500, Val: 0.3880, Test: 0.3560
Epoch: 21, Loss: 1.8917, Train: 0.5071, Val: 0.4380, Test: 0.4020
Epoch: 22, Loss: 1.8696, Train: 0.4429, Val: 0.4400, Test: 0.3950
Epoch: 23, Loss: 1.8143, Train: 0.5643, Val: 0.4760, Test: 0.4380
Epoch: 24, Loss: 1.7373, Train: 0.4500, Val: 0.3360, Test: 0.3270
Epoch: 25, Loss: 1.6463, Train: 0.3857, Val: 0.3580, Test: 0.3520
Epoch: 26, Loss: 1.5598, Train: 0.4071, Val: 0.3900, Test: 0.3980
Epoch: 27, Loss: 1.5010, Train: 0.5786, Val: 0.4440, Test: 0.4580
Epoch: 28, Loss: 1.3764, Train: 0.6357, Val: 0.5060, Test: 0.5280
Epoch: 29, Loss: 1.2613, Train: 0.7071, Val: 0.5540, Test: 0.5740
Epoch: 30, Loss: 1.1619, Train: 0.8286, Val: 0.6280, Test: 0.6330
Epoch: 31, Loss: 1.0541, Train: 0.8357, Val: 0.7460, Test: 0.7030
Epoch: 32, Loss: 0.9286, Train: 0.9000, Val: 0.7160, Test: 0.7260
Epoch: 33, Loss: 0.9140, Train: 0.8571, Val: 0.7060, Test: 0.6620
Epoch: 34, Loss: 0.8357, Train: 0.8929, Val: 0.7160, Test: 0.6850
Epoch: 35, Loss: 0.6882, Train: 0.9214, Val: 0.6980, Test: 0.7160
Epoch: 36, Loss: 0.6960, Train: 0.9714, Val: 0.7220, Test: 0.7120
Epoch: 37, Loss: 0.6026, Train: 0.9571, Val: 0.7360, Test: 0.7350
Epoch: 38, Loss: 0.5662, Train: 0.9000, Val: 0.6980, Test: 0.7100
Epoch: 39, Loss: 0.5719, Train: 0.9286, Val: 0.6940, Test: 0.6800
Epoch: 40, Loss: 0.5458, Train: 0.9857, Val: 0.7220, Test: 0.7170
Epoch: 41, Loss: 0.4228, Train: 0.9643, Val: 0.7240, Test: 0.7340
Epoch: 42, Loss: 0.3794, Train: 0.9857, Val: 0.7340, Test: 0.7530
Epoch: 43, Loss: 0.3538, Train: 0.9857, Val: 0.7440, Test: 0.7460
Epoch: 44, Loss: 0.2926, Train: 0.9714, Val: 0.7420, Test: 0.7360
Epoch: 45, Loss: 0.2482, Train: 0.9786, Val: 0.7380, Test: 0.7450
Epoch: 46, Loss: 0.2232, Train: 0.9857, Val: 0.7500, Test: 0.7510
Epoch: 47, Loss: 0.1664, Train: 0.9929, Val: 0.7360, Test: 0.7360
Epoch: 48, Loss: 0.1603, Train: 0.9929, Val: 0.7360, Test: 0.7290
Epoch: 49, Loss: 0.1140, Train: 0.9929, Val: 0.7480, Test: 0.7600
Epoch: 50, Loss: 0.1209, Train: 0.9929, Val: 0.7560, Test: 0.7580
MAD:  0.5639
Best Test Accuracy: 0.7600, Val Accuracy: 0.7480, Train Accuracy: 0.9929
Training completed.
Average Test Accuracy:  0.7734000000000001 ± 0.010451794104363145
Average MAD:  0.52883 ± 0.04843581422872954

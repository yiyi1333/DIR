Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9896, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 2, Loss: 1.9611, Train: 0.1571, Val: 0.1620, Test: 0.1500
Epoch: 3, Loss: 1.9386, Train: 0.1643, Val: 0.1800, Test: 0.1670
Epoch: 4, Loss: 1.9399, Train: 0.2000, Val: 0.1820, Test: 0.1570
Epoch: 5, Loss: 1.9541, Train: 0.1929, Val: 0.1580, Test: 0.1680
Epoch: 6, Loss: 1.9259, Train: 0.1786, Val: 0.1240, Test: 0.1350
Epoch: 7, Loss: 1.9376, Train: 0.2143, Val: 0.1220, Test: 0.1580
Epoch: 8, Loss: 1.9536, Train: 0.3214, Val: 0.1620, Test: 0.2040
Epoch: 9, Loss: 1.9282, Train: 0.4571, Val: 0.2300, Test: 0.2660
Epoch: 10, Loss: 1.9311, Train: 0.5000, Val: 0.3260, Test: 0.3350
Epoch: 11, Loss: 1.9140, Train: 0.5214, Val: 0.3340, Test: 0.3580
Epoch: 12, Loss: 1.9400, Train: 0.4929, Val: 0.2980, Test: 0.3090
Epoch: 13, Loss: 1.9206, Train: 0.4500, Val: 0.3180, Test: 0.3180
Epoch: 14, Loss: 1.9087, Train: 0.4714, Val: 0.3600, Test: 0.3750
Epoch: 15, Loss: 1.9308, Train: 0.5286, Val: 0.3820, Test: 0.4040
Epoch: 16, Loss: 1.9108, Train: 0.5500, Val: 0.3540, Test: 0.3680
Epoch: 17, Loss: 1.8804, Train: 0.5357, Val: 0.3500, Test: 0.3520
Epoch: 18, Loss: 1.8911, Train: 0.6000, Val: 0.3940, Test: 0.4030
Epoch: 19, Loss: 1.8432, Train: 0.6000, Val: 0.4040, Test: 0.4090
Epoch: 20, Loss: 1.7787, Train: 0.5714, Val: 0.4000, Test: 0.4140
Epoch: 21, Loss: 1.7507, Train: 0.6214, Val: 0.3980, Test: 0.4330
Epoch: 22, Loss: 1.7249, Train: 0.6643, Val: 0.4220, Test: 0.4430
Epoch: 23, Loss: 1.6391, Train: 0.6500, Val: 0.4260, Test: 0.4390
Epoch: 24, Loss: 1.5729, Train: 0.6357, Val: 0.4240, Test: 0.4290
Epoch: 25, Loss: 1.4891, Train: 0.6286, Val: 0.3920, Test: 0.4190
Epoch: 26, Loss: 1.4750, Train: 0.6429, Val: 0.3980, Test: 0.4240
Epoch: 27, Loss: 1.3331, Train: 0.6500, Val: 0.3940, Test: 0.4310
Epoch: 28, Loss: 1.3013, Train: 0.6429, Val: 0.4040, Test: 0.4250
Epoch: 29, Loss: 1.1920, Train: 0.6500, Val: 0.4340, Test: 0.4410
Epoch: 30, Loss: 1.1362, Train: 0.7143, Val: 0.4660, Test: 0.4900
Epoch: 31, Loss: 1.0410, Train: 0.7429, Val: 0.5200, Test: 0.5320
Epoch: 32, Loss: 0.9855, Train: 0.7786, Val: 0.5960, Test: 0.5990
Epoch: 33, Loss: 0.9193, Train: 0.8357, Val: 0.6220, Test: 0.6520
Epoch: 34, Loss: 0.8590, Train: 0.8929, Val: 0.6620, Test: 0.6920
Epoch: 35, Loss: 0.8591, Train: 0.9214, Val: 0.6820, Test: 0.7130
Epoch: 36, Loss: 0.6874, Train: 0.9357, Val: 0.6780, Test: 0.7010
Epoch: 37, Loss: 0.6396, Train: 0.8714, Val: 0.6520, Test: 0.6690
Epoch: 38, Loss: 0.6128, Train: 0.9357, Val: 0.6760, Test: 0.7050
Epoch: 39, Loss: 0.5099, Train: 0.9214, Val: 0.6700, Test: 0.6790
Epoch: 40, Loss: 0.5382, Train: 0.9786, Val: 0.7300, Test: 0.7220
Epoch: 41, Loss: 0.4783, Train: 0.9357, Val: 0.6780, Test: 0.7080
Epoch: 42, Loss: 0.5067, Train: 0.9571, Val: 0.7300, Test: 0.7140
Epoch: 43, Loss: 0.4212, Train: 0.9571, Val: 0.7040, Test: 0.7020
Epoch: 44, Loss: 0.3644, Train: 0.9571, Val: 0.7020, Test: 0.7060
Epoch: 45, Loss: 0.3332, Train: 0.9571, Val: 0.7020, Test: 0.7150
Epoch: 46, Loss: 0.3491, Train: 0.9571, Val: 0.7100, Test: 0.7230
Epoch: 47, Loss: 0.2585, Train: 0.9786, Val: 0.7380, Test: 0.7570
Epoch: 48, Loss: 0.2426, Train: 0.9929, Val: 0.7500, Test: 0.7460
Epoch: 49, Loss: 0.1904, Train: 0.9929, Val: 0.7460, Test: 0.7290
Epoch: 50, Loss: 0.2021, Train: 0.9929, Val: 0.7580, Test: 0.7280
MAD:  0.8281
Best Test Accuracy: 0.7570, Val Accuracy: 0.7380, Train Accuracy: 0.9786
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9668, Train: 0.2000, Val: 0.2780, Test: 0.2900
Epoch: 2, Loss: 1.9577, Train: 0.1929, Val: 0.1420, Test: 0.1550
Epoch: 3, Loss: 1.9661, Train: 0.2071, Val: 0.1200, Test: 0.1170
Epoch: 4, Loss: 1.9839, Train: 0.2286, Val: 0.1220, Test: 0.1180
Epoch: 5, Loss: 1.9539, Train: 0.1714, Val: 0.0840, Test: 0.0900
Epoch: 6, Loss: 1.9437, Train: 0.1714, Val: 0.0760, Test: 0.0880
Epoch: 7, Loss: 1.9530, Train: 0.1929, Val: 0.1100, Test: 0.1180
Epoch: 8, Loss: 1.9310, Train: 0.2214, Val: 0.1320, Test: 0.1440
Epoch: 9, Loss: 1.9633, Train: 0.2286, Val: 0.1260, Test: 0.1410
Epoch: 10, Loss: 1.9408, Train: 0.1929, Val: 0.1220, Test: 0.1400
Epoch: 11, Loss: 1.9403, Train: 0.1857, Val: 0.1220, Test: 0.1350
Epoch: 12, Loss: 1.9139, Train: 0.2071, Val: 0.1240, Test: 0.1430
Epoch: 13, Loss: 1.9041, Train: 0.3071, Val: 0.1760, Test: 0.1920
Epoch: 14, Loss: 1.9297, Train: 0.4786, Val: 0.3040, Test: 0.3260
Epoch: 15, Loss: 1.8878, Train: 0.5071, Val: 0.3720, Test: 0.3790
Epoch: 16, Loss: 1.8914, Train: 0.5214, Val: 0.3780, Test: 0.3790
Epoch: 17, Loss: 1.8639, Train: 0.4857, Val: 0.3340, Test: 0.3300
Epoch: 18, Loss: 1.8206, Train: 0.4500, Val: 0.3220, Test: 0.3190
Epoch: 19, Loss: 1.8229, Train: 0.4857, Val: 0.3200, Test: 0.3280
Epoch: 20, Loss: 1.7965, Train: 0.5286, Val: 0.3580, Test: 0.3530
Epoch: 21, Loss: 1.7309, Train: 0.5071, Val: 0.3920, Test: 0.3850
Epoch: 22, Loss: 1.6926, Train: 0.5571, Val: 0.3580, Test: 0.3610
Epoch: 23, Loss: 1.6168, Train: 0.4857, Val: 0.3220, Test: 0.3360
Epoch: 24, Loss: 1.5516, Train: 0.4714, Val: 0.3080, Test: 0.3230
Epoch: 25, Loss: 1.4265, Train: 0.4643, Val: 0.2920, Test: 0.3110
Epoch: 26, Loss: 1.3843, Train: 0.4571, Val: 0.2920, Test: 0.3120
Epoch: 27, Loss: 1.3233, Train: 0.4571, Val: 0.3060, Test: 0.3160
Epoch: 28, Loss: 1.2664, Train: 0.4857, Val: 0.2980, Test: 0.3270
Epoch: 29, Loss: 1.2126, Train: 0.5214, Val: 0.3240, Test: 0.3590
Epoch: 30, Loss: 1.1211, Train: 0.5857, Val: 0.4180, Test: 0.4320
Epoch: 31, Loss: 1.1019, Train: 0.6643, Val: 0.4740, Test: 0.4980
Epoch: 32, Loss: 1.0343, Train: 0.7357, Val: 0.5280, Test: 0.5680
Epoch: 33, Loss: 0.9708, Train: 0.7143, Val: 0.5060, Test: 0.5510
Epoch: 34, Loss: 0.9581, Train: 0.7429, Val: 0.5420, Test: 0.5800
Epoch: 35, Loss: 0.9222, Train: 0.8286, Val: 0.6160, Test: 0.6670
Epoch: 36, Loss: 0.8423, Train: 0.8500, Val: 0.6680, Test: 0.6780
Epoch: 37, Loss: 0.8722, Train: 0.9000, Val: 0.6720, Test: 0.6980
Epoch: 38, Loss: 0.7919, Train: 0.7786, Val: 0.6040, Test: 0.6070
Epoch: 39, Loss: 0.6629, Train: 0.7857, Val: 0.6100, Test: 0.6000
Epoch: 40, Loss: 0.6590, Train: 0.9500, Val: 0.7100, Test: 0.7130
Epoch: 41, Loss: 0.5147, Train: 0.9429, Val: 0.6980, Test: 0.7210
Epoch: 42, Loss: 0.5690, Train: 0.9643, Val: 0.7160, Test: 0.7340
Epoch: 43, Loss: 0.4823, Train: 0.9357, Val: 0.7300, Test: 0.7160
Epoch: 44, Loss: 0.4883, Train: 0.9500, Val: 0.7560, Test: 0.7310
Epoch: 45, Loss: 0.4528, Train: 0.9571, Val: 0.7500, Test: 0.7450
Epoch: 46, Loss: 0.4053, Train: 0.9643, Val: 0.7340, Test: 0.7590
Epoch: 47, Loss: 0.3914, Train: 0.9714, Val: 0.7580, Test: 0.7630
Epoch: 48, Loss: 0.3034, Train: 0.9714, Val: 0.7580, Test: 0.7580
Epoch: 49, Loss: 0.3129, Train: 0.9786, Val: 0.7520, Test: 0.7530
Epoch: 50, Loss: 0.2632, Train: 0.9786, Val: 0.7660, Test: 0.7630
MAD:  0.8613
Best Test Accuracy: 0.7630, Val Accuracy: 0.7580, Train Accuracy: 0.9714
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9763, Train: 0.1286, Val: 0.0760, Test: 0.0740
Epoch: 2, Loss: 1.9474, Train: 0.1357, Val: 0.0700, Test: 0.0710
Epoch: 3, Loss: 1.9436, Train: 0.1357, Val: 0.0600, Test: 0.0640
Epoch: 4, Loss: 1.9488, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 5, Loss: 1.9562, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 6, Loss: 1.9477, Train: 0.1929, Val: 0.0800, Test: 0.0840
Epoch: 7, Loss: 1.9422, Train: 0.3500, Val: 0.1920, Test: 0.2270
Epoch: 8, Loss: 1.9571, Train: 0.2429, Val: 0.1380, Test: 0.1560
Epoch: 9, Loss: 1.9225, Train: 0.2214, Val: 0.0840, Test: 0.1200
Epoch: 10, Loss: 1.9267, Train: 0.2000, Val: 0.0760, Test: 0.1130
Epoch: 11, Loss: 1.9207, Train: 0.2214, Val: 0.0720, Test: 0.1050
Epoch: 12, Loss: 1.9001, Train: 0.2571, Val: 0.1140, Test: 0.1450
Epoch: 13, Loss: 1.9211, Train: 0.4500, Val: 0.2440, Test: 0.2690
Epoch: 14, Loss: 1.9094, Train: 0.4857, Val: 0.2800, Test: 0.2980
Epoch: 15, Loss: 1.9015, Train: 0.4857, Val: 0.2840, Test: 0.3030
Epoch: 16, Loss: 1.8810, Train: 0.4714, Val: 0.2920, Test: 0.3180
Epoch: 17, Loss: 1.8617, Train: 0.5000, Val: 0.3320, Test: 0.3420
Epoch: 18, Loss: 1.8194, Train: 0.5143, Val: 0.3840, Test: 0.3730
Epoch: 19, Loss: 1.7614, Train: 0.5214, Val: 0.3920, Test: 0.3900
Epoch: 20, Loss: 1.7324, Train: 0.4643, Val: 0.3940, Test: 0.3890
Epoch: 21, Loss: 1.7245, Train: 0.4143, Val: 0.3980, Test: 0.3810
Epoch: 22, Loss: 1.6439, Train: 0.4071, Val: 0.4080, Test: 0.3660
Epoch: 23, Loss: 1.5770, Train: 0.3786, Val: 0.3760, Test: 0.3550
Epoch: 24, Loss: 1.5155, Train: 0.3929, Val: 0.3920, Test: 0.3720
Epoch: 25, Loss: 1.4794, Train: 0.4929, Val: 0.4700, Test: 0.4510
Epoch: 26, Loss: 1.4354, Train: 0.6714, Val: 0.5580, Test: 0.5470
Epoch: 27, Loss: 1.3701, Train: 0.7071, Val: 0.5660, Test: 0.5720
Epoch: 28, Loss: 1.3497, Train: 0.7429, Val: 0.5140, Test: 0.5600
Epoch: 29, Loss: 1.2820, Train: 0.7571, Val: 0.5480, Test: 0.5430
Epoch: 30, Loss: 1.1650, Train: 0.7643, Val: 0.5360, Test: 0.5300
Epoch: 31, Loss: 1.1672, Train: 0.7429, Val: 0.5180, Test: 0.5040
Epoch: 32, Loss: 1.0684, Train: 0.7429, Val: 0.5200, Test: 0.5160
Epoch: 33, Loss: 1.0243, Train: 0.7714, Val: 0.5240, Test: 0.5220
Epoch: 34, Loss: 0.9628, Train: 0.8000, Val: 0.5360, Test: 0.5320
Epoch: 35, Loss: 0.8221, Train: 0.8000, Val: 0.5440, Test: 0.5250
Epoch: 36, Loss: 0.7883, Train: 0.8143, Val: 0.5520, Test: 0.5420
Epoch: 37, Loss: 0.7906, Train: 0.8143, Val: 0.5560, Test: 0.5530
Epoch: 38, Loss: 0.6811, Train: 0.8214, Val: 0.5580, Test: 0.5570
Epoch: 39, Loss: 0.6674, Train: 0.8357, Val: 0.5700, Test: 0.5600
Epoch: 40, Loss: 0.5607, Train: 0.8214, Val: 0.5620, Test: 0.5630
Epoch: 41, Loss: 0.5273, Train: 0.8214, Val: 0.5740, Test: 0.5730
Epoch: 42, Loss: 0.5191, Train: 0.8429, Val: 0.5940, Test: 0.6090
Epoch: 43, Loss: 0.4364, Train: 0.9214, Val: 0.6540, Test: 0.6760
Epoch: 44, Loss: 0.4378, Train: 0.9429, Val: 0.7020, Test: 0.7250
Epoch: 45, Loss: 0.4436, Train: 0.9500, Val: 0.6940, Test: 0.7250
Epoch: 46, Loss: 0.4082, Train: 0.9643, Val: 0.7060, Test: 0.7410
Epoch: 47, Loss: 0.3520, Train: 0.9643, Val: 0.6960, Test: 0.7390
Epoch: 48, Loss: 0.3376, Train: 0.9714, Val: 0.7160, Test: 0.7220
Epoch: 49, Loss: 0.2941, Train: 0.9714, Val: 0.6800, Test: 0.6950
Epoch: 50, Loss: 0.3063, Train: 0.9571, Val: 0.6720, Test: 0.6960
MAD:  0.8318
Best Test Accuracy: 0.7410, Val Accuracy: 0.7060, Train Accuracy: 0.9643
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9910, Train: 0.1429, Val: 0.1260, Test: 0.1340
Epoch: 2, Loss: 1.9666, Train: 0.1857, Val: 0.1360, Test: 0.1720
Epoch: 3, Loss: 1.9431, Train: 0.2143, Val: 0.1480, Test: 0.1640
Epoch: 4, Loss: 1.9829, Train: 0.2143, Val: 0.1300, Test: 0.1500
Epoch: 5, Loss: 1.9480, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 6, Loss: 1.9529, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 7, Loss: 1.9229, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 8, Loss: 1.9450, Train: 0.1786, Val: 0.0860, Test: 0.0940
Epoch: 9, Loss: 1.9507, Train: 0.2286, Val: 0.1220, Test: 0.1430
Epoch: 10, Loss: 1.9413, Train: 0.2214, Val: 0.1400, Test: 0.1470
Epoch: 11, Loss: 1.9591, Train: 0.2357, Val: 0.1360, Test: 0.1510
Epoch: 12, Loss: 1.9544, Train: 0.2643, Val: 0.1620, Test: 0.1660
Epoch: 13, Loss: 1.9201, Train: 0.4500, Val: 0.3300, Test: 0.3390
Epoch: 14, Loss: 1.9219, Train: 0.4214, Val: 0.3540, Test: 0.3510
Epoch: 15, Loss: 1.9350, Train: 0.5786, Val: 0.4300, Test: 0.4320
Epoch: 16, Loss: 1.9105, Train: 0.5214, Val: 0.3220, Test: 0.3610
Epoch: 17, Loss: 1.8920, Train: 0.5000, Val: 0.2820, Test: 0.2930
Epoch: 18, Loss: 1.8979, Train: 0.4143, Val: 0.2340, Test: 0.2540
Epoch: 19, Loss: 1.9144, Train: 0.4500, Val: 0.2500, Test: 0.2700
Epoch: 20, Loss: 1.8611, Train: 0.5643, Val: 0.3200, Test: 0.3430
Epoch: 21, Loss: 1.8279, Train: 0.6143, Val: 0.4160, Test: 0.4420
Epoch: 22, Loss: 1.8521, Train: 0.6857, Val: 0.5680, Test: 0.5920
Epoch: 23, Loss: 1.8009, Train: 0.7000, Val: 0.6000, Test: 0.6070
Epoch: 24, Loss: 1.7521, Train: 0.7214, Val: 0.6120, Test: 0.6330
Epoch: 25, Loss: 1.7307, Train: 0.7429, Val: 0.5880, Test: 0.6410
Epoch: 26, Loss: 1.6436, Train: 0.7857, Val: 0.5960, Test: 0.6770
Epoch: 27, Loss: 1.6083, Train: 0.7929, Val: 0.6200, Test: 0.6870
Epoch: 28, Loss: 1.5147, Train: 0.7714, Val: 0.6360, Test: 0.7040
Epoch: 29, Loss: 1.4317, Train: 0.7571, Val: 0.6400, Test: 0.6890
Epoch: 30, Loss: 1.3539, Train: 0.8143, Val: 0.6680, Test: 0.7060
Epoch: 31, Loss: 1.2648, Train: 0.7786, Val: 0.6420, Test: 0.6660
Epoch: 32, Loss: 1.1938, Train: 0.7786, Val: 0.6060, Test: 0.6130
Epoch: 33, Loss: 1.1023, Train: 0.8214, Val: 0.5980, Test: 0.6100
Epoch: 34, Loss: 1.0463, Train: 0.8000, Val: 0.5880, Test: 0.6140
Epoch: 35, Loss: 1.0222, Train: 0.8500, Val: 0.6020, Test: 0.6150
Epoch: 36, Loss: 0.9110, Train: 0.8571, Val: 0.6280, Test: 0.6300
Epoch: 37, Loss: 0.8378, Train: 0.8643, Val: 0.6260, Test: 0.6320
Epoch: 38, Loss: 0.8424, Train: 0.8857, Val: 0.6960, Test: 0.6780
Epoch: 39, Loss: 0.6880, Train: 0.9357, Val: 0.7480, Test: 0.7260
Epoch: 40, Loss: 0.6825, Train: 0.9571, Val: 0.7400, Test: 0.7490
Epoch: 41, Loss: 0.5326, Train: 0.9643, Val: 0.7480, Test: 0.7540
Epoch: 42, Loss: 0.5245, Train: 0.9786, Val: 0.7540, Test: 0.7550
Epoch: 43, Loss: 0.4897, Train: 0.9714, Val: 0.7580, Test: 0.7580
Epoch: 44, Loss: 0.4058, Train: 0.9857, Val: 0.7540, Test: 0.7670
Epoch: 45, Loss: 0.4014, Train: 0.9857, Val: 0.7440, Test: 0.7640
Epoch: 46, Loss: 0.3334, Train: 0.9857, Val: 0.7520, Test: 0.7640
Epoch: 47, Loss: 0.2863, Train: 0.9857, Val: 0.7520, Test: 0.7520
Epoch: 48, Loss: 0.2702, Train: 1.0000, Val: 0.7520, Test: 0.7650
Epoch: 49, Loss: 0.2022, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 50, Loss: 0.2001, Train: 1.0000, Val: 0.7520, Test: 0.7700
MAD:  0.8717
Best Test Accuracy: 0.7710, Val Accuracy: 0.7500, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0152, Train: 0.2000, Val: 0.1340, Test: 0.1300
Epoch: 2, Loss: 1.9718, Train: 0.2143, Val: 0.1620, Test: 0.1810
Epoch: 3, Loss: 1.9718, Train: 0.1571, Val: 0.1440, Test: 0.1520
Epoch: 4, Loss: 1.9672, Train: 0.1929, Val: 0.1720, Test: 0.1720
Epoch: 5, Loss: 1.9871, Train: 0.2357, Val: 0.1980, Test: 0.2140
Epoch: 6, Loss: 1.9876, Train: 0.3714, Val: 0.2000, Test: 0.2360
Epoch: 7, Loss: 1.9650, Train: 0.3429, Val: 0.2120, Test: 0.2270
Epoch: 8, Loss: 1.9520, Train: 0.1571, Val: 0.1580, Test: 0.1500
Epoch: 9, Loss: 1.9470, Train: 0.1429, Val: 0.1560, Test: 0.1450
Epoch: 10, Loss: 1.9173, Train: 0.1643, Val: 0.1640, Test: 0.1490
Epoch: 11, Loss: 1.9486, Train: 0.2500, Val: 0.2260, Test: 0.1990
Epoch: 12, Loss: 1.9534, Train: 0.2643, Val: 0.1760, Test: 0.1760
Epoch: 13, Loss: 1.9338, Train: 0.3429, Val: 0.2640, Test: 0.2500
Epoch: 14, Loss: 1.9437, Train: 0.3214, Val: 0.2700, Test: 0.2690
Epoch: 15, Loss: 1.9503, Train: 0.2857, Val: 0.2020, Test: 0.2000
Epoch: 16, Loss: 1.9185, Train: 0.2571, Val: 0.1680, Test: 0.1800
Epoch: 17, Loss: 1.9240, Train: 0.2643, Val: 0.1740, Test: 0.1850
Epoch: 18, Loss: 1.9080, Train: 0.2929, Val: 0.1800, Test: 0.2010
Epoch: 19, Loss: 1.9124, Train: 0.3286, Val: 0.1980, Test: 0.2150
Epoch: 20, Loss: 1.9159, Train: 0.4143, Val: 0.2580, Test: 0.2680
Epoch: 21, Loss: 1.8764, Train: 0.4929, Val: 0.2900, Test: 0.2980
Epoch: 22, Loss: 1.8710, Train: 0.4786, Val: 0.2860, Test: 0.3000
Epoch: 23, Loss: 1.8777, Train: 0.4857, Val: 0.2940, Test: 0.3070
Epoch: 24, Loss: 1.8364, Train: 0.5357, Val: 0.3400, Test: 0.3300
Epoch: 25, Loss: 1.8156, Train: 0.5571, Val: 0.3540, Test: 0.3380
Epoch: 26, Loss: 1.7840, Train: 0.5286, Val: 0.3220, Test: 0.3230
Epoch: 27, Loss: 1.7604, Train: 0.5071, Val: 0.3020, Test: 0.3040
Epoch: 28, Loss: 1.6906, Train: 0.4929, Val: 0.2920, Test: 0.3010
Epoch: 29, Loss: 1.6295, Train: 0.5214, Val: 0.3020, Test: 0.3220
Epoch: 30, Loss: 1.5944, Train: 0.5357, Val: 0.3160, Test: 0.3250
Epoch: 31, Loss: 1.5205, Train: 0.5429, Val: 0.3020, Test: 0.3290
Epoch: 32, Loss: 1.4237, Train: 0.5286, Val: 0.3020, Test: 0.3230
Epoch: 33, Loss: 1.3478, Train: 0.5429, Val: 0.3200, Test: 0.3240
Epoch: 34, Loss: 1.3251, Train: 0.5500, Val: 0.3400, Test: 0.3370
Epoch: 35, Loss: 1.2808, Train: 0.5857, Val: 0.3620, Test: 0.3600
Epoch: 36, Loss: 1.2549, Train: 0.6857, Val: 0.4500, Test: 0.4510
Epoch: 37, Loss: 1.1202, Train: 0.7571, Val: 0.4900, Test: 0.5020
Epoch: 38, Loss: 1.1249, Train: 0.8286, Val: 0.5940, Test: 0.5920
Epoch: 39, Loss: 1.0179, Train: 0.7286, Val: 0.5280, Test: 0.5080
Epoch: 40, Loss: 0.9171, Train: 0.7071, Val: 0.5060, Test: 0.4950
Epoch: 41, Loss: 0.9258, Train: 0.7071, Val: 0.4960, Test: 0.4930
Epoch: 42, Loss: 0.8758, Train: 0.7214, Val: 0.5180, Test: 0.5090
Epoch: 43, Loss: 0.8385, Train: 0.7286, Val: 0.5140, Test: 0.5220
Epoch: 44, Loss: 0.7087, Train: 0.8429, Val: 0.5800, Test: 0.5860
Epoch: 45, Loss: 0.7105, Train: 0.8500, Val: 0.5520, Test: 0.5680
Epoch: 46, Loss: 0.6790, Train: 0.8357, Val: 0.5820, Test: 0.5800
Epoch: 47, Loss: 0.6410, Train: 0.8429, Val: 0.5860, Test: 0.5830
Epoch: 48, Loss: 0.5321, Train: 0.8714, Val: 0.6040, Test: 0.6140
Epoch: 49, Loss: 0.5305, Train: 0.8429, Val: 0.6020, Test: 0.6140
Epoch: 50, Loss: 0.5021, Train: 0.8571, Val: 0.6060, Test: 0.6130
MAD:  0.8521
Best Test Accuracy: 0.6140, Val Accuracy: 0.6040, Train Accuracy: 0.8714
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9796, Train: 0.1429, Val: 0.1280, Test: 0.1290
Epoch: 2, Loss: 1.9566, Train: 0.1357, Val: 0.0560, Test: 0.0690
Epoch: 3, Loss: 1.9483, Train: 0.1500, Val: 0.0520, Test: 0.0620
Epoch: 4, Loss: 1.9606, Train: 0.1357, Val: 0.0580, Test: 0.0630
Epoch: 5, Loss: 1.9720, Train: 0.1500, Val: 0.0600, Test: 0.0640
Epoch: 6, Loss: 1.9654, Train: 0.1500, Val: 0.0580, Test: 0.0660
Epoch: 7, Loss: 1.9735, Train: 0.2000, Val: 0.0980, Test: 0.1090
Epoch: 8, Loss: 1.9465, Train: 0.1500, Val: 0.0740, Test: 0.0940
Epoch: 9, Loss: 1.9385, Train: 0.1429, Val: 0.0740, Test: 0.0930
Epoch: 10, Loss: 1.9482, Train: 0.1429, Val: 0.0740, Test: 0.0930
Epoch: 11, Loss: 1.9299, Train: 0.1429, Val: 0.0740, Test: 0.0930
Epoch: 12, Loss: 1.9358, Train: 0.1429, Val: 0.0740, Test: 0.0930
Epoch: 13, Loss: 1.9209, Train: 0.1571, Val: 0.0760, Test: 0.0950
Epoch: 14, Loss: 1.9432, Train: 0.2143, Val: 0.0960, Test: 0.1180
Epoch: 15, Loss: 1.9273, Train: 0.2929, Val: 0.2400, Test: 0.2680
Epoch: 16, Loss: 1.8968, Train: 0.3214, Val: 0.4120, Test: 0.4180
Epoch: 17, Loss: 1.9143, Train: 0.2643, Val: 0.3920, Test: 0.3890
Epoch: 18, Loss: 1.9105, Train: 0.2429, Val: 0.3600, Test: 0.3720
Epoch: 19, Loss: 1.8945, Train: 0.2714, Val: 0.3760, Test: 0.3870
Epoch: 20, Loss: 1.8877, Train: 0.3929, Val: 0.4740, Test: 0.4620
Epoch: 21, Loss: 1.9040, Train: 0.5429, Val: 0.5220, Test: 0.5350
Epoch: 22, Loss: 1.8667, Train: 0.6500, Val: 0.4940, Test: 0.5150
Epoch: 23, Loss: 1.8450, Train: 0.6357, Val: 0.4320, Test: 0.4600
Epoch: 24, Loss: 1.8294, Train: 0.6929, Val: 0.4940, Test: 0.5130
Epoch: 25, Loss: 1.8064, Train: 0.7357, Val: 0.5040, Test: 0.5220
Epoch: 26, Loss: 1.7779, Train: 0.6929, Val: 0.5140, Test: 0.5200
Epoch: 27, Loss: 1.6799, Train: 0.6643, Val: 0.4880, Test: 0.4970
Epoch: 28, Loss: 1.6708, Train: 0.6643, Val: 0.4840, Test: 0.4700
Epoch: 29, Loss: 1.5712, Train: 0.6714, Val: 0.4880, Test: 0.4750
Epoch: 30, Loss: 1.4897, Train: 0.7000, Val: 0.4900, Test: 0.4790
Epoch: 31, Loss: 1.4250, Train: 0.6786, Val: 0.4780, Test: 0.4690
Epoch: 32, Loss: 1.3286, Train: 0.7286, Val: 0.5500, Test: 0.5440
Epoch: 33, Loss: 1.1972, Train: 0.7929, Val: 0.6140, Test: 0.6090
Epoch: 34, Loss: 1.1266, Train: 0.8357, Val: 0.6280, Test: 0.6280
Epoch: 35, Loss: 1.0711, Train: 0.8000, Val: 0.6560, Test: 0.6320
Epoch: 36, Loss: 0.9916, Train: 0.7786, Val: 0.6200, Test: 0.6130
Epoch: 37, Loss: 0.8675, Train: 0.8643, Val: 0.6680, Test: 0.6650
Epoch: 38, Loss: 0.8083, Train: 0.9286, Val: 0.7000, Test: 0.6900
Epoch: 39, Loss: 0.7245, Train: 0.9286, Val: 0.7140, Test: 0.7010
Epoch: 40, Loss: 0.6573, Train: 0.9286, Val: 0.6880, Test: 0.6900
Epoch: 41, Loss: 0.5852, Train: 0.9643, Val: 0.7560, Test: 0.7440
Epoch: 42, Loss: 0.4906, Train: 0.9714, Val: 0.7140, Test: 0.7080
Epoch: 43, Loss: 0.4188, Train: 0.9643, Val: 0.7360, Test: 0.7270
Epoch: 44, Loss: 0.3923, Train: 0.9714, Val: 0.7420, Test: 0.7580
Epoch: 45, Loss: 0.3756, Train: 0.9643, Val: 0.7280, Test: 0.7390
Epoch: 46, Loss: 0.3759, Train: 0.9786, Val: 0.7600, Test: 0.7640
Epoch: 47, Loss: 0.2496, Train: 0.9786, Val: 0.7540, Test: 0.7690
Epoch: 48, Loss: 0.2606, Train: 0.9857, Val: 0.7600, Test: 0.7650
Epoch: 49, Loss: 0.2061, Train: 0.9786, Val: 0.7640, Test: 0.7810
Epoch: 50, Loss: 0.1744, Train: 0.9786, Val: 0.7460, Test: 0.7770
MAD:  0.9083
Best Test Accuracy: 0.7810, Val Accuracy: 0.7640, Train Accuracy: 0.9786
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9852, Train: 0.1500, Val: 0.2520, Test: 0.2670
Epoch: 2, Loss: 1.9543, Train: 0.1500, Val: 0.1660, Test: 0.1560
Epoch: 3, Loss: 1.9505, Train: 0.1786, Val: 0.1600, Test: 0.1530
Epoch: 4, Loss: 1.9651, Train: 0.2857, Val: 0.1340, Test: 0.1610
Epoch: 5, Loss: 1.9434, Train: 0.2714, Val: 0.1140, Test: 0.1460
Epoch: 6, Loss: 1.9740, Train: 0.2857, Val: 0.1480, Test: 0.1610
Epoch: 7, Loss: 1.9406, Train: 0.2571, Val: 0.1380, Test: 0.1370
Epoch: 8, Loss: 1.9388, Train: 0.1857, Val: 0.1140, Test: 0.1130
Epoch: 9, Loss: 1.9505, Train: 0.1857, Val: 0.1140, Test: 0.1100
Epoch: 10, Loss: 1.9430, Train: 0.2000, Val: 0.1200, Test: 0.1160
Epoch: 11, Loss: 1.9087, Train: 0.2500, Val: 0.1320, Test: 0.1300
Epoch: 12, Loss: 1.9270, Train: 0.3643, Val: 0.2340, Test: 0.2470
Epoch: 13, Loss: 1.8962, Train: 0.3786, Val: 0.2500, Test: 0.2450
Epoch: 14, Loss: 1.9405, Train: 0.3714, Val: 0.2320, Test: 0.2350
Epoch: 15, Loss: 1.9059, Train: 0.3357, Val: 0.2320, Test: 0.2260
Epoch: 16, Loss: 1.8714, Train: 0.3500, Val: 0.2580, Test: 0.2530
Epoch: 17, Loss: 1.8523, Train: 0.4214, Val: 0.2620, Test: 0.2640
Epoch: 18, Loss: 1.8155, Train: 0.4071, Val: 0.2540, Test: 0.2640
Epoch: 19, Loss: 1.7660, Train: 0.4143, Val: 0.2720, Test: 0.2650
Epoch: 20, Loss: 1.7681, Train: 0.4214, Val: 0.2640, Test: 0.2630
Epoch: 21, Loss: 1.7014, Train: 0.4214, Val: 0.2480, Test: 0.2690
Epoch: 22, Loss: 1.6638, Train: 0.3929, Val: 0.2720, Test: 0.2580
Epoch: 23, Loss: 1.6142, Train: 0.4286, Val: 0.2680, Test: 0.2610
Epoch: 24, Loss: 1.5833, Train: 0.4286, Val: 0.2720, Test: 0.2680
Epoch: 25, Loss: 1.5222, Train: 0.4357, Val: 0.2840, Test: 0.2740
Epoch: 26, Loss: 1.4496, Train: 0.4429, Val: 0.3100, Test: 0.2980
Epoch: 27, Loss: 1.3323, Train: 0.5214, Val: 0.3460, Test: 0.3310
Epoch: 28, Loss: 1.3224, Train: 0.5500, Val: 0.4100, Test: 0.3910
Epoch: 29, Loss: 1.2719, Train: 0.5929, Val: 0.4200, Test: 0.3850
Epoch: 30, Loss: 1.1712, Train: 0.6714, Val: 0.4880, Test: 0.4410
Epoch: 31, Loss: 1.1356, Train: 0.8071, Val: 0.6180, Test: 0.5850
Epoch: 32, Loss: 1.0604, Train: 0.7500, Val: 0.5800, Test: 0.5750
Epoch: 33, Loss: 1.1098, Train: 0.8143, Val: 0.5980, Test: 0.5860
Epoch: 34, Loss: 1.0125, Train: 0.8214, Val: 0.6040, Test: 0.5780
Epoch: 35, Loss: 0.9349, Train: 0.8214, Val: 0.5980, Test: 0.5750
Epoch: 36, Loss: 0.8862, Train: 0.8000, Val: 0.5980, Test: 0.6000
Epoch: 37, Loss: 0.8326, Train: 0.7929, Val: 0.6040, Test: 0.5960
Epoch: 38, Loss: 0.7656, Train: 0.8143, Val: 0.5820, Test: 0.5840
Epoch: 39, Loss: 0.7758, Train: 0.8500, Val: 0.6020, Test: 0.5920
Epoch: 40, Loss: 0.7359, Train: 0.9000, Val: 0.6500, Test: 0.6370
Epoch: 41, Loss: 0.6705, Train: 0.9000, Val: 0.6980, Test: 0.6830
Epoch: 42, Loss: 0.6412, Train: 0.9500, Val: 0.7000, Test: 0.7070
Epoch: 43, Loss: 0.5681, Train: 0.9429, Val: 0.7080, Test: 0.7270
Epoch: 44, Loss: 0.5377, Train: 0.9429, Val: 0.7140, Test: 0.7300
Epoch: 45, Loss: 0.4945, Train: 0.9571, Val: 0.7300, Test: 0.7550
Epoch: 46, Loss: 0.4571, Train: 0.9571, Val: 0.7400, Test: 0.7560
Epoch: 47, Loss: 0.4011, Train: 0.9500, Val: 0.7380, Test: 0.7490
Epoch: 48, Loss: 0.3561, Train: 0.9643, Val: 0.7400, Test: 0.7630
Epoch: 49, Loss: 0.3168, Train: 0.9714, Val: 0.7600, Test: 0.7640
Epoch: 50, Loss: 0.2765, Train: 0.9857, Val: 0.7740, Test: 0.7850
MAD:  0.8508
Best Test Accuracy: 0.7850, Val Accuracy: 0.7740, Train Accuracy: 0.9857
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9852, Train: 0.1643, Val: 0.0700, Test: 0.0590
Epoch: 2, Loss: 1.9678, Train: 0.1500, Val: 0.1520, Test: 0.1410
Epoch: 3, Loss: 1.9577, Train: 0.1429, Val: 0.1600, Test: 0.1450
Epoch: 4, Loss: 1.9457, Train: 0.1929, Val: 0.1640, Test: 0.1590
Epoch: 5, Loss: 1.9634, Train: 0.3000, Val: 0.1840, Test: 0.1900
Epoch: 6, Loss: 1.9375, Train: 0.2643, Val: 0.1660, Test: 0.1690
Epoch: 7, Loss: 1.9613, Train: 0.1786, Val: 0.1140, Test: 0.1070
Epoch: 8, Loss: 1.9319, Train: 0.1786, Val: 0.1140, Test: 0.1060
Epoch: 9, Loss: 1.9409, Train: 0.1857, Val: 0.1140, Test: 0.1070
Epoch: 10, Loss: 1.9344, Train: 0.1857, Val: 0.1160, Test: 0.1090
Epoch: 11, Loss: 1.9098, Train: 0.3143, Val: 0.1940, Test: 0.1930
Epoch: 12, Loss: 1.9097, Train: 0.4429, Val: 0.3200, Test: 0.3170
Epoch: 13, Loss: 1.9066, Train: 0.4714, Val: 0.3300, Test: 0.3210
Epoch: 14, Loss: 1.9140, Train: 0.4286, Val: 0.2680, Test: 0.2690
Epoch: 15, Loss: 1.8674, Train: 0.4571, Val: 0.2820, Test: 0.2840
Epoch: 16, Loss: 1.8797, Train: 0.5071, Val: 0.3360, Test: 0.3490
Epoch: 17, Loss: 1.8378, Train: 0.6000, Val: 0.4580, Test: 0.4630
Epoch: 18, Loss: 1.8084, Train: 0.6143, Val: 0.5040, Test: 0.4910
Epoch: 19, Loss: 1.7728, Train: 0.6286, Val: 0.5140, Test: 0.5030
Epoch: 20, Loss: 1.7682, Train: 0.6286, Val: 0.4840, Test: 0.4830
Epoch: 21, Loss: 1.6902, Train: 0.5929, Val: 0.4240, Test: 0.4400
Epoch: 22, Loss: 1.6463, Train: 0.5429, Val: 0.4080, Test: 0.4200
Epoch: 23, Loss: 1.5369, Train: 0.5857, Val: 0.4140, Test: 0.4530
Epoch: 24, Loss: 1.5182, Train: 0.5214, Val: 0.4040, Test: 0.4250
Epoch: 25, Loss: 1.4637, Train: 0.4929, Val: 0.3640, Test: 0.3840
Epoch: 26, Loss: 1.4112, Train: 0.5143, Val: 0.4140, Test: 0.4320
Epoch: 27, Loss: 1.3367, Train: 0.5500, Val: 0.5060, Test: 0.4970
Epoch: 28, Loss: 1.2673, Train: 0.6714, Val: 0.5840, Test: 0.5610
Epoch: 29, Loss: 1.1395, Train: 0.7643, Val: 0.6220, Test: 0.6260
Epoch: 30, Loss: 1.0709, Train: 0.7857, Val: 0.6560, Test: 0.6510
Epoch: 31, Loss: 1.0655, Train: 0.8214, Val: 0.7060, Test: 0.7120
Epoch: 32, Loss: 1.0110, Train: 0.8643, Val: 0.7080, Test: 0.7070
Epoch: 33, Loss: 0.8927, Train: 0.8714, Val: 0.7300, Test: 0.7420
Epoch: 34, Loss: 0.8933, Train: 0.8500, Val: 0.7380, Test: 0.7440
Epoch: 35, Loss: 0.7971, Train: 0.8357, Val: 0.7040, Test: 0.7450
Epoch: 36, Loss: 0.7374, Train: 0.8857, Val: 0.7340, Test: 0.7540
Epoch: 37, Loss: 0.7303, Train: 0.9214, Val: 0.7320, Test: 0.7350
Epoch: 38, Loss: 0.6037, Train: 0.9214, Val: 0.7420, Test: 0.7350
Epoch: 39, Loss: 0.6083, Train: 0.9500, Val: 0.7600, Test: 0.7480
Epoch: 40, Loss: 0.5019, Train: 0.9714, Val: 0.7440, Test: 0.7560
Epoch: 41, Loss: 0.4857, Train: 0.9571, Val: 0.7420, Test: 0.7510
Epoch: 42, Loss: 0.4857, Train: 0.9643, Val: 0.7540, Test: 0.7490
Epoch: 43, Loss: 0.3769, Train: 0.9643, Val: 0.7520, Test: 0.7570
Epoch: 44, Loss: 0.3676, Train: 0.9786, Val: 0.7460, Test: 0.7520
Epoch: 45, Loss: 0.2923, Train: 0.9786, Val: 0.7620, Test: 0.7730
Epoch: 46, Loss: 0.2822, Train: 0.9857, Val: 0.7500, Test: 0.7620
Epoch: 47, Loss: 0.2080, Train: 0.9643, Val: 0.7420, Test: 0.7350
Epoch: 48, Loss: 0.2574, Train: 0.9714, Val: 0.7300, Test: 0.7340
Epoch: 49, Loss: 0.2026, Train: 0.9929, Val: 0.7360, Test: 0.7500
Epoch: 50, Loss: 0.1856, Train: 0.9857, Val: 0.7240, Test: 0.7390
MAD:  0.8997
Best Test Accuracy: 0.7730, Val Accuracy: 0.7620, Train Accuracy: 0.9786
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9531, Train: 0.2286, Val: 0.2100, Test: 0.2390
Epoch: 2, Loss: 1.9812, Train: 0.2071, Val: 0.1540, Test: 0.1660
Epoch: 3, Loss: 1.9656, Train: 0.2500, Val: 0.2000, Test: 0.2160
Epoch: 4, Loss: 1.9533, Train: 0.1786, Val: 0.3080, Test: 0.3200
Epoch: 5, Loss: 1.9255, Train: 0.1571, Val: 0.3160, Test: 0.3200
Epoch: 6, Loss: 1.9259, Train: 0.1786, Val: 0.3140, Test: 0.3230
Epoch: 7, Loss: 1.9578, Train: 0.2071, Val: 0.3240, Test: 0.3320
Epoch: 8, Loss: 1.9453, Train: 0.2929, Val: 0.3720, Test: 0.3940
Epoch: 9, Loss: 1.9463, Train: 0.3214, Val: 0.3620, Test: 0.3760
Epoch: 10, Loss: 1.9307, Train: 0.4000, Val: 0.3080, Test: 0.3030
Epoch: 11, Loss: 1.9148, Train: 0.4929, Val: 0.3240, Test: 0.3370
Epoch: 12, Loss: 1.8693, Train: 0.5714, Val: 0.3420, Test: 0.3620
Epoch: 13, Loss: 1.9155, Train: 0.5643, Val: 0.3580, Test: 0.3610
Epoch: 14, Loss: 1.9091, Train: 0.5143, Val: 0.3280, Test: 0.3540
Epoch: 15, Loss: 1.8526, Train: 0.5286, Val: 0.3680, Test: 0.3750
Epoch: 16, Loss: 1.8588, Train: 0.5429, Val: 0.3540, Test: 0.3720
Epoch: 17, Loss: 1.8261, Train: 0.5143, Val: 0.3900, Test: 0.3830
Epoch: 18, Loss: 1.7878, Train: 0.4929, Val: 0.3880, Test: 0.3710
Epoch: 19, Loss: 1.7860, Train: 0.5000, Val: 0.3560, Test: 0.3610
Epoch: 20, Loss: 1.6578, Train: 0.5286, Val: 0.3620, Test: 0.3730
Epoch: 21, Loss: 1.6323, Train: 0.6000, Val: 0.3820, Test: 0.3980
Epoch: 22, Loss: 1.6110, Train: 0.6143, Val: 0.4360, Test: 0.4370
Epoch: 23, Loss: 1.5112, Train: 0.6214, Val: 0.4740, Test: 0.4610
Epoch: 24, Loss: 1.4747, Train: 0.6357, Val: 0.4640, Test: 0.4560
Epoch: 25, Loss: 1.3710, Train: 0.7214, Val: 0.5000, Test: 0.4950
Epoch: 26, Loss: 1.2628, Train: 0.7286, Val: 0.5140, Test: 0.5170
Epoch: 27, Loss: 1.2010, Train: 0.7714, Val: 0.5620, Test: 0.5660
Epoch: 28, Loss: 1.1320, Train: 0.7786, Val: 0.5560, Test: 0.5670
Epoch: 29, Loss: 1.0923, Train: 0.7714, Val: 0.5560, Test: 0.5700
Epoch: 30, Loss: 1.0485, Train: 0.8000, Val: 0.5920, Test: 0.5990
Epoch: 31, Loss: 0.9193, Train: 0.8143, Val: 0.5980, Test: 0.6170
Epoch: 32, Loss: 0.9378, Train: 0.8000, Val: 0.6160, Test: 0.6040
Epoch: 33, Loss: 0.8398, Train: 0.7714, Val: 0.5680, Test: 0.5570
Epoch: 34, Loss: 0.7907, Train: 0.7429, Val: 0.5420, Test: 0.5330
Epoch: 35, Loss: 0.7766, Train: 0.7786, Val: 0.5740, Test: 0.5490
Epoch: 36, Loss: 0.6116, Train: 0.8000, Val: 0.6440, Test: 0.6050
Epoch: 37, Loss: 0.6070, Train: 0.9214, Val: 0.6960, Test: 0.6760
Epoch: 38, Loss: 0.5686, Train: 0.9143, Val: 0.7120, Test: 0.6910
Epoch: 39, Loss: 0.5675, Train: 0.9286, Val: 0.7220, Test: 0.7130
Epoch: 40, Loss: 0.5038, Train: 0.9357, Val: 0.7100, Test: 0.6880
Epoch: 41, Loss: 0.4887, Train: 0.9500, Val: 0.7100, Test: 0.6860
Epoch: 42, Loss: 0.4663, Train: 0.9429, Val: 0.7260, Test: 0.6950
Epoch: 43, Loss: 0.3816, Train: 0.9214, Val: 0.7360, Test: 0.7150
Epoch: 44, Loss: 0.3995, Train: 0.9143, Val: 0.7360, Test: 0.7280
Epoch: 45, Loss: 0.3866, Train: 0.9571, Val: 0.7380, Test: 0.7280
Epoch: 46, Loss: 0.3372, Train: 0.9643, Val: 0.7300, Test: 0.7290
Epoch: 47, Loss: 0.3015, Train: 0.9571, Val: 0.7200, Test: 0.7310
Epoch: 48, Loss: 0.3129, Train: 0.9643, Val: 0.7200, Test: 0.7250
Epoch: 49, Loss: 0.2719, Train: 0.9571, Val: 0.7200, Test: 0.7180
Epoch: 50, Loss: 0.2413, Train: 0.9714, Val: 0.7500, Test: 0.7400
MAD:  0.9198
Best Test Accuracy: 0.7400, Val Accuracy: 0.7500, Train Accuracy: 0.9714
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=2)
      (conv2): GATv2Conv(128, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(512, 128, heads=2)
      (conv2): GATv2Conv(512, 128, heads=2)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(512, 256, heads=2)
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9910, Train: 0.1214, Val: 0.0980, Test: 0.0970
Epoch: 2, Loss: 1.9876, Train: 0.1357, Val: 0.1120, Test: 0.1010
Epoch: 3, Loss: 1.9432, Train: 0.1429, Val: 0.1140, Test: 0.1100
Epoch: 4, Loss: 1.9347, Train: 0.1500, Val: 0.1620, Test: 0.1450
Epoch: 5, Loss: 1.9771, Train: 0.2500, Val: 0.3340, Test: 0.3160
Epoch: 6, Loss: 1.9765, Train: 0.1857, Val: 0.3480, Test: 0.3450
Epoch: 7, Loss: 1.9662, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 8, Loss: 1.9403, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 9, Loss: 1.9689, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 10, Loss: 1.9232, Train: 0.1429, Val: 0.3180, Test: 0.3210
Epoch: 11, Loss: 1.9372, Train: 0.1857, Val: 0.3480, Test: 0.3490
Epoch: 12, Loss: 1.9147, Train: 0.2571, Val: 0.3400, Test: 0.3650
Epoch: 13, Loss: 1.9371, Train: 0.2286, Val: 0.2020, Test: 0.2230
Epoch: 14, Loss: 1.8962, Train: 0.2429, Val: 0.1520, Test: 0.1690
Epoch: 15, Loss: 1.9248, Train: 0.3429, Val: 0.2460, Test: 0.2540
Epoch: 16, Loss: 1.9163, Train: 0.4429, Val: 0.3240, Test: 0.3200
Epoch: 17, Loss: 1.9062, Train: 0.4643, Val: 0.3740, Test: 0.3590
Epoch: 18, Loss: 1.8884, Train: 0.4429, Val: 0.3940, Test: 0.3830
Epoch: 19, Loss: 1.8626, Train: 0.3929, Val: 0.3520, Test: 0.3450
Epoch: 20, Loss: 1.8642, Train: 0.4071, Val: 0.3360, Test: 0.3260
Epoch: 21, Loss: 1.7985, Train: 0.4786, Val: 0.3960, Test: 0.3700
Epoch: 22, Loss: 1.7925, Train: 0.4929, Val: 0.4120, Test: 0.3900
Epoch: 23, Loss: 1.7609, Train: 0.3929, Val: 0.3200, Test: 0.2870
Epoch: 24, Loss: 1.6813, Train: 0.3357, Val: 0.2800, Test: 0.2610
Epoch: 25, Loss: 1.6223, Train: 0.3000, Val: 0.2720, Test: 0.2610
Epoch: 26, Loss: 1.5479, Train: 0.3071, Val: 0.2760, Test: 0.2580
Epoch: 27, Loss: 1.5260, Train: 0.3214, Val: 0.2880, Test: 0.2590
Epoch: 28, Loss: 1.4498, Train: 0.3143, Val: 0.2880, Test: 0.2860
Epoch: 29, Loss: 1.4358, Train: 0.3714, Val: 0.3060, Test: 0.3130
Epoch: 30, Loss: 1.3714, Train: 0.3786, Val: 0.3300, Test: 0.3260
Epoch: 31, Loss: 1.3193, Train: 0.5143, Val: 0.4020, Test: 0.3820
Epoch: 32, Loss: 1.2820, Train: 0.5857, Val: 0.4400, Test: 0.4270
Epoch: 33, Loss: 1.2732, Train: 0.6429, Val: 0.4780, Test: 0.4870
Epoch: 34, Loss: 1.1780, Train: 0.7500, Val: 0.5320, Test: 0.5560
Epoch: 35, Loss: 1.1393, Train: 0.7714, Val: 0.5700, Test: 0.5760
Epoch: 36, Loss: 1.1291, Train: 0.7643, Val: 0.5740, Test: 0.5720
Epoch: 37, Loss: 1.0163, Train: 0.7857, Val: 0.6080, Test: 0.5820
Epoch: 38, Loss: 0.9879, Train: 0.7500, Val: 0.5620, Test: 0.5530
Epoch: 39, Loss: 0.8800, Train: 0.7357, Val: 0.5260, Test: 0.5360
Epoch: 40, Loss: 0.9367, Train: 0.7071, Val: 0.5280, Test: 0.5240
Epoch: 41, Loss: 0.8739, Train: 0.7714, Val: 0.5380, Test: 0.5460
Epoch: 42, Loss: 0.8260, Train: 0.8429, Val: 0.5940, Test: 0.5910
Epoch: 43, Loss: 0.7073, Train: 0.7071, Val: 0.5240, Test: 0.5230
Epoch: 44, Loss: 0.7770, Train: 0.8571, Val: 0.6060, Test: 0.6070
Epoch: 45, Loss: 0.6736, Train: 0.9214, Val: 0.6780, Test: 0.6830
Epoch: 46, Loss: 0.6505, Train: 0.9000, Val: 0.6580, Test: 0.6740
Epoch: 47, Loss: 0.6003, Train: 0.9429, Val: 0.6840, Test: 0.6860
Epoch: 48, Loss: 0.5304, Train: 0.9214, Val: 0.7100, Test: 0.7330
Epoch: 49, Loss: 0.5143, Train: 0.9000, Val: 0.7140, Test: 0.7150
Epoch: 50, Loss: 0.5591, Train: 0.9143, Val: 0.6820, Test: 0.7170
MAD:  0.9667
Best Test Accuracy: 0.7330, Val Accuracy: 0.7100, Train Accuracy: 0.9214
Training completed.
Average Test Accuracy:  0.7458 ± 0.047078232762073825
Average MAD:  0.87903 ± 0.04164970708180312

Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9973, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 2, Loss: 1.9124, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 3, Loss: 1.9315, Train: 0.2571, Val: 0.2240, Test: 0.2170
Epoch: 4, Loss: 1.9567, Train: 0.4500, Val: 0.2020, Test: 0.2400
Epoch: 5, Loss: 1.9783, Train: 0.4500, Val: 0.3560, Test: 0.3530
Epoch: 6, Loss: 1.9213, Train: 0.1714, Val: 0.1760, Test: 0.1640
Epoch: 7, Loss: 1.9078, Train: 0.3143, Val: 0.2880, Test: 0.2690
Epoch: 8, Loss: 1.8945, Train: 0.3857, Val: 0.2320, Test: 0.2400
Epoch: 9, Loss: 1.8576, Train: 0.5429, Val: 0.3660, Test: 0.3790
Epoch: 10, Loss: 1.8483, Train: 0.7357, Val: 0.5620, Test: 0.5900
Epoch: 11, Loss: 1.7482, Train: 0.7500, Val: 0.5760, Test: 0.5610
Epoch: 12, Loss: 1.6156, Train: 0.6357, Val: 0.4820, Test: 0.4600
Epoch: 13, Loss: 1.3695, Train: 0.6786, Val: 0.5240, Test: 0.5140
Epoch: 14, Loss: 1.1436, Train: 0.8286, Val: 0.6320, Test: 0.6400
Epoch: 15, Loss: 0.9197, Train: 0.8643, Val: 0.6500, Test: 0.6810
Epoch: 16, Loss: 0.8219, Train: 0.8429, Val: 0.6060, Test: 0.6090
Epoch: 17, Loss: 0.6629, Train: 0.8357, Val: 0.6080, Test: 0.5990
Epoch: 18, Loss: 0.6845, Train: 0.9571, Val: 0.7480, Test: 0.7590
Epoch: 19, Loss: 0.3660, Train: 0.8929, Val: 0.6720, Test: 0.6960
Epoch: 20, Loss: 0.5176, Train: 0.9429, Val: 0.7140, Test: 0.7450
Epoch: 21, Loss: 0.3628, Train: 0.9571, Val: 0.7320, Test: 0.7600
Epoch: 22, Loss: 0.3029, Train: 0.9500, Val: 0.7420, Test: 0.7740
Epoch: 23, Loss: 0.2404, Train: 0.9857, Val: 0.7460, Test: 0.7720
Epoch: 24, Loss: 0.1876, Train: 0.9714, Val: 0.7240, Test: 0.7430
Epoch: 25, Loss: 0.1797, Train: 0.9929, Val: 0.7240, Test: 0.7460
Epoch: 26, Loss: 0.1013, Train: 0.9929, Val: 0.7440, Test: 0.7620
Epoch: 27, Loss: 0.1080, Train: 0.9857, Val: 0.7520, Test: 0.7700
Epoch: 28, Loss: 0.0676, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 29, Loss: 0.0785, Train: 0.9857, Val: 0.7520, Test: 0.7730
Epoch: 30, Loss: 0.0481, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 31, Loss: 0.0458, Train: 1.0000, Val: 0.7320, Test: 0.7700
Epoch: 32, Loss: 0.0369, Train: 0.9857, Val: 0.7300, Test: 0.7640
Epoch: 33, Loss: 0.0671, Train: 1.0000, Val: 0.7320, Test: 0.7680
Epoch: 34, Loss: 0.0170, Train: 1.0000, Val: 0.7240, Test: 0.7530
Epoch: 35, Loss: 0.0150, Train: 1.0000, Val: 0.7200, Test: 0.7420
Epoch: 36, Loss: 0.0446, Train: 1.0000, Val: 0.7100, Test: 0.7350
Epoch: 37, Loss: 0.0278, Train: 0.9857, Val: 0.7140, Test: 0.7260
Epoch: 38, Loss: 0.0340, Train: 1.0000, Val: 0.7280, Test: 0.7510
Epoch: 39, Loss: 0.0114, Train: 0.9857, Val: 0.7400, Test: 0.7700
Epoch: 40, Loss: 0.0708, Train: 1.0000, Val: 0.7160, Test: 0.7480
Epoch: 41, Loss: 0.0035, Train: 0.9929, Val: 0.6980, Test: 0.7100
Epoch: 42, Loss: 0.0265, Train: 0.9857, Val: 0.6820, Test: 0.6980
Epoch: 43, Loss: 0.0724, Train: 0.9929, Val: 0.7000, Test: 0.7370
Epoch: 44, Loss: 0.0074, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 45, Loss: 0.0186, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 46, Loss: 0.0062, Train: 1.0000, Val: 0.7300, Test: 0.7680
Epoch: 47, Loss: 0.0144, Train: 1.0000, Val: 0.7200, Test: 0.7650
Epoch: 48, Loss: 0.0355, Train: 1.0000, Val: 0.7400, Test: 0.7540
Epoch: 49, Loss: 0.0081, Train: 1.0000, Val: 0.7320, Test: 0.7400
Epoch: 50, Loss: 0.0138, Train: 1.0000, Val: 0.7220, Test: 0.7320
MAD:  0.5551
Best Test Accuracy: 0.7760, Val Accuracy: 0.7440, Train Accuracy: 1.0000
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9583, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 2, Loss: 1.9827, Train: 0.1857, Val: 0.1240, Test: 0.1380
Epoch: 3, Loss: 1.9439, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 4, Loss: 1.9423, Train: 0.1571, Val: 0.1560, Test: 0.1460
Epoch: 5, Loss: 1.9247, Train: 0.2429, Val: 0.1720, Test: 0.1550
Epoch: 6, Loss: 1.9541, Train: 0.2714, Val: 0.2340, Test: 0.2260
Epoch: 7, Loss: 1.8924, Train: 0.4143, Val: 0.2700, Test: 0.2790
Epoch: 8, Loss: 1.9423, Train: 0.2286, Val: 0.1140, Test: 0.1230
Epoch: 9, Loss: 1.9177, Train: 0.2429, Val: 0.1520, Test: 0.1550
Epoch: 10, Loss: 1.8480, Train: 0.3929, Val: 0.3560, Test: 0.3520
Epoch: 11, Loss: 1.8326, Train: 0.4214, Val: 0.3540, Test: 0.3280
Epoch: 12, Loss: 1.7109, Train: 0.4071, Val: 0.2740, Test: 0.2700
Epoch: 13, Loss: 1.6241, Train: 0.5643, Val: 0.3860, Test: 0.3740
Epoch: 14, Loss: 1.4506, Train: 0.6357, Val: 0.4360, Test: 0.4350
Epoch: 15, Loss: 1.3881, Train: 0.5000, Val: 0.3840, Test: 0.3660
Epoch: 16, Loss: 1.3029, Train: 0.5357, Val: 0.4000, Test: 0.3840
Epoch: 17, Loss: 1.2697, Train: 0.5357, Val: 0.4100, Test: 0.4140
Epoch: 18, Loss: 1.1560, Train: 0.6214, Val: 0.4580, Test: 0.4790
Epoch: 19, Loss: 1.0334, Train: 0.7571, Val: 0.5820, Test: 0.6100
Epoch: 20, Loss: 1.0426, Train: 0.8143, Val: 0.5720, Test: 0.6040
Epoch: 21, Loss: 0.8961, Train: 0.7143, Val: 0.4760, Test: 0.4790
Epoch: 22, Loss: 0.9627, Train: 0.8000, Val: 0.6060, Test: 0.6300
Epoch: 23, Loss: 0.7649, Train: 0.7500, Val: 0.6060, Test: 0.6130
Epoch: 24, Loss: 0.7277, Train: 0.8429, Val: 0.5520, Test: 0.6070
Epoch: 25, Loss: 0.6429, Train: 0.8929, Val: 0.5680, Test: 0.6010
Epoch: 26, Loss: 0.6008, Train: 0.9286, Val: 0.6080, Test: 0.6300
Epoch: 27, Loss: 0.5236, Train: 0.9500, Val: 0.6400, Test: 0.6310
Epoch: 28, Loss: 0.3981, Train: 0.9786, Val: 0.6760, Test: 0.7310
Epoch: 29, Loss: 0.2910, Train: 0.9714, Val: 0.6760, Test: 0.6950
Epoch: 30, Loss: 0.2661, Train: 0.9714, Val: 0.6860, Test: 0.7300
Epoch: 31, Loss: 0.1731, Train: 0.9857, Val: 0.6980, Test: 0.7390
Epoch: 32, Loss: 0.1659, Train: 0.9857, Val: 0.7080, Test: 0.7430
Epoch: 33, Loss: 0.1258, Train: 0.9714, Val: 0.6940, Test: 0.7330
Epoch: 34, Loss: 0.0925, Train: 0.9643, Val: 0.6700, Test: 0.7110
Epoch: 35, Loss: 0.1143, Train: 1.0000, Val: 0.6920, Test: 0.7290
Epoch: 36, Loss: 0.0647, Train: 1.0000, Val: 0.7120, Test: 0.7390
Epoch: 37, Loss: 0.0568, Train: 1.0000, Val: 0.7000, Test: 0.7240
Epoch: 38, Loss: 0.0361, Train: 1.0000, Val: 0.6980, Test: 0.7250
Epoch: 39, Loss: 0.0380, Train: 1.0000, Val: 0.7100, Test: 0.7340
Epoch: 40, Loss: 0.0231, Train: 1.0000, Val: 0.7140, Test: 0.7280
Epoch: 41, Loss: 0.0194, Train: 1.0000, Val: 0.7020, Test: 0.7300
Epoch: 42, Loss: 0.0173, Train: 1.0000, Val: 0.7120, Test: 0.7430
Epoch: 43, Loss: 0.0161, Train: 1.0000, Val: 0.7220, Test: 0.7490
Epoch: 44, Loss: 0.0148, Train: 1.0000, Val: 0.7320, Test: 0.7510
Epoch: 45, Loss: 0.0084, Train: 1.0000, Val: 0.7280, Test: 0.7550
Epoch: 46, Loss: 0.0164, Train: 1.0000, Val: 0.7300, Test: 0.7580
Epoch: 47, Loss: 0.0200, Train: 1.0000, Val: 0.7240, Test: 0.7490
Epoch: 48, Loss: 0.0094, Train: 1.0000, Val: 0.7120, Test: 0.7400
Epoch: 49, Loss: 0.0046, Train: 1.0000, Val: 0.6980, Test: 0.7280
Epoch: 50, Loss: 0.0161, Train: 1.0000, Val: 0.7080, Test: 0.7380
MAD:  0.579
Best Test Accuracy: 0.7580, Val Accuracy: 0.7300, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9683, Train: 0.1857, Val: 0.2840, Test: 0.2690
Epoch: 2, Loss: 1.9394, Train: 0.2143, Val: 0.1540, Test: 0.1650
Epoch: 3, Loss: 1.9310, Train: 0.3214, Val: 0.1800, Test: 0.1770
Epoch: 4, Loss: 1.9459, Train: 0.1714, Val: 0.1700, Test: 0.1690
Epoch: 5, Loss: 1.9435, Train: 0.3714, Val: 0.2600, Test: 0.2420
Epoch: 6, Loss: 1.9407, Train: 0.3143, Val: 0.1640, Test: 0.1710
Epoch: 7, Loss: 1.9184, Train: 0.1429, Val: 0.1620, Test: 0.1520
Epoch: 8, Loss: 1.9258, Train: 0.4071, Val: 0.2900, Test: 0.2970
Epoch: 9, Loss: 1.8801, Train: 0.4643, Val: 0.2860, Test: 0.3040
Epoch: 10, Loss: 1.8136, Train: 0.6071, Val: 0.4000, Test: 0.4100
Epoch: 11, Loss: 1.7446, Train: 0.8214, Val: 0.5840, Test: 0.5730
Epoch: 12, Loss: 1.6131, Train: 0.7857, Val: 0.5040, Test: 0.5110
Epoch: 13, Loss: 1.4248, Train: 0.7643, Val: 0.5440, Test: 0.5480
Epoch: 14, Loss: 1.1739, Train: 0.8500, Val: 0.6800, Test: 0.6660
Epoch: 15, Loss: 0.9494, Train: 0.8286, Val: 0.5520, Test: 0.5670
Epoch: 16, Loss: 0.8551, Train: 0.9143, Val: 0.7580, Test: 0.7650
Epoch: 17, Loss: 0.5935, Train: 0.9357, Val: 0.7620, Test: 0.7660
Epoch: 18, Loss: 0.5278, Train: 0.9571, Val: 0.7380, Test: 0.7480
Epoch: 19, Loss: 0.3854, Train: 0.9500, Val: 0.6820, Test: 0.6840
Epoch: 20, Loss: 0.3015, Train: 0.9643, Val: 0.7440, Test: 0.7310
Epoch: 21, Loss: 0.2560, Train: 0.9857, Val: 0.7520, Test: 0.7580
Epoch: 22, Loss: 0.1505, Train: 0.9929, Val: 0.7440, Test: 0.7450
Epoch: 23, Loss: 0.1272, Train: 0.9929, Val: 0.7620, Test: 0.7600
Epoch: 24, Loss: 0.0944, Train: 0.9929, Val: 0.7520, Test: 0.7590
Epoch: 25, Loss: 0.0607, Train: 1.0000, Val: 0.7480, Test: 0.7480
Epoch: 26, Loss: 0.0560, Train: 1.0000, Val: 0.7460, Test: 0.7490
Epoch: 27, Loss: 0.0256, Train: 0.9929, Val: 0.7400, Test: 0.7490
Epoch: 28, Loss: 0.0227, Train: 1.0000, Val: 0.7420, Test: 0.7590
Epoch: 29, Loss: 0.0246, Train: 0.9929, Val: 0.7520, Test: 0.7590
Epoch: 30, Loss: 0.0272, Train: 1.0000, Val: 0.7440, Test: 0.7820
Epoch: 31, Loss: 0.0114, Train: 1.0000, Val: 0.7440, Test: 0.7790
Epoch: 32, Loss: 0.0303, Train: 0.9929, Val: 0.7480, Test: 0.7790
Epoch: 33, Loss: 0.0500, Train: 0.9714, Val: 0.7560, Test: 0.7870
Epoch: 34, Loss: 0.1803, Train: 0.9929, Val: 0.7540, Test: 0.7690
Epoch: 35, Loss: 0.0463, Train: 0.9714, Val: 0.6980, Test: 0.7130
Epoch: 36, Loss: 0.1136, Train: 0.9857, Val: 0.7020, Test: 0.7220
Epoch: 37, Loss: 0.0486, Train: 1.0000, Val: 0.7260, Test: 0.7500
Epoch: 38, Loss: 0.0174, Train: 1.0000, Val: 0.7480, Test: 0.7660
Epoch: 39, Loss: 0.0050, Train: 1.0000, Val: 0.7700, Test: 0.7640
Epoch: 40, Loss: 0.0621, Train: 0.9929, Val: 0.7560, Test: 0.7530
Epoch: 41, Loss: 0.0135, Train: 1.0000, Val: 0.7520, Test: 0.7660
Epoch: 42, Loss: 0.0195, Train: 1.0000, Val: 0.7420, Test: 0.7760
Epoch: 43, Loss: 0.0108, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 44, Loss: 0.0035, Train: 1.0000, Val: 0.7240, Test: 0.7610
Epoch: 45, Loss: 0.0181, Train: 1.0000, Val: 0.7220, Test: 0.7550
Epoch: 46, Loss: 0.0350, Train: 1.0000, Val: 0.7300, Test: 0.7460
Epoch: 47, Loss: 0.0074, Train: 1.0000, Val: 0.7280, Test: 0.7420
Epoch: 48, Loss: 0.0056, Train: 1.0000, Val: 0.7360, Test: 0.7320
Epoch: 49, Loss: 0.0141, Train: 1.0000, Val: 0.7240, Test: 0.7200
Epoch: 50, Loss: 0.0138, Train: 1.0000, Val: 0.7240, Test: 0.7240
MAD:  0.5608
Best Test Accuracy: 0.7870, Val Accuracy: 0.7560, Train Accuracy: 0.9714
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9695, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 2, Loss: 1.9820, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 3, Loss: 1.9943, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 4, Loss: 1.9119, Train: 0.2786, Val: 0.2620, Test: 0.2550
Epoch: 5, Loss: 1.9134, Train: 0.2643, Val: 0.2000, Test: 0.1800
Epoch: 6, Loss: 1.9510, Train: 0.3357, Val: 0.1720, Test: 0.1900
Epoch: 7, Loss: 1.9441, Train: 0.3214, Val: 0.1300, Test: 0.1510
Epoch: 8, Loss: 1.8972, Train: 0.3786, Val: 0.2380, Test: 0.2490
Epoch: 9, Loss: 1.8787, Train: 0.4000, Val: 0.2160, Test: 0.2470
Epoch: 10, Loss: 1.8524, Train: 0.5286, Val: 0.3900, Test: 0.4730
Epoch: 11, Loss: 1.7144, Train: 0.5500, Val: 0.3440, Test: 0.3570
Epoch: 12, Loss: 1.6580, Train: 0.4214, Val: 0.2440, Test: 0.2360
Epoch: 13, Loss: 1.5764, Train: 0.5357, Val: 0.2940, Test: 0.3260
Epoch: 14, Loss: 1.4875, Train: 0.4500, Val: 0.2560, Test: 0.2650
Epoch: 15, Loss: 1.4433, Train: 0.7071, Val: 0.4920, Test: 0.4760
Epoch: 16, Loss: 1.2770, Train: 0.6714, Val: 0.4140, Test: 0.4180
Epoch: 17, Loss: 1.1637, Train: 0.7500, Val: 0.5240, Test: 0.5210
Epoch: 18, Loss: 1.0079, Train: 0.7929, Val: 0.5540, Test: 0.5510
Epoch: 19, Loss: 0.8989, Train: 0.7500, Val: 0.6060, Test: 0.6040
Epoch: 20, Loss: 0.8103, Train: 0.8929, Val: 0.7140, Test: 0.7250
Epoch: 21, Loss: 0.6245, Train: 0.8786, Val: 0.7280, Test: 0.7260
Epoch: 22, Loss: 0.5045, Train: 0.9500, Val: 0.7680, Test: 0.7840
Epoch: 23, Loss: 0.3642, Train: 0.9643, Val: 0.7760, Test: 0.8020
Epoch: 24, Loss: 0.3976, Train: 0.9714, Val: 0.7820, Test: 0.8030
Epoch: 25, Loss: 0.2536, Train: 0.9786, Val: 0.7580, Test: 0.7880
Epoch: 26, Loss: 0.1872, Train: 0.9857, Val: 0.7500, Test: 0.7830
Epoch: 27, Loss: 0.1134, Train: 0.9929, Val: 0.7660, Test: 0.7910
Epoch: 28, Loss: 0.0978, Train: 1.0000, Val: 0.7640, Test: 0.7910
Epoch: 29, Loss: 0.0824, Train: 0.9929, Val: 0.7660, Test: 0.7920
Epoch: 30, Loss: 0.0657, Train: 0.9929, Val: 0.7660, Test: 0.7910
Epoch: 31, Loss: 0.0518, Train: 1.0000, Val: 0.7620, Test: 0.7870
Epoch: 32, Loss: 0.0562, Train: 0.9929, Val: 0.7640, Test: 0.7880
Epoch: 33, Loss: 0.0809, Train: 0.9786, Val: 0.7300, Test: 0.7610
Epoch: 34, Loss: 0.0762, Train: 0.9857, Val: 0.7200, Test: 0.7430
Epoch: 35, Loss: 0.0655, Train: 1.0000, Val: 0.7420, Test: 0.7500
Epoch: 36, Loss: 0.0240, Train: 0.9857, Val: 0.7380, Test: 0.7430
Epoch: 37, Loss: 0.0860, Train: 1.0000, Val: 0.7340, Test: 0.7570
Epoch: 38, Loss: 0.0150, Train: 0.9929, Val: 0.7160, Test: 0.7530
Epoch: 39, Loss: 0.0128, Train: 0.9857, Val: 0.7140, Test: 0.7560
Epoch: 40, Loss: 0.0433, Train: 0.9929, Val: 0.7460, Test: 0.7740
Epoch: 41, Loss: 0.0292, Train: 0.9857, Val: 0.7580, Test: 0.7830
Epoch: 42, Loss: 0.0478, Train: 0.9929, Val: 0.7480, Test: 0.7840
Epoch: 43, Loss: 0.0166, Train: 1.0000, Val: 0.7400, Test: 0.7780
Epoch: 44, Loss: 0.0294, Train: 0.9857, Val: 0.7280, Test: 0.7610
Epoch: 45, Loss: 0.0354, Train: 0.9857, Val: 0.7240, Test: 0.7560
Epoch: 46, Loss: 0.0313, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 47, Loss: 0.0106, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 48, Loss: 0.0114, Train: 0.9929, Val: 0.7460, Test: 0.7810
Epoch: 49, Loss: 0.0138, Train: 0.9857, Val: 0.7420, Test: 0.7890
Epoch: 50, Loss: 0.0340, Train: 1.0000, Val: 0.7500, Test: 0.7880
MAD:  0.4567
Best Test Accuracy: 0.8030, Val Accuracy: 0.7820, Train Accuracy: 0.9714
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9811, Train: 0.1786, Val: 0.2920, Test: 0.2920
Epoch: 2, Loss: 1.9565, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 3, Loss: 1.9129, Train: 0.2286, Val: 0.2100, Test: 0.2270
Epoch: 4, Loss: 1.9479, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 5, Loss: 1.9483, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 6, Loss: 1.9410, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 7, Loss: 1.9520, Train: 0.3214, Val: 0.3160, Test: 0.2910
Epoch: 8, Loss: 1.9401, Train: 0.4500, Val: 0.3640, Test: 0.3340
Epoch: 9, Loss: 1.9038, Train: 0.4071, Val: 0.2720, Test: 0.2690
Epoch: 10, Loss: 1.8784, Train: 0.3786, Val: 0.2520, Test: 0.2580
Epoch: 11, Loss: 1.8615, Train: 0.3357, Val: 0.2320, Test: 0.2560
Epoch: 12, Loss: 1.7818, Train: 0.3643, Val: 0.2440, Test: 0.2560
Epoch: 13, Loss: 1.7016, Train: 0.4357, Val: 0.2840, Test: 0.2930
Epoch: 14, Loss: 1.5443, Train: 0.5143, Val: 0.4080, Test: 0.4060
Epoch: 15, Loss: 1.4197, Train: 0.5500, Val: 0.4900, Test: 0.4660
Epoch: 16, Loss: 1.3039, Train: 0.6000, Val: 0.4600, Test: 0.4760
Epoch: 17, Loss: 1.2392, Train: 0.7714, Val: 0.6000, Test: 0.5950
Epoch: 18, Loss: 1.0735, Train: 0.8286, Val: 0.6720, Test: 0.6640
Epoch: 19, Loss: 0.9086, Train: 0.8000, Val: 0.6000, Test: 0.6030
Epoch: 20, Loss: 0.8688, Train: 0.9214, Val: 0.7380, Test: 0.7450
Epoch: 21, Loss: 0.6962, Train: 0.9286, Val: 0.7400, Test: 0.7450
Epoch: 22, Loss: 0.5723, Train: 0.9643, Val: 0.7580, Test: 0.7520
Epoch: 23, Loss: 0.4932, Train: 0.9429, Val: 0.6980, Test: 0.7090
Epoch: 24, Loss: 0.4530, Train: 0.9643, Val: 0.7660, Test: 0.7680
Epoch: 25, Loss: 0.3316, Train: 0.9857, Val: 0.7700, Test: 0.7770
Epoch: 26, Loss: 0.2680, Train: 0.9929, Val: 0.7600, Test: 0.7660
Epoch: 27, Loss: 0.1753, Train: 0.9786, Val: 0.7320, Test: 0.7400
Epoch: 28, Loss: 0.1951, Train: 0.9786, Val: 0.7420, Test: 0.7470
Epoch: 29, Loss: 0.1731, Train: 0.9929, Val: 0.7660, Test: 0.7800
Epoch: 30, Loss: 0.1108, Train: 0.9714, Val: 0.7780, Test: 0.7830
Epoch: 31, Loss: 0.1393, Train: 1.0000, Val: 0.7700, Test: 0.7960
Epoch: 32, Loss: 0.1071, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 33, Loss: 0.0431, Train: 0.9857, Val: 0.7260, Test: 0.7420
Epoch: 34, Loss: 0.0776, Train: 0.9929, Val: 0.7220, Test: 0.7470
Epoch: 35, Loss: 0.0440, Train: 0.9929, Val: 0.7500, Test: 0.7680
Epoch: 36, Loss: 0.0343, Train: 0.9929, Val: 0.7640, Test: 0.7890
Epoch: 37, Loss: 0.0622, Train: 1.0000, Val: 0.7580, Test: 0.7870
Epoch: 38, Loss: 0.0279, Train: 0.9929, Val: 0.7760, Test: 0.7810
Epoch: 39, Loss: 0.0391, Train: 1.0000, Val: 0.7600, Test: 0.7790
Epoch: 40, Loss: 0.0301, Train: 1.0000, Val: 0.7520, Test: 0.7800
Epoch: 41, Loss: 0.0154, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 42, Loss: 0.0279, Train: 1.0000, Val: 0.7220, Test: 0.7560
Epoch: 43, Loss: 0.0291, Train: 0.9929, Val: 0.7320, Test: 0.7530
Epoch: 44, Loss: 0.0085, Train: 0.9857, Val: 0.7380, Test: 0.7400
Epoch: 45, Loss: 0.0399, Train: 1.0000, Val: 0.7360, Test: 0.7520
Epoch: 46, Loss: 0.0050, Train: 0.9929, Val: 0.7280, Test: 0.7520
Epoch: 47, Loss: 0.0110, Train: 0.9929, Val: 0.7240, Test: 0.7490
Epoch: 48, Loss: 0.0441, Train: 0.9929, Val: 0.7320, Test: 0.7480
Epoch: 49, Loss: 0.0225, Train: 1.0000, Val: 0.7240, Test: 0.7430
Epoch: 50, Loss: 0.0035, Train: 1.0000, Val: 0.7280, Test: 0.7420
MAD:  0.5225
Best Test Accuracy: 0.7960, Val Accuracy: 0.7700, Train Accuracy: 1.0000
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9833, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 2, Loss: 1.9732, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 3, Loss: 1.9321, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 4, Loss: 1.9391, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 5, Loss: 1.9416, Train: 0.2643, Val: 0.4300, Test: 0.4200
Epoch: 6, Loss: 1.9448, Train: 0.1714, Val: 0.1660, Test: 0.1580
Epoch: 7, Loss: 1.9233, Train: 0.2714, Val: 0.2000, Test: 0.1990
Epoch: 8, Loss: 1.8996, Train: 0.2643, Val: 0.1260, Test: 0.1260
Epoch: 9, Loss: 1.8798, Train: 0.5643, Val: 0.5820, Test: 0.5700
Epoch: 10, Loss: 1.8329, Train: 0.5143, Val: 0.5280, Test: 0.5420
Epoch: 11, Loss: 1.6600, Train: 0.5857, Val: 0.4700, Test: 0.4690
Epoch: 12, Loss: 1.5448, Train: 0.4143, Val: 0.2940, Test: 0.3010
Epoch: 13, Loss: 1.3992, Train: 0.6000, Val: 0.4260, Test: 0.4370
Epoch: 14, Loss: 1.2262, Train: 0.7214, Val: 0.4880, Test: 0.5210
Epoch: 15, Loss: 1.1582, Train: 0.7357, Val: 0.4840, Test: 0.5070
Epoch: 16, Loss: 0.9674, Train: 0.7429, Val: 0.5880, Test: 0.6020
Epoch: 17, Loss: 0.8831, Train: 0.8714, Val: 0.6220, Test: 0.6660
Epoch: 18, Loss: 0.8888, Train: 0.9214, Val: 0.6700, Test: 0.6970
Epoch: 19, Loss: 0.7002, Train: 0.9000, Val: 0.6340, Test: 0.6650
Epoch: 20, Loss: 0.5719, Train: 0.9786, Val: 0.6840, Test: 0.6990
Epoch: 21, Loss: 0.4103, Train: 0.9714, Val: 0.6800, Test: 0.7010
Epoch: 22, Loss: 0.3639, Train: 0.9571, Val: 0.7020, Test: 0.7050
Epoch: 23, Loss: 0.2995, Train: 0.9857, Val: 0.7440, Test: 0.7520
Epoch: 24, Loss: 0.2148, Train: 0.9857, Val: 0.7540, Test: 0.7630
Epoch: 25, Loss: 0.2018, Train: 0.9857, Val: 0.7380, Test: 0.7550
Epoch: 26, Loss: 0.1534, Train: 0.9857, Val: 0.7340, Test: 0.7520
Epoch: 27, Loss: 0.1337, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 28, Loss: 0.0974, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 29, Loss: 0.0834, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 30, Loss: 0.0880, Train: 0.9929, Val: 0.7460, Test: 0.7690
Epoch: 31, Loss: 0.0554, Train: 0.9929, Val: 0.7520, Test: 0.7750
Epoch: 32, Loss: 0.0404, Train: 0.9929, Val: 0.7460, Test: 0.7790
Epoch: 33, Loss: 0.0349, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 34, Loss: 0.0315, Train: 1.0000, Val: 0.7540, Test: 0.7650
Epoch: 35, Loss: 0.0302, Train: 0.9857, Val: 0.7360, Test: 0.7610
Epoch: 36, Loss: 0.0512, Train: 0.9857, Val: 0.7340, Test: 0.7570
Epoch: 37, Loss: 0.0457, Train: 0.9929, Val: 0.7660, Test: 0.7790
Epoch: 38, Loss: 0.0245, Train: 0.9929, Val: 0.7620, Test: 0.7920
Epoch: 39, Loss: 0.0633, Train: 1.0000, Val: 0.7580, Test: 0.7890
Epoch: 40, Loss: 0.0115, Train: 0.9929, Val: 0.7360, Test: 0.7680
Epoch: 41, Loss: 0.0342, Train: 1.0000, Val: 0.7300, Test: 0.7510
Epoch: 42, Loss: 0.0238, Train: 1.0000, Val: 0.7080, Test: 0.7400
Epoch: 43, Loss: 0.0255, Train: 1.0000, Val: 0.7220, Test: 0.7500
Epoch: 44, Loss: 0.0417, Train: 1.0000, Val: 0.7240, Test: 0.7700
Epoch: 45, Loss: 0.0111, Train: 1.0000, Val: 0.7220, Test: 0.7720
Epoch: 46, Loss: 0.0169, Train: 1.0000, Val: 0.7400, Test: 0.7790
Epoch: 47, Loss: 0.0043, Train: 1.0000, Val: 0.7680, Test: 0.7820
Epoch: 48, Loss: 0.0071, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 49, Loss: 0.0057, Train: 1.0000, Val: 0.7420, Test: 0.7750
Epoch: 50, Loss: 0.0108, Train: 1.0000, Val: 0.7440, Test: 0.7740
MAD:  0.537
Best Test Accuracy: 0.7920, Val Accuracy: 0.7620, Train Accuracy: 0.9929
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9977, Train: 0.2000, Val: 0.2080, Test: 0.1720
Epoch: 2, Loss: 1.9382, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 3, Loss: 1.9642, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 4, Loss: 1.9324, Train: 0.2071, Val: 0.1220, Test: 0.1480
Epoch: 5, Loss: 1.9396, Train: 0.1643, Val: 0.1140, Test: 0.1060
Epoch: 6, Loss: 1.9375, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 7, Loss: 1.9273, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 8, Loss: 1.9279, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 9, Loss: 1.9668, Train: 0.3571, Val: 0.3400, Test: 0.3200
Epoch: 10, Loss: 1.8898, Train: 0.5214, Val: 0.3240, Test: 0.3420
Epoch: 11, Loss: 1.8938, Train: 0.4857, Val: 0.2820, Test: 0.2750
Epoch: 12, Loss: 1.8411, Train: 0.6429, Val: 0.4920, Test: 0.5140
Epoch: 13, Loss: 1.7397, Train: 0.4286, Val: 0.3160, Test: 0.3360
Epoch: 14, Loss: 1.6448, Train: 0.5143, Val: 0.3400, Test: 0.3520
Epoch: 15, Loss: 1.4754, Train: 0.6000, Val: 0.4140, Test: 0.4000
Epoch: 16, Loss: 1.4127, Train: 0.6357, Val: 0.4540, Test: 0.4310
Epoch: 17, Loss: 1.2533, Train: 0.6357, Val: 0.4420, Test: 0.4480
Epoch: 18, Loss: 1.1861, Train: 0.6429, Val: 0.4320, Test: 0.4230
Epoch: 19, Loss: 1.2629, Train: 0.5429, Val: 0.3420, Test: 0.3660
Epoch: 20, Loss: 1.0445, Train: 0.7071, Val: 0.4560, Test: 0.4900
Epoch: 21, Loss: 0.9974, Train: 0.9357, Val: 0.7440, Test: 0.7480
Epoch: 22, Loss: 0.6972, Train: 0.7929, Val: 0.6420, Test: 0.6540
Epoch: 23, Loss: 0.7638, Train: 0.9429, Val: 0.7180, Test: 0.7260
Epoch: 24, Loss: 0.5593, Train: 0.9429, Val: 0.7100, Test: 0.7180
Epoch: 25, Loss: 0.4762, Train: 0.9143, Val: 0.6780, Test: 0.7010
Epoch: 26, Loss: 0.3737, Train: 0.9786, Val: 0.7240, Test: 0.7510
Epoch: 27, Loss: 0.2499, Train: 0.9643, Val: 0.7440, Test: 0.7640
Epoch: 28, Loss: 0.2312, Train: 0.9786, Val: 0.7520, Test: 0.7760
Epoch: 29, Loss: 0.2056, Train: 0.9929, Val: 0.7720, Test: 0.7650
Epoch: 30, Loss: 0.1344, Train: 0.9929, Val: 0.7580, Test: 0.7630
Epoch: 31, Loss: 0.1130, Train: 0.9929, Val: 0.7780, Test: 0.7750
Epoch: 32, Loss: 0.1215, Train: 1.0000, Val: 0.7720, Test: 0.7760
Epoch: 33, Loss: 0.0597, Train: 1.0000, Val: 0.7520, Test: 0.7640
Epoch: 34, Loss: 0.0581, Train: 0.9929, Val: 0.7460, Test: 0.7560
Epoch: 35, Loss: 0.0426, Train: 0.9857, Val: 0.7300, Test: 0.7450
Epoch: 36, Loss: 0.0293, Train: 0.9857, Val: 0.7360, Test: 0.7480
Epoch: 37, Loss: 0.0298, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 38, Loss: 0.0256, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 39, Loss: 0.0188, Train: 1.0000, Val: 0.7560, Test: 0.7820
Epoch: 40, Loss: 0.0184, Train: 0.9929, Val: 0.7600, Test: 0.7840
Epoch: 41, Loss: 0.0127, Train: 1.0000, Val: 0.7560, Test: 0.7830
Epoch: 42, Loss: 0.0114, Train: 1.0000, Val: 0.7580, Test: 0.7700
Epoch: 43, Loss: 0.0097, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 44, Loss: 0.0212, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 45, Loss: 0.0161, Train: 0.9929, Val: 0.7700, Test: 0.7810
Epoch: 46, Loss: 0.0336, Train: 0.9857, Val: 0.7720, Test: 0.7730
Epoch: 47, Loss: 0.0170, Train: 1.0000, Val: 0.7720, Test: 0.7610
Epoch: 48, Loss: 0.0099, Train: 1.0000, Val: 0.7620, Test: 0.7670
Epoch: 49, Loss: 0.0040, Train: 0.9929, Val: 0.7480, Test: 0.7500
Epoch: 50, Loss: 0.0571, Train: 1.0000, Val: 0.7480, Test: 0.7890
MAD:  0.6072
Best Test Accuracy: 0.7900, Val Accuracy: 0.7680, Train Accuracy: 1.0000
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9642, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 2, Loss: 1.9585, Train: 0.2786, Val: 0.4340, Test: 0.4340
Epoch: 3, Loss: 1.9657, Train: 0.1714, Val: 0.1140, Test: 0.1150
Epoch: 4, Loss: 1.9533, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 5, Loss: 1.8972, Train: 0.2286, Val: 0.1400, Test: 0.1330
Epoch: 6, Loss: 1.9451, Train: 0.3643, Val: 0.2560, Test: 0.2610
Epoch: 7, Loss: 1.8951, Train: 0.3643, Val: 0.2360, Test: 0.2410
Epoch: 8, Loss: 1.9044, Train: 0.3071, Val: 0.2060, Test: 0.2080
Epoch: 9, Loss: 1.8587, Train: 0.5429, Val: 0.3700, Test: 0.3780
Epoch: 10, Loss: 1.8173, Train: 0.5929, Val: 0.4340, Test: 0.4460
Epoch: 11, Loss: 1.6894, Train: 0.5714, Val: 0.4220, Test: 0.4440
Epoch: 12, Loss: 1.5394, Train: 0.5714, Val: 0.4760, Test: 0.4640
Epoch: 13, Loss: 1.3760, Train: 0.6071, Val: 0.5260, Test: 0.5040
Epoch: 14, Loss: 1.1858, Train: 0.6286, Val: 0.4760, Test: 0.4670
Epoch: 15, Loss: 1.0285, Train: 0.7000, Val: 0.5440, Test: 0.5260
Epoch: 16, Loss: 0.9305, Train: 0.7357, Val: 0.5300, Test: 0.5440
Epoch: 17, Loss: 0.8775, Train: 0.6929, Val: 0.5660, Test: 0.5630
Epoch: 18, Loss: 0.7901, Train: 0.8643, Val: 0.6500, Test: 0.6580
Epoch: 19, Loss: 0.6338, Train: 0.9071, Val: 0.6860, Test: 0.7050
Epoch: 20, Loss: 0.4867, Train: 0.9571, Val: 0.7300, Test: 0.7400
Epoch: 21, Loss: 0.3750, Train: 0.9786, Val: 0.7360, Test: 0.7510
Epoch: 22, Loss: 0.3615, Train: 0.9786, Val: 0.7420, Test: 0.7470
Epoch: 23, Loss: 0.3128, Train: 0.9643, Val: 0.7220, Test: 0.7610
Epoch: 24, Loss: 0.2544, Train: 0.9714, Val: 0.7560, Test: 0.7550
Epoch: 25, Loss: 0.2119, Train: 0.9857, Val: 0.7500, Test: 0.7560
Epoch: 26, Loss: 0.2269, Train: 0.9857, Val: 0.7520, Test: 0.7590
Epoch: 27, Loss: 0.1283, Train: 0.9714, Val: 0.7480, Test: 0.7460
Epoch: 28, Loss: 0.1515, Train: 0.9929, Val: 0.7540, Test: 0.7600
Epoch: 29, Loss: 0.1221, Train: 0.9929, Val: 0.7560, Test: 0.7650
Epoch: 30, Loss: 0.1156, Train: 0.9929, Val: 0.7600, Test: 0.7580
Epoch: 31, Loss: 0.0750, Train: 0.9929, Val: 0.7440, Test: 0.7500
Epoch: 32, Loss: 0.0931, Train: 1.0000, Val: 0.7620, Test: 0.7590
Epoch: 33, Loss: 0.0740, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 34, Loss: 0.0520, Train: 1.0000, Val: 0.7500, Test: 0.7660
Epoch: 35, Loss: 0.0365, Train: 0.9929, Val: 0.7300, Test: 0.7490
Epoch: 36, Loss: 0.0403, Train: 1.0000, Val: 0.7240, Test: 0.7410
Epoch: 37, Loss: 0.0402, Train: 1.0000, Val: 0.7300, Test: 0.7350
Epoch: 38, Loss: 0.0327, Train: 1.0000, Val: 0.7420, Test: 0.7510
Epoch: 39, Loss: 0.0217, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 40, Loss: 0.0155, Train: 1.0000, Val: 0.7540, Test: 0.7690
Epoch: 41, Loss: 0.0188, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 42, Loss: 0.0302, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 43, Loss: 0.0125, Train: 1.0000, Val: 0.7400, Test: 0.7540
Epoch: 44, Loss: 0.0165, Train: 1.0000, Val: 0.7320, Test: 0.7560
Epoch: 45, Loss: 0.0085, Train: 1.0000, Val: 0.7340, Test: 0.7570
Epoch: 46, Loss: 0.0245, Train: 1.0000, Val: 0.7460, Test: 0.7500
Epoch: 47, Loss: 0.0125, Train: 1.0000, Val: 0.7300, Test: 0.7370
Epoch: 48, Loss: 0.0119, Train: 0.9857, Val: 0.7260, Test: 0.7300
Epoch: 49, Loss: 0.0528, Train: 0.9857, Val: 0.7000, Test: 0.7230
Epoch: 50, Loss: 0.0171, Train: 0.9857, Val: 0.6720, Test: 0.6680
MAD:  0.5759
Best Test Accuracy: 0.7690, Val Accuracy: 0.7580, Train Accuracy: 1.0000
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9823, Train: 0.1643, Val: 0.3080, Test: 0.3030
Epoch: 2, Loss: 1.9733, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 3, Loss: 2.0011, Train: 0.2643, Val: 0.2400, Test: 0.2140
Epoch: 4, Loss: 1.9348, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 5, Loss: 1.9304, Train: 0.3071, Val: 0.3480, Test: 0.3850
Epoch: 6, Loss: 1.9404, Train: 0.2071, Val: 0.1620, Test: 0.1620
Epoch: 7, Loss: 1.9468, Train: 0.3000, Val: 0.2280, Test: 0.2150
Epoch: 8, Loss: 1.8737, Train: 0.3000, Val: 0.2180, Test: 0.1990
Epoch: 9, Loss: 1.8470, Train: 0.4000, Val: 0.2800, Test: 0.2590
Epoch: 10, Loss: 1.8212, Train: 0.7000, Val: 0.4580, Test: 0.4780
Epoch: 11, Loss: 1.6748, Train: 0.5643, Val: 0.4340, Test: 0.4180
Epoch: 12, Loss: 1.5836, Train: 0.6571, Val: 0.4600, Test: 0.4360
Epoch: 13, Loss: 1.2730, Train: 0.6786, Val: 0.4420, Test: 0.4420
Epoch: 14, Loss: 1.1516, Train: 0.7071, Val: 0.5100, Test: 0.4850
Epoch: 15, Loss: 0.9784, Train: 0.7429, Val: 0.5260, Test: 0.5090
Epoch: 16, Loss: 0.9043, Train: 0.7714, Val: 0.5460, Test: 0.5200
Epoch: 17, Loss: 0.8250, Train: 0.8357, Val: 0.6240, Test: 0.6110
Epoch: 18, Loss: 0.7891, Train: 0.8357, Val: 0.6520, Test: 0.6880
Epoch: 19, Loss: 0.5741, Train: 0.8714, Val: 0.7020, Test: 0.6890
Epoch: 20, Loss: 0.5068, Train: 0.8786, Val: 0.6860, Test: 0.6780
Epoch: 21, Loss: 0.4405, Train: 0.9214, Val: 0.6860, Test: 0.6810
Epoch: 22, Loss: 0.3919, Train: 0.9429, Val: 0.7200, Test: 0.7160
Epoch: 23, Loss: 0.3327, Train: 0.9357, Val: 0.7520, Test: 0.7400
Epoch: 24, Loss: 0.3077, Train: 0.9714, Val: 0.7640, Test: 0.7670
Epoch: 25, Loss: 0.2788, Train: 0.9500, Val: 0.7680, Test: 0.7680
Epoch: 26, Loss: 0.1585, Train: 0.9500, Val: 0.7440, Test: 0.7420
Epoch: 27, Loss: 0.1902, Train: 0.9786, Val: 0.7600, Test: 0.7410
Epoch: 28, Loss: 0.1471, Train: 0.9643, Val: 0.7380, Test: 0.7270
Epoch: 29, Loss: 0.1396, Train: 0.9786, Val: 0.7700, Test: 0.7710
Epoch: 30, Loss: 0.1044, Train: 0.9357, Val: 0.7700, Test: 0.7660
Epoch: 31, Loss: 0.1093, Train: 0.9714, Val: 0.7760, Test: 0.7650
Epoch: 32, Loss: 0.0749, Train: 0.9786, Val: 0.7560, Test: 0.7440
Epoch: 33, Loss: 0.0710, Train: 0.9571, Val: 0.6780, Test: 0.6890
Epoch: 34, Loss: 0.1854, Train: 0.9929, Val: 0.7500, Test: 0.7460
Epoch: 35, Loss: 0.0647, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 36, Loss: 0.0296, Train: 0.9500, Val: 0.7160, Test: 0.7430
Epoch: 37, Loss: 0.0875, Train: 0.9857, Val: 0.7680, Test: 0.7690
Epoch: 38, Loss: 0.0410, Train: 0.9929, Val: 0.7580, Test: 0.7680
Epoch: 39, Loss: 0.0334, Train: 1.0000, Val: 0.7460, Test: 0.7540
Epoch: 40, Loss: 0.0278, Train: 1.0000, Val: 0.7400, Test: 0.7280
Epoch: 41, Loss: 0.0177, Train: 1.0000, Val: 0.7140, Test: 0.7120
Epoch: 42, Loss: 0.0215, Train: 1.0000, Val: 0.7260, Test: 0.7170
Epoch: 43, Loss: 0.0312, Train: 1.0000, Val: 0.7240, Test: 0.7280
Epoch: 44, Loss: 0.0286, Train: 1.0000, Val: 0.7240, Test: 0.7480
Epoch: 45, Loss: 0.0082, Train: 0.9929, Val: 0.7200, Test: 0.7570
Epoch: 46, Loss: 0.0142, Train: 1.0000, Val: 0.7320, Test: 0.7580
Epoch: 47, Loss: 0.0084, Train: 1.0000, Val: 0.7340, Test: 0.7720
Epoch: 48, Loss: 0.0278, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 49, Loss: 0.0064, Train: 0.9929, Val: 0.7440, Test: 0.7720
Epoch: 50, Loss: 0.0095, Train: 0.9857, Val: 0.7320, Test: 0.7620
MAD:  0.5777
Best Test Accuracy: 0.7750, Val Accuracy: 0.7520, Train Accuracy: 1.0000
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-4): 4 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (5): GATv2Conv(2048, 256, heads=8)
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0221, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 2, Loss: 1.9828, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 3, Loss: 1.9460, Train: 0.2071, Val: 0.1520, Test: 0.1760
Epoch: 4, Loss: 1.9552, Train: 0.2000, Val: 0.1340, Test: 0.1520
Epoch: 5, Loss: 1.9338, Train: 0.2857, Val: 0.1260, Test: 0.1440
Epoch: 6, Loss: 1.9254, Train: 0.4286, Val: 0.2920, Test: 0.2780
Epoch: 7, Loss: 1.9510, Train: 0.2214, Val: 0.1900, Test: 0.1900
Epoch: 8, Loss: 1.9523, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 9, Loss: 1.9188, Train: 0.1571, Val: 0.1580, Test: 0.1470
Epoch: 10, Loss: 1.9264, Train: 0.3571, Val: 0.3700, Test: 0.3720
Epoch: 11, Loss: 1.8998, Train: 0.3786, Val: 0.2280, Test: 0.2490
Epoch: 12, Loss: 1.9005, Train: 0.2714, Val: 0.1100, Test: 0.1250
Epoch: 13, Loss: 1.8705, Train: 0.4643, Val: 0.2560, Test: 0.2830
Epoch: 14, Loss: 1.8295, Train: 0.4929, Val: 0.2500, Test: 0.2750
Epoch: 15, Loss: 1.7949, Train: 0.5000, Val: 0.2560, Test: 0.2810
Epoch: 16, Loss: 1.6765, Train: 0.6714, Val: 0.3980, Test: 0.4280
Epoch: 17, Loss: 1.5329, Train: 0.6571, Val: 0.4060, Test: 0.4350
Epoch: 18, Loss: 1.3646, Train: 0.7000, Val: 0.5180, Test: 0.5470
Epoch: 19, Loss: 1.1392, Train: 0.7000, Val: 0.4580, Test: 0.4770
Epoch: 20, Loss: 0.9696, Train: 0.8071, Val: 0.5680, Test: 0.5650
Epoch: 21, Loss: 0.8138, Train: 0.8357, Val: 0.6360, Test: 0.6280
Epoch: 22, Loss: 0.7052, Train: 0.8429, Val: 0.6000, Test: 0.6030
Epoch: 23, Loss: 0.6434, Train: 0.8286, Val: 0.5840, Test: 0.5870
Epoch: 24, Loss: 0.5002, Train: 0.8429, Val: 0.5620, Test: 0.5740
Epoch: 25, Loss: 0.4613, Train: 0.9071, Val: 0.6400, Test: 0.6490
Epoch: 26, Loss: 0.3956, Train: 0.9357, Val: 0.7020, Test: 0.7060
Epoch: 27, Loss: 0.3639, Train: 0.9571, Val: 0.7300, Test: 0.7430
Epoch: 28, Loss: 0.2694, Train: 0.9571, Val: 0.7480, Test: 0.7310
Epoch: 29, Loss: 0.2505, Train: 0.9786, Val: 0.7460, Test: 0.7680
Epoch: 30, Loss: 0.2075, Train: 0.9714, Val: 0.7360, Test: 0.7460
Epoch: 31, Loss: 0.2140, Train: 0.9929, Val: 0.7480, Test: 0.7600
Epoch: 32, Loss: 0.1656, Train: 0.9714, Val: 0.7460, Test: 0.7610
Epoch: 33, Loss: 0.1294, Train: 0.9643, Val: 0.7220, Test: 0.7250
Epoch: 34, Loss: 0.1762, Train: 0.9786, Val: 0.7460, Test: 0.7510
Epoch: 35, Loss: 0.1246, Train: 0.9929, Val: 0.7040, Test: 0.7310
Epoch: 36, Loss: 0.0832, Train: 0.9714, Val: 0.6980, Test: 0.7200
Epoch: 37, Loss: 0.1054, Train: 0.9857, Val: 0.7140, Test: 0.7300
Epoch: 38, Loss: 0.0847, Train: 1.0000, Val: 0.7360, Test: 0.7540
Epoch: 39, Loss: 0.0520, Train: 0.9929, Val: 0.7420, Test: 0.7560
Epoch: 40, Loss: 0.0860, Train: 0.9857, Val: 0.7660, Test: 0.7720
Epoch: 41, Loss: 0.0745, Train: 0.9786, Val: 0.7360, Test: 0.7430
Epoch: 42, Loss: 0.0815, Train: 0.9929, Val: 0.7360, Test: 0.7480
Epoch: 43, Loss: 0.0364, Train: 1.0000, Val: 0.7300, Test: 0.7480
Epoch: 44, Loss: 0.0205, Train: 1.0000, Val: 0.7320, Test: 0.7510
Epoch: 45, Loss: 0.0231, Train: 0.9857, Val: 0.7340, Test: 0.7480
Epoch: 46, Loss: 0.0589, Train: 1.0000, Val: 0.7360, Test: 0.7510
Epoch: 47, Loss: 0.0137, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 48, Loss: 0.0071, Train: 1.0000, Val: 0.7440, Test: 0.7370
Epoch: 49, Loss: 0.0180, Train: 1.0000, Val: 0.7160, Test: 0.7210
Epoch: 50, Loss: 0.0095, Train: 1.0000, Val: 0.7060, Test: 0.7100
MAD:  0.5471
Best Test Accuracy: 0.7720, Val Accuracy: 0.7660, Train Accuracy: 0.9857
Training completed.
Average Test Accuracy:  0.7818000000000002 ± 0.013249905660041519
Average MAD:  0.5519000000000001 ± 0.03911015213470793

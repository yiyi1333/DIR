Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0012, Train: 0.3000, Val: 0.2400, Test: 0.2480
Epoch: 2, Loss: 1.8894, Train: 0.5000, Val: 0.3640, Test: 0.3780
Epoch: 3, Loss: 1.8174, Train: 0.6929, Val: 0.5420, Test: 0.5320
Epoch: 4, Loss: 1.7789, Train: 0.7929, Val: 0.6120, Test: 0.6160
Epoch: 5, Loss: 1.6620, Train: 0.8286, Val: 0.6420, Test: 0.6500
Epoch: 6, Loss: 1.6076, Train: 0.8714, Val: 0.6860, Test: 0.6710
Epoch: 7, Loss: 1.5127, Train: 0.8714, Val: 0.7100, Test: 0.6900
Epoch: 8, Loss: 1.4125, Train: 0.9071, Val: 0.7180, Test: 0.6980
Epoch: 9, Loss: 1.3557, Train: 0.9286, Val: 0.7160, Test: 0.7060
Epoch: 10, Loss: 1.2944, Train: 0.9500, Val: 0.7100, Test: 0.7250
Epoch: 11, Loss: 1.2509, Train: 0.9500, Val: 0.7220, Test: 0.7400
Epoch: 12, Loss: 1.1239, Train: 0.9714, Val: 0.7340, Test: 0.7570
Epoch: 13, Loss: 1.0588, Train: 0.9786, Val: 0.7300, Test: 0.7730
Epoch: 14, Loss: 0.9730, Train: 0.9929, Val: 0.7440, Test: 0.7730
Epoch: 15, Loss: 0.9288, Train: 0.9929, Val: 0.7580, Test: 0.7770
Epoch: 16, Loss: 0.8321, Train: 0.9929, Val: 0.7560, Test: 0.7870
Epoch: 17, Loss: 0.7997, Train: 0.9929, Val: 0.7620, Test: 0.7910
Epoch: 18, Loss: 0.7300, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 19, Loss: 0.6294, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 20, Loss: 0.6023, Train: 1.0000, Val: 0.7620, Test: 0.7930
Epoch: 21, Loss: 0.5514, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 22, Loss: 0.4691, Train: 1.0000, Val: 0.7600, Test: 0.7890
Epoch: 23, Loss: 0.4002, Train: 1.0000, Val: 0.7620, Test: 0.7930
Epoch: 24, Loss: 0.3787, Train: 1.0000, Val: 0.7580, Test: 0.7910
Epoch: 25, Loss: 0.3628, Train: 1.0000, Val: 0.7580, Test: 0.7920
Epoch: 26, Loss: 0.3126, Train: 1.0000, Val: 0.7600, Test: 0.7900
Epoch: 27, Loss: 0.2858, Train: 1.0000, Val: 0.7580, Test: 0.7920
Epoch: 28, Loss: 0.2408, Train: 1.0000, Val: 0.7580, Test: 0.7890
Epoch: 29, Loss: 0.2003, Train: 1.0000, Val: 0.7540, Test: 0.7870
Epoch: 30, Loss: 0.1718, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 31, Loss: 0.1690, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 32, Loss: 0.1310, Train: 1.0000, Val: 0.7560, Test: 0.7850
Epoch: 33, Loss: 0.1261, Train: 1.0000, Val: 0.7620, Test: 0.7870
Epoch: 34, Loss: 0.1234, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 35, Loss: 0.1010, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 36, Loss: 0.1056, Train: 1.0000, Val: 0.7580, Test: 0.7860
Epoch: 37, Loss: 0.1039, Train: 1.0000, Val: 0.7600, Test: 0.7860
Epoch: 38, Loss: 0.0679, Train: 1.0000, Val: 0.7580, Test: 0.7870
Epoch: 39, Loss: 0.0691, Train: 1.0000, Val: 0.7560, Test: 0.7840
Epoch: 40, Loss: 0.0725, Train: 1.0000, Val: 0.7560, Test: 0.7860
Epoch: 41, Loss: 0.0444, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 42, Loss: 0.0439, Train: 1.0000, Val: 0.7500, Test: 0.7820
Epoch: 43, Loss: 0.0450, Train: 1.0000, Val: 0.7500, Test: 0.7830
Epoch: 44, Loss: 0.0341, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 45, Loss: 0.0351, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 46, Loss: 0.0388, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 47, Loss: 0.0305, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 48, Loss: 0.0308, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 49, Loss: 0.0282, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 50, Loss: 0.0226, Train: 1.0000, Val: 0.7540, Test: 0.7830
MAD:  0.9195
Best Test Accuracy: 0.7930, Val Accuracy: 0.7660, Train Accuracy: 1.0000
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0124, Train: 0.1929, Val: 0.0840, Test: 0.1000
Epoch: 2, Loss: 1.9090, Train: 0.4357, Val: 0.1460, Test: 0.1730
Epoch: 3, Loss: 1.8146, Train: 0.7000, Val: 0.3520, Test: 0.3430
Epoch: 4, Loss: 1.7122, Train: 0.8214, Val: 0.5080, Test: 0.4870
Epoch: 5, Loss: 1.6137, Train: 0.9000, Val: 0.6400, Test: 0.5870
Epoch: 6, Loss: 1.5772, Train: 0.9214, Val: 0.6700, Test: 0.6190
Epoch: 7, Loss: 1.4703, Train: 0.9429, Val: 0.6760, Test: 0.6370
Epoch: 8, Loss: 1.3863, Train: 0.9643, Val: 0.6860, Test: 0.6620
Epoch: 9, Loss: 1.2633, Train: 0.9714, Val: 0.7000, Test: 0.6690
Epoch: 10, Loss: 1.2450, Train: 0.9857, Val: 0.7100, Test: 0.6830
Epoch: 11, Loss: 1.1106, Train: 0.9857, Val: 0.7260, Test: 0.6980
Epoch: 12, Loss: 1.0879, Train: 0.9929, Val: 0.7320, Test: 0.7200
Epoch: 13, Loss: 0.9754, Train: 0.9929, Val: 0.7480, Test: 0.7330
Epoch: 14, Loss: 0.8936, Train: 0.9929, Val: 0.7520, Test: 0.7450
Epoch: 15, Loss: 0.8424, Train: 0.9929, Val: 0.7520, Test: 0.7510
Epoch: 16, Loss: 0.7807, Train: 0.9929, Val: 0.7500, Test: 0.7500
Epoch: 17, Loss: 0.7221, Train: 0.9929, Val: 0.7520, Test: 0.7530
Epoch: 18, Loss: 0.6657, Train: 0.9929, Val: 0.7540, Test: 0.7640
Epoch: 19, Loss: 0.5798, Train: 0.9929, Val: 0.7560, Test: 0.7640
Epoch: 20, Loss: 0.5454, Train: 1.0000, Val: 0.7540, Test: 0.7630
Epoch: 21, Loss: 0.5210, Train: 1.0000, Val: 0.7540, Test: 0.7680
Epoch: 22, Loss: 0.4010, Train: 1.0000, Val: 0.7540, Test: 0.7690
Epoch: 23, Loss: 0.3417, Train: 1.0000, Val: 0.7640, Test: 0.7710
Epoch: 24, Loss: 0.3236, Train: 1.0000, Val: 0.7660, Test: 0.7730
Epoch: 25, Loss: 0.2931, Train: 1.0000, Val: 0.7660, Test: 0.7740
Epoch: 26, Loss: 0.2793, Train: 1.0000, Val: 0.7640, Test: 0.7730
Epoch: 27, Loss: 0.2446, Train: 1.0000, Val: 0.7620, Test: 0.7710
Epoch: 28, Loss: 0.1943, Train: 1.0000, Val: 0.7560, Test: 0.7660
Epoch: 29, Loss: 0.1728, Train: 1.0000, Val: 0.7560, Test: 0.7680
Epoch: 30, Loss: 0.1683, Train: 1.0000, Val: 0.7520, Test: 0.7660
Epoch: 31, Loss: 0.1248, Train: 1.0000, Val: 0.7500, Test: 0.7660
Epoch: 32, Loss: 0.1229, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 33, Loss: 0.0821, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 34, Loss: 0.0870, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 35, Loss: 0.0764, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 36, Loss: 0.0689, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 37, Loss: 0.0552, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 38, Loss: 0.0541, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 39, Loss: 0.0658, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 40, Loss: 0.0449, Train: 1.0000, Val: 0.7520, Test: 0.7730
Epoch: 41, Loss: 0.0321, Train: 1.0000, Val: 0.7580, Test: 0.7720
Epoch: 42, Loss: 0.0394, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 43, Loss: 0.0471, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 44, Loss: 0.0367, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 45, Loss: 0.0292, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 46, Loss: 0.0239, Train: 1.0000, Val: 0.7540, Test: 0.7660
Epoch: 47, Loss: 0.0284, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 48, Loss: 0.0156, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 49, Loss: 0.0264, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 50, Loss: 0.0158, Train: 1.0000, Val: 0.7500, Test: 0.7600
MAD:  0.9204
Best Test Accuracy: 0.7740, Val Accuracy: 0.7660, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0095, Train: 0.3286, Val: 0.2520, Test: 0.2250
Epoch: 2, Loss: 1.9302, Train: 0.6429, Val: 0.3900, Test: 0.4230
Epoch: 3, Loss: 1.8732, Train: 0.7857, Val: 0.4660, Test: 0.4720
Epoch: 4, Loss: 1.7798, Train: 0.8286, Val: 0.4740, Test: 0.4950
Epoch: 5, Loss: 1.7126, Train: 0.9214, Val: 0.5080, Test: 0.5270
Epoch: 6, Loss: 1.6623, Train: 0.9500, Val: 0.5880, Test: 0.6140
Epoch: 7, Loss: 1.5696, Train: 0.9571, Val: 0.6500, Test: 0.6860
Epoch: 8, Loss: 1.5044, Train: 0.9643, Val: 0.6720, Test: 0.7160
Epoch: 9, Loss: 1.4548, Train: 0.9643, Val: 0.6880, Test: 0.7240
Epoch: 10, Loss: 1.3600, Train: 0.9714, Val: 0.6900, Test: 0.7320
Epoch: 11, Loss: 1.3148, Train: 0.9786, Val: 0.6920, Test: 0.7330
Epoch: 12, Loss: 1.2143, Train: 0.9714, Val: 0.6980, Test: 0.7270
Epoch: 13, Loss: 1.1259, Train: 0.9857, Val: 0.7000, Test: 0.7240
Epoch: 14, Loss: 1.0165, Train: 0.9857, Val: 0.7100, Test: 0.7340
Epoch: 15, Loss: 0.9635, Train: 0.9929, Val: 0.7280, Test: 0.7420
Epoch: 16, Loss: 0.8993, Train: 0.9929, Val: 0.7360, Test: 0.7510
Epoch: 17, Loss: 0.8305, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 18, Loss: 0.7348, Train: 1.0000, Val: 0.7540, Test: 0.7670
Epoch: 19, Loss: 0.6538, Train: 1.0000, Val: 0.7580, Test: 0.7730
Epoch: 20, Loss: 0.5990, Train: 1.0000, Val: 0.7600, Test: 0.7810
Epoch: 21, Loss: 0.5205, Train: 1.0000, Val: 0.7600, Test: 0.7820
Epoch: 22, Loss: 0.5231, Train: 1.0000, Val: 0.7580, Test: 0.7840
Epoch: 23, Loss: 0.3987, Train: 1.0000, Val: 0.7580, Test: 0.7840
Epoch: 24, Loss: 0.3520, Train: 1.0000, Val: 0.7580, Test: 0.7830
Epoch: 25, Loss: 0.3496, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 26, Loss: 0.2599, Train: 1.0000, Val: 0.7540, Test: 0.7800
Epoch: 27, Loss: 0.2581, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 28, Loss: 0.2313, Train: 1.0000, Val: 0.7560, Test: 0.7720
Epoch: 29, Loss: 0.1859, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 30, Loss: 0.1631, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 31, Loss: 0.1709, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 32, Loss: 0.1499, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 33, Loss: 0.1046, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 34, Loss: 0.0922, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 35, Loss: 0.0946, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 36, Loss: 0.0808, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 37, Loss: 0.0676, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 38, Loss: 0.0611, Train: 1.0000, Val: 0.7520, Test: 0.7790
Epoch: 39, Loss: 0.0595, Train: 1.0000, Val: 0.7520, Test: 0.7820
Epoch: 40, Loss: 0.0424, Train: 1.0000, Val: 0.7520, Test: 0.7820
Epoch: 41, Loss: 0.0442, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 42, Loss: 0.0368, Train: 1.0000, Val: 0.7560, Test: 0.7820
Epoch: 43, Loss: 0.0419, Train: 1.0000, Val: 0.7560, Test: 0.7820
Epoch: 44, Loss: 0.0472, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 45, Loss: 0.0233, Train: 1.0000, Val: 0.7500, Test: 0.7830
Epoch: 46, Loss: 0.0295, Train: 1.0000, Val: 0.7520, Test: 0.7820
Epoch: 47, Loss: 0.0239, Train: 1.0000, Val: 0.7500, Test: 0.7800
Epoch: 48, Loss: 0.0239, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 49, Loss: 0.0235, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 50, Loss: 0.0198, Train: 1.0000, Val: 0.7520, Test: 0.7720
MAD:  0.911
Best Test Accuracy: 0.7840, Val Accuracy: 0.7580, Train Accuracy: 1.0000
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9675, Train: 0.2429, Val: 0.2060, Test: 0.1830
Epoch: 2, Loss: 1.8850, Train: 0.6929, Val: 0.4440, Test: 0.4720
Epoch: 3, Loss: 1.8096, Train: 0.8071, Val: 0.5340, Test: 0.5780
Epoch: 4, Loss: 1.7206, Train: 0.9143, Val: 0.5940, Test: 0.6450
Epoch: 5, Loss: 1.6467, Train: 0.9429, Val: 0.6400, Test: 0.6840
Epoch: 6, Loss: 1.5744, Train: 0.9714, Val: 0.6780, Test: 0.7170
Epoch: 7, Loss: 1.4388, Train: 0.9786, Val: 0.7000, Test: 0.7310
Epoch: 8, Loss: 1.3755, Train: 0.9786, Val: 0.7060, Test: 0.7440
Epoch: 9, Loss: 1.2989, Train: 0.9786, Val: 0.7220, Test: 0.7550
Epoch: 10, Loss: 1.2285, Train: 0.9786, Val: 0.7260, Test: 0.7590
Epoch: 11, Loss: 1.1162, Train: 0.9857, Val: 0.7320, Test: 0.7640
Epoch: 12, Loss: 1.0728, Train: 0.9857, Val: 0.7360, Test: 0.7710
Epoch: 13, Loss: 1.0179, Train: 0.9857, Val: 0.7440, Test: 0.7740
Epoch: 14, Loss: 0.8787, Train: 0.9929, Val: 0.7460, Test: 0.7700
Epoch: 15, Loss: 0.7953, Train: 0.9929, Val: 0.7460, Test: 0.7780
Epoch: 16, Loss: 0.7765, Train: 0.9929, Val: 0.7480, Test: 0.7850
Epoch: 17, Loss: 0.6695, Train: 0.9929, Val: 0.7460, Test: 0.7830
Epoch: 18, Loss: 0.6496, Train: 0.9929, Val: 0.7480, Test: 0.7860
Epoch: 19, Loss: 0.5772, Train: 0.9929, Val: 0.7560, Test: 0.7860
Epoch: 20, Loss: 0.5185, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 21, Loss: 0.4262, Train: 1.0000, Val: 0.7680, Test: 0.7940
Epoch: 22, Loss: 0.4139, Train: 1.0000, Val: 0.7680, Test: 0.8030
Epoch: 23, Loss: 0.3780, Train: 1.0000, Val: 0.7660, Test: 0.7940
Epoch: 24, Loss: 0.3449, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 25, Loss: 0.2924, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 26, Loss: 0.2293, Train: 1.0000, Val: 0.7640, Test: 0.7890
Epoch: 27, Loss: 0.2107, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 28, Loss: 0.1798, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 29, Loss: 0.1864, Train: 1.0000, Val: 0.7660, Test: 0.7910
Epoch: 30, Loss: 0.1625, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 31, Loss: 0.1318, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 32, Loss: 0.1144, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 33, Loss: 0.1010, Train: 1.0000, Val: 0.7660, Test: 0.7830
Epoch: 34, Loss: 0.1292, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 35, Loss: 0.0904, Train: 1.0000, Val: 0.7700, Test: 0.7830
Epoch: 36, Loss: 0.0618, Train: 1.0000, Val: 0.7700, Test: 0.7820
Epoch: 37, Loss: 0.0658, Train: 1.0000, Val: 0.7700, Test: 0.7810
Epoch: 38, Loss: 0.0751, Train: 1.0000, Val: 0.7720, Test: 0.7820
Epoch: 39, Loss: 0.0551, Train: 1.0000, Val: 0.7740, Test: 0.7820
Epoch: 40, Loss: 0.0420, Train: 1.0000, Val: 0.7700, Test: 0.7810
Epoch: 41, Loss: 0.0490, Train: 1.0000, Val: 0.7680, Test: 0.7810
Epoch: 42, Loss: 0.0423, Train: 1.0000, Val: 0.7680, Test: 0.7820
Epoch: 43, Loss: 0.0392, Train: 1.0000, Val: 0.7640, Test: 0.7830
Epoch: 44, Loss: 0.0328, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 45, Loss: 0.0363, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 46, Loss: 0.0292, Train: 1.0000, Val: 0.7700, Test: 0.7850
Epoch: 47, Loss: 0.0311, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 48, Loss: 0.0247, Train: 1.0000, Val: 0.7660, Test: 0.7880
Epoch: 49, Loss: 0.0236, Train: 1.0000, Val: 0.7640, Test: 0.7890
Epoch: 50, Loss: 0.0236, Train: 1.0000, Val: 0.7660, Test: 0.7890
MAD:  0.961
Best Test Accuracy: 0.8030, Val Accuracy: 0.7680, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9817, Train: 0.3857, Val: 0.3640, Test: 0.3600
Epoch: 2, Loss: 1.9154, Train: 0.6786, Val: 0.5220, Test: 0.5380
Epoch: 3, Loss: 1.8362, Train: 0.8500, Val: 0.6240, Test: 0.6160
Epoch: 4, Loss: 1.7478, Train: 0.9071, Val: 0.6740, Test: 0.6640
Epoch: 5, Loss: 1.6581, Train: 0.9357, Val: 0.7200, Test: 0.7090
Epoch: 6, Loss: 1.5728, Train: 0.9714, Val: 0.7260, Test: 0.7340
Epoch: 7, Loss: 1.4961, Train: 0.9857, Val: 0.7480, Test: 0.7560
Epoch: 8, Loss: 1.3807, Train: 0.9857, Val: 0.7600, Test: 0.7660
Epoch: 9, Loss: 1.3328, Train: 0.9857, Val: 0.7500, Test: 0.7670
Epoch: 10, Loss: 1.2465, Train: 0.9857, Val: 0.7520, Test: 0.7720
Epoch: 11, Loss: 1.0841, Train: 0.9929, Val: 0.7540, Test: 0.7840
Epoch: 12, Loss: 1.0938, Train: 0.9929, Val: 0.7640, Test: 0.7910
Epoch: 13, Loss: 1.0557, Train: 0.9929, Val: 0.7740, Test: 0.7940
Epoch: 14, Loss: 0.9200, Train: 0.9929, Val: 0.7680, Test: 0.7980
Epoch: 15, Loss: 0.8303, Train: 0.9929, Val: 0.7720, Test: 0.7960
Epoch: 16, Loss: 0.7883, Train: 1.0000, Val: 0.7720, Test: 0.7970
Epoch: 17, Loss: 0.7077, Train: 1.0000, Val: 0.7740, Test: 0.7880
Epoch: 18, Loss: 0.6258, Train: 1.0000, Val: 0.7740, Test: 0.7880
Epoch: 19, Loss: 0.5585, Train: 1.0000, Val: 0.7800, Test: 0.7870
Epoch: 20, Loss: 0.5105, Train: 1.0000, Val: 0.7800, Test: 0.7900
Epoch: 21, Loss: 0.4573, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 22, Loss: 0.4172, Train: 1.0000, Val: 0.7800, Test: 0.7870
Epoch: 23, Loss: 0.3373, Train: 1.0000, Val: 0.7760, Test: 0.7880
Epoch: 24, Loss: 0.2920, Train: 1.0000, Val: 0.7740, Test: 0.7890
Epoch: 25, Loss: 0.2702, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 26, Loss: 0.2489, Train: 1.0000, Val: 0.7760, Test: 0.7960
Epoch: 27, Loss: 0.2187, Train: 1.0000, Val: 0.7740, Test: 0.7960
Epoch: 28, Loss: 0.2005, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 29, Loss: 0.1668, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 30, Loss: 0.1401, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 31, Loss: 0.1351, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 32, Loss: 0.1159, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 33, Loss: 0.0913, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 34, Loss: 0.1043, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 35, Loss: 0.0785, Train: 1.0000, Val: 0.7720, Test: 0.7910
Epoch: 36, Loss: 0.0687, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 37, Loss: 0.0594, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 38, Loss: 0.0619, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 39, Loss: 0.0431, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 40, Loss: 0.0394, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 41, Loss: 0.0532, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 42, Loss: 0.0433, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 43, Loss: 0.0386, Train: 1.0000, Val: 0.7680, Test: 0.7860
Epoch: 44, Loss: 0.0318, Train: 1.0000, Val: 0.7720, Test: 0.7860
Epoch: 45, Loss: 0.0293, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 46, Loss: 0.0354, Train: 1.0000, Val: 0.7680, Test: 0.7840
Epoch: 47, Loss: 0.0294, Train: 1.0000, Val: 0.7680, Test: 0.7860
Epoch: 48, Loss: 0.0283, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 49, Loss: 0.0254, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 50, Loss: 0.0197, Train: 1.0000, Val: 0.7660, Test: 0.7870
MAD:  0.8938
Best Test Accuracy: 0.7980, Val Accuracy: 0.7680, Train Accuracy: 0.9929
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9777, Train: 0.4571, Val: 0.2240, Test: 0.2360
Epoch: 2, Loss: 1.8954, Train: 0.6571, Val: 0.3740, Test: 0.3800
Epoch: 3, Loss: 1.8188, Train: 0.7857, Val: 0.4740, Test: 0.4690
Epoch: 4, Loss: 1.7518, Train: 0.8286, Val: 0.5420, Test: 0.5290
Epoch: 5, Loss: 1.7160, Train: 0.8786, Val: 0.6040, Test: 0.5750
Epoch: 6, Loss: 1.5925, Train: 0.9357, Val: 0.6620, Test: 0.6420
Epoch: 7, Loss: 1.5059, Train: 0.9643, Val: 0.6800, Test: 0.6730
Epoch: 8, Loss: 1.4204, Train: 0.9786, Val: 0.6880, Test: 0.6940
Epoch: 9, Loss: 1.3715, Train: 0.9857, Val: 0.7000, Test: 0.7170
Epoch: 10, Loss: 1.3056, Train: 0.9929, Val: 0.7160, Test: 0.7410
Epoch: 11, Loss: 1.2090, Train: 0.9929, Val: 0.7260, Test: 0.7580
Epoch: 12, Loss: 1.1504, Train: 0.9929, Val: 0.7360, Test: 0.7660
Epoch: 13, Loss: 1.0787, Train: 0.9929, Val: 0.7320, Test: 0.7730
Epoch: 14, Loss: 0.9547, Train: 0.9929, Val: 0.7340, Test: 0.7760
Epoch: 15, Loss: 0.9378, Train: 0.9929, Val: 0.7380, Test: 0.7770
Epoch: 16, Loss: 0.8123, Train: 0.9929, Val: 0.7440, Test: 0.7810
Epoch: 17, Loss: 0.7926, Train: 0.9929, Val: 0.7540, Test: 0.7810
Epoch: 18, Loss: 0.6835, Train: 0.9929, Val: 0.7580, Test: 0.7830
Epoch: 19, Loss: 0.6293, Train: 0.9929, Val: 0.7580, Test: 0.7840
Epoch: 20, Loss: 0.5772, Train: 0.9929, Val: 0.7640, Test: 0.7880
Epoch: 21, Loss: 0.5378, Train: 1.0000, Val: 0.7620, Test: 0.7910
Epoch: 22, Loss: 0.4637, Train: 1.0000, Val: 0.7660, Test: 0.7980
Epoch: 23, Loss: 0.4206, Train: 1.0000, Val: 0.7640, Test: 0.7980
Epoch: 24, Loss: 0.3811, Train: 1.0000, Val: 0.7660, Test: 0.8020
Epoch: 25, Loss: 0.3234, Train: 1.0000, Val: 0.7620, Test: 0.8030
Epoch: 26, Loss: 0.2758, Train: 1.0000, Val: 0.7640, Test: 0.8040
Epoch: 27, Loss: 0.2463, Train: 1.0000, Val: 0.7580, Test: 0.8010
Epoch: 28, Loss: 0.2236, Train: 1.0000, Val: 0.7580, Test: 0.8010
Epoch: 29, Loss: 0.1977, Train: 1.0000, Val: 0.7560, Test: 0.7980
Epoch: 30, Loss: 0.1782, Train: 1.0000, Val: 0.7580, Test: 0.7980
Epoch: 31, Loss: 0.1891, Train: 1.0000, Val: 0.7580, Test: 0.7980
Epoch: 32, Loss: 0.1456, Train: 1.0000, Val: 0.7600, Test: 0.8000
Epoch: 33, Loss: 0.1171, Train: 1.0000, Val: 0.7600, Test: 0.7980
Epoch: 34, Loss: 0.0964, Train: 1.0000, Val: 0.7620, Test: 0.7980
Epoch: 35, Loss: 0.0972, Train: 1.0000, Val: 0.7620, Test: 0.7950
Epoch: 36, Loss: 0.0940, Train: 1.0000, Val: 0.7600, Test: 0.7960
Epoch: 37, Loss: 0.0678, Train: 1.0000, Val: 0.7600, Test: 0.7980
Epoch: 38, Loss: 0.0716, Train: 1.0000, Val: 0.7620, Test: 0.8000
Epoch: 39, Loss: 0.0587, Train: 1.0000, Val: 0.7640, Test: 0.7990
Epoch: 40, Loss: 0.0563, Train: 1.0000, Val: 0.7660, Test: 0.7980
Epoch: 41, Loss: 0.0454, Train: 1.0000, Val: 0.7660, Test: 0.7990
Epoch: 42, Loss: 0.0460, Train: 1.0000, Val: 0.7680, Test: 0.7990
Epoch: 43, Loss: 0.0398, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 44, Loss: 0.0405, Train: 1.0000, Val: 0.7700, Test: 0.7970
Epoch: 45, Loss: 0.0349, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 46, Loss: 0.0281, Train: 1.0000, Val: 0.7700, Test: 0.7960
Epoch: 47, Loss: 0.0308, Train: 1.0000, Val: 0.7700, Test: 0.7950
Epoch: 48, Loss: 0.0228, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 49, Loss: 0.0201, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 50, Loss: 0.0250, Train: 1.0000, Val: 0.7680, Test: 0.7960
MAD:  0.9234
Best Test Accuracy: 0.8040, Val Accuracy: 0.7640, Train Accuracy: 1.0000
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9792, Train: 0.3500, Val: 0.2560, Test: 0.2220
Epoch: 2, Loss: 1.9071, Train: 0.5429, Val: 0.3740, Test: 0.3550
Epoch: 3, Loss: 1.8212, Train: 0.6357, Val: 0.4500, Test: 0.4160
Epoch: 4, Loss: 1.7722, Train: 0.7929, Val: 0.5300, Test: 0.5000
Epoch: 5, Loss: 1.7001, Train: 0.9143, Val: 0.6260, Test: 0.6170
Epoch: 6, Loss: 1.6064, Train: 0.9714, Val: 0.6940, Test: 0.6880
Epoch: 7, Loss: 1.4873, Train: 0.9857, Val: 0.7460, Test: 0.7320
Epoch: 8, Loss: 1.4150, Train: 0.9857, Val: 0.7540, Test: 0.7450
Epoch: 9, Loss: 1.3337, Train: 0.9786, Val: 0.7440, Test: 0.7570
Epoch: 10, Loss: 1.2822, Train: 0.9786, Val: 0.7540, Test: 0.7560
Epoch: 11, Loss: 1.1536, Train: 0.9786, Val: 0.7600, Test: 0.7590
Epoch: 12, Loss: 1.1087, Train: 0.9786, Val: 0.7600, Test: 0.7620
Epoch: 13, Loss: 1.0306, Train: 0.9786, Val: 0.7620, Test: 0.7670
Epoch: 14, Loss: 0.9346, Train: 0.9857, Val: 0.7740, Test: 0.7780
Epoch: 15, Loss: 0.8478, Train: 0.9929, Val: 0.7760, Test: 0.7860
Epoch: 16, Loss: 0.7798, Train: 0.9929, Val: 0.7680, Test: 0.7870
Epoch: 17, Loss: 0.7051, Train: 0.9929, Val: 0.7760, Test: 0.7900
Epoch: 18, Loss: 0.6380, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 19, Loss: 0.5338, Train: 1.0000, Val: 0.7800, Test: 0.7950
Epoch: 20, Loss: 0.4531, Train: 1.0000, Val: 0.7780, Test: 0.7980
Epoch: 21, Loss: 0.3925, Train: 1.0000, Val: 0.7700, Test: 0.7980
Epoch: 22, Loss: 0.4046, Train: 1.0000, Val: 0.7720, Test: 0.7970
Epoch: 23, Loss: 0.3251, Train: 1.0000, Val: 0.7740, Test: 0.7960
Epoch: 24, Loss: 0.2851, Train: 1.0000, Val: 0.7720, Test: 0.7980
Epoch: 25, Loss: 0.2749, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 26, Loss: 0.2553, Train: 1.0000, Val: 0.7680, Test: 0.7970
Epoch: 27, Loss: 0.1857, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 28, Loss: 0.1667, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 29, Loss: 0.1640, Train: 1.0000, Val: 0.7720, Test: 0.7970
Epoch: 30, Loss: 0.1506, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 31, Loss: 0.1336, Train: 1.0000, Val: 0.7720, Test: 0.8000
Epoch: 32, Loss: 0.1043, Train: 1.0000, Val: 0.7720, Test: 0.8010
Epoch: 33, Loss: 0.1009, Train: 1.0000, Val: 0.7660, Test: 0.8050
Epoch: 34, Loss: 0.0769, Train: 1.0000, Val: 0.7700, Test: 0.8060
Epoch: 35, Loss: 0.0669, Train: 1.0000, Val: 0.7720, Test: 0.8060
Epoch: 36, Loss: 0.0800, Train: 1.0000, Val: 0.7700, Test: 0.8040
Epoch: 37, Loss: 0.0679, Train: 1.0000, Val: 0.7720, Test: 0.8050
Epoch: 38, Loss: 0.0636, Train: 1.0000, Val: 0.7720, Test: 0.8060
Epoch: 39, Loss: 0.0403, Train: 1.0000, Val: 0.7740, Test: 0.8030
Epoch: 40, Loss: 0.0638, Train: 1.0000, Val: 0.7760, Test: 0.8040
Epoch: 41, Loss: 0.0477, Train: 1.0000, Val: 0.7740, Test: 0.8060
Epoch: 42, Loss: 0.0474, Train: 1.0000, Val: 0.7760, Test: 0.8090
Epoch: 43, Loss: 0.0361, Train: 1.0000, Val: 0.7780, Test: 0.8080
Epoch: 44, Loss: 0.0287, Train: 1.0000, Val: 0.7780, Test: 0.8070
Epoch: 45, Loss: 0.0258, Train: 1.0000, Val: 0.7800, Test: 0.8080
Epoch: 46, Loss: 0.0219, Train: 1.0000, Val: 0.7800, Test: 0.8080
Epoch: 47, Loss: 0.0284, Train: 1.0000, Val: 0.7800, Test: 0.8060
Epoch: 48, Loss: 0.0315, Train: 1.0000, Val: 0.7820, Test: 0.8040
Epoch: 49, Loss: 0.0189, Train: 1.0000, Val: 0.7820, Test: 0.8050
Epoch: 50, Loss: 0.0181, Train: 1.0000, Val: 0.7840, Test: 0.8050
MAD:  0.8321
Best Test Accuracy: 0.8090, Val Accuracy: 0.7760, Train Accuracy: 1.0000
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9581, Train: 0.4286, Val: 0.4080, Test: 0.3520
Epoch: 2, Loss: 1.8763, Train: 0.6571, Val: 0.5040, Test: 0.4690
Epoch: 3, Loss: 1.8019, Train: 0.8500, Val: 0.5920, Test: 0.5660
Epoch: 4, Loss: 1.7439, Train: 0.9143, Val: 0.6300, Test: 0.6340
Epoch: 5, Loss: 1.6241, Train: 0.9429, Val: 0.6420, Test: 0.6620
Epoch: 6, Loss: 1.5827, Train: 0.9429, Val: 0.6560, Test: 0.6720
Epoch: 7, Loss: 1.4898, Train: 0.9714, Val: 0.6720, Test: 0.7040
Epoch: 8, Loss: 1.3905, Train: 0.9714, Val: 0.6880, Test: 0.7160
Epoch: 9, Loss: 1.3528, Train: 0.9714, Val: 0.7000, Test: 0.7130
Epoch: 10, Loss: 1.2481, Train: 0.9786, Val: 0.7060, Test: 0.7180
Epoch: 11, Loss: 1.1823, Train: 0.9857, Val: 0.7240, Test: 0.7300
Epoch: 12, Loss: 1.0918, Train: 0.9929, Val: 0.7280, Test: 0.7470
Epoch: 13, Loss: 1.0303, Train: 0.9929, Val: 0.7300, Test: 0.7590
Epoch: 14, Loss: 1.0117, Train: 0.9929, Val: 0.7440, Test: 0.7650
Epoch: 15, Loss: 0.8929, Train: 0.9929, Val: 0.7440, Test: 0.7690
Epoch: 16, Loss: 0.7838, Train: 0.9929, Val: 0.7420, Test: 0.7720
Epoch: 17, Loss: 0.7345, Train: 0.9929, Val: 0.7440, Test: 0.7730
Epoch: 18, Loss: 0.6562, Train: 0.9929, Val: 0.7440, Test: 0.7740
Epoch: 19, Loss: 0.6246, Train: 0.9929, Val: 0.7480, Test: 0.7740
Epoch: 20, Loss: 0.5472, Train: 0.9929, Val: 0.7560, Test: 0.7750
Epoch: 21, Loss: 0.4878, Train: 1.0000, Val: 0.7580, Test: 0.7770
Epoch: 22, Loss: 0.4494, Train: 1.0000, Val: 0.7640, Test: 0.7770
Epoch: 23, Loss: 0.4341, Train: 1.0000, Val: 0.7660, Test: 0.7770
Epoch: 24, Loss: 0.3548, Train: 1.0000, Val: 0.7600, Test: 0.7780
Epoch: 25, Loss: 0.3445, Train: 1.0000, Val: 0.7600, Test: 0.7820
Epoch: 26, Loss: 0.2664, Train: 1.0000, Val: 0.7540, Test: 0.7800
Epoch: 27, Loss: 0.2674, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 28, Loss: 0.2405, Train: 1.0000, Val: 0.7580, Test: 0.7810
Epoch: 29, Loss: 0.1693, Train: 1.0000, Val: 0.7620, Test: 0.7850
Epoch: 30, Loss: 0.1604, Train: 1.0000, Val: 0.7560, Test: 0.7860
Epoch: 31, Loss: 0.1621, Train: 1.0000, Val: 0.7580, Test: 0.7900
Epoch: 32, Loss: 0.1169, Train: 1.0000, Val: 0.7640, Test: 0.7890
Epoch: 33, Loss: 0.1283, Train: 1.0000, Val: 0.7640, Test: 0.7910
Epoch: 34, Loss: 0.0993, Train: 1.0000, Val: 0.7620, Test: 0.7900
Epoch: 35, Loss: 0.0964, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 36, Loss: 0.0758, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 37, Loss: 0.0779, Train: 1.0000, Val: 0.7600, Test: 0.7900
Epoch: 38, Loss: 0.0656, Train: 1.0000, Val: 0.7580, Test: 0.7890
Epoch: 39, Loss: 0.0527, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 40, Loss: 0.0547, Train: 1.0000, Val: 0.7560, Test: 0.7900
Epoch: 41, Loss: 0.0568, Train: 1.0000, Val: 0.7580, Test: 0.7910
Epoch: 42, Loss: 0.0567, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 43, Loss: 0.0593, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 44, Loss: 0.0354, Train: 1.0000, Val: 0.7560, Test: 0.7870
Epoch: 45, Loss: 0.0400, Train: 1.0000, Val: 0.7560, Test: 0.7870
Epoch: 46, Loss: 0.0389, Train: 1.0000, Val: 0.7540, Test: 0.7870
Epoch: 47, Loss: 0.0277, Train: 1.0000, Val: 0.7480, Test: 0.7880
Epoch: 48, Loss: 0.0306, Train: 1.0000, Val: 0.7460, Test: 0.7890
Epoch: 49, Loss: 0.0302, Train: 1.0000, Val: 0.7480, Test: 0.7910
Epoch: 50, Loss: 0.0243, Train: 1.0000, Val: 0.7500, Test: 0.7910
MAD:  0.9549
Best Test Accuracy: 0.7910, Val Accuracy: 0.7640, Train Accuracy: 1.0000
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9857, Train: 0.2857, Val: 0.3780, Test: 0.3600
Epoch: 2, Loss: 1.8940, Train: 0.5500, Val: 0.4600, Test: 0.4850
Epoch: 3, Loss: 1.8209, Train: 0.8000, Val: 0.5480, Test: 0.5850
Epoch: 4, Loss: 1.7447, Train: 0.8857, Val: 0.6460, Test: 0.6440
Epoch: 5, Loss: 1.6970, Train: 0.9214, Val: 0.6740, Test: 0.6840
Epoch: 6, Loss: 1.6276, Train: 0.9357, Val: 0.7140, Test: 0.7080
Epoch: 7, Loss: 1.5496, Train: 0.9429, Val: 0.7100, Test: 0.7450
Epoch: 8, Loss: 1.4728, Train: 0.9500, Val: 0.7240, Test: 0.7580
Epoch: 9, Loss: 1.3616, Train: 0.9571, Val: 0.7340, Test: 0.7630
Epoch: 10, Loss: 1.2898, Train: 0.9929, Val: 0.7400, Test: 0.7630
Epoch: 11, Loss: 1.2083, Train: 0.9929, Val: 0.7540, Test: 0.7590
Epoch: 12, Loss: 1.1829, Train: 0.9929, Val: 0.7480, Test: 0.7620
Epoch: 13, Loss: 1.1118, Train: 0.9929, Val: 0.7420, Test: 0.7550
Epoch: 14, Loss: 1.0584, Train: 0.9929, Val: 0.7420, Test: 0.7550
Epoch: 15, Loss: 0.9766, Train: 0.9929, Val: 0.7440, Test: 0.7540
Epoch: 16, Loss: 0.8511, Train: 0.9929, Val: 0.7520, Test: 0.7570
Epoch: 17, Loss: 0.8160, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 18, Loss: 0.7017, Train: 1.0000, Val: 0.7520, Test: 0.7710
Epoch: 19, Loss: 0.6263, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 20, Loss: 0.5775, Train: 1.0000, Val: 0.7580, Test: 0.7830
Epoch: 21, Loss: 0.5107, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 22, Loss: 0.4975, Train: 1.0000, Val: 0.7660, Test: 0.7950
Epoch: 23, Loss: 0.4377, Train: 1.0000, Val: 0.7660, Test: 0.7960
Epoch: 24, Loss: 0.3918, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 25, Loss: 0.3355, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 26, Loss: 0.2585, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 27, Loss: 0.2258, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 28, Loss: 0.2286, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 29, Loss: 0.2167, Train: 1.0000, Val: 0.7720, Test: 0.7950
Epoch: 30, Loss: 0.1475, Train: 1.0000, Val: 0.7720, Test: 0.7950
Epoch: 31, Loss: 0.1664, Train: 1.0000, Val: 0.7780, Test: 0.7930
Epoch: 32, Loss: 0.1474, Train: 1.0000, Val: 0.7740, Test: 0.7890
Epoch: 33, Loss: 0.1121, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 34, Loss: 0.0962, Train: 1.0000, Val: 0.7740, Test: 0.7900
Epoch: 35, Loss: 0.0929, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 36, Loss: 0.0724, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 37, Loss: 0.0799, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 38, Loss: 0.0675, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 39, Loss: 0.0645, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 40, Loss: 0.0548, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 41, Loss: 0.0528, Train: 1.0000, Val: 0.7720, Test: 0.7910
Epoch: 42, Loss: 0.0409, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 43, Loss: 0.0291, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 44, Loss: 0.0353, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 45, Loss: 0.0353, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 46, Loss: 0.0376, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 47, Loss: 0.0301, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 48, Loss: 0.0258, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 49, Loss: 0.0169, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 50, Loss: 0.0189, Train: 1.0000, Val: 0.7720, Test: 0.7930
MAD:  0.9357
Best Test Accuracy: 0.7960, Val Accuracy: 0.7660, Train Accuracy: 1.0000
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=1)
      (conv2): GATv2Conv(128, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): ParallelGNNBlock(
      (conv1): GATv2Conv(256, 128, heads=1)
      (conv2): GATv2Conv(256, 128, heads=1)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=256, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9773, Train: 0.3429, Val: 0.3060, Test: 0.2690
Epoch: 2, Loss: 1.8905, Train: 0.7143, Val: 0.4140, Test: 0.3930
Epoch: 3, Loss: 1.8399, Train: 0.8571, Val: 0.5180, Test: 0.5070
Epoch: 4, Loss: 1.7466, Train: 0.9071, Val: 0.5860, Test: 0.5690
Epoch: 5, Loss: 1.6717, Train: 0.9643, Val: 0.6580, Test: 0.6280
Epoch: 6, Loss: 1.6110, Train: 0.9643, Val: 0.6800, Test: 0.6610
Epoch: 7, Loss: 1.5481, Train: 0.9643, Val: 0.6940, Test: 0.6820
Epoch: 8, Loss: 1.4414, Train: 0.9714, Val: 0.7060, Test: 0.7040
Epoch: 9, Loss: 1.3583, Train: 0.9857, Val: 0.7240, Test: 0.7230
Epoch: 10, Loss: 1.3016, Train: 0.9929, Val: 0.7400, Test: 0.7440
Epoch: 11, Loss: 1.2134, Train: 0.9929, Val: 0.7560, Test: 0.7610
Epoch: 12, Loss: 1.1550, Train: 0.9929, Val: 0.7700, Test: 0.7690
Epoch: 13, Loss: 1.0921, Train: 0.9929, Val: 0.7700, Test: 0.7800
Epoch: 14, Loss: 1.0062, Train: 0.9929, Val: 0.7700, Test: 0.7880
Epoch: 15, Loss: 0.9157, Train: 0.9929, Val: 0.7700, Test: 0.7880
Epoch: 16, Loss: 0.8378, Train: 0.9929, Val: 0.7660, Test: 0.7850
Epoch: 17, Loss: 0.8037, Train: 0.9929, Val: 0.7580, Test: 0.7830
Epoch: 18, Loss: 0.6343, Train: 0.9929, Val: 0.7600, Test: 0.7820
Epoch: 19, Loss: 0.6203, Train: 0.9929, Val: 0.7680, Test: 0.7850
Epoch: 20, Loss: 0.5837, Train: 1.0000, Val: 0.7720, Test: 0.7880
Epoch: 21, Loss: 0.5101, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 22, Loss: 0.4550, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 23, Loss: 0.4031, Train: 1.0000, Val: 0.7720, Test: 0.7850
Epoch: 24, Loss: 0.3473, Train: 1.0000, Val: 0.7780, Test: 0.7840
Epoch: 25, Loss: 0.3072, Train: 1.0000, Val: 0.7720, Test: 0.7830
Epoch: 26, Loss: 0.2581, Train: 1.0000, Val: 0.7700, Test: 0.7810
Epoch: 27, Loss: 0.2477, Train: 1.0000, Val: 0.7720, Test: 0.7830
Epoch: 28, Loss: 0.2086, Train: 1.0000, Val: 0.7740, Test: 0.7860
Epoch: 29, Loss: 0.1994, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 30, Loss: 0.1676, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 31, Loss: 0.1550, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 32, Loss: 0.1425, Train: 1.0000, Val: 0.7620, Test: 0.7880
Epoch: 33, Loss: 0.0925, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 34, Loss: 0.0906, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 35, Loss: 0.0987, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 36, Loss: 0.0862, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 37, Loss: 0.0764, Train: 1.0000, Val: 0.7760, Test: 0.7880
Epoch: 38, Loss: 0.0654, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 39, Loss: 0.0456, Train: 1.0000, Val: 0.7780, Test: 0.7860
Epoch: 40, Loss: 0.0403, Train: 1.0000, Val: 0.7780, Test: 0.7830
Epoch: 41, Loss: 0.0415, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 42, Loss: 0.0373, Train: 1.0000, Val: 0.7760, Test: 0.7890
Epoch: 43, Loss: 0.0484, Train: 1.0000, Val: 0.7740, Test: 0.7890
Epoch: 44, Loss: 0.0325, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 45, Loss: 0.0329, Train: 1.0000, Val: 0.7660, Test: 0.7870
Epoch: 46, Loss: 0.0345, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 47, Loss: 0.0267, Train: 1.0000, Val: 0.7680, Test: 0.7840
Epoch: 48, Loss: 0.0249, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 49, Loss: 0.0240, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 50, Loss: 0.0290, Train: 1.0000, Val: 0.7640, Test: 0.7840
MAD:  0.8445
Best Test Accuracy: 0.7900, Val Accuracy: 0.7640, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.7942 ± 0.009754998718605766
Average MAD:  0.9096300000000002 ± 0.040343773993021526

Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0202, Train: 0.1929, Val: 0.1740, Test: 0.1570
Epoch: 2, Loss: 1.9530, Train: 0.2286, Val: 0.2380, Test: 0.2180
Epoch: 3, Loss: 1.9427, Train: 0.1429, Val: 0.1640, Test: 0.1500
Epoch: 4, Loss: 1.9332, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 5, Loss: 1.9339, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 6, Loss: 1.9568, Train: 0.1500, Val: 0.0720, Test: 0.0920
Epoch: 7, Loss: 1.9302, Train: 0.1571, Val: 0.3160, Test: 0.3260
Epoch: 8, Loss: 1.9457, Train: 0.1929, Val: 0.3220, Test: 0.3290
Epoch: 9, Loss: 1.9489, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 10, Loss: 1.9442, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 11, Loss: 1.9510, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 12, Loss: 1.9646, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 13, Loss: 1.9636, Train: 0.2214, Val: 0.0920, Test: 0.1000
Epoch: 14, Loss: 1.9434, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 15, Loss: 1.9677, Train: 0.2571, Val: 0.3280, Test: 0.3580
Epoch: 16, Loss: 1.9319, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 17, Loss: 1.9312, Train: 0.1500, Val: 0.0720, Test: 0.0940
Epoch: 18, Loss: 1.9229, Train: 0.2500, Val: 0.3100, Test: 0.3450
Epoch: 19, Loss: 1.9355, Train: 0.3143, Val: 0.2340, Test: 0.2320
Epoch: 20, Loss: 1.9185, Train: 0.1857, Val: 0.1220, Test: 0.1350
Epoch: 21, Loss: 1.9343, Train: 0.2286, Val: 0.1440, Test: 0.1470
Epoch: 22, Loss: 1.9257, Train: 0.2500, Val: 0.1440, Test: 0.1530
Epoch: 23, Loss: 1.9032, Train: 0.5429, Val: 0.3500, Test: 0.3370
Epoch: 24, Loss: 1.8913, Train: 0.4714, Val: 0.4900, Test: 0.5140
Epoch: 25, Loss: 1.8604, Train: 0.3929, Val: 0.4200, Test: 0.4130
Epoch: 26, Loss: 1.8363, Train: 0.3286, Val: 0.3380, Test: 0.3360
Epoch: 27, Loss: 1.7201, Train: 0.4000, Val: 0.2780, Test: 0.2910
Epoch: 28, Loss: 1.6758, Train: 0.5143, Val: 0.3080, Test: 0.3190
Epoch: 29, Loss: 1.5630, Train: 0.5000, Val: 0.3080, Test: 0.3280
Epoch: 30, Loss: 1.4920, Train: 0.5357, Val: 0.3440, Test: 0.3630
Epoch: 31, Loss: 1.4133, Train: 0.5857, Val: 0.4340, Test: 0.4820
Epoch: 32, Loss: 1.3498, Train: 0.6429, Val: 0.4840, Test: 0.5270
Epoch: 33, Loss: 1.2913, Train: 0.8071, Val: 0.6340, Test: 0.6680
Epoch: 34, Loss: 1.0707, Train: 0.6643, Val: 0.5320, Test: 0.5750
Epoch: 35, Loss: 0.9893, Train: 0.9357, Val: 0.7240, Test: 0.7500
Epoch: 36, Loss: 0.8259, Train: 0.9286, Val: 0.6840, Test: 0.7310
Epoch: 37, Loss: 0.7285, Train: 0.9643, Val: 0.7060, Test: 0.7270
Epoch: 38, Loss: 0.5494, Train: 0.9071, Val: 0.6320, Test: 0.6480
Epoch: 39, Loss: 0.5495, Train: 0.9571, Val: 0.6780, Test: 0.7060
Epoch: 40, Loss: 0.4074, Train: 0.9714, Val: 0.7300, Test: 0.7560
Epoch: 41, Loss: 0.3419, Train: 0.9786, Val: 0.7240, Test: 0.7710
Epoch: 42, Loss: 0.2525, Train: 0.9643, Val: 0.7360, Test: 0.7660
Epoch: 43, Loss: 0.2301, Train: 0.9643, Val: 0.7360, Test: 0.7540
Epoch: 44, Loss: 0.1987, Train: 0.9857, Val: 0.7240, Test: 0.7760
Epoch: 45, Loss: 0.1812, Train: 0.9929, Val: 0.7420, Test: 0.7850
Epoch: 46, Loss: 0.1049, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 47, Loss: 0.0876, Train: 0.9929, Val: 0.7080, Test: 0.7500
Epoch: 48, Loss: 0.1112, Train: 0.9857, Val: 0.7160, Test: 0.7500
Epoch: 49, Loss: 0.0650, Train: 0.9714, Val: 0.7060, Test: 0.7470
Epoch: 50, Loss: 0.1040, Train: 1.0000, Val: 0.7200, Test: 0.7620
MAD:  0.489
Best Test Accuracy: 0.7850, Val Accuracy: 0.7420, Train Accuracy: 0.9929
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9709, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 2, Loss: 1.9750, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 3, Loss: 1.9463, Train: 0.1500, Val: 0.1560, Test: 0.1440
Epoch: 4, Loss: 1.9464, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 5, Loss: 1.9442, Train: 0.2929, Val: 0.2120, Test: 0.2220
Epoch: 6, Loss: 1.9387, Train: 0.1500, Val: 0.1640, Test: 0.1510
Epoch: 7, Loss: 1.9436, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 8, Loss: 1.9532, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 9, Loss: 1.9568, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 10, Loss: 1.9403, Train: 0.2357, Val: 0.1240, Test: 0.1390
Epoch: 11, Loss: 1.9588, Train: 0.2571, Val: 0.2080, Test: 0.1920
Epoch: 12, Loss: 1.9268, Train: 0.2214, Val: 0.1220, Test: 0.1170
Epoch: 13, Loss: 1.9402, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 14, Loss: 1.9174, Train: 0.1500, Val: 0.0620, Test: 0.0690
Epoch: 15, Loss: 1.9070, Train: 0.3643, Val: 0.2320, Test: 0.2410
Epoch: 16, Loss: 1.9206, Train: 0.4429, Val: 0.2580, Test: 0.2730
Epoch: 17, Loss: 1.8233, Train: 0.4786, Val: 0.2820, Test: 0.2890
Epoch: 18, Loss: 1.7659, Train: 0.4786, Val: 0.3080, Test: 0.3280
Epoch: 19, Loss: 1.6596, Train: 0.3500, Val: 0.3060, Test: 0.3090
Epoch: 20, Loss: 1.5310, Train: 0.4071, Val: 0.2720, Test: 0.2950
Epoch: 21, Loss: 1.5363, Train: 0.4429, Val: 0.2900, Test: 0.2960
Epoch: 22, Loss: 1.4784, Train: 0.5071, Val: 0.3120, Test: 0.3290
Epoch: 23, Loss: 1.3970, Train: 0.6357, Val: 0.4160, Test: 0.3890
Epoch: 24, Loss: 1.2279, Train: 0.8071, Val: 0.5540, Test: 0.5620
Epoch: 25, Loss: 1.1642, Train: 0.7357, Val: 0.5300, Test: 0.5380
Epoch: 26, Loss: 1.1914, Train: 0.8786, Val: 0.6380, Test: 0.6680
Epoch: 27, Loss: 0.9002, Train: 0.8643, Val: 0.6160, Test: 0.6410
Epoch: 28, Loss: 0.8065, Train: 0.8071, Val: 0.5580, Test: 0.5630
Epoch: 29, Loss: 0.6864, Train: 0.9500, Val: 0.6680, Test: 0.6670
Epoch: 30, Loss: 0.5222, Train: 0.9214, Val: 0.6660, Test: 0.6690
Epoch: 31, Loss: 0.5324, Train: 0.9571, Val: 0.7280, Test: 0.7180
Epoch: 32, Loss: 0.3855, Train: 0.9714, Val: 0.7280, Test: 0.7220
Epoch: 33, Loss: 0.3483, Train: 0.9643, Val: 0.6780, Test: 0.6550
Epoch: 34, Loss: 0.2860, Train: 0.9500, Val: 0.7120, Test: 0.6900
Epoch: 35, Loss: 0.2688, Train: 0.9929, Val: 0.7400, Test: 0.7410
Epoch: 36, Loss: 0.2125, Train: 0.9643, Val: 0.7320, Test: 0.7270
Epoch: 37, Loss: 0.1654, Train: 0.9857, Val: 0.7360, Test: 0.7130
Epoch: 38, Loss: 0.1528, Train: 0.9786, Val: 0.7100, Test: 0.7150
Epoch: 39, Loss: 0.1505, Train: 0.9786, Val: 0.7120, Test: 0.7080
Epoch: 40, Loss: 0.1201, Train: 1.0000, Val: 0.7280, Test: 0.7450
Epoch: 41, Loss: 0.0776, Train: 0.9929, Val: 0.7360, Test: 0.7420
Epoch: 42, Loss: 0.0812, Train: 1.0000, Val: 0.7280, Test: 0.7500
Epoch: 43, Loss: 0.0618, Train: 1.0000, Val: 0.7240, Test: 0.7330
Epoch: 44, Loss: 0.0520, Train: 1.0000, Val: 0.7220, Test: 0.7210
Epoch: 45, Loss: 0.0371, Train: 1.0000, Val: 0.7260, Test: 0.7170
Epoch: 46, Loss: 0.0369, Train: 1.0000, Val: 0.7320, Test: 0.7360
Epoch: 47, Loss: 0.0194, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 48, Loss: 0.0177, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 49, Loss: 0.0109, Train: 0.9929, Val: 0.7380, Test: 0.7610
Epoch: 50, Loss: 0.0202, Train: 1.0000, Val: 0.7480, Test: 0.7630
MAD:  0.588
Best Test Accuracy: 0.7630, Val Accuracy: 0.7480, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9886, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 2, Loss: 1.9698, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 3, Loss: 1.9401, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 4, Loss: 1.9496, Train: 0.2000, Val: 0.1420, Test: 0.1520
Epoch: 5, Loss: 1.9620, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 6, Loss: 1.9229, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 7, Loss: 1.9629, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 8, Loss: 1.9504, Train: 0.2429, Val: 0.2160, Test: 0.2020
Epoch: 9, Loss: 1.9602, Train: 0.2429, Val: 0.2160, Test: 0.1980
Epoch: 10, Loss: 1.9377, Train: 0.2071, Val: 0.1920, Test: 0.1700
Epoch: 11, Loss: 1.9513, Train: 0.2071, Val: 0.1840, Test: 0.1570
Epoch: 12, Loss: 1.9543, Train: 0.2357, Val: 0.1760, Test: 0.1580
Epoch: 13, Loss: 1.9334, Train: 0.4071, Val: 0.2380, Test: 0.2580
Epoch: 14, Loss: 1.9344, Train: 0.1571, Val: 0.0600, Test: 0.0710
Epoch: 15, Loss: 1.9305, Train: 0.2643, Val: 0.0900, Test: 0.1110
Epoch: 16, Loss: 1.8947, Train: 0.2429, Val: 0.1120, Test: 0.1170
Epoch: 17, Loss: 1.8971, Train: 0.3643, Val: 0.1960, Test: 0.1940
Epoch: 18, Loss: 1.8565, Train: 0.6571, Val: 0.6260, Test: 0.6150
Epoch: 19, Loss: 1.8123, Train: 0.5786, Val: 0.6060, Test: 0.5790
Epoch: 20, Loss: 1.7398, Train: 0.5143, Val: 0.4520, Test: 0.4090
Epoch: 21, Loss: 1.6460, Train: 0.4786, Val: 0.3940, Test: 0.3660
Epoch: 22, Loss: 1.5086, Train: 0.4857, Val: 0.4140, Test: 0.3850
Epoch: 23, Loss: 1.3693, Train: 0.5286, Val: 0.4200, Test: 0.3840
Epoch: 24, Loss: 1.3024, Train: 0.5214, Val: 0.4360, Test: 0.3980
Epoch: 25, Loss: 1.2717, Train: 0.6500, Val: 0.4860, Test: 0.4760
Epoch: 26, Loss: 1.0729, Train: 0.7214, Val: 0.5500, Test: 0.5560
Epoch: 27, Loss: 0.9306, Train: 0.9143, Val: 0.7040, Test: 0.6820
Epoch: 28, Loss: 0.8286, Train: 0.9214, Val: 0.7340, Test: 0.7440
Epoch: 29, Loss: 0.6732, Train: 0.9429, Val: 0.7600, Test: 0.7590
Epoch: 30, Loss: 0.5518, Train: 0.9357, Val: 0.7220, Test: 0.7460
Epoch: 31, Loss: 0.4980, Train: 0.9429, Val: 0.6920, Test: 0.6910
Epoch: 32, Loss: 0.4000, Train: 0.9286, Val: 0.6680, Test: 0.6850
Epoch: 33, Loss: 0.3688, Train: 0.9857, Val: 0.7240, Test: 0.7320
Epoch: 34, Loss: 0.2302, Train: 0.9500, Val: 0.6940, Test: 0.6820
Epoch: 35, Loss: 0.2347, Train: 0.9929, Val: 0.7100, Test: 0.7380
Epoch: 36, Loss: 0.1681, Train: 0.9571, Val: 0.6260, Test: 0.6600
Epoch: 37, Loss: 0.1678, Train: 0.9857, Val: 0.7280, Test: 0.7560
Epoch: 38, Loss: 0.0853, Train: 0.9714, Val: 0.7340, Test: 0.7380
Epoch: 39, Loss: 0.1629, Train: 0.9714, Val: 0.7460, Test: 0.7590
Epoch: 40, Loss: 0.1544, Train: 1.0000, Val: 0.7220, Test: 0.7600
Epoch: 41, Loss: 0.0569, Train: 0.9714, Val: 0.6620, Test: 0.6990
Epoch: 42, Loss: 0.1219, Train: 0.9857, Val: 0.7200, Test: 0.7260
Epoch: 43, Loss: 0.0644, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 44, Loss: 0.0492, Train: 1.0000, Val: 0.7640, Test: 0.7610
Epoch: 45, Loss: 0.0390, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 46, Loss: 0.0529, Train: 1.0000, Val: 0.7560, Test: 0.7700
Epoch: 47, Loss: 0.0165, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 48, Loss: 0.0188, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 49, Loss: 0.0133, Train: 0.9857, Val: 0.7200, Test: 0.7410
Epoch: 50, Loss: 0.0176, Train: 1.0000, Val: 0.7200, Test: 0.7430
MAD:  0.5762
Best Test Accuracy: 0.7730, Val Accuracy: 0.7500, Train Accuracy: 1.0000
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9838, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 2, Loss: 1.9783, Train: 0.1786, Val: 0.3220, Test: 0.3260
Epoch: 3, Loss: 1.9561, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 4, Loss: 1.9440, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 5, Loss: 1.9432, Train: 0.3500, Val: 0.2920, Test: 0.2840
Epoch: 6, Loss: 1.9652, Train: 0.1571, Val: 0.0800, Test: 0.1020
Epoch: 7, Loss: 1.9217, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 8, Loss: 1.9621, Train: 0.2571, Val: 0.1560, Test: 0.1730
Epoch: 9, Loss: 1.9187, Train: 0.2786, Val: 0.2500, Test: 0.2500
Epoch: 10, Loss: 1.9502, Train: 0.1500, Val: 0.1680, Test: 0.1550
Epoch: 11, Loss: 1.9186, Train: 0.3929, Val: 0.3880, Test: 0.3490
Epoch: 12, Loss: 1.9175, Train: 0.2071, Val: 0.1540, Test: 0.1330
Epoch: 13, Loss: 1.8933, Train: 0.2000, Val: 0.1380, Test: 0.1310
Epoch: 14, Loss: 1.9082, Train: 0.3786, Val: 0.2680, Test: 0.2710
Epoch: 15, Loss: 1.8814, Train: 0.3714, Val: 0.2860, Test: 0.2610
Epoch: 16, Loss: 1.8512, Train: 0.3429, Val: 0.2640, Test: 0.2710
Epoch: 17, Loss: 1.7834, Train: 0.3286, Val: 0.2540, Test: 0.2620
Epoch: 18, Loss: 1.6826, Train: 0.4357, Val: 0.2940, Test: 0.2910
Epoch: 19, Loss: 1.5901, Train: 0.3714, Val: 0.3180, Test: 0.3030
Epoch: 20, Loss: 1.5426, Train: 0.3286, Val: 0.2100, Test: 0.2350
Epoch: 21, Loss: 1.4832, Train: 0.4286, Val: 0.3060, Test: 0.3060
Epoch: 22, Loss: 1.4438, Train: 0.5143, Val: 0.3640, Test: 0.3820
Epoch: 23, Loss: 1.3228, Train: 0.5571, Val: 0.4520, Test: 0.4470
Epoch: 24, Loss: 1.2793, Train: 0.6214, Val: 0.4440, Test: 0.4450
Epoch: 25, Loss: 1.1588, Train: 0.7071, Val: 0.5000, Test: 0.5070
Epoch: 26, Loss: 1.0226, Train: 0.7286, Val: 0.5560, Test: 0.5450
Epoch: 27, Loss: 0.9377, Train: 0.7929, Val: 0.5700, Test: 0.5600
Epoch: 28, Loss: 0.9169, Train: 0.8071, Val: 0.6240, Test: 0.6340
Epoch: 29, Loss: 0.8037, Train: 0.8929, Val: 0.6800, Test: 0.6690
Epoch: 30, Loss: 0.6974, Train: 0.9071, Val: 0.7060, Test: 0.7240
Epoch: 31, Loss: 0.5724, Train: 0.9571, Val: 0.7700, Test: 0.7620
Epoch: 32, Loss: 0.5948, Train: 0.9643, Val: 0.7380, Test: 0.7560
Epoch: 33, Loss: 0.4376, Train: 0.9500, Val: 0.7540, Test: 0.7670
Epoch: 34, Loss: 0.4310, Train: 0.9786, Val: 0.7720, Test: 0.7900
Epoch: 35, Loss: 0.3224, Train: 0.9500, Val: 0.7640, Test: 0.7670
Epoch: 36, Loss: 0.2928, Train: 0.9714, Val: 0.7520, Test: 0.7590
Epoch: 37, Loss: 0.2216, Train: 0.9571, Val: 0.7300, Test: 0.7420
Epoch: 38, Loss: 0.2649, Train: 0.9857, Val: 0.7600, Test: 0.7780
Epoch: 39, Loss: 0.1108, Train: 0.9643, Val: 0.7740, Test: 0.7720
Epoch: 40, Loss: 0.1663, Train: 1.0000, Val: 0.7900, Test: 0.8030
Epoch: 41, Loss: 0.1248, Train: 1.0000, Val: 0.7620, Test: 0.7850
Epoch: 42, Loss: 0.1004, Train: 0.9643, Val: 0.7440, Test: 0.7660
Epoch: 43, Loss: 0.1331, Train: 1.0000, Val: 0.7460, Test: 0.7840
Epoch: 44, Loss: 0.0802, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 45, Loss: 0.0648, Train: 0.9857, Val: 0.7820, Test: 0.7940
Epoch: 46, Loss: 0.0734, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 47, Loss: 0.0408, Train: 1.0000, Val: 0.7620, Test: 0.7580
Epoch: 48, Loss: 0.0385, Train: 0.9929, Val: 0.7420, Test: 0.7530
Epoch: 49, Loss: 0.0426, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 50, Loss: 0.0465, Train: 1.0000, Val: 0.7700, Test: 0.7930
MAD:  0.5323
Best Test Accuracy: 0.8030, Val Accuracy: 0.7900, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9661, Train: 0.2143, Val: 0.1780, Test: 0.1900
Epoch: 2, Loss: 1.9963, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 3, Loss: 1.9515, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 4, Loss: 1.9569, Train: 0.1929, Val: 0.1240, Test: 0.1390
Epoch: 5, Loss: 1.9708, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 6, Loss: 1.9554, Train: 0.2786, Val: 0.2540, Test: 0.2290
Epoch: 7, Loss: 1.9621, Train: 0.2571, Val: 0.4420, Test: 0.4250
Epoch: 8, Loss: 1.9351, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 9, Loss: 1.9603, Train: 0.2643, Val: 0.3340, Test: 0.3430
Epoch: 10, Loss: 1.9646, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 11, Loss: 1.9448, Train: 0.3357, Val: 0.2060, Test: 0.2210
Epoch: 12, Loss: 1.9409, Train: 0.2571, Val: 0.1300, Test: 0.1330
Epoch: 13, Loss: 1.9500, Train: 0.2500, Val: 0.1400, Test: 0.1470
Epoch: 14, Loss: 1.9453, Train: 0.2500, Val: 0.1380, Test: 0.1470
Epoch: 15, Loss: 1.9157, Train: 0.2714, Val: 0.1440, Test: 0.1640
Epoch: 16, Loss: 1.9024, Train: 0.3143, Val: 0.1740, Test: 0.1880
Epoch: 17, Loss: 1.8564, Train: 0.5071, Val: 0.4600, Test: 0.4360
Epoch: 18, Loss: 1.8362, Train: 0.4214, Val: 0.3440, Test: 0.3300
Epoch: 19, Loss: 1.7356, Train: 0.3429, Val: 0.2720, Test: 0.2690
Epoch: 20, Loss: 1.6467, Train: 0.3714, Val: 0.2840, Test: 0.2710
Epoch: 21, Loss: 1.5868, Train: 0.2857, Val: 0.2240, Test: 0.2230
Epoch: 22, Loss: 1.6121, Train: 0.3571, Val: 0.2480, Test: 0.2570
Epoch: 23, Loss: 1.5684, Train: 0.3857, Val: 0.2960, Test: 0.2860
Epoch: 24, Loss: 1.4653, Train: 0.4571, Val: 0.3560, Test: 0.3330
Epoch: 25, Loss: 1.4032, Train: 0.4929, Val: 0.3280, Test: 0.3090
Epoch: 26, Loss: 1.3702, Train: 0.5786, Val: 0.4220, Test: 0.4010
Epoch: 27, Loss: 1.3703, Train: 0.6500, Val: 0.4720, Test: 0.4760
Epoch: 28, Loss: 1.2828, Train: 0.4500, Val: 0.4440, Test: 0.4270
Epoch: 29, Loss: 1.2649, Train: 0.6000, Val: 0.5060, Test: 0.4900
Epoch: 30, Loss: 1.2447, Train: 0.7143, Val: 0.5620, Test: 0.5300
Epoch: 31, Loss: 1.0437, Train: 0.8000, Val: 0.5440, Test: 0.4920
Epoch: 32, Loss: 1.0097, Train: 0.8214, Val: 0.6380, Test: 0.5740
Epoch: 33, Loss: 0.9197, Train: 0.7857, Val: 0.6020, Test: 0.5780
Epoch: 34, Loss: 0.8180, Train: 0.8214, Val: 0.6560, Test: 0.6270
Epoch: 35, Loss: 0.7601, Train: 0.9143, Val: 0.6900, Test: 0.6480
Epoch: 36, Loss: 0.6269, Train: 0.8786, Val: 0.6740, Test: 0.6730
Epoch: 37, Loss: 0.6240, Train: 0.8857, Val: 0.7000, Test: 0.6720
Epoch: 38, Loss: 0.5579, Train: 0.9500, Val: 0.7300, Test: 0.7270
Epoch: 39, Loss: 0.4975, Train: 0.9286, Val: 0.7520, Test: 0.7230
Epoch: 40, Loss: 0.4181, Train: 0.8786, Val: 0.7060, Test: 0.6810
Epoch: 41, Loss: 0.4003, Train: 0.9357, Val: 0.7360, Test: 0.7080
Epoch: 42, Loss: 0.3001, Train: 0.9286, Val: 0.7340, Test: 0.7150
Epoch: 43, Loss: 0.3047, Train: 0.9929, Val: 0.7480, Test: 0.7470
Epoch: 44, Loss: 0.2095, Train: 0.9786, Val: 0.7440, Test: 0.7370
Epoch: 45, Loss: 0.2379, Train: 0.9929, Val: 0.7640, Test: 0.7670
Epoch: 46, Loss: 0.1722, Train: 1.0000, Val: 0.7600, Test: 0.7600
Epoch: 47, Loss: 0.1217, Train: 1.0000, Val: 0.7620, Test: 0.7550
Epoch: 48, Loss: 0.0979, Train: 1.0000, Val: 0.7520, Test: 0.7500
Epoch: 49, Loss: 0.1175, Train: 1.0000, Val: 0.7520, Test: 0.7640
Epoch: 50, Loss: 0.0667, Train: 0.9929, Val: 0.7600, Test: 0.7720
MAD:  0.5635
Best Test Accuracy: 0.7720, Val Accuracy: 0.7600, Train Accuracy: 0.9929
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9903, Train: 0.2214, Val: 0.3120, Test: 0.3300
Epoch: 2, Loss: 1.9540, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 3, Loss: 1.9648, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 4, Loss: 1.9581, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 5, Loss: 1.9779, Train: 0.2500, Val: 0.3280, Test: 0.3580
Epoch: 6, Loss: 1.9462, Train: 0.1429, Val: 0.0780, Test: 0.0910
Epoch: 7, Loss: 1.9438, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 8, Loss: 1.9444, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 9, Loss: 1.9233, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 10, Loss: 1.9328, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 11, Loss: 1.9207, Train: 0.2500, Val: 0.3540, Test: 0.3710
Epoch: 12, Loss: 1.9383, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 13, Loss: 1.9282, Train: 0.2500, Val: 0.3360, Test: 0.3440
Epoch: 14, Loss: 1.9026, Train: 0.2643, Val: 0.2880, Test: 0.3290
Epoch: 15, Loss: 1.8914, Train: 0.3857, Val: 0.3820, Test: 0.3970
Epoch: 16, Loss: 1.7865, Train: 0.4357, Val: 0.3080, Test: 0.3050
Epoch: 17, Loss: 1.7199, Train: 0.4357, Val: 0.3240, Test: 0.3300
Epoch: 18, Loss: 1.5821, Train: 0.4429, Val: 0.3080, Test: 0.3300
Epoch: 19, Loss: 1.4751, Train: 0.5000, Val: 0.3480, Test: 0.3770
Epoch: 20, Loss: 1.3562, Train: 0.5357, Val: 0.3620, Test: 0.3760
Epoch: 21, Loss: 1.2958, Train: 0.4786, Val: 0.3880, Test: 0.4020
Epoch: 22, Loss: 1.3898, Train: 0.8071, Val: 0.6740, Test: 0.6670
Epoch: 23, Loss: 1.1911, Train: 0.7000, Val: 0.5560, Test: 0.5900
Epoch: 24, Loss: 1.0594, Train: 0.7000, Val: 0.4820, Test: 0.4750
Epoch: 25, Loss: 0.9987, Train: 0.8286, Val: 0.5680, Test: 0.5840
Epoch: 26, Loss: 0.9095, Train: 0.8357, Val: 0.5980, Test: 0.6230
Epoch: 27, Loss: 0.8467, Train: 0.8500, Val: 0.6080, Test: 0.6140
Epoch: 28, Loss: 0.6777, Train: 0.8714, Val: 0.6060, Test: 0.6080
Epoch: 29, Loss: 0.5843, Train: 0.8643, Val: 0.5360, Test: 0.5670
Epoch: 30, Loss: 0.5538, Train: 0.9857, Val: 0.6880, Test: 0.7100
Epoch: 31, Loss: 0.4774, Train: 0.9429, Val: 0.6660, Test: 0.6760
Epoch: 32, Loss: 0.4010, Train: 0.9500, Val: 0.6900, Test: 0.7170
Epoch: 33, Loss: 0.3735, Train: 0.9786, Val: 0.7200, Test: 0.7380
Epoch: 34, Loss: 0.2849, Train: 0.9786, Val: 0.7140, Test: 0.7240
Epoch: 35, Loss: 0.2742, Train: 0.9857, Val: 0.7000, Test: 0.7220
Epoch: 36, Loss: 0.2378, Train: 0.9857, Val: 0.7100, Test: 0.7380
Epoch: 37, Loss: 0.2496, Train: 1.0000, Val: 0.7160, Test: 0.7420
Epoch: 38, Loss: 0.1586, Train: 1.0000, Val: 0.7220, Test: 0.7400
Epoch: 39, Loss: 0.1023, Train: 0.9643, Val: 0.7080, Test: 0.7180
Epoch: 40, Loss: 0.1455, Train: 1.0000, Val: 0.7420, Test: 0.7540
Epoch: 41, Loss: 0.1224, Train: 0.9857, Val: 0.7380, Test: 0.7680
Epoch: 42, Loss: 0.1310, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 43, Loss: 0.0710, Train: 0.9929, Val: 0.7280, Test: 0.7490
Epoch: 44, Loss: 0.0780, Train: 0.9929, Val: 0.7220, Test: 0.7510
Epoch: 45, Loss: 0.0688, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 46, Loss: 0.0466, Train: 1.0000, Val: 0.7580, Test: 0.7770
Epoch: 47, Loss: 0.0625, Train: 0.9786, Val: 0.7480, Test: 0.7750
Epoch: 48, Loss: 0.0894, Train: 1.0000, Val: 0.7180, Test: 0.7450
Epoch: 49, Loss: 0.0680, Train: 0.9929, Val: 0.7340, Test: 0.7440
Epoch: 50, Loss: 0.0492, Train: 1.0000, Val: 0.7500, Test: 0.7710
MAD:  0.5223
Best Test Accuracy: 0.7770, Val Accuracy: 0.7580, Train Accuracy: 1.0000
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9836, Train: 0.1857, Val: 0.2100, Test: 0.2220
Epoch: 2, Loss: 1.9747, Train: 0.1429, Val: 0.1640, Test: 0.1490
Epoch: 3, Loss: 1.9384, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 4, Loss: 1.9670, Train: 0.1929, Val: 0.1140, Test: 0.1220
Epoch: 5, Loss: 1.9318, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 6, Loss: 1.9309, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 7, Loss: 1.9360, Train: 0.2714, Val: 0.1920, Test: 0.1970
Epoch: 8, Loss: 1.9285, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 9, Loss: 1.9910, Train: 0.2786, Val: 0.2260, Test: 0.2180
Epoch: 10, Loss: 1.9255, Train: 0.1929, Val: 0.0740, Test: 0.0990
Epoch: 11, Loss: 1.9591, Train: 0.1857, Val: 0.0660, Test: 0.0830
Epoch: 12, Loss: 1.9159, Train: 0.1929, Val: 0.0680, Test: 0.0850
Epoch: 13, Loss: 1.9155, Train: 0.1714, Val: 0.3160, Test: 0.3210
Epoch: 14, Loss: 1.9307, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 15, Loss: 1.9096, Train: 0.2214, Val: 0.3400, Test: 0.3480
Epoch: 16, Loss: 1.8865, Train: 0.4929, Val: 0.3780, Test: 0.3860
Epoch: 17, Loss: 1.8317, Train: 0.4500, Val: 0.3460, Test: 0.3330
Epoch: 18, Loss: 1.7558, Train: 0.3429, Val: 0.2980, Test: 0.2960
Epoch: 19, Loss: 1.6798, Train: 0.3214, Val: 0.2880, Test: 0.2790
Epoch: 20, Loss: 1.5738, Train: 0.3857, Val: 0.3100, Test: 0.3240
Epoch: 21, Loss: 1.4687, Train: 0.3286, Val: 0.2460, Test: 0.2440
Epoch: 22, Loss: 1.4781, Train: 0.4857, Val: 0.3580, Test: 0.3690
Epoch: 23, Loss: 1.3624, Train: 0.4143, Val: 0.3260, Test: 0.3220
Epoch: 24, Loss: 1.4891, Train: 0.6571, Val: 0.4160, Test: 0.4430
Epoch: 25, Loss: 1.2070, Train: 0.4786, Val: 0.3220, Test: 0.3200
Epoch: 26, Loss: 1.2084, Train: 0.5643, Val: 0.4240, Test: 0.4310
Epoch: 27, Loss: 1.0406, Train: 0.6286, Val: 0.5020, Test: 0.5330
Epoch: 28, Loss: 1.0568, Train: 0.6500, Val: 0.5000, Test: 0.5160
Epoch: 29, Loss: 0.9725, Train: 0.6857, Val: 0.5460, Test: 0.5370
Epoch: 30, Loss: 0.8802, Train: 0.6929, Val: 0.5600, Test: 0.5550
Epoch: 31, Loss: 0.8539, Train: 0.8429, Val: 0.6260, Test: 0.6370
Epoch: 32, Loss: 0.6705, Train: 0.6214, Val: 0.4860, Test: 0.4740
Epoch: 33, Loss: 0.8000, Train: 0.8929, Val: 0.6460, Test: 0.6650
Epoch: 34, Loss: 0.6516, Train: 0.8571, Val: 0.6460, Test: 0.6750
Epoch: 35, Loss: 0.6523, Train: 0.8857, Val: 0.6300, Test: 0.6510
Epoch: 36, Loss: 0.5526, Train: 0.9357, Val: 0.6380, Test: 0.6440
Epoch: 37, Loss: 0.4755, Train: 0.8929, Val: 0.5920, Test: 0.6100
Epoch: 38, Loss: 0.5137, Train: 0.9143, Val: 0.6380, Test: 0.6500
Epoch: 39, Loss: 0.4528, Train: 0.9643, Val: 0.6740, Test: 0.6710
Epoch: 40, Loss: 0.3387, Train: 0.9714, Val: 0.6300, Test: 0.6350
Epoch: 41, Loss: 0.3436, Train: 0.9857, Val: 0.6680, Test: 0.6540
Epoch: 42, Loss: 0.3200, Train: 0.9857, Val: 0.6800, Test: 0.6840
Epoch: 43, Loss: 0.2407, Train: 0.9857, Val: 0.6800, Test: 0.6900
Epoch: 44, Loss: 0.2226, Train: 0.9643, Val: 0.6880, Test: 0.7060
Epoch: 45, Loss: 0.2412, Train: 1.0000, Val: 0.6840, Test: 0.6990
Epoch: 46, Loss: 0.1566, Train: 0.9929, Val: 0.6500, Test: 0.6570
Epoch: 47, Loss: 0.2271, Train: 0.9929, Val: 0.6860, Test: 0.7130
Epoch: 48, Loss: 0.1487, Train: 0.9929, Val: 0.6880, Test: 0.7260
Epoch: 49, Loss: 0.1288, Train: 1.0000, Val: 0.6800, Test: 0.7170
Epoch: 50, Loss: 0.1466, Train: 1.0000, Val: 0.6980, Test: 0.7090
MAD:  0.546
Best Test Accuracy: 0.7260, Val Accuracy: 0.6880, Train Accuracy: 0.9929
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9755, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 2, Loss: 1.9964, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 3, Loss: 1.9587, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 4, Loss: 1.9616, Train: 0.1429, Val: 0.3160, Test: 0.3200
Epoch: 5, Loss: 1.9714, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 6, Loss: 1.9548, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 7, Loss: 1.9495, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 8, Loss: 1.9251, Train: 0.2786, Val: 0.1840, Test: 0.2100
Epoch: 9, Loss: 1.9432, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 10, Loss: 1.9694, Train: 0.2071, Val: 0.1180, Test: 0.1150
Epoch: 11, Loss: 1.9646, Train: 0.1500, Val: 0.0620, Test: 0.0650
Epoch: 12, Loss: 1.9368, Train: 0.2143, Val: 0.3220, Test: 0.3320
Epoch: 13, Loss: 1.9512, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 14, Loss: 1.9340, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 15, Loss: 1.9269, Train: 0.1929, Val: 0.3160, Test: 0.3300
Epoch: 16, Loss: 1.9220, Train: 0.5214, Val: 0.4620, Test: 0.4850
Epoch: 17, Loss: 1.9127, Train: 0.4071, Val: 0.2500, Test: 0.2450
Epoch: 18, Loss: 1.9046, Train: 0.4857, Val: 0.3660, Test: 0.3370
Epoch: 19, Loss: 1.8754, Train: 0.5214, Val: 0.3920, Test: 0.3710
Epoch: 20, Loss: 1.8326, Train: 0.5214, Val: 0.4080, Test: 0.3850
Epoch: 21, Loss: 1.7749, Train: 0.5286, Val: 0.4180, Test: 0.3880
Epoch: 22, Loss: 1.7170, Train: 0.5643, Val: 0.4300, Test: 0.3970
Epoch: 23, Loss: 1.5456, Train: 0.5500, Val: 0.4240, Test: 0.3840
Epoch: 24, Loss: 1.4272, Train: 0.5643, Val: 0.4400, Test: 0.3990
Epoch: 25, Loss: 1.3464, Train: 0.5571, Val: 0.4280, Test: 0.3900
Epoch: 26, Loss: 1.2467, Train: 0.5643, Val: 0.4460, Test: 0.4060
Epoch: 27, Loss: 1.1350, Train: 0.7357, Val: 0.5240, Test: 0.4960
Epoch: 28, Loss: 1.0730, Train: 0.7357, Val: 0.5260, Test: 0.5070
Epoch: 29, Loss: 0.9953, Train: 0.8071, Val: 0.5440, Test: 0.5310
Epoch: 30, Loss: 0.8879, Train: 0.8214, Val: 0.5500, Test: 0.5430
Epoch: 31, Loss: 0.7887, Train: 0.8571, Val: 0.5800, Test: 0.5590
Epoch: 32, Loss: 0.7997, Train: 0.8214, Val: 0.5780, Test: 0.5730
Epoch: 33, Loss: 0.7551, Train: 0.9071, Val: 0.6540, Test: 0.6220
Epoch: 34, Loss: 0.6211, Train: 0.8286, Val: 0.6640, Test: 0.6300
Epoch: 35, Loss: 0.6234, Train: 0.8429, Val: 0.5900, Test: 0.5940
Epoch: 36, Loss: 0.5705, Train: 0.8357, Val: 0.5660, Test: 0.5750
Epoch: 37, Loss: 0.4860, Train: 0.8714, Val: 0.6120, Test: 0.6140
Epoch: 38, Loss: 0.4340, Train: 0.9429, Val: 0.7140, Test: 0.6900
Epoch: 39, Loss: 0.3619, Train: 0.9643, Val: 0.6820, Test: 0.6550
Epoch: 40, Loss: 0.3466, Train: 0.9643, Val: 0.6640, Test: 0.6600
Epoch: 41, Loss: 0.3013, Train: 0.9857, Val: 0.7080, Test: 0.6920
Epoch: 42, Loss: 0.2217, Train: 0.9929, Val: 0.7080, Test: 0.6950
Epoch: 43, Loss: 0.2501, Train: 0.9857, Val: 0.7180, Test: 0.7150
Epoch: 44, Loss: 0.1980, Train: 0.9571, Val: 0.7160, Test: 0.7030
Epoch: 45, Loss: 0.2060, Train: 0.9857, Val: 0.7620, Test: 0.7390
Epoch: 46, Loss: 0.1402, Train: 0.9857, Val: 0.7500, Test: 0.7390
Epoch: 47, Loss: 0.1513, Train: 0.9929, Val: 0.7600, Test: 0.7410
Epoch: 48, Loss: 0.1173, Train: 0.9929, Val: 0.7380, Test: 0.7280
Epoch: 49, Loss: 0.1001, Train: 0.9929, Val: 0.7220, Test: 0.7230
Epoch: 50, Loss: 0.1323, Train: 0.9929, Val: 0.7360, Test: 0.7290
MAD:  0.4945
Best Test Accuracy: 0.7410, Val Accuracy: 0.7600, Train Accuracy: 0.9929
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9760, Train: 0.1571, Val: 0.0740, Test: 0.0960
Epoch: 2, Loss: 1.9625, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 3, Loss: 1.9672, Train: 0.2214, Val: 0.2560, Test: 0.2600
Epoch: 4, Loss: 1.9156, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 5, Loss: 1.9868, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 6, Loss: 1.9392, Train: 0.1429, Val: 0.1640, Test: 0.1490
Epoch: 7, Loss: 1.9435, Train: 0.2143, Val: 0.1520, Test: 0.1760
Epoch: 8, Loss: 1.9727, Train: 0.2929, Val: 0.2640, Test: 0.2590
Epoch: 9, Loss: 1.9574, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 10, Loss: 1.9352, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 11, Loss: 1.9193, Train: 0.1429, Val: 0.0600, Test: 0.0750
Epoch: 12, Loss: 1.9357, Train: 0.2000, Val: 0.0740, Test: 0.1020
Epoch: 13, Loss: 1.9385, Train: 0.1929, Val: 0.0880, Test: 0.1140
Epoch: 14, Loss: 1.9554, Train: 0.2714, Val: 0.2460, Test: 0.2750
Epoch: 15, Loss: 1.9301, Train: 0.3929, Val: 0.4200, Test: 0.4480
Epoch: 16, Loss: 1.9282, Train: 0.3000, Val: 0.2820, Test: 0.2730
Epoch: 17, Loss: 1.8889, Train: 0.2571, Val: 0.2280, Test: 0.2320
Epoch: 18, Loss: 1.8651, Train: 0.3071, Val: 0.3380, Test: 0.2990
Epoch: 19, Loss: 1.8096, Train: 0.3929, Val: 0.3740, Test: 0.3450
Epoch: 20, Loss: 1.7168, Train: 0.3143, Val: 0.2780, Test: 0.2490
Epoch: 21, Loss: 1.6845, Train: 0.3000, Val: 0.2820, Test: 0.2480
Epoch: 22, Loss: 1.5674, Train: 0.3143, Val: 0.2720, Test: 0.2380
Epoch: 23, Loss: 1.5641, Train: 0.3000, Val: 0.2760, Test: 0.2370
Epoch: 24, Loss: 1.5285, Train: 0.3214, Val: 0.2960, Test: 0.2590
Epoch: 25, Loss: 1.4872, Train: 0.3500, Val: 0.3600, Test: 0.3110
Epoch: 26, Loss: 1.4901, Train: 0.5000, Val: 0.5560, Test: 0.5300
Epoch: 27, Loss: 1.4241, Train: 0.4000, Val: 0.5260, Test: 0.4950
Epoch: 28, Loss: 1.3926, Train: 0.3857, Val: 0.3620, Test: 0.3720
Epoch: 29, Loss: 1.4297, Train: 0.4357, Val: 0.4540, Test: 0.4320
Epoch: 30, Loss: 1.3785, Train: 0.4357, Val: 0.4580, Test: 0.4380
Epoch: 31, Loss: 1.2871, Train: 0.4429, Val: 0.4760, Test: 0.4510
Epoch: 32, Loss: 1.2543, Train: 0.4143, Val: 0.4640, Test: 0.4310
Epoch: 33, Loss: 1.2089, Train: 0.5286, Val: 0.4500, Test: 0.4640
Epoch: 34, Loss: 1.0871, Train: 0.6786, Val: 0.5460, Test: 0.5780
Epoch: 35, Loss: 1.0561, Train: 0.6643, Val: 0.5500, Test: 0.5690
Epoch: 36, Loss: 1.0104, Train: 0.7214, Val: 0.5580, Test: 0.5730
Epoch: 37, Loss: 0.9275, Train: 0.8786, Val: 0.6520, Test: 0.6800
Epoch: 38, Loss: 0.7669, Train: 0.8429, Val: 0.6680, Test: 0.6790
Epoch: 39, Loss: 0.7788, Train: 0.8357, Val: 0.6880, Test: 0.6990
Epoch: 40, Loss: 0.6653, Train: 0.9000, Val: 0.6720, Test: 0.7010
Epoch: 41, Loss: 0.6024, Train: 0.8571, Val: 0.6900, Test: 0.7060
Epoch: 42, Loss: 0.5731, Train: 0.8643, Val: 0.6620, Test: 0.6770
Epoch: 43, Loss: 0.5127, Train: 0.8643, Val: 0.6720, Test: 0.6690
Epoch: 44, Loss: 0.5342, Train: 0.8643, Val: 0.6500, Test: 0.6640
Epoch: 45, Loss: 0.3867, Train: 0.8429, Val: 0.6080, Test: 0.6130
Epoch: 46, Loss: 0.3828, Train: 0.9143, Val: 0.6700, Test: 0.6960
Epoch: 47, Loss: 0.3792, Train: 0.8500, Val: 0.6460, Test: 0.6680
Epoch: 48, Loss: 0.3063, Train: 0.8643, Val: 0.6620, Test: 0.6710
Epoch: 49, Loss: 0.3410, Train: 0.8571, Val: 0.6480, Test: 0.6620
Epoch: 50, Loss: 0.3224, Train: 0.8786, Val: 0.6920, Test: 0.7100
MAD:  0.4915
Best Test Accuracy: 0.7100, Val Accuracy: 0.6920, Train Accuracy: 0.8786
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=8)
      (conv2): GATv2Conv(128, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-6): 6 x ParallelGNNBlock(
      (conv1): GATv2Conv(2048, 128, heads=8)
      (conv2): GATv2Conv(2048, 128, heads=8)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0000, Train: 0.2143, Val: 0.1200, Test: 0.1150
Epoch: 2, Loss: 1.9904, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 3, Loss: 1.9188, Train: 0.3429, Val: 0.1640, Test: 0.1840
Epoch: 4, Loss: 1.9715, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 5, Loss: 1.9609, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 6, Loss: 1.9546, Train: 0.2000, Val: 0.1600, Test: 0.1550
Epoch: 7, Loss: 1.9479, Train: 0.1786, Val: 0.1140, Test: 0.1060
Epoch: 8, Loss: 1.9612, Train: 0.2857, Val: 0.1560, Test: 0.1390
Epoch: 9, Loss: 1.9526, Train: 0.2286, Val: 0.1360, Test: 0.1460
Epoch: 10, Loss: 1.9321, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 11, Loss: 1.9503, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 12, Loss: 1.9620, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 13, Loss: 1.9481, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 14, Loss: 1.9574, Train: 0.2571, Val: 0.1560, Test: 0.1920
Epoch: 15, Loss: 1.9469, Train: 0.3143, Val: 0.2320, Test: 0.2240
Epoch: 16, Loss: 1.9334, Train: 0.2643, Val: 0.1460, Test: 0.1430
Epoch: 17, Loss: 1.9368, Train: 0.2357, Val: 0.1340, Test: 0.1190
Epoch: 18, Loss: 1.9433, Train: 0.2571, Val: 0.1440, Test: 0.1500
Epoch: 19, Loss: 1.9145, Train: 0.3000, Val: 0.1760, Test: 0.1940
Epoch: 20, Loss: 1.8880, Train: 0.3500, Val: 0.2680, Test: 0.2790
Epoch: 21, Loss: 1.9269, Train: 0.5929, Val: 0.5740, Test: 0.5900
Epoch: 22, Loss: 1.8559, Train: 0.5429, Val: 0.5100, Test: 0.5210
Epoch: 23, Loss: 1.8368, Train: 0.5643, Val: 0.4620, Test: 0.4750
Epoch: 24, Loss: 1.7736, Train: 0.6571, Val: 0.5600, Test: 0.5800
Epoch: 25, Loss: 1.6848, Train: 0.5214, Val: 0.3920, Test: 0.4040
Epoch: 26, Loss: 1.5524, Train: 0.5143, Val: 0.2960, Test: 0.3000
Epoch: 27, Loss: 1.4116, Train: 0.4929, Val: 0.2420, Test: 0.2730
Epoch: 28, Loss: 1.3699, Train: 0.6143, Val: 0.3560, Test: 0.3550
Epoch: 29, Loss: 1.1994, Train: 0.6429, Val: 0.4320, Test: 0.4590
Epoch: 30, Loss: 1.1049, Train: 0.7000, Val: 0.5120, Test: 0.5520
Epoch: 31, Loss: 1.0866, Train: 0.8071, Val: 0.6040, Test: 0.6210
Epoch: 32, Loss: 1.0364, Train: 0.8143, Val: 0.6040, Test: 0.6260
Epoch: 33, Loss: 0.7643, Train: 0.8214, Val: 0.5800, Test: 0.6120
Epoch: 34, Loss: 0.7494, Train: 0.9000, Val: 0.6520, Test: 0.6720
Epoch: 35, Loss: 0.6876, Train: 0.9071, Val: 0.6180, Test: 0.6500
Epoch: 36, Loss: 0.6143, Train: 0.8000, Val: 0.6000, Test: 0.6270
Epoch: 37, Loss: 0.6467, Train: 0.9643, Val: 0.6820, Test: 0.6960
Epoch: 38, Loss: 0.4871, Train: 0.9500, Val: 0.6760, Test: 0.6910
Epoch: 39, Loss: 0.4310, Train: 0.9571, Val: 0.7000, Test: 0.7300
Epoch: 40, Loss: 0.3999, Train: 0.9571, Val: 0.7160, Test: 0.7400
Epoch: 41, Loss: 0.3371, Train: 0.9786, Val: 0.7040, Test: 0.7150
Epoch: 42, Loss: 0.2933, Train: 0.9786, Val: 0.7220, Test: 0.7360
Epoch: 43, Loss: 0.2645, Train: 0.9714, Val: 0.7280, Test: 0.7570
Epoch: 44, Loss: 0.2521, Train: 0.9857, Val: 0.7280, Test: 0.7600
Epoch: 45, Loss: 0.1645, Train: 0.9929, Val: 0.6980, Test: 0.7120
Epoch: 46, Loss: 0.1692, Train: 0.9929, Val: 0.7160, Test: 0.7460
Epoch: 47, Loss: 0.1270, Train: 1.0000, Val: 0.7140, Test: 0.7550
Epoch: 48, Loss: 0.0855, Train: 0.9643, Val: 0.7140, Test: 0.7490
Epoch: 49, Loss: 0.1651, Train: 0.9857, Val: 0.7260, Test: 0.7600
Epoch: 50, Loss: 0.1724, Train: 0.9929, Val: 0.6980, Test: 0.7100
MAD:  0.5216
Best Test Accuracy: 0.7600, Val Accuracy: 0.7280, Train Accuracy: 0.9857
Training completed.
Average Test Accuracy:  0.761 ± 0.026630809225406598
Average MAD:  0.53249 ± 0.03372436656187927

Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9792, Train: 0.1429, Val: 0.0720, Test: 0.0900
Epoch: 2, Loss: 1.9682, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 3, Loss: 1.9357, Train: 0.1714, Val: 0.3240, Test: 0.3220
Epoch: 4, Loss: 1.9698, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 5, Loss: 1.9569, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 6, Loss: 1.9881, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 7, Loss: 1.9278, Train: 0.2571, Val: 0.3600, Test: 0.3550
Epoch: 8, Loss: 1.9225, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 9, Loss: 1.9445, Train: 0.1714, Val: 0.1160, Test: 0.1140
Epoch: 10, Loss: 1.9728, Train: 0.3071, Val: 0.2260, Test: 0.2220
Epoch: 11, Loss: 1.9542, Train: 0.2214, Val: 0.1920, Test: 0.1810
Epoch: 12, Loss: 1.9615, Train: 0.3214, Val: 0.2140, Test: 0.2190
Epoch: 13, Loss: 1.9390, Train: 0.2857, Val: 0.1120, Test: 0.1440
Epoch: 14, Loss: 1.9724, Train: 0.2643, Val: 0.1200, Test: 0.1390
Epoch: 15, Loss: 1.9479, Train: 0.2214, Val: 0.0880, Test: 0.1000
Epoch: 16, Loss: 1.9224, Train: 0.3143, Val: 0.3880, Test: 0.3970
Epoch: 17, Loss: 1.9239, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 18, Loss: 1.9224, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 19, Loss: 1.9194, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 20, Loss: 1.9166, Train: 0.2000, Val: 0.3440, Test: 0.3330
Epoch: 21, Loss: 1.8944, Train: 0.4643, Val: 0.4920, Test: 0.5120
Epoch: 22, Loss: 1.8811, Train: 0.4286, Val: 0.3280, Test: 0.3450
Epoch: 23, Loss: 1.8323, Train: 0.5714, Val: 0.4040, Test: 0.4210
Epoch: 24, Loss: 1.7865, Train: 0.6000, Val: 0.4440, Test: 0.4370
Epoch: 25, Loss: 1.6790, Train: 0.6357, Val: 0.4460, Test: 0.4580
Epoch: 26, Loss: 1.5695, Train: 0.6429, Val: 0.4300, Test: 0.4490
Epoch: 27, Loss: 1.4570, Train: 0.6643, Val: 0.4540, Test: 0.4570
Epoch: 28, Loss: 1.3369, Train: 0.6500, Val: 0.4680, Test: 0.4540
Epoch: 29, Loss: 1.2512, Train: 0.6500, Val: 0.4420, Test: 0.4480
Epoch: 30, Loss: 1.1338, Train: 0.7000, Val: 0.4900, Test: 0.4890
Epoch: 31, Loss: 1.0724, Train: 0.7286, Val: 0.5360, Test: 0.5270
Epoch: 32, Loss: 0.9299, Train: 0.7357, Val: 0.5360, Test: 0.5420
Epoch: 33, Loss: 0.8756, Train: 0.7929, Val: 0.5980, Test: 0.6210
Epoch: 34, Loss: 0.8620, Train: 0.7500, Val: 0.5600, Test: 0.5610
Epoch: 35, Loss: 0.7871, Train: 0.8286, Val: 0.6560, Test: 0.6670
Epoch: 36, Loss: 0.6981, Train: 0.8214, Val: 0.6560, Test: 0.6770
Epoch: 37, Loss: 0.6554, Train: 0.8429, Val: 0.6680, Test: 0.6650
Epoch: 38, Loss: 0.6283, Train: 0.8500, Val: 0.7060, Test: 0.6960
Epoch: 39, Loss: 0.5667, Train: 0.9071, Val: 0.6940, Test: 0.7190
Epoch: 40, Loss: 0.5737, Train: 0.9214, Val: 0.7280, Test: 0.7200
Epoch: 41, Loss: 0.4935, Train: 0.8571, Val: 0.6820, Test: 0.6780
Epoch: 42, Loss: 0.4920, Train: 0.8929, Val: 0.7220, Test: 0.7100
Epoch: 43, Loss: 0.3741, Train: 0.8500, Val: 0.6940, Test: 0.7040
Epoch: 44, Loss: 0.4677, Train: 0.8786, Val: 0.7160, Test: 0.7090
Epoch: 45, Loss: 0.3337, Train: 0.8571, Val: 0.6600, Test: 0.6600
Epoch: 46, Loss: 0.4248, Train: 0.9214, Val: 0.6940, Test: 0.6880
Epoch: 47, Loss: 0.3584, Train: 0.9429, Val: 0.6700, Test: 0.7060
Epoch: 48, Loss: 0.3529, Train: 0.9714, Val: 0.7700, Test: 0.7890
Epoch: 49, Loss: 0.2497, Train: 0.9214, Val: 0.7520, Test: 0.7770
Epoch: 50, Loss: 0.2781, Train: 0.9857, Val: 0.7680, Test: 0.7740
MAD:  0.5033
Best Test Accuracy: 0.7890, Val Accuracy: 0.7700, Train Accuracy: 0.9714
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9845, Train: 0.1714, Val: 0.2200, Test: 0.2370
Epoch: 2, Loss: 1.9641, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 3, Loss: 1.9666, Train: 0.1429, Val: 0.0720, Test: 0.0920
Epoch: 4, Loss: 1.9677, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 5, Loss: 1.9618, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 6, Loss: 1.9429, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 7, Loss: 1.9684, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 8, Loss: 1.9344, Train: 0.2643, Val: 0.1820, Test: 0.2060
Epoch: 9, Loss: 1.9452, Train: 0.1643, Val: 0.1220, Test: 0.1370
Epoch: 10, Loss: 1.9370, Train: 0.1429, Val: 0.1220, Test: 0.1320
Epoch: 11, Loss: 1.9323, Train: 0.3071, Val: 0.2400, Test: 0.2490
Epoch: 12, Loss: 1.9173, Train: 0.3214, Val: 0.2640, Test: 0.2640
Epoch: 13, Loss: 1.9158, Train: 0.3429, Val: 0.2600, Test: 0.2600
Epoch: 14, Loss: 1.9088, Train: 0.3714, Val: 0.3340, Test: 0.3360
Epoch: 15, Loss: 1.8875, Train: 0.3357, Val: 0.3540, Test: 0.3450
Epoch: 16, Loss: 1.8699, Train: 0.3571, Val: 0.3540, Test: 0.3410
Epoch: 17, Loss: 1.8110, Train: 0.3214, Val: 0.3260, Test: 0.3200
Epoch: 18, Loss: 1.7535, Train: 0.3357, Val: 0.3140, Test: 0.3100
Epoch: 19, Loss: 1.6881, Train: 0.3143, Val: 0.2580, Test: 0.2450
Epoch: 20, Loss: 1.6271, Train: 0.3286, Val: 0.2260, Test: 0.2240
Epoch: 21, Loss: 1.5577, Train: 0.3500, Val: 0.2700, Test: 0.2610
Epoch: 22, Loss: 1.5636, Train: 0.3357, Val: 0.2520, Test: 0.2640
Epoch: 23, Loss: 1.4537, Train: 0.4000, Val: 0.2980, Test: 0.2750
Epoch: 24, Loss: 1.4591, Train: 0.4857, Val: 0.3960, Test: 0.3790
Epoch: 25, Loss: 1.3859, Train: 0.6000, Val: 0.4640, Test: 0.4830
Epoch: 26, Loss: 1.3196, Train: 0.6714, Val: 0.5020, Test: 0.4970
Epoch: 27, Loss: 1.2140, Train: 0.6286, Val: 0.4540, Test: 0.4900
Epoch: 28, Loss: 1.2394, Train: 0.6643, Val: 0.4960, Test: 0.5100
Epoch: 29, Loss: 1.0379, Train: 0.7214, Val: 0.5440, Test: 0.5730
Epoch: 30, Loss: 1.0922, Train: 0.6857, Val: 0.5300, Test: 0.5500
Epoch: 31, Loss: 0.9624, Train: 0.7786, Val: 0.5900, Test: 0.6220
Epoch: 32, Loss: 0.8905, Train: 0.8429, Val: 0.6180, Test: 0.6410
Epoch: 33, Loss: 0.7765, Train: 0.8429, Val: 0.6180, Test: 0.6300
Epoch: 34, Loss: 0.7791, Train: 0.8857, Val: 0.6340, Test: 0.6640
Epoch: 35, Loss: 0.7161, Train: 0.9000, Val: 0.6520, Test: 0.6780
Epoch: 36, Loss: 0.6588, Train: 0.8500, Val: 0.6480, Test: 0.6350
Epoch: 37, Loss: 0.5759, Train: 0.9071, Val: 0.6700, Test: 0.6890
Epoch: 38, Loss: 0.5133, Train: 0.9000, Val: 0.6940, Test: 0.7290
Epoch: 39, Loss: 0.5129, Train: 0.9357, Val: 0.6980, Test: 0.7290
Epoch: 40, Loss: 0.4314, Train: 0.9357, Val: 0.6740, Test: 0.6940
Epoch: 41, Loss: 0.3486, Train: 0.9571, Val: 0.6980, Test: 0.7190
Epoch: 42, Loss: 0.3222, Train: 0.9643, Val: 0.6920, Test: 0.7440
Epoch: 43, Loss: 0.3414, Train: 0.9929, Val: 0.6960, Test: 0.7360
Epoch: 44, Loss: 0.3214, Train: 0.9500, Val: 0.6980, Test: 0.6970
Epoch: 45, Loss: 0.2334, Train: 0.9357, Val: 0.6780, Test: 0.6890
Epoch: 46, Loss: 0.2712, Train: 0.9929, Val: 0.7260, Test: 0.7580
Epoch: 47, Loss: 0.1926, Train: 0.9571, Val: 0.6600, Test: 0.7130
Epoch: 48, Loss: 0.2937, Train: 0.9857, Val: 0.7220, Test: 0.7560
Epoch: 49, Loss: 0.1914, Train: 0.9571, Val: 0.7060, Test: 0.7560
Epoch: 50, Loss: 0.1474, Train: 0.9571, Val: 0.6920, Test: 0.7300
MAD:  0.4955
Best Test Accuracy: 0.7580, Val Accuracy: 0.7260, Train Accuracy: 0.9929
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9581, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 2, Loss: 1.9250, Train: 0.2000, Val: 0.1240, Test: 0.1380
Epoch: 3, Loss: 1.9789, Train: 0.1500, Val: 0.0800, Test: 0.1090
Epoch: 4, Loss: 1.9506, Train: 0.1429, Val: 0.1660, Test: 0.1730
Epoch: 5, Loss: 1.9496, Train: 0.1571, Val: 0.1620, Test: 0.1480
Epoch: 6, Loss: 1.9935, Train: 0.1500, Val: 0.1240, Test: 0.1310
Epoch: 7, Loss: 1.9210, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 8, Loss: 1.9379, Train: 0.1714, Val: 0.1220, Test: 0.1320
Epoch: 9, Loss: 1.9358, Train: 0.1500, Val: 0.0580, Test: 0.0650
Epoch: 10, Loss: 1.9257, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 11, Loss: 1.9426, Train: 0.1571, Val: 0.0600, Test: 0.0730
Epoch: 12, Loss: 1.9424, Train: 0.3071, Val: 0.1580, Test: 0.1790
Epoch: 13, Loss: 1.9617, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 14, Loss: 1.9329, Train: 0.1500, Val: 0.0740, Test: 0.0920
Epoch: 15, Loss: 1.9387, Train: 0.1571, Val: 0.1560, Test: 0.1530
Epoch: 16, Loss: 1.9518, Train: 0.1571, Val: 0.1560, Test: 0.1470
Epoch: 17, Loss: 1.9127, Train: 0.3071, Val: 0.2200, Test: 0.2280
Epoch: 18, Loss: 1.9399, Train: 0.3500, Val: 0.2160, Test: 0.2190
Epoch: 19, Loss: 1.9170, Train: 0.3000, Val: 0.1440, Test: 0.1660
Epoch: 20, Loss: 1.9293, Train: 0.2857, Val: 0.1220, Test: 0.1480
Epoch: 21, Loss: 1.8977, Train: 0.2857, Val: 0.1240, Test: 0.1520
Epoch: 22, Loss: 1.9040, Train: 0.3929, Val: 0.2420, Test: 0.2870
Epoch: 23, Loss: 1.8713, Train: 0.2857, Val: 0.3720, Test: 0.3620
Epoch: 24, Loss: 1.8437, Train: 0.3214, Val: 0.4020, Test: 0.4200
Epoch: 25, Loss: 1.8061, Train: 0.3714, Val: 0.4040, Test: 0.4350
Epoch: 26, Loss: 1.7384, Train: 0.3500, Val: 0.3940, Test: 0.4230
Epoch: 27, Loss: 1.6680, Train: 0.3357, Val: 0.3840, Test: 0.4100
Epoch: 28, Loss: 1.5390, Train: 0.3714, Val: 0.4140, Test: 0.4460
Epoch: 29, Loss: 1.4494, Train: 0.4286, Val: 0.4620, Test: 0.4910
Epoch: 30, Loss: 1.4127, Train: 0.3286, Val: 0.3780, Test: 0.3830
Epoch: 31, Loss: 1.4425, Train: 0.4357, Val: 0.4900, Test: 0.4960
Epoch: 32, Loss: 1.3608, Train: 0.6786, Val: 0.5780, Test: 0.5920
Epoch: 33, Loss: 1.1752, Train: 0.5857, Val: 0.4980, Test: 0.4770
Epoch: 34, Loss: 1.1732, Train: 0.6143, Val: 0.5620, Test: 0.5470
Epoch: 35, Loss: 1.1231, Train: 0.6286, Val: 0.6440, Test: 0.6310
Epoch: 36, Loss: 1.0613, Train: 0.6143, Val: 0.6040, Test: 0.5910
Epoch: 37, Loss: 1.0048, Train: 0.6571, Val: 0.6560, Test: 0.6380
Epoch: 38, Loss: 0.9428, Train: 0.6857, Val: 0.6300, Test: 0.6190
Epoch: 39, Loss: 0.8681, Train: 0.6786, Val: 0.6260, Test: 0.6260
Epoch: 40, Loss: 0.8513, Train: 0.6857, Val: 0.6440, Test: 0.6390
Epoch: 41, Loss: 0.8444, Train: 0.7214, Val: 0.6360, Test: 0.6470
Epoch: 42, Loss: 0.7718, Train: 0.7786, Val: 0.6340, Test: 0.6550
Epoch: 43, Loss: 0.8096, Train: 0.7786, Val: 0.5960, Test: 0.6170
Epoch: 44, Loss: 0.7373, Train: 0.7857, Val: 0.5740, Test: 0.6140
Epoch: 45, Loss: 0.7177, Train: 0.7429, Val: 0.5900, Test: 0.6130
Epoch: 46, Loss: 0.7015, Train: 0.7071, Val: 0.5880, Test: 0.5990
Epoch: 47, Loss: 0.6674, Train: 0.8357, Val: 0.5980, Test: 0.6170
Epoch: 48, Loss: 0.6894, Train: 0.8500, Val: 0.6120, Test: 0.6400
Epoch: 49, Loss: 0.5864, Train: 0.8286, Val: 0.6280, Test: 0.6540
Epoch: 50, Loss: 0.5815, Train: 0.8357, Val: 0.6240, Test: 0.6540
MAD:  0.4099
Best Test Accuracy: 0.6550, Val Accuracy: 0.6340, Train Accuracy: 0.7786
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9765, Train: 0.1643, Val: 0.0800, Test: 0.0720
Epoch: 2, Loss: 1.9993, Train: 0.1643, Val: 0.0620, Test: 0.0670
Epoch: 3, Loss: 1.9832, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 4, Loss: 1.9568, Train: 0.1714, Val: 0.0760, Test: 0.0810
Epoch: 5, Loss: 1.9490, Train: 0.1429, Val: 0.3160, Test: 0.3210
Epoch: 6, Loss: 1.9536, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 7, Loss: 1.9633, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 8, Loss: 1.9659, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 9, Loss: 1.9300, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 10, Loss: 1.9333, Train: 0.1357, Val: 0.3160, Test: 0.3150
Epoch: 11, Loss: 1.9421, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 12, Loss: 1.9408, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 13, Loss: 1.9366, Train: 0.1714, Val: 0.0720, Test: 0.0800
Epoch: 14, Loss: 1.9598, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 15, Loss: 1.9149, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 16, Loss: 1.9463, Train: 0.1571, Val: 0.0680, Test: 0.0690
Epoch: 17, Loss: 1.9644, Train: 0.1643, Val: 0.1560, Test: 0.1460
Epoch: 18, Loss: 1.9051, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 19, Loss: 1.9554, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 20, Loss: 1.9642, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 21, Loss: 1.9748, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 22, Loss: 1.9415, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 23, Loss: 1.9399, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 24, Loss: 1.9443, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 25, Loss: 1.9163, Train: 0.2000, Val: 0.2980, Test: 0.2990
Epoch: 26, Loss: 1.9191, Train: 0.2000, Val: 0.3160, Test: 0.3310
Epoch: 27, Loss: 1.9535, Train: 0.2071, Val: 0.3200, Test: 0.3320
Epoch: 28, Loss: 1.9309, Train: 0.2214, Val: 0.3300, Test: 0.3350
Epoch: 29, Loss: 1.9321, Train: 0.2500, Val: 0.3380, Test: 0.3390
Epoch: 30, Loss: 1.9231, Train: 0.4786, Val: 0.2980, Test: 0.2990
Epoch: 31, Loss: 1.9047, Train: 0.4429, Val: 0.2400, Test: 0.2380
Epoch: 32, Loss: 1.8696, Train: 0.3786, Val: 0.1740, Test: 0.2180
Epoch: 33, Loss: 1.8602, Train: 0.3286, Val: 0.1480, Test: 0.1750
Epoch: 34, Loss: 1.7928, Train: 0.3714, Val: 0.2580, Test: 0.2860
Epoch: 35, Loss: 1.7453, Train: 0.3786, Val: 0.3120, Test: 0.3490
Epoch: 36, Loss: 1.6863, Train: 0.3286, Val: 0.2320, Test: 0.2280
Epoch: 37, Loss: 1.6381, Train: 0.4286, Val: 0.3440, Test: 0.3770
Epoch: 38, Loss: 1.5556, Train: 0.4429, Val: 0.3460, Test: 0.3680
Epoch: 39, Loss: 1.4436, Train: 0.4929, Val: 0.3780, Test: 0.4160
Epoch: 40, Loss: 1.3927, Train: 0.5286, Val: 0.3240, Test: 0.3590
Epoch: 41, Loss: 1.2281, Train: 0.6357, Val: 0.5400, Test: 0.5370
Epoch: 42, Loss: 1.2792, Train: 0.6429, Val: 0.4320, Test: 0.4460
Epoch: 43, Loss: 1.1347, Train: 0.6143, Val: 0.4520, Test: 0.4420
Epoch: 44, Loss: 1.0508, Train: 0.6929, Val: 0.5180, Test: 0.5140
Epoch: 45, Loss: 0.9849, Train: 0.7000, Val: 0.4880, Test: 0.4840
Epoch: 46, Loss: 0.9382, Train: 0.6143, Val: 0.4740, Test: 0.4570
Epoch: 47, Loss: 0.8593, Train: 0.7071, Val: 0.5000, Test: 0.4960
Epoch: 48, Loss: 0.8298, Train: 0.6214, Val: 0.4260, Test: 0.4190
Epoch: 49, Loss: 0.8473, Train: 0.7429, Val: 0.4920, Test: 0.4820
Epoch: 50, Loss: 0.7057, Train: 0.7357, Val: 0.4880, Test: 0.4780
MAD:  0.2952
Best Test Accuracy: 0.5370, Val Accuracy: 0.5400, Train Accuracy: 0.6357
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0137, Train: 0.1429, Val: 0.1160, Test: 0.1050
Epoch: 2, Loss: 1.9474, Train: 0.1571, Val: 0.1220, Test: 0.1350
Epoch: 3, Loss: 1.9473, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 4, Loss: 1.9326, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 5, Loss: 1.9875, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 6, Loss: 1.9360, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 7, Loss: 1.9765, Train: 0.1500, Val: 0.0760, Test: 0.0900
Epoch: 8, Loss: 1.9286, Train: 0.1500, Val: 0.1560, Test: 0.1440
Epoch: 9, Loss: 1.9375, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 10, Loss: 1.9646, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 11, Loss: 1.9745, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 12, Loss: 1.9537, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 13, Loss: 1.9450, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 14, Loss: 1.9757, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 15, Loss: 1.9552, Train: 0.1786, Val: 0.1700, Test: 0.1600
Epoch: 16, Loss: 1.9345, Train: 0.1714, Val: 0.1860, Test: 0.1860
Epoch: 17, Loss: 1.9409, Train: 0.1500, Val: 0.3140, Test: 0.3230
Epoch: 18, Loss: 1.9270, Train: 0.1857, Val: 0.3340, Test: 0.3340
Epoch: 19, Loss: 1.9386, Train: 0.2571, Val: 0.3800, Test: 0.3870
Epoch: 20, Loss: 1.9327, Train: 0.2714, Val: 0.3740, Test: 0.3740
Epoch: 21, Loss: 1.9057, Train: 0.2286, Val: 0.2540, Test: 0.2500
Epoch: 22, Loss: 1.9390, Train: 0.1929, Val: 0.1880, Test: 0.1900
Epoch: 23, Loss: 1.9246, Train: 0.3214, Val: 0.3660, Test: 0.3780
Epoch: 24, Loss: 1.8834, Train: 0.4214, Val: 0.4260, Test: 0.4470
Epoch: 25, Loss: 1.8735, Train: 0.4214, Val: 0.4120, Test: 0.4480
Epoch: 26, Loss: 1.8792, Train: 0.4429, Val: 0.3340, Test: 0.3540
Epoch: 27, Loss: 1.7993, Train: 0.2643, Val: 0.2360, Test: 0.2220
Epoch: 28, Loss: 1.7264, Train: 0.3429, Val: 0.2600, Test: 0.2410
Epoch: 29, Loss: 1.6853, Train: 0.2857, Val: 0.2440, Test: 0.2280
Epoch: 30, Loss: 1.6216, Train: 0.3214, Val: 0.2780, Test: 0.2610
Epoch: 31, Loss: 1.5629, Train: 0.4500, Val: 0.4740, Test: 0.4660
Epoch: 32, Loss: 1.5073, Train: 0.5143, Val: 0.4220, Test: 0.4390
Epoch: 33, Loss: 1.4332, Train: 0.5286, Val: 0.4200, Test: 0.4170
Epoch: 34, Loss: 1.4106, Train: 0.6286, Val: 0.4700, Test: 0.4440
Epoch: 35, Loss: 1.4732, Train: 0.7357, Val: 0.4960, Test: 0.4750
Epoch: 36, Loss: 1.3123, Train: 0.7143, Val: 0.5100, Test: 0.5150
Epoch: 37, Loss: 1.2154, Train: 0.7071, Val: 0.4900, Test: 0.4670
Epoch: 38, Loss: 1.1596, Train: 0.7500, Val: 0.5260, Test: 0.5320
Epoch: 39, Loss: 1.0293, Train: 0.7786, Val: 0.5340, Test: 0.5360
Epoch: 40, Loss: 0.9117, Train: 0.7643, Val: 0.5360, Test: 0.5470
Epoch: 41, Loss: 0.8267, Train: 0.7857, Val: 0.5580, Test: 0.5580
Epoch: 42, Loss: 0.7344, Train: 0.8286, Val: 0.5840, Test: 0.5650
Epoch: 43, Loss: 0.6319, Train: 0.8429, Val: 0.5720, Test: 0.5630
Epoch: 44, Loss: 0.5885, Train: 0.8500, Val: 0.5680, Test: 0.5730
Epoch: 45, Loss: 0.5503, Train: 0.9286, Val: 0.6660, Test: 0.6740
Epoch: 46, Loss: 0.4462, Train: 0.9214, Val: 0.7380, Test: 0.7320
Epoch: 47, Loss: 0.4108, Train: 0.9500, Val: 0.7400, Test: 0.7520
Epoch: 48, Loss: 0.3404, Train: 0.9571, Val: 0.7320, Test: 0.7040
Epoch: 49, Loss: 0.3396, Train: 0.9500, Val: 0.6940, Test: 0.6880
Epoch: 50, Loss: 0.3095, Train: 0.9500, Val: 0.6900, Test: 0.6880
MAD:  0.5056
Best Test Accuracy: 0.7520, Val Accuracy: 0.7400, Train Accuracy: 0.9500
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9937, Train: 0.2000, Val: 0.1680, Test: 0.1620
Epoch: 2, Loss: 1.9910, Train: 0.1143, Val: 0.2140, Test: 0.2130
Epoch: 3, Loss: 1.9633, Train: 0.1429, Val: 0.2780, Test: 0.2790
Epoch: 4, Loss: 1.9546, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 5, Loss: 1.9596, Train: 0.1500, Val: 0.1620, Test: 0.1470
Epoch: 6, Loss: 1.9712, Train: 0.1429, Val: 0.1580, Test: 0.1440
Epoch: 7, Loss: 1.9396, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 8, Loss: 1.9411, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 9, Loss: 1.9334, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 10, Loss: 1.9578, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 11, Loss: 1.9402, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 12, Loss: 1.9380, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 13, Loss: 1.9465, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 14, Loss: 1.9709, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 15, Loss: 1.9401, Train: 0.2500, Val: 0.1460, Test: 0.1350
Epoch: 16, Loss: 1.9368, Train: 0.2786, Val: 0.1580, Test: 0.1470
Epoch: 17, Loss: 1.9353, Train: 0.2000, Val: 0.1220, Test: 0.1160
Epoch: 18, Loss: 1.9422, Train: 0.1571, Val: 0.1340, Test: 0.1380
Epoch: 19, Loss: 1.9332, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 20, Loss: 1.9406, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 21, Loss: 1.9534, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 22, Loss: 1.9635, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 23, Loss: 1.9310, Train: 0.1786, Val: 0.1560, Test: 0.1470
Epoch: 24, Loss: 1.9337, Train: 0.1929, Val: 0.1560, Test: 0.1550
Epoch: 25, Loss: 1.9281, Train: 0.2357, Val: 0.1720, Test: 0.1600
Epoch: 26, Loss: 1.9236, Train: 0.2857, Val: 0.2000, Test: 0.2000
Epoch: 27, Loss: 1.9323, Train: 0.2071, Val: 0.3280, Test: 0.3330
Epoch: 28, Loss: 1.9231, Train: 0.3000, Val: 0.3640, Test: 0.3740
Epoch: 29, Loss: 1.9089, Train: 0.3429, Val: 0.3940, Test: 0.3770
Epoch: 30, Loss: 1.8854, Train: 0.2571, Val: 0.1600, Test: 0.1470
Epoch: 31, Loss: 1.8291, Train: 0.2857, Val: 0.2040, Test: 0.1740
Epoch: 32, Loss: 1.8405, Train: 0.3857, Val: 0.2640, Test: 0.2330
Epoch: 33, Loss: 1.8047, Train: 0.5357, Val: 0.3120, Test: 0.3360
Epoch: 34, Loss: 1.7099, Train: 0.4214, Val: 0.2500, Test: 0.2300
Epoch: 35, Loss: 1.6736, Train: 0.5429, Val: 0.3040, Test: 0.3390
Epoch: 36, Loss: 1.5753, Train: 0.5571, Val: 0.3320, Test: 0.3250
Epoch: 37, Loss: 1.4888, Train: 0.5500, Val: 0.3500, Test: 0.3210
Epoch: 38, Loss: 1.3927, Train: 0.5857, Val: 0.3800, Test: 0.3550
Epoch: 39, Loss: 1.3697, Train: 0.5500, Val: 0.3580, Test: 0.3410
Epoch: 40, Loss: 1.3423, Train: 0.6000, Val: 0.3780, Test: 0.3770
Epoch: 41, Loss: 1.1666, Train: 0.4714, Val: 0.3280, Test: 0.3240
Epoch: 42, Loss: 1.2909, Train: 0.7000, Val: 0.5020, Test: 0.4810
Epoch: 43, Loss: 1.0675, Train: 0.7643, Val: 0.5200, Test: 0.5120
Epoch: 44, Loss: 1.0881, Train: 0.7643, Val: 0.5300, Test: 0.5420
Epoch: 45, Loss: 0.9158, Train: 0.6643, Val: 0.4940, Test: 0.4870
Epoch: 46, Loss: 0.9629, Train: 0.7429, Val: 0.5400, Test: 0.5510
Epoch: 47, Loss: 0.7760, Train: 0.8071, Val: 0.5980, Test: 0.6190
Epoch: 48, Loss: 0.7722, Train: 0.8429, Val: 0.6380, Test: 0.6400
Epoch: 49, Loss: 0.7411, Train: 0.9357, Val: 0.7040, Test: 0.7360
Epoch: 50, Loss: 0.5918, Train: 0.9286, Val: 0.6760, Test: 0.7190
MAD:  0.4094
Best Test Accuracy: 0.7360, Val Accuracy: 0.7040, Train Accuracy: 0.9357
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9842, Train: 0.1429, Val: 0.0780, Test: 0.0930
Epoch: 2, Loss: 1.9642, Train: 0.1571, Val: 0.0740, Test: 0.0940
Epoch: 3, Loss: 1.9572, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 4, Loss: 1.9557, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 5, Loss: 1.9640, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 6, Loss: 1.9476, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 7, Loss: 1.9512, Train: 0.1500, Val: 0.1360, Test: 0.1360
Epoch: 8, Loss: 1.9299, Train: 0.1500, Val: 0.0580, Test: 0.0660
Epoch: 9, Loss: 1.9571, Train: 0.1571, Val: 0.0820, Test: 0.0980
Epoch: 10, Loss: 1.9368, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 11, Loss: 1.9681, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 12, Loss: 1.9430, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 13, Loss: 1.9419, Train: 0.2857, Val: 0.2120, Test: 0.2150
Epoch: 14, Loss: 1.9449, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 15, Loss: 1.9672, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 16, Loss: 1.9358, Train: 0.1857, Val: 0.0720, Test: 0.0950
Epoch: 17, Loss: 1.9399, Train: 0.2143, Val: 0.0760, Test: 0.1030
Epoch: 18, Loss: 1.9290, Train: 0.4500, Val: 0.3080, Test: 0.2870
Epoch: 19, Loss: 1.9220, Train: 0.5000, Val: 0.3480, Test: 0.3420
Epoch: 20, Loss: 1.9090, Train: 0.4500, Val: 0.2940, Test: 0.2840
Epoch: 21, Loss: 1.9083, Train: 0.4000, Val: 0.3100, Test: 0.2840
Epoch: 22, Loss: 1.8881, Train: 0.4286, Val: 0.3160, Test: 0.2970
Epoch: 23, Loss: 1.8953, Train: 0.3357, Val: 0.2440, Test: 0.2310
Epoch: 24, Loss: 1.8627, Train: 0.3929, Val: 0.2960, Test: 0.2880
Epoch: 25, Loss: 1.7953, Train: 0.3286, Val: 0.2440, Test: 0.2440
Epoch: 26, Loss: 1.7120, Train: 0.3429, Val: 0.2440, Test: 0.2440
Epoch: 27, Loss: 1.6381, Train: 0.3429, Val: 0.2460, Test: 0.2420
Epoch: 28, Loss: 1.5967, Train: 0.3143, Val: 0.2400, Test: 0.2430
Epoch: 29, Loss: 1.5529, Train: 0.4071, Val: 0.2900, Test: 0.2990
Epoch: 30, Loss: 1.5037, Train: 0.4571, Val: 0.3120, Test: 0.3290
Epoch: 31, Loss: 1.4199, Train: 0.4929, Val: 0.3360, Test: 0.3450
Epoch: 32, Loss: 1.3664, Train: 0.6143, Val: 0.4540, Test: 0.4620
Epoch: 33, Loss: 1.2288, Train: 0.6571, Val: 0.4780, Test: 0.4640
Epoch: 34, Loss: 1.1467, Train: 0.6500, Val: 0.4400, Test: 0.4390
Epoch: 35, Loss: 1.1635, Train: 0.5714, Val: 0.4060, Test: 0.3970
Epoch: 36, Loss: 1.3053, Train: 0.8000, Val: 0.6000, Test: 0.6040
Epoch: 37, Loss: 0.9488, Train: 0.6357, Val: 0.4480, Test: 0.4600
Epoch: 38, Loss: 0.9646, Train: 0.7143, Val: 0.4700, Test: 0.5040
Epoch: 39, Loss: 0.8771, Train: 0.8571, Val: 0.6500, Test: 0.6320
Epoch: 40, Loss: 0.7482, Train: 0.8643, Val: 0.6900, Test: 0.6680
Epoch: 41, Loss: 0.6825, Train: 0.8357, Val: 0.6440, Test: 0.6630
Epoch: 42, Loss: 0.6839, Train: 0.9286, Val: 0.7400, Test: 0.7240
Epoch: 43, Loss: 0.5550, Train: 0.9571, Val: 0.7460, Test: 0.7370
Epoch: 44, Loss: 0.4495, Train: 0.9286, Val: 0.7160, Test: 0.7250
Epoch: 45, Loss: 0.4645, Train: 0.9786, Val: 0.7380, Test: 0.7630
Epoch: 46, Loss: 0.3502, Train: 0.9571, Val: 0.7420, Test: 0.7690
Epoch: 47, Loss: 0.3657, Train: 0.9643, Val: 0.7380, Test: 0.7610
Epoch: 48, Loss: 0.2759, Train: 0.9786, Val: 0.7640, Test: 0.7490
Epoch: 49, Loss: 0.2347, Train: 0.9786, Val: 0.7600, Test: 0.7520
Epoch: 50, Loss: 0.2175, Train: 0.9857, Val: 0.7560, Test: 0.7500
MAD:  0.4909
Best Test Accuracy: 0.7690, Val Accuracy: 0.7420, Train Accuracy: 0.9571
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9866, Train: 0.1143, Val: 0.1160, Test: 0.1260
Epoch: 2, Loss: 1.9431, Train: 0.1500, Val: 0.1220, Test: 0.1320
Epoch: 3, Loss: 1.9392, Train: 0.1500, Val: 0.1240, Test: 0.1320
Epoch: 4, Loss: 1.9429, Train: 0.2429, Val: 0.3300, Test: 0.3320
Epoch: 5, Loss: 1.9483, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 6, Loss: 1.9859, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 7, Loss: 1.9827, Train: 0.1429, Val: 0.1140, Test: 0.1030
Epoch: 8, Loss: 1.9520, Train: 0.1500, Val: 0.0740, Test: 0.0920
Epoch: 9, Loss: 1.9223, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 10, Loss: 1.9436, Train: 0.1786, Val: 0.0720, Test: 0.0940
Epoch: 11, Loss: 1.9272, Train: 0.2214, Val: 0.0820, Test: 0.1080
Epoch: 12, Loss: 1.9503, Train: 0.2714, Val: 0.1880, Test: 0.1830
Epoch: 13, Loss: 1.9543, Train: 0.3571, Val: 0.2300, Test: 0.2330
Epoch: 14, Loss: 1.8931, Train: 0.2429, Val: 0.1240, Test: 0.1310
Epoch: 15, Loss: 1.9619, Train: 0.3357, Val: 0.2000, Test: 0.2080
Epoch: 16, Loss: 1.9247, Train: 0.2786, Val: 0.1640, Test: 0.1750
Epoch: 17, Loss: 1.9014, Train: 0.2929, Val: 0.1660, Test: 0.1750
Epoch: 18, Loss: 1.9292, Train: 0.4071, Val: 0.2220, Test: 0.2450
Epoch: 19, Loss: 1.8521, Train: 0.3643, Val: 0.1800, Test: 0.2160
Epoch: 20, Loss: 1.8089, Train: 0.3571, Val: 0.1820, Test: 0.1910
Epoch: 21, Loss: 1.7346, Train: 0.2929, Val: 0.1340, Test: 0.1680
Epoch: 22, Loss: 1.6747, Train: 0.3000, Val: 0.1240, Test: 0.1600
Epoch: 23, Loss: 1.6345, Train: 0.3286, Val: 0.1460, Test: 0.1750
Epoch: 24, Loss: 1.6093, Train: 0.3643, Val: 0.1740, Test: 0.1940
Epoch: 25, Loss: 1.5932, Train: 0.3500, Val: 0.1680, Test: 0.1920
Epoch: 26, Loss: 1.5365, Train: 0.4143, Val: 0.1940, Test: 0.2110
Epoch: 27, Loss: 1.4875, Train: 0.4500, Val: 0.2120, Test: 0.2390
Epoch: 28, Loss: 1.3970, Train: 0.4571, Val: 0.2460, Test: 0.2640
Epoch: 29, Loss: 1.4348, Train: 0.4571, Val: 0.2480, Test: 0.2570
Epoch: 30, Loss: 1.3530, Train: 0.5214, Val: 0.2660, Test: 0.3010
Epoch: 31, Loss: 1.3526, Train: 0.4143, Val: 0.2720, Test: 0.2740
Epoch: 32, Loss: 1.2887, Train: 0.6643, Val: 0.3980, Test: 0.4240
Epoch: 33, Loss: 1.2003, Train: 0.7643, Val: 0.5140, Test: 0.5460
Epoch: 34, Loss: 1.2043, Train: 0.6571, Val: 0.4600, Test: 0.4610
Epoch: 35, Loss: 1.1581, Train: 0.5857, Val: 0.4640, Test: 0.4400
Epoch: 36, Loss: 1.0918, Train: 0.6071, Val: 0.5400, Test: 0.4830
Epoch: 37, Loss: 1.0482, Train: 0.6429, Val: 0.5320, Test: 0.4990
Epoch: 38, Loss: 0.9978, Train: 0.7143, Val: 0.6020, Test: 0.5690
Epoch: 39, Loss: 0.9166, Train: 0.8071, Val: 0.6520, Test: 0.6230
Epoch: 40, Loss: 0.9480, Train: 0.7857, Val: 0.6640, Test: 0.6500
Epoch: 41, Loss: 0.8225, Train: 0.7857, Val: 0.6760, Test: 0.6380
Epoch: 42, Loss: 0.8177, Train: 0.8071, Val: 0.6760, Test: 0.6440
Epoch: 43, Loss: 0.7185, Train: 0.8286, Val: 0.6400, Test: 0.6440
Epoch: 44, Loss: 0.6576, Train: 0.8357, Val: 0.6380, Test: 0.6440
Epoch: 45, Loss: 0.6361, Train: 0.8786, Val: 0.6840, Test: 0.6870
Epoch: 46, Loss: 0.5857, Train: 0.9286, Val: 0.7340, Test: 0.7130
Epoch: 47, Loss: 0.5082, Train: 0.9643, Val: 0.7400, Test: 0.7330
Epoch: 48, Loss: 0.4542, Train: 0.9000, Val: 0.7020, Test: 0.6880
Epoch: 49, Loss: 0.5652, Train: 0.9643, Val: 0.7380, Test: 0.7330
Epoch: 50, Loss: 0.3199, Train: 0.9143, Val: 0.6740, Test: 0.6950
MAD:  0.4905
Best Test Accuracy: 0.7330, Val Accuracy: 0.7400, Train Accuracy: 0.9643
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9361, Train: 0.1500, Val: 0.1280, Test: 0.1410
Epoch: 2, Loss: 1.9402, Train: 0.1929, Val: 0.2240, Test: 0.2070
Epoch: 3, Loss: 1.9456, Train: 0.1429, Val: 0.1720, Test: 0.1500
Epoch: 4, Loss: 1.9668, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 5, Loss: 1.9396, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 6, Loss: 1.9443, Train: 0.1429, Val: 0.1560, Test: 0.1470
Epoch: 7, Loss: 1.9379, Train: 0.2357, Val: 0.2240, Test: 0.2020
Epoch: 8, Loss: 1.9659, Train: 0.1643, Val: 0.1240, Test: 0.1350
Epoch: 9, Loss: 1.9382, Train: 0.1500, Val: 0.1220, Test: 0.1330
Epoch: 10, Loss: 1.9564, Train: 0.2286, Val: 0.0840, Test: 0.1100
Epoch: 11, Loss: 1.9551, Train: 0.2929, Val: 0.1140, Test: 0.1440
Epoch: 12, Loss: 1.9294, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 13, Loss: 1.9450, Train: 0.2071, Val: 0.0740, Test: 0.0800
Epoch: 14, Loss: 1.9312, Train: 0.2571, Val: 0.1380, Test: 0.1260
Epoch: 15, Loss: 1.8971, Train: 0.4357, Val: 0.2740, Test: 0.2690
Epoch: 16, Loss: 1.9333, Train: 0.2143, Val: 0.1660, Test: 0.1680
Epoch: 17, Loss: 1.9504, Train: 0.2429, Val: 0.1880, Test: 0.1730
Epoch: 18, Loss: 1.9023, Train: 0.4286, Val: 0.3040, Test: 0.2970
Epoch: 19, Loss: 1.8529, Train: 0.6929, Val: 0.5080, Test: 0.4860
Epoch: 20, Loss: 1.8429, Train: 0.6357, Val: 0.5140, Test: 0.4910
Epoch: 21, Loss: 1.8093, Train: 0.6786, Val: 0.5160, Test: 0.4980
Epoch: 22, Loss: 1.7335, Train: 0.7143, Val: 0.5500, Test: 0.5240
Epoch: 23, Loss: 1.6068, Train: 0.7214, Val: 0.6200, Test: 0.6100
Epoch: 24, Loss: 1.4929, Train: 0.7071, Val: 0.5940, Test: 0.5950
Epoch: 25, Loss: 1.3436, Train: 0.6929, Val: 0.5780, Test: 0.5840
Epoch: 26, Loss: 1.2063, Train: 0.6714, Val: 0.5380, Test: 0.5360
Epoch: 27, Loss: 1.0613, Train: 0.6714, Val: 0.5460, Test: 0.5120
Epoch: 28, Loss: 0.9522, Train: 0.6929, Val: 0.5340, Test: 0.5130
Epoch: 29, Loss: 0.8753, Train: 0.8000, Val: 0.5880, Test: 0.5710
Epoch: 30, Loss: 0.7552, Train: 0.9143, Val: 0.6760, Test: 0.6860
Epoch: 31, Loss: 0.7313, Train: 0.8500, Val: 0.7020, Test: 0.7010
Epoch: 32, Loss: 0.6607, Train: 0.8714, Val: 0.7140, Test: 0.7100
Epoch: 33, Loss: 0.6034, Train: 0.9286, Val: 0.7620, Test: 0.7430
Epoch: 34, Loss: 0.5224, Train: 0.9357, Val: 0.7100, Test: 0.6980
Epoch: 35, Loss: 0.4342, Train: 0.9143, Val: 0.7080, Test: 0.7020
Epoch: 36, Loss: 0.4115, Train: 0.9571, Val: 0.7480, Test: 0.7490
Epoch: 37, Loss: 0.3687, Train: 0.9500, Val: 0.7460, Test: 0.7620
Epoch: 38, Loss: 0.3213, Train: 0.9286, Val: 0.6980, Test: 0.7180
Epoch: 39, Loss: 0.3278, Train: 0.9357, Val: 0.7020, Test: 0.7060
Epoch: 40, Loss: 0.3169, Train: 0.9857, Val: 0.7320, Test: 0.7400
Epoch: 41, Loss: 0.2081, Train: 0.9643, Val: 0.7440, Test: 0.7380
Epoch: 42, Loss: 0.2661, Train: 0.9857, Val: 0.7500, Test: 0.7520
Epoch: 43, Loss: 0.1722, Train: 0.9643, Val: 0.7160, Test: 0.7230
Epoch: 44, Loss: 0.1937, Train: 0.9929, Val: 0.7220, Test: 0.7210
Epoch: 45, Loss: 0.2022, Train: 0.9786, Val: 0.7420, Test: 0.7300
Epoch: 46, Loss: 0.1355, Train: 1.0000, Val: 0.7560, Test: 0.7670
Epoch: 47, Loss: 0.0964, Train: 0.9857, Val: 0.7620, Test: 0.7800
Epoch: 48, Loss: 0.1310, Train: 0.9857, Val: 0.7540, Test: 0.7640
Epoch: 49, Loss: 0.1033, Train: 0.9786, Val: 0.7340, Test: 0.7190
Epoch: 50, Loss: 0.0701, Train: 0.9500, Val: 0.7140, Test: 0.7000
MAD:  0.5307
Best Test Accuracy: 0.7800, Val Accuracy: 0.7620, Train Accuracy: 0.9857
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-8): 8 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (9): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0051, Train: 0.1429, Val: 0.0740, Test: 0.0930
Epoch: 2, Loss: 1.9656, Train: 0.1500, Val: 0.3140, Test: 0.3190
Epoch: 3, Loss: 1.9368, Train: 0.1429, Val: 0.3160, Test: 0.3190
Epoch: 4, Loss: 1.9609, Train: 0.1500, Val: 0.1600, Test: 0.1490
Epoch: 5, Loss: 1.9638, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 6, Loss: 1.9591, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 7, Loss: 1.9704, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 8, Loss: 1.9533, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 9, Loss: 1.9283, Train: 0.1429, Val: 0.1560, Test: 0.1440
Epoch: 10, Loss: 1.9308, Train: 0.1500, Val: 0.1560, Test: 0.1460
Epoch: 11, Loss: 1.9738, Train: 0.2286, Val: 0.1500, Test: 0.1610
Epoch: 12, Loss: 1.9528, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 13, Loss: 1.9473, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 14, Loss: 1.9561, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 15, Loss: 1.9429, Train: 0.1429, Val: 0.0720, Test: 0.0910
Epoch: 16, Loss: 1.9412, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 17, Loss: 1.9573, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 18, Loss: 1.9400, Train: 0.1429, Val: 0.1220, Test: 0.1300
Epoch: 19, Loss: 1.9404, Train: 0.2357, Val: 0.2180, Test: 0.2320
Epoch: 20, Loss: 1.9598, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 21, Loss: 1.9373, Train: 0.1429, Val: 0.1620, Test: 0.1490
Epoch: 22, Loss: 1.9473, Train: 0.2429, Val: 0.2280, Test: 0.2190
Epoch: 23, Loss: 1.9427, Train: 0.2786, Val: 0.3180, Test: 0.2830
Epoch: 24, Loss: 1.9482, Train: 0.3857, Val: 0.3960, Test: 0.3780
Epoch: 25, Loss: 1.9249, Train: 0.2643, Val: 0.1620, Test: 0.1790
Epoch: 26, Loss: 1.9344, Train: 0.2929, Val: 0.1560, Test: 0.1810
Epoch: 27, Loss: 1.8980, Train: 0.3929, Val: 0.2280, Test: 0.2440
Epoch: 28, Loss: 1.8886, Train: 0.4071, Val: 0.2600, Test: 0.2600
Epoch: 29, Loss: 1.8584, Train: 0.3714, Val: 0.2620, Test: 0.2530
Epoch: 30, Loss: 1.8414, Train: 0.3929, Val: 0.2640, Test: 0.2680
Epoch: 31, Loss: 1.7622, Train: 0.3071, Val: 0.2420, Test: 0.2510
Epoch: 32, Loss: 1.7153, Train: 0.2857, Val: 0.2500, Test: 0.2450
Epoch: 33, Loss: 1.6602, Train: 0.3571, Val: 0.3200, Test: 0.2820
Epoch: 34, Loss: 1.6128, Train: 0.3214, Val: 0.2940, Test: 0.2820
Epoch: 35, Loss: 1.5673, Train: 0.3000, Val: 0.2780, Test: 0.2450
Epoch: 36, Loss: 1.5254, Train: 0.4357, Val: 0.3780, Test: 0.3390
Epoch: 37, Loss: 1.4992, Train: 0.5000, Val: 0.3380, Test: 0.3520
Epoch: 38, Loss: 1.3845, Train: 0.4357, Val: 0.3180, Test: 0.3030
Epoch: 39, Loss: 1.5949, Train: 0.5786, Val: 0.3840, Test: 0.3950
Epoch: 40, Loss: 1.4078, Train: 0.4429, Val: 0.2960, Test: 0.2980
Epoch: 41, Loss: 1.3961, Train: 0.4643, Val: 0.3920, Test: 0.3730
Epoch: 42, Loss: 1.3221, Train: 0.5071, Val: 0.4180, Test: 0.3880
Epoch: 43, Loss: 1.2958, Train: 0.4714, Val: 0.4360, Test: 0.4020
Epoch: 44, Loss: 1.3200, Train: 0.6643, Val: 0.5660, Test: 0.5420
Epoch: 45, Loss: 1.1694, Train: 0.5143, Val: 0.3900, Test: 0.4180
Epoch: 46, Loss: 1.2223, Train: 0.6500, Val: 0.5320, Test: 0.5580
Epoch: 47, Loss: 1.0909, Train: 0.6071, Val: 0.5180, Test: 0.5050
Epoch: 48, Loss: 1.1987, Train: 0.7714, Val: 0.6220, Test: 0.6140
Epoch: 49, Loss: 0.9246, Train: 0.5929, Val: 0.4600, Test: 0.4690
Epoch: 50, Loss: 0.9898, Train: 0.7714, Val: 0.5320, Test: 0.5550
MAD:  0.3406
Best Test Accuracy: 0.6140, Val Accuracy: 0.6220, Train Accuracy: 0.7714
Training completed.
Average Test Accuracy:  0.7123000000000002 ± 0.07874776187295739
Average MAD:  0.44716000000000006 ± 0.07545539344539924

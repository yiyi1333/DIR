Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9573, Train: 0.2429, Val: 0.1460, Test: 0.1650
Epoch: 2, Loss: 1.9278, Train: 0.3143, Val: 0.1360, Test: 0.1570
Epoch: 3, Loss: 1.8971, Train: 0.5786, Val: 0.3660, Test: 0.3600
Epoch: 4, Loss: 1.8355, Train: 0.6143, Val: 0.4280, Test: 0.4180
Epoch: 5, Loss: 1.8598, Train: 0.7000, Val: 0.4700, Test: 0.4830
Epoch: 6, Loss: 1.7703, Train: 0.8429, Val: 0.6060, Test: 0.6300
Epoch: 7, Loss: 1.6587, Train: 0.8214, Val: 0.5940, Test: 0.6010
Epoch: 8, Loss: 1.5591, Train: 0.8714, Val: 0.6600, Test: 0.6640
Epoch: 9, Loss: 1.3827, Train: 0.8857, Val: 0.6620, Test: 0.6960
Epoch: 10, Loss: 1.2229, Train: 0.9214, Val: 0.7080, Test: 0.7310
Epoch: 11, Loss: 1.0184, Train: 0.9286, Val: 0.7140, Test: 0.7240
Epoch: 12, Loss: 0.8162, Train: 0.9429, Val: 0.7460, Test: 0.7470
Epoch: 13, Loss: 0.7341, Train: 0.9429, Val: 0.7440, Test: 0.7650
Epoch: 14, Loss: 0.5650, Train: 0.9643, Val: 0.7480, Test: 0.7720
Epoch: 15, Loss: 0.4953, Train: 0.9786, Val: 0.7540, Test: 0.7770
Epoch: 16, Loss: 0.3590, Train: 0.9857, Val: 0.7740, Test: 0.7980
Epoch: 17, Loss: 0.2467, Train: 0.9786, Val: 0.7740, Test: 0.7940
Epoch: 18, Loss: 0.2293, Train: 0.9857, Val: 0.7580, Test: 0.7890
Epoch: 19, Loss: 0.1732, Train: 0.9857, Val: 0.7560, Test: 0.7830
Epoch: 20, Loss: 0.1389, Train: 0.9929, Val: 0.7440, Test: 0.7780
Epoch: 21, Loss: 0.0930, Train: 0.9929, Val: 0.7480, Test: 0.7820
Epoch: 22, Loss: 0.0955, Train: 0.9929, Val: 0.7560, Test: 0.7960
Epoch: 23, Loss: 0.0731, Train: 0.9929, Val: 0.7640, Test: 0.7950
Epoch: 24, Loss: 0.0490, Train: 1.0000, Val: 0.7600, Test: 0.8030
Epoch: 25, Loss: 0.0346, Train: 1.0000, Val: 0.7600, Test: 0.8010
Epoch: 26, Loss: 0.0299, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 27, Loss: 0.0655, Train: 1.0000, Val: 0.7720, Test: 0.8000
Epoch: 28, Loss: 0.0158, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 29, Loss: 0.0202, Train: 1.0000, Val: 0.7460, Test: 0.7810
Epoch: 30, Loss: 0.0289, Train: 0.9929, Val: 0.7460, Test: 0.7750
Epoch: 31, Loss: 0.0168, Train: 1.0000, Val: 0.7520, Test: 0.7780
Epoch: 32, Loss: 0.0103, Train: 1.0000, Val: 0.7580, Test: 0.7860
Epoch: 33, Loss: 0.0092, Train: 1.0000, Val: 0.7580, Test: 0.7920
Epoch: 34, Loss: 0.0140, Train: 1.0000, Val: 0.7620, Test: 0.7840
Epoch: 35, Loss: 0.0046, Train: 1.0000, Val: 0.7500, Test: 0.7820
Epoch: 36, Loss: 0.0068, Train: 1.0000, Val: 0.7420, Test: 0.7770
Epoch: 37, Loss: 0.0108, Train: 1.0000, Val: 0.7280, Test: 0.7700
Epoch: 38, Loss: 0.0084, Train: 1.0000, Val: 0.7320, Test: 0.7710
Epoch: 39, Loss: 0.0041, Train: 1.0000, Val: 0.7360, Test: 0.7670
Epoch: 40, Loss: 0.0073, Train: 1.0000, Val: 0.7300, Test: 0.7630
Epoch: 41, Loss: 0.0028, Train: 0.9929, Val: 0.7320, Test: 0.7620
Epoch: 42, Loss: 0.0445, Train: 1.0000, Val: 0.7360, Test: 0.7620
Epoch: 43, Loss: 0.0022, Train: 1.0000, Val: 0.7260, Test: 0.7440
Epoch: 44, Loss: 0.0022, Train: 0.9857, Val: 0.7300, Test: 0.7300
Epoch: 45, Loss: 0.1192, Train: 1.0000, Val: 0.7360, Test: 0.7590
Epoch: 46, Loss: 0.0020, Train: 1.0000, Val: 0.7480, Test: 0.7660
Epoch: 47, Loss: 0.0027, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 48, Loss: 0.0043, Train: 0.9929, Val: 0.7680, Test: 0.7740
Epoch: 49, Loss: 0.0502, Train: 0.9929, Val: 0.7640, Test: 0.7710
Epoch: 50, Loss: 0.0236, Train: 1.0000, Val: 0.7520, Test: 0.7670
MAD:  0.5677
Best Test Accuracy: 0.8030, Val Accuracy: 0.7600, Train Accuracy: 1.0000
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0043, Train: 0.2214, Val: 0.1400, Test: 0.1590
Epoch: 2, Loss: 1.9527, Train: 0.2786, Val: 0.1980, Test: 0.2110
Epoch: 3, Loss: 1.8950, Train: 0.3214, Val: 0.1720, Test: 0.1930
Epoch: 4, Loss: 1.8486, Train: 0.5929, Val: 0.3740, Test: 0.3880
Epoch: 5, Loss: 1.8212, Train: 0.7643, Val: 0.5480, Test: 0.5660
Epoch: 6, Loss: 1.7552, Train: 0.8429, Val: 0.6320, Test: 0.6500
Epoch: 7, Loss: 1.6392, Train: 0.8857, Val: 0.7040, Test: 0.7030
Epoch: 8, Loss: 1.5179, Train: 0.8929, Val: 0.7140, Test: 0.7360
Epoch: 9, Loss: 1.2949, Train: 0.8929, Val: 0.7080, Test: 0.7180
Epoch: 10, Loss: 1.1067, Train: 0.8857, Val: 0.6640, Test: 0.6960
Epoch: 11, Loss: 0.8917, Train: 0.9071, Val: 0.6940, Test: 0.7190
Epoch: 12, Loss: 0.7583, Train: 0.9143, Val: 0.7200, Test: 0.7390
Epoch: 13, Loss: 0.6445, Train: 0.9357, Val: 0.7400, Test: 0.7480
Epoch: 14, Loss: 0.4697, Train: 0.9786, Val: 0.7580, Test: 0.7820
Epoch: 15, Loss: 0.3532, Train: 0.9857, Val: 0.7640, Test: 0.7760
Epoch: 16, Loss: 0.2775, Train: 0.9929, Val: 0.7560, Test: 0.7770
Epoch: 17, Loss: 0.2093, Train: 0.9929, Val: 0.7520, Test: 0.7760
Epoch: 18, Loss: 0.1781, Train: 0.9929, Val: 0.7680, Test: 0.7790
Epoch: 19, Loss: 0.1277, Train: 0.9929, Val: 0.7740, Test: 0.7830
Epoch: 20, Loss: 0.0849, Train: 0.9857, Val: 0.7620, Test: 0.7730
Epoch: 21, Loss: 0.0867, Train: 1.0000, Val: 0.7580, Test: 0.7730
Epoch: 22, Loss: 0.0621, Train: 1.0000, Val: 0.7580, Test: 0.7650
Epoch: 23, Loss: 0.0401, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 24, Loss: 0.0370, Train: 1.0000, Val: 0.7520, Test: 0.7650
Epoch: 25, Loss: 0.0225, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 26, Loss: 0.0258, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 27, Loss: 0.0119, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 28, Loss: 0.0168, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 29, Loss: 0.0093, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 30, Loss: 0.0102, Train: 1.0000, Val: 0.7480, Test: 0.7770
Epoch: 31, Loss: 0.0060, Train: 1.0000, Val: 0.7540, Test: 0.7800
Epoch: 32, Loss: 0.0082, Train: 1.0000, Val: 0.7560, Test: 0.7850
Epoch: 33, Loss: 0.0097, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 34, Loss: 0.0068, Train: 1.0000, Val: 0.7540, Test: 0.7650
Epoch: 35, Loss: 0.0083, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 36, Loss: 0.0044, Train: 1.0000, Val: 0.7380, Test: 0.7510
Epoch: 37, Loss: 0.0187, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 38, Loss: 0.0027, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 39, Loss: 0.0020, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 40, Loss: 0.0146, Train: 0.9929, Val: 0.7600, Test: 0.7770
Epoch: 41, Loss: 0.0312, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 42, Loss: 0.0042, Train: 0.9857, Val: 0.7460, Test: 0.7480
Epoch: 43, Loss: 0.0360, Train: 0.9929, Val: 0.7240, Test: 0.7320
Epoch: 44, Loss: 0.0202, Train: 0.9929, Val: 0.7060, Test: 0.7170
Epoch: 45, Loss: 0.0247, Train: 0.9786, Val: 0.7000, Test: 0.7140
Epoch: 46, Loss: 0.0278, Train: 0.9929, Val: 0.7240, Test: 0.7420
Epoch: 47, Loss: 0.0024, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 48, Loss: 0.0095, Train: 1.0000, Val: 0.7720, Test: 0.7800
Epoch: 49, Loss: 0.0031, Train: 0.9929, Val: 0.7660, Test: 0.7920
Epoch: 50, Loss: 0.0195, Train: 1.0000, Val: 0.7660, Test: 0.7900
MAD:  0.5638
Best Test Accuracy: 0.7920, Val Accuracy: 0.7660, Train Accuracy: 0.9929
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9994, Train: 0.1500, Val: 0.3220, Test: 0.3220
Epoch: 2, Loss: 1.9480, Train: 0.3214, Val: 0.2040, Test: 0.1760
Epoch: 3, Loss: 1.9308, Train: 0.4571, Val: 0.3000, Test: 0.2980
Epoch: 4, Loss: 1.9081, Train: 0.3786, Val: 0.1960, Test: 0.2100
Epoch: 5, Loss: 1.8740, Train: 0.6214, Val: 0.3800, Test: 0.4080
Epoch: 6, Loss: 1.8005, Train: 0.8071, Val: 0.6120, Test: 0.5950
Epoch: 7, Loss: 1.7510, Train: 0.6929, Val: 0.5280, Test: 0.5190
Epoch: 8, Loss: 1.6718, Train: 0.7929, Val: 0.6600, Test: 0.6490
Epoch: 9, Loss: 1.4942, Train: 0.9000, Val: 0.7200, Test: 0.7430
Epoch: 10, Loss: 1.3886, Train: 0.9071, Val: 0.7560, Test: 0.7840
Epoch: 11, Loss: 1.2230, Train: 0.9286, Val: 0.7620, Test: 0.8060
Epoch: 12, Loss: 0.9885, Train: 0.9071, Val: 0.7620, Test: 0.7890
Epoch: 13, Loss: 0.8148, Train: 0.9429, Val: 0.7640, Test: 0.8030
Epoch: 14, Loss: 0.6602, Train: 0.9500, Val: 0.7720, Test: 0.7900
Epoch: 15, Loss: 0.5412, Train: 0.9643, Val: 0.7660, Test: 0.7870
Epoch: 16, Loss: 0.4861, Train: 0.9714, Val: 0.7780, Test: 0.7840
Epoch: 17, Loss: 0.3837, Train: 0.9714, Val: 0.7760, Test: 0.7880
Epoch: 18, Loss: 0.2801, Train: 0.9786, Val: 0.7720, Test: 0.7850
Epoch: 19, Loss: 0.2486, Train: 0.9857, Val: 0.7680, Test: 0.7860
Epoch: 20, Loss: 0.1710, Train: 0.9929, Val: 0.7660, Test: 0.7870
Epoch: 21, Loss: 0.1428, Train: 0.9929, Val: 0.7740, Test: 0.7890
Epoch: 22, Loss: 0.0941, Train: 0.9929, Val: 0.7800, Test: 0.8000
Epoch: 23, Loss: 0.0677, Train: 1.0000, Val: 0.7720, Test: 0.8030
Epoch: 24, Loss: 0.0563, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 25, Loss: 0.0438, Train: 1.0000, Val: 0.7740, Test: 0.7900
Epoch: 26, Loss: 0.0400, Train: 1.0000, Val: 0.7720, Test: 0.7840
Epoch: 27, Loss: 0.0231, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 28, Loss: 0.0222, Train: 1.0000, Val: 0.7680, Test: 0.7820
Epoch: 29, Loss: 0.0155, Train: 1.0000, Val: 0.7700, Test: 0.7820
Epoch: 30, Loss: 0.0245, Train: 1.0000, Val: 0.7640, Test: 0.7740
Epoch: 31, Loss: 0.0185, Train: 1.0000, Val: 0.7560, Test: 0.7720
Epoch: 32, Loss: 0.0202, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 33, Loss: 0.0116, Train: 1.0000, Val: 0.7540, Test: 0.7780
Epoch: 34, Loss: 0.0297, Train: 1.0000, Val: 0.7600, Test: 0.7780
Epoch: 35, Loss: 0.0054, Train: 0.9857, Val: 0.7500, Test: 0.7740
Epoch: 36, Loss: 0.0146, Train: 0.9857, Val: 0.7440, Test: 0.7630
Epoch: 37, Loss: 0.0454, Train: 1.0000, Val: 0.7320, Test: 0.7350
Epoch: 38, Loss: 0.0210, Train: 0.9929, Val: 0.7360, Test: 0.7270
Epoch: 39, Loss: 0.0103, Train: 0.9929, Val: 0.7280, Test: 0.7270
Epoch: 40, Loss: 0.0152, Train: 1.0000, Val: 0.7200, Test: 0.7330
Epoch: 41, Loss: 0.0201, Train: 1.0000, Val: 0.7280, Test: 0.7430
Epoch: 42, Loss: 0.0102, Train: 1.0000, Val: 0.7420, Test: 0.7560
Epoch: 43, Loss: 0.0058, Train: 1.0000, Val: 0.7540, Test: 0.7660
Epoch: 44, Loss: 0.0070, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 45, Loss: 0.0194, Train: 1.0000, Val: 0.7540, Test: 0.7660
Epoch: 46, Loss: 0.0107, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 47, Loss: 0.0069, Train: 1.0000, Val: 0.7560, Test: 0.7840
Epoch: 48, Loss: 0.0036, Train: 1.0000, Val: 0.7560, Test: 0.7800
Epoch: 49, Loss: 0.0029, Train: 1.0000, Val: 0.7600, Test: 0.7770
Epoch: 50, Loss: 0.0045, Train: 1.0000, Val: 0.7540, Test: 0.7730
MAD:  0.2908
Best Test Accuracy: 0.8060, Val Accuracy: 0.7620, Train Accuracy: 0.9286
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9813, Train: 0.2500, Val: 0.1400, Test: 0.1630
Epoch: 2, Loss: 1.9233, Train: 0.4214, Val: 0.2360, Test: 0.2560
Epoch: 3, Loss: 1.9326, Train: 0.5357, Val: 0.3560, Test: 0.3340
Epoch: 4, Loss: 1.8734, Train: 0.5429, Val: 0.3600, Test: 0.3520
Epoch: 5, Loss: 1.8243, Train: 0.7571, Val: 0.5620, Test: 0.5550
Epoch: 6, Loss: 1.7847, Train: 0.8429, Val: 0.6800, Test: 0.6950
Epoch: 7, Loss: 1.7092, Train: 0.8929, Val: 0.6700, Test: 0.6630
Epoch: 8, Loss: 1.5552, Train: 0.8857, Val: 0.6780, Test: 0.6780
Epoch: 9, Loss: 1.3893, Train: 0.8571, Val: 0.6660, Test: 0.6630
Epoch: 10, Loss: 1.1951, Train: 0.8714, Val: 0.6640, Test: 0.6550
Epoch: 11, Loss: 0.9893, Train: 0.9071, Val: 0.7240, Test: 0.7170
Epoch: 12, Loss: 0.8089, Train: 0.9286, Val: 0.7580, Test: 0.7540
Epoch: 13, Loss: 0.6689, Train: 0.9500, Val: 0.7740, Test: 0.7750
Epoch: 14, Loss: 0.5384, Train: 0.9500, Val: 0.7820, Test: 0.7870
Epoch: 15, Loss: 0.4080, Train: 0.9643, Val: 0.7660, Test: 0.7790
Epoch: 16, Loss: 0.3180, Train: 0.9714, Val: 0.7600, Test: 0.7730
Epoch: 17, Loss: 0.2637, Train: 0.9714, Val: 0.7640, Test: 0.7790
Epoch: 18, Loss: 0.1912, Train: 0.9929, Val: 0.7700, Test: 0.7830
Epoch: 19, Loss: 0.1356, Train: 0.9929, Val: 0.7820, Test: 0.7870
Epoch: 20, Loss: 0.0983, Train: 1.0000, Val: 0.7740, Test: 0.7880
Epoch: 21, Loss: 0.0869, Train: 1.0000, Val: 0.7680, Test: 0.7810
Epoch: 22, Loss: 0.0646, Train: 1.0000, Val: 0.7680, Test: 0.7740
Epoch: 23, Loss: 0.0542, Train: 1.0000, Val: 0.7720, Test: 0.7720
Epoch: 24, Loss: 0.0337, Train: 1.0000, Val: 0.7740, Test: 0.7730
Epoch: 25, Loss: 0.0332, Train: 1.0000, Val: 0.7840, Test: 0.7840
Epoch: 26, Loss: 0.0257, Train: 1.0000, Val: 0.7780, Test: 0.7850
Epoch: 27, Loss: 0.0180, Train: 0.9929, Val: 0.7560, Test: 0.7760
Epoch: 28, Loss: 0.0389, Train: 1.0000, Val: 0.7680, Test: 0.7740
Epoch: 29, Loss: 0.0408, Train: 1.0000, Val: 0.7740, Test: 0.7800
Epoch: 30, Loss: 0.0103, Train: 0.9929, Val: 0.7700, Test: 0.7730
Epoch: 31, Loss: 0.0097, Train: 0.9929, Val: 0.7660, Test: 0.7570
Epoch: 32, Loss: 0.0370, Train: 1.0000, Val: 0.7600, Test: 0.7640
Epoch: 33, Loss: 0.0123, Train: 1.0000, Val: 0.7500, Test: 0.7550
Epoch: 34, Loss: 0.0070, Train: 0.9929, Val: 0.7260, Test: 0.7420
Epoch: 35, Loss: 0.0397, Train: 1.0000, Val: 0.7420, Test: 0.7560
Epoch: 36, Loss: 0.0076, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 37, Loss: 0.0135, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 38, Loss: 0.0036, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 39, Loss: 0.0044, Train: 1.0000, Val: 0.7640, Test: 0.7710
Epoch: 40, Loss: 0.0084, Train: 1.0000, Val: 0.7580, Test: 0.7730
Epoch: 41, Loss: 0.0054, Train: 1.0000, Val: 0.7660, Test: 0.7730
Epoch: 42, Loss: 0.0046, Train: 1.0000, Val: 0.7600, Test: 0.7730
Epoch: 43, Loss: 0.0033, Train: 1.0000, Val: 0.7560, Test: 0.7710
Epoch: 44, Loss: 0.0031, Train: 1.0000, Val: 0.7600, Test: 0.7720
Epoch: 45, Loss: 0.0048, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 46, Loss: 0.0035, Train: 1.0000, Val: 0.7620, Test: 0.7640
Epoch: 47, Loss: 0.0090, Train: 1.0000, Val: 0.7620, Test: 0.7660
Epoch: 48, Loss: 0.0044, Train: 1.0000, Val: 0.7600, Test: 0.7680
Epoch: 49, Loss: 0.0034, Train: 1.0000, Val: 0.7560, Test: 0.7720
Epoch: 50, Loss: 0.0033, Train: 1.0000, Val: 0.7520, Test: 0.7720
MAD:  0.5362
Best Test Accuracy: 0.7880, Val Accuracy: 0.7740, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0011, Train: 0.2214, Val: 0.1720, Test: 0.1630
Epoch: 2, Loss: 1.9026, Train: 0.1786, Val: 0.1420, Test: 0.1500
Epoch: 3, Loss: 1.9122, Train: 0.1714, Val: 0.1220, Test: 0.1360
Epoch: 4, Loss: 1.8634, Train: 0.4857, Val: 0.3100, Test: 0.3220
Epoch: 5, Loss: 1.8269, Train: 0.7429, Val: 0.4660, Test: 0.4420
Epoch: 6, Loss: 1.8417, Train: 0.7643, Val: 0.4980, Test: 0.4950
Epoch: 7, Loss: 1.7393, Train: 0.7857, Val: 0.6420, Test: 0.6390
Epoch: 8, Loss: 1.6099, Train: 0.8571, Val: 0.7160, Test: 0.7310
Epoch: 9, Loss: 1.4666, Train: 0.8929, Val: 0.7400, Test: 0.7470
Epoch: 10, Loss: 1.2866, Train: 0.9000, Val: 0.7160, Test: 0.7080
Epoch: 11, Loss: 1.0604, Train: 0.9071, Val: 0.7040, Test: 0.6810
Epoch: 12, Loss: 0.9204, Train: 0.9286, Val: 0.7340, Test: 0.7180
Epoch: 13, Loss: 0.7344, Train: 0.9643, Val: 0.7440, Test: 0.7390
Epoch: 14, Loss: 0.6093, Train: 0.9714, Val: 0.7660, Test: 0.7480
Epoch: 15, Loss: 0.4456, Train: 0.9714, Val: 0.7740, Test: 0.7530
Epoch: 16, Loss: 0.3468, Train: 0.9571, Val: 0.7760, Test: 0.7500
Epoch: 17, Loss: 0.3001, Train: 0.9857, Val: 0.7820, Test: 0.7630
Epoch: 18, Loss: 0.2090, Train: 0.9929, Val: 0.7840, Test: 0.7860
Epoch: 19, Loss: 0.1366, Train: 0.9929, Val: 0.7920, Test: 0.7970
Epoch: 20, Loss: 0.1099, Train: 0.9929, Val: 0.7780, Test: 0.7880
Epoch: 21, Loss: 0.1051, Train: 1.0000, Val: 0.7780, Test: 0.7760
Epoch: 22, Loss: 0.0480, Train: 1.0000, Val: 0.7740, Test: 0.7710
Epoch: 23, Loss: 0.0639, Train: 1.0000, Val: 0.7680, Test: 0.7740
Epoch: 24, Loss: 0.0483, Train: 1.0000, Val: 0.7660, Test: 0.7700
Epoch: 25, Loss: 0.0347, Train: 1.0000, Val: 0.7620, Test: 0.7760
Epoch: 26, Loss: 0.0217, Train: 1.0000, Val: 0.7660, Test: 0.7730
Epoch: 27, Loss: 0.0233, Train: 1.0000, Val: 0.7680, Test: 0.7710
Epoch: 28, Loss: 0.0143, Train: 1.0000, Val: 0.7700, Test: 0.7640
Epoch: 29, Loss: 0.0099, Train: 1.0000, Val: 0.7640, Test: 0.7680
Epoch: 30, Loss: 0.0275, Train: 1.0000, Val: 0.7540, Test: 0.7640
Epoch: 31, Loss: 0.0204, Train: 1.0000, Val: 0.7500, Test: 0.7530
Epoch: 32, Loss: 0.0225, Train: 1.0000, Val: 0.7540, Test: 0.7640
Epoch: 33, Loss: 0.0097, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 34, Loss: 0.0104, Train: 1.0000, Val: 0.7600, Test: 0.7600
Epoch: 35, Loss: 0.0055, Train: 1.0000, Val: 0.7640, Test: 0.7640
Epoch: 36, Loss: 0.0100, Train: 1.0000, Val: 0.7660, Test: 0.7710
Epoch: 37, Loss: 0.0043, Train: 1.0000, Val: 0.7680, Test: 0.7820
Epoch: 38, Loss: 0.0056, Train: 1.0000, Val: 0.7640, Test: 0.7840
Epoch: 39, Loss: 0.0034, Train: 1.0000, Val: 0.7640, Test: 0.7780
Epoch: 40, Loss: 0.0042, Train: 1.0000, Val: 0.7680, Test: 0.7860
Epoch: 41, Loss: 0.0046, Train: 1.0000, Val: 0.7740, Test: 0.7850
Epoch: 42, Loss: 0.0084, Train: 1.0000, Val: 0.7620, Test: 0.7820
Epoch: 43, Loss: 0.0047, Train: 1.0000, Val: 0.7580, Test: 0.7720
Epoch: 44, Loss: 0.0042, Train: 1.0000, Val: 0.7500, Test: 0.7660
Epoch: 45, Loss: 0.0023, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 46, Loss: 0.0052, Train: 0.9929, Val: 0.7360, Test: 0.7510
Epoch: 47, Loss: 0.0031, Train: 0.9857, Val: 0.7300, Test: 0.7500
Epoch: 48, Loss: 0.0247, Train: 1.0000, Val: 0.7560, Test: 0.7640
Epoch: 49, Loss: 0.0040, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 50, Loss: 0.0066, Train: 1.0000, Val: 0.7600, Test: 0.7860
MAD:  0.5319
Best Test Accuracy: 0.7970, Val Accuracy: 0.7920, Train Accuracy: 0.9929
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0178, Train: 0.1429, Val: 0.1680, Test: 0.1520
Epoch: 2, Loss: 1.9385, Train: 0.1500, Val: 0.1620, Test: 0.1490
Epoch: 3, Loss: 1.9289, Train: 0.3929, Val: 0.3120, Test: 0.3000
Epoch: 4, Loss: 1.9260, Train: 0.3214, Val: 0.2580, Test: 0.2400
Epoch: 5, Loss: 1.8523, Train: 0.4214, Val: 0.2920, Test: 0.2840
Epoch: 6, Loss: 1.7760, Train: 0.4857, Val: 0.3260, Test: 0.3200
Epoch: 7, Loss: 1.7127, Train: 0.7071, Val: 0.4280, Test: 0.4320
Epoch: 8, Loss: 1.6374, Train: 0.7714, Val: 0.5480, Test: 0.5190
Epoch: 9, Loss: 1.4626, Train: 0.7857, Val: 0.5400, Test: 0.5290
Epoch: 10, Loss: 1.3191, Train: 0.7786, Val: 0.5440, Test: 0.5480
Epoch: 11, Loss: 1.1501, Train: 0.8357, Val: 0.6040, Test: 0.5840
Epoch: 12, Loss: 0.9590, Train: 0.8929, Val: 0.6980, Test: 0.6980
Epoch: 13, Loss: 0.7657, Train: 0.9357, Val: 0.7580, Test: 0.7510
Epoch: 14, Loss: 0.6247, Train: 0.9286, Val: 0.7420, Test: 0.7470
Epoch: 15, Loss: 0.4888, Train: 0.9500, Val: 0.7540, Test: 0.7540
Epoch: 16, Loss: 0.4435, Train: 0.9643, Val: 0.7660, Test: 0.7590
Epoch: 17, Loss: 0.4120, Train: 0.9857, Val: 0.7700, Test: 0.7760
Epoch: 18, Loss: 0.2623, Train: 0.9857, Val: 0.7780, Test: 0.7820
Epoch: 19, Loss: 0.2217, Train: 1.0000, Val: 0.7820, Test: 0.7810
Epoch: 20, Loss: 0.1775, Train: 1.0000, Val: 0.7880, Test: 0.7800
Epoch: 21, Loss: 0.1334, Train: 0.9929, Val: 0.7720, Test: 0.7710
Epoch: 22, Loss: 0.1233, Train: 0.9929, Val: 0.7640, Test: 0.7730
Epoch: 23, Loss: 0.0672, Train: 1.0000, Val: 0.7640, Test: 0.7740
Epoch: 24, Loss: 0.0502, Train: 1.0000, Val: 0.7800, Test: 0.7830
Epoch: 25, Loss: 0.0533, Train: 1.0000, Val: 0.7740, Test: 0.7850
Epoch: 26, Loss: 0.0520, Train: 1.0000, Val: 0.7560, Test: 0.7790
Epoch: 27, Loss: 0.0277, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 28, Loss: 0.0276, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 29, Loss: 0.0148, Train: 1.0000, Val: 0.7620, Test: 0.7680
Epoch: 30, Loss: 0.0148, Train: 1.0000, Val: 0.7660, Test: 0.7710
Epoch: 31, Loss: 0.0272, Train: 1.0000, Val: 0.7640, Test: 0.7670
Epoch: 32, Loss: 0.0184, Train: 1.0000, Val: 0.7620, Test: 0.7720
Epoch: 33, Loss: 0.0125, Train: 1.0000, Val: 0.7640, Test: 0.7730
Epoch: 34, Loss: 0.0071, Train: 1.0000, Val: 0.7660, Test: 0.7830
Epoch: 35, Loss: 0.0072, Train: 1.0000, Val: 0.7740, Test: 0.7810
Epoch: 36, Loss: 0.0060, Train: 1.0000, Val: 0.7720, Test: 0.7790
Epoch: 37, Loss: 0.0050, Train: 1.0000, Val: 0.7700, Test: 0.7820
Epoch: 38, Loss: 0.0126, Train: 1.0000, Val: 0.7760, Test: 0.7900
Epoch: 39, Loss: 0.0048, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 40, Loss: 0.0074, Train: 1.0000, Val: 0.7740, Test: 0.7850
Epoch: 41, Loss: 0.0071, Train: 1.0000, Val: 0.7620, Test: 0.7860
Epoch: 42, Loss: 0.0041, Train: 1.0000, Val: 0.7560, Test: 0.7760
Epoch: 43, Loss: 0.0048, Train: 1.0000, Val: 0.7560, Test: 0.7690
Epoch: 44, Loss: 0.0131, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 45, Loss: 0.0037, Train: 1.0000, Val: 0.7760, Test: 0.7900
Epoch: 46, Loss: 0.0029, Train: 0.9929, Val: 0.7720, Test: 0.7930
Epoch: 47, Loss: 0.0043, Train: 0.9929, Val: 0.7740, Test: 0.7880
Epoch: 48, Loss: 0.0214, Train: 1.0000, Val: 0.7600, Test: 0.7740
Epoch: 49, Loss: 0.0035, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 50, Loss: 0.0123, Train: 1.0000, Val: 0.7300, Test: 0.7590
MAD:  0.6319
Best Test Accuracy: 0.7930, Val Accuracy: 0.7720, Train Accuracy: 0.9929
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9552, Train: 0.3143, Val: 0.2840, Test: 0.2760
Epoch: 2, Loss: 1.9335, Train: 0.3786, Val: 0.1980, Test: 0.2160
Epoch: 3, Loss: 1.9267, Train: 0.3000, Val: 0.1440, Test: 0.1680
Epoch: 4, Loss: 1.8745, Train: 0.4786, Val: 0.3720, Test: 0.3640
Epoch: 5, Loss: 1.8579, Train: 0.6357, Val: 0.5280, Test: 0.5150
Epoch: 6, Loss: 1.7686, Train: 0.7714, Val: 0.6100, Test: 0.6310
Epoch: 7, Loss: 1.7374, Train: 0.8571, Val: 0.6140, Test: 0.6320
Epoch: 8, Loss: 1.6303, Train: 0.8214, Val: 0.6140, Test: 0.6040
Epoch: 9, Loss: 1.4806, Train: 0.8071, Val: 0.5320, Test: 0.5180
Epoch: 10, Loss: 1.2563, Train: 0.8357, Val: 0.5660, Test: 0.5550
Epoch: 11, Loss: 1.0710, Train: 0.8429, Val: 0.5940, Test: 0.5740
Epoch: 12, Loss: 0.9151, Train: 0.8571, Val: 0.6040, Test: 0.6030
Epoch: 13, Loss: 0.7994, Train: 0.8929, Val: 0.6240, Test: 0.6370
Epoch: 14, Loss: 0.6361, Train: 0.9429, Val: 0.7320, Test: 0.7080
Epoch: 15, Loss: 0.4958, Train: 0.9714, Val: 0.7820, Test: 0.7490
Epoch: 16, Loss: 0.4133, Train: 0.9857, Val: 0.7860, Test: 0.7650
Epoch: 17, Loss: 0.3058, Train: 0.9857, Val: 0.7820, Test: 0.7690
Epoch: 18, Loss: 0.2423, Train: 0.9929, Val: 0.7860, Test: 0.7690
Epoch: 19, Loss: 0.2041, Train: 1.0000, Val: 0.7800, Test: 0.7720
Epoch: 20, Loss: 0.1295, Train: 1.0000, Val: 0.7800, Test: 0.7660
Epoch: 21, Loss: 0.1019, Train: 0.9929, Val: 0.7700, Test: 0.7630
Epoch: 22, Loss: 0.0884, Train: 0.9929, Val: 0.7500, Test: 0.7570
Epoch: 23, Loss: 0.0581, Train: 1.0000, Val: 0.7500, Test: 0.7490
Epoch: 24, Loss: 0.0441, Train: 1.0000, Val: 0.7460, Test: 0.7510
Epoch: 25, Loss: 0.0418, Train: 0.9929, Val: 0.7540, Test: 0.7530
Epoch: 26, Loss: 0.0305, Train: 0.9929, Val: 0.7500, Test: 0.7560
Epoch: 27, Loss: 0.0574, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 28, Loss: 0.0135, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 29, Loss: 0.0427, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 30, Loss: 0.0106, Train: 1.0000, Val: 0.7480, Test: 0.7530
Epoch: 31, Loss: 0.0319, Train: 0.9929, Val: 0.7580, Test: 0.7660
Epoch: 32, Loss: 0.0153, Train: 1.0000, Val: 0.7540, Test: 0.7670
Epoch: 33, Loss: 0.0059, Train: 1.0000, Val: 0.7540, Test: 0.7650
Epoch: 34, Loss: 0.0070, Train: 1.0000, Val: 0.7540, Test: 0.7630
Epoch: 35, Loss: 0.0055, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 36, Loss: 0.0082, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 37, Loss: 0.0062, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 38, Loss: 0.0041, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 39, Loss: 0.0044, Train: 1.0000, Val: 0.7380, Test: 0.7610
Epoch: 40, Loss: 0.0223, Train: 1.0000, Val: 0.7320, Test: 0.7490
Epoch: 41, Loss: 0.0031, Train: 1.0000, Val: 0.7320, Test: 0.7410
Epoch: 42, Loss: 0.0029, Train: 1.0000, Val: 0.7360, Test: 0.7330
Epoch: 43, Loss: 0.0197, Train: 0.9929, Val: 0.7120, Test: 0.7150
Epoch: 44, Loss: 0.0059, Train: 0.9929, Val: 0.7120, Test: 0.7150
Epoch: 45, Loss: 0.0119, Train: 0.9929, Val: 0.7120, Test: 0.7170
Epoch: 46, Loss: 0.0353, Train: 1.0000, Val: 0.7200, Test: 0.7320
Epoch: 47, Loss: 0.0154, Train: 0.9929, Val: 0.7400, Test: 0.7410
Epoch: 48, Loss: 0.0408, Train: 1.0000, Val: 0.7360, Test: 0.7360
Epoch: 49, Loss: 0.0119, Train: 1.0000, Val: 0.7260, Test: 0.7330
Epoch: 50, Loss: 0.0086, Train: 1.0000, Val: 0.7280, Test: 0.7330
MAD:  0.5158
Best Test Accuracy: 0.7720, Val Accuracy: 0.7800, Train Accuracy: 1.0000
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9971, Train: 0.1429, Val: 0.0580, Test: 0.0640
Epoch: 2, Loss: 1.9126, Train: 0.3571, Val: 0.2500, Test: 0.2350
Epoch: 3, Loss: 1.9307, Train: 0.3857, Val: 0.2680, Test: 0.2630
Epoch: 4, Loss: 1.8827, Train: 0.5643, Val: 0.3740, Test: 0.3650
Epoch: 5, Loss: 1.8289, Train: 0.6643, Val: 0.4500, Test: 0.4320
Epoch: 6, Loss: 1.7765, Train: 0.8786, Val: 0.6840, Test: 0.7010
Epoch: 7, Loss: 1.6669, Train: 0.8714, Val: 0.7160, Test: 0.7250
Epoch: 8, Loss: 1.5492, Train: 0.8429, Val: 0.6220, Test: 0.6200
Epoch: 9, Loss: 1.3863, Train: 0.8500, Val: 0.6080, Test: 0.6070
Epoch: 10, Loss: 1.1722, Train: 0.8929, Val: 0.6460, Test: 0.6630
Epoch: 11, Loss: 1.0836, Train: 0.9000, Val: 0.7160, Test: 0.6910
Epoch: 12, Loss: 0.8607, Train: 0.9000, Val: 0.6900, Test: 0.7000
Epoch: 13, Loss: 0.7600, Train: 0.9214, Val: 0.7460, Test: 0.7370
Epoch: 14, Loss: 0.6204, Train: 0.9429, Val: 0.7680, Test: 0.7630
Epoch: 15, Loss: 0.5171, Train: 0.9571, Val: 0.7580, Test: 0.7650
Epoch: 16, Loss: 0.4204, Train: 0.9786, Val: 0.7620, Test: 0.7760
Epoch: 17, Loss: 0.3536, Train: 0.9786, Val: 0.7740, Test: 0.7830
Epoch: 18, Loss: 0.2868, Train: 0.9929, Val: 0.7820, Test: 0.7950
Epoch: 19, Loss: 0.2263, Train: 0.9857, Val: 0.7800, Test: 0.8000
Epoch: 20, Loss: 0.1803, Train: 0.9857, Val: 0.7520, Test: 0.7780
Epoch: 21, Loss: 0.1369, Train: 0.9857, Val: 0.7340, Test: 0.7680
Epoch: 22, Loss: 0.0948, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 23, Loss: 0.0736, Train: 1.0000, Val: 0.7580, Test: 0.7750
Epoch: 24, Loss: 0.0596, Train: 1.0000, Val: 0.7760, Test: 0.7850
Epoch: 25, Loss: 0.0456, Train: 1.0000, Val: 0.7760, Test: 0.7840
Epoch: 26, Loss: 0.0427, Train: 1.0000, Val: 0.7620, Test: 0.7800
Epoch: 27, Loss: 0.0321, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 28, Loss: 0.0173, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 29, Loss: 0.0246, Train: 1.0000, Val: 0.7540, Test: 0.7670
Epoch: 30, Loss: 0.0126, Train: 1.0000, Val: 0.7560, Test: 0.7640
Epoch: 31, Loss: 0.0081, Train: 1.0000, Val: 0.7520, Test: 0.7540
Epoch: 32, Loss: 0.0130, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 33, Loss: 0.0074, Train: 1.0000, Val: 0.7560, Test: 0.7610
Epoch: 34, Loss: 0.0066, Train: 0.9929, Val: 0.7580, Test: 0.7590
Epoch: 35, Loss: 0.0074, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 36, Loss: 0.0222, Train: 1.0000, Val: 0.7520, Test: 0.7540
Epoch: 37, Loss: 0.0038, Train: 0.9929, Val: 0.7440, Test: 0.7400
Epoch: 38, Loss: 0.0951, Train: 0.9857, Val: 0.7400, Test: 0.7430
Epoch: 39, Loss: 0.0454, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 40, Loss: 0.0064, Train: 1.0000, Val: 0.7560, Test: 0.7560
Epoch: 41, Loss: 0.0123, Train: 0.9929, Val: 0.7420, Test: 0.7510
Epoch: 42, Loss: 0.0180, Train: 0.9929, Val: 0.7440, Test: 0.7440
Epoch: 43, Loss: 0.0303, Train: 1.0000, Val: 0.7440, Test: 0.7490
Epoch: 44, Loss: 0.0053, Train: 1.0000, Val: 0.7460, Test: 0.7490
Epoch: 45, Loss: 0.0034, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 46, Loss: 0.0019, Train: 1.0000, Val: 0.7400, Test: 0.7500
Epoch: 47, Loss: 0.0083, Train: 1.0000, Val: 0.7380, Test: 0.7440
Epoch: 48, Loss: 0.0042, Train: 1.0000, Val: 0.7320, Test: 0.7390
Epoch: 49, Loss: 0.0030, Train: 1.0000, Val: 0.7300, Test: 0.7360
Epoch: 50, Loss: 0.0034, Train: 1.0000, Val: 0.7320, Test: 0.7370
MAD:  0.5337
Best Test Accuracy: 0.8000, Val Accuracy: 0.7800, Train Accuracy: 0.9857
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9578, Train: 0.2286, Val: 0.1700, Test: 0.2140
Epoch: 2, Loss: 1.9393, Train: 0.2000, Val: 0.1420, Test: 0.1570
Epoch: 3, Loss: 1.9128, Train: 0.4286, Val: 0.2600, Test: 0.2710
Epoch: 4, Loss: 1.8803, Train: 0.4500, Val: 0.2980, Test: 0.3120
Epoch: 5, Loss: 1.8358, Train: 0.6786, Val: 0.5580, Test: 0.5600
Epoch: 6, Loss: 1.7893, Train: 0.7571, Val: 0.6460, Test: 0.6770
Epoch: 7, Loss: 1.6929, Train: 0.9214, Val: 0.7280, Test: 0.7750
Epoch: 8, Loss: 1.5628, Train: 0.9071, Val: 0.6920, Test: 0.7150
Epoch: 9, Loss: 1.3944, Train: 0.9071, Val: 0.7040, Test: 0.7300
Epoch: 10, Loss: 1.2019, Train: 0.9286, Val: 0.7160, Test: 0.7560
Epoch: 11, Loss: 0.9845, Train: 0.9286, Val: 0.7200, Test: 0.7600
Epoch: 12, Loss: 0.8040, Train: 0.9500, Val: 0.7260, Test: 0.7660
Epoch: 13, Loss: 0.6389, Train: 0.9500, Val: 0.7320, Test: 0.7670
Epoch: 14, Loss: 0.5603, Train: 0.9714, Val: 0.7460, Test: 0.7780
Epoch: 15, Loss: 0.4054, Train: 0.9857, Val: 0.7640, Test: 0.7800
Epoch: 16, Loss: 0.3325, Train: 0.9929, Val: 0.7520, Test: 0.7750
Epoch: 17, Loss: 0.2761, Train: 0.9929, Val: 0.7400, Test: 0.7580
Epoch: 18, Loss: 0.2076, Train: 0.9929, Val: 0.7440, Test: 0.7650
Epoch: 19, Loss: 0.1612, Train: 0.9929, Val: 0.7500, Test: 0.7710
Epoch: 20, Loss: 0.1195, Train: 1.0000, Val: 0.7660, Test: 0.7720
Epoch: 21, Loss: 0.0961, Train: 1.0000, Val: 0.7620, Test: 0.7770
Epoch: 22, Loss: 0.0705, Train: 0.9929, Val: 0.7580, Test: 0.7730
Epoch: 23, Loss: 0.0512, Train: 1.0000, Val: 0.7600, Test: 0.7780
Epoch: 24, Loss: 0.0400, Train: 1.0000, Val: 0.7640, Test: 0.7800
Epoch: 25, Loss: 0.0260, Train: 1.0000, Val: 0.7740, Test: 0.7850
Epoch: 26, Loss: 0.0381, Train: 1.0000, Val: 0.7640, Test: 0.7870
Epoch: 27, Loss: 0.0272, Train: 1.0000, Val: 0.7580, Test: 0.7830
Epoch: 28, Loss: 0.0159, Train: 1.0000, Val: 0.7580, Test: 0.7850
Epoch: 29, Loss: 0.0215, Train: 1.0000, Val: 0.7620, Test: 0.7900
Epoch: 30, Loss: 0.0160, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 31, Loss: 0.0130, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 32, Loss: 0.0137, Train: 1.0000, Val: 0.7780, Test: 0.7860
Epoch: 33, Loss: 0.0074, Train: 1.0000, Val: 0.7780, Test: 0.7870
Epoch: 34, Loss: 0.0100, Train: 1.0000, Val: 0.7820, Test: 0.7850
Epoch: 35, Loss: 0.0056, Train: 1.0000, Val: 0.7760, Test: 0.7890
Epoch: 36, Loss: 0.0050, Train: 1.0000, Val: 0.7680, Test: 0.7840
Epoch: 37, Loss: 0.0070, Train: 1.0000, Val: 0.7660, Test: 0.7820
Epoch: 38, Loss: 0.0049, Train: 1.0000, Val: 0.7660, Test: 0.7770
Epoch: 39, Loss: 0.0037, Train: 1.0000, Val: 0.7640, Test: 0.7790
Epoch: 40, Loss: 0.0051, Train: 1.0000, Val: 0.7620, Test: 0.7750
Epoch: 41, Loss: 0.0019, Train: 1.0000, Val: 0.7620, Test: 0.7770
Epoch: 42, Loss: 0.0043, Train: 1.0000, Val: 0.7560, Test: 0.7750
Epoch: 43, Loss: 0.0027, Train: 1.0000, Val: 0.7560, Test: 0.7710
Epoch: 44, Loss: 0.0016, Train: 1.0000, Val: 0.7520, Test: 0.7700
Epoch: 45, Loss: 0.0016, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 46, Loss: 0.0035, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 47, Loss: 0.0015, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 48, Loss: 0.0027, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 49, Loss: 0.0025, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 50, Loss: 0.0021, Train: 1.0000, Val: 0.7480, Test: 0.7720
MAD:  0.5879
Best Test Accuracy: 0.7910, Val Accuracy: 0.7700, Train Accuracy: 1.0000
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GATv2Conv(128, 128, heads=4)
      (conv2): GATv2Conv(128, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1-3): 3 x ParallelGNNBlock(
      (conv1): GATv2Conv(1024, 128, heads=4)
      (conv2): GATv2Conv(1024, 128, heads=4)
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (4): GATv2Conv(1024, 256, heads=4)
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=64, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.3, inplace=False)
    (3): Linear(in_features=64, out_features=7, bias=True)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9725, Train: 0.2786, Val: 0.2560, Test: 0.2380
Epoch: 2, Loss: 1.9243, Train: 0.1500, Val: 0.1580, Test: 0.1440
Epoch: 3, Loss: 1.9321, Train: 0.3143, Val: 0.2500, Test: 0.2300
Epoch: 4, Loss: 1.8990, Train: 0.4071, Val: 0.2340, Test: 0.1960
Epoch: 5, Loss: 1.8320, Train: 0.4143, Val: 0.2240, Test: 0.2280
Epoch: 6, Loss: 1.8482, Train: 0.5929, Val: 0.4260, Test: 0.4420
Epoch: 7, Loss: 1.7995, Train: 0.6429, Val: 0.5940, Test: 0.5910
Epoch: 8, Loss: 1.7391, Train: 0.6857, Val: 0.6340, Test: 0.6170
Epoch: 9, Loss: 1.5341, Train: 0.8643, Val: 0.7740, Test: 0.7500
Epoch: 10, Loss: 1.4369, Train: 0.7714, Val: 0.6280, Test: 0.5950
Epoch: 11, Loss: 1.2160, Train: 0.8357, Val: 0.6320, Test: 0.5930
Epoch: 12, Loss: 1.0582, Train: 0.8786, Val: 0.7300, Test: 0.7280
Epoch: 13, Loss: 0.9032, Train: 0.9357, Val: 0.7760, Test: 0.7710
Epoch: 14, Loss: 0.7609, Train: 0.9571, Val: 0.8060, Test: 0.7880
Epoch: 15, Loss: 0.6123, Train: 0.9786, Val: 0.8040, Test: 0.8150
Epoch: 16, Loss: 0.4274, Train: 0.9786, Val: 0.7940, Test: 0.8080
Epoch: 17, Loss: 0.3895, Train: 0.9786, Val: 0.7740, Test: 0.7950
Epoch: 18, Loss: 0.2728, Train: 0.9786, Val: 0.7700, Test: 0.7900
Epoch: 19, Loss: 0.1874, Train: 0.9786, Val: 0.7840, Test: 0.7980
Epoch: 20, Loss: 0.1684, Train: 0.9857, Val: 0.7880, Test: 0.8010
Epoch: 21, Loss: 0.1336, Train: 0.9929, Val: 0.7840, Test: 0.8020
Epoch: 22, Loss: 0.0855, Train: 0.9929, Val: 0.7720, Test: 0.7940
Epoch: 23, Loss: 0.0622, Train: 0.9929, Val: 0.7720, Test: 0.7980
Epoch: 24, Loss: 0.0533, Train: 1.0000, Val: 0.7860, Test: 0.8000
Epoch: 25, Loss: 0.0309, Train: 1.0000, Val: 0.7840, Test: 0.7930
Epoch: 26, Loss: 0.0227, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 27, Loss: 0.0226, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 28, Loss: 0.0160, Train: 1.0000, Val: 0.7820, Test: 0.7910
Epoch: 29, Loss: 0.0107, Train: 1.0000, Val: 0.7820, Test: 0.7890
Epoch: 30, Loss: 0.0108, Train: 1.0000, Val: 0.7820, Test: 0.7850
Epoch: 31, Loss: 0.0073, Train: 1.0000, Val: 0.7820, Test: 0.7890
Epoch: 32, Loss: 0.0079, Train: 1.0000, Val: 0.7860, Test: 0.7880
Epoch: 33, Loss: 0.0085, Train: 1.0000, Val: 0.7900, Test: 0.7900
Epoch: 34, Loss: 0.0037, Train: 1.0000, Val: 0.7880, Test: 0.7890
Epoch: 35, Loss: 0.0049, Train: 1.0000, Val: 0.7920, Test: 0.7920
Epoch: 36, Loss: 0.0028, Train: 1.0000, Val: 0.7920, Test: 0.7890
Epoch: 37, Loss: 0.0035, Train: 1.0000, Val: 0.7880, Test: 0.7870
Epoch: 38, Loss: 0.0049, Train: 1.0000, Val: 0.7840, Test: 0.7860
Epoch: 39, Loss: 0.0021, Train: 1.0000, Val: 0.7880, Test: 0.7840
Epoch: 40, Loss: 0.0023, Train: 1.0000, Val: 0.7840, Test: 0.7830
Epoch: 41, Loss: 0.0067, Train: 1.0000, Val: 0.7880, Test: 0.7840
Epoch: 42, Loss: 0.0022, Train: 1.0000, Val: 0.7840, Test: 0.7900
Epoch: 43, Loss: 0.0027, Train: 1.0000, Val: 0.7800, Test: 0.7880
Epoch: 44, Loss: 0.0020, Train: 1.0000, Val: 0.7800, Test: 0.7950
Epoch: 45, Loss: 0.0017, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 46, Loss: 0.0011, Train: 1.0000, Val: 0.7760, Test: 0.7940
Epoch: 47, Loss: 0.0027, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 48, Loss: 0.0011, Train: 1.0000, Val: 0.7780, Test: 0.7900
Epoch: 49, Loss: 0.0016, Train: 1.0000, Val: 0.7860, Test: 0.7890
Epoch: 50, Loss: 0.0021, Train: 1.0000, Val: 0.7860, Test: 0.7820
MAD:  0.4394
Best Test Accuracy: 0.8150, Val Accuracy: 0.8040, Train Accuracy: 0.9786
Training completed.
Average Test Accuracy:  0.7957000000000001 ± 0.011027692415006857
Average MAD:  0.5199100000000001 ± 0.0899800250055533

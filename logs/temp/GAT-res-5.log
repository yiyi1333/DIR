/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9419, Train: 0.2929, Val: 0.2420, Test: 0.2310
Epoch: 2, Loss: 1.9063, Train: 0.5214, Val: 0.3360, Test: 0.3310
Epoch: 3, Loss: 1.9288, Train: 0.7214, Val: 0.4580, Test: 0.4410
Epoch: 4, Loss: 1.8568, Train: 0.7571, Val: 0.4540, Test: 0.4730
Epoch: 5, Loss: 1.7700, Train: 0.7857, Val: 0.4600, Test: 0.4710
Epoch: 6, Loss: 1.7501, Train: 0.7857, Val: 0.4800, Test: 0.4820
Epoch: 7, Loss: 1.7208, Train: 0.8071, Val: 0.5140, Test: 0.4960
Epoch: 8, Loss: 1.6183, Train: 0.8643, Val: 0.5360, Test: 0.5320
Epoch: 9, Loss: 1.5139, Train: 0.8714, Val: 0.5500, Test: 0.5550
Epoch: 10, Loss: 1.5092, Train: 0.8714, Val: 0.5760, Test: 0.5860
Epoch: 11, Loss: 1.3637, Train: 0.8857, Val: 0.6080, Test: 0.6090
Epoch: 12, Loss: 1.4049, Train: 0.8857, Val: 0.6280, Test: 0.6290
Epoch: 13, Loss: 1.2308, Train: 0.8929, Val: 0.6500, Test: 0.6430
Epoch: 14, Loss: 1.2288, Train: 0.8929, Val: 0.6520, Test: 0.6550
Epoch: 15, Loss: 1.0955, Train: 0.9000, Val: 0.6680, Test: 0.6730
Epoch: 16, Loss: 0.9858, Train: 0.9000, Val: 0.6920, Test: 0.6910
Epoch: 17, Loss: 0.9459, Train: 0.9071, Val: 0.7140, Test: 0.7120
Epoch: 18, Loss: 0.8880, Train: 0.9071, Val: 0.7200, Test: 0.7290
Epoch: 19, Loss: 0.7875, Train: 0.9214, Val: 0.7300, Test: 0.7400
Epoch: 20, Loss: 0.8329, Train: 0.9214, Val: 0.7400, Test: 0.7440
Epoch: 21, Loss: 0.6520, Train: 0.9357, Val: 0.7360, Test: 0.7450
Epoch: 22, Loss: 0.6331, Train: 0.9429, Val: 0.7520, Test: 0.7510
Epoch: 23, Loss: 0.5724, Train: 0.9714, Val: 0.7700, Test: 0.7570
Epoch: 24, Loss: 0.5484, Train: 0.9714, Val: 0.7760, Test: 0.7630
Epoch: 25, Loss: 0.5333, Train: 0.9643, Val: 0.7740, Test: 0.7740
Epoch: 26, Loss: 0.4992, Train: 0.9643, Val: 0.7800, Test: 0.7780
Epoch: 27, Loss: 0.3759, Train: 0.9643, Val: 0.7820, Test: 0.7780
Epoch: 28, Loss: 0.3918, Train: 0.9714, Val: 0.7740, Test: 0.7780
Epoch: 29, Loss: 0.3606, Train: 0.9786, Val: 0.7720, Test: 0.7830
Epoch: 30, Loss: 0.3193, Train: 0.9857, Val: 0.7720, Test: 0.7790
Epoch: 31, Loss: 0.3150, Train: 0.9857, Val: 0.7680, Test: 0.7750
Epoch: 32, Loss: 0.2647, Train: 0.9786, Val: 0.7720, Test: 0.7740
Epoch: 33, Loss: 0.2682, Train: 0.9714, Val: 0.7680, Test: 0.7680
Epoch: 34, Loss: 0.2235, Train: 0.9714, Val: 0.7680, Test: 0.7650
Epoch: 35, Loss: 0.1839, Train: 0.9857, Val: 0.7760, Test: 0.7710
Epoch: 36, Loss: 0.2035, Train: 0.9929, Val: 0.7800, Test: 0.7760
Epoch: 37, Loss: 0.2003, Train: 0.9929, Val: 0.7800, Test: 0.7840
Epoch: 38, Loss: 0.1687, Train: 0.9929, Val: 0.7820, Test: 0.7880
Epoch: 39, Loss: 0.1573, Train: 1.0000, Val: 0.7820, Test: 0.7920
Epoch: 40, Loss: 0.1640, Train: 1.0000, Val: 0.7880, Test: 0.7950
Epoch: 41, Loss: 0.1105, Train: 1.0000, Val: 0.7880, Test: 0.7970
Epoch: 42, Loss: 0.1566, Train: 1.0000, Val: 0.7920, Test: 0.7960
Epoch: 43, Loss: 0.1016, Train: 1.0000, Val: 0.7920, Test: 0.7970
Epoch: 44, Loss: 0.1323, Train: 1.0000, Val: 0.7860, Test: 0.7960
Epoch: 45, Loss: 0.0911, Train: 1.0000, Val: 0.7800, Test: 0.7940
Epoch: 46, Loss: 0.0997, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 47, Loss: 0.1000, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 48, Loss: 0.0987, Train: 1.0000, Val: 0.7620, Test: 0.7820
Epoch: 49, Loss: 0.0736, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 50, Loss: 0.0586, Train: 1.0000, Val: 0.7620, Test: 0.7820
MAD:  0.7904
Best Test Accuracy: 0.7970, Val Accuracy: 0.7880, Train Accuracy: 1.0000
Training completed.
Seed:  1
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0519, Train: 0.1500, Val: 0.1480, Test: 0.1390
Epoch: 2, Loss: 1.9852, Train: 0.4000, Val: 0.2820, Test: 0.2970
Epoch: 3, Loss: 1.8896, Train: 0.6357, Val: 0.4060, Test: 0.4180
Epoch: 4, Loss: 1.8404, Train: 0.7214, Val: 0.4820, Test: 0.5180
Epoch: 5, Loss: 1.7927, Train: 0.7786, Val: 0.5240, Test: 0.5700
Epoch: 6, Loss: 1.7699, Train: 0.8429, Val: 0.5500, Test: 0.6160
Epoch: 7, Loss: 1.7192, Train: 0.9000, Val: 0.5960, Test: 0.6650
Epoch: 8, Loss: 1.6404, Train: 0.9214, Val: 0.6400, Test: 0.6970
Epoch: 9, Loss: 1.5902, Train: 0.9357, Val: 0.6740, Test: 0.7080
Epoch: 10, Loss: 1.4760, Train: 0.9357, Val: 0.6960, Test: 0.7320
Epoch: 11, Loss: 1.4095, Train: 0.9429, Val: 0.7080, Test: 0.7460
Epoch: 12, Loss: 1.2971, Train: 0.9429, Val: 0.7180, Test: 0.7510
Epoch: 13, Loss: 1.2652, Train: 0.9429, Val: 0.7300, Test: 0.7530
Epoch: 14, Loss: 1.0854, Train: 0.9429, Val: 0.7300, Test: 0.7650
Epoch: 15, Loss: 1.0777, Train: 0.9429, Val: 0.7420, Test: 0.7700
Epoch: 16, Loss: 0.9202, Train: 0.9429, Val: 0.7580, Test: 0.7740
Epoch: 17, Loss: 0.9126, Train: 0.9571, Val: 0.7620, Test: 0.7720
Epoch: 18, Loss: 0.7326, Train: 0.9643, Val: 0.7580, Test: 0.7780
Epoch: 19, Loss: 0.7129, Train: 0.9643, Val: 0.7560, Test: 0.7770
Epoch: 20, Loss: 0.6622, Train: 0.9643, Val: 0.7580, Test: 0.7850
Epoch: 21, Loss: 0.5706, Train: 0.9571, Val: 0.7640, Test: 0.7920
Epoch: 22, Loss: 0.5304, Train: 0.9643, Val: 0.7680, Test: 0.8000
Epoch: 23, Loss: 0.5272, Train: 0.9857, Val: 0.7720, Test: 0.8050
Epoch: 24, Loss: 0.3169, Train: 0.9929, Val: 0.7720, Test: 0.8040
Epoch: 25, Loss: 0.3702, Train: 0.9929, Val: 0.7740, Test: 0.8090
Epoch: 26, Loss: 0.3433, Train: 0.9929, Val: 0.7760, Test: 0.8090
Epoch: 27, Loss: 0.3555, Train: 0.9929, Val: 0.7680, Test: 0.8090
Epoch: 28, Loss: 0.2487, Train: 0.9929, Val: 0.7620, Test: 0.8020
Epoch: 29, Loss: 0.1838, Train: 0.9857, Val: 0.7680, Test: 0.8030
Epoch: 30, Loss: 0.2315, Train: 0.9929, Val: 0.7640, Test: 0.7990
Epoch: 31, Loss: 0.2285, Train: 0.9929, Val: 0.7600, Test: 0.7920
Epoch: 32, Loss: 0.2115, Train: 0.9929, Val: 0.7640, Test: 0.7910
Epoch: 33, Loss: 0.1758, Train: 0.9857, Val: 0.7640, Test: 0.7880
Epoch: 34, Loss: 0.2169, Train: 0.9929, Val: 0.7620, Test: 0.7940
Epoch: 35, Loss: 0.1696, Train: 0.9929, Val: 0.7620, Test: 0.7940
Epoch: 36, Loss: 0.1508, Train: 1.0000, Val: 0.7620, Test: 0.7920
Epoch: 37, Loss: 0.0966, Train: 1.0000, Val: 0.7600, Test: 0.7930
Epoch: 38, Loss: 0.1039, Train: 1.0000, Val: 0.7620, Test: 0.7970
Epoch: 39, Loss: 0.1335, Train: 1.0000, Val: 0.7640, Test: 0.8000
Epoch: 40, Loss: 0.0916, Train: 1.0000, Val: 0.7660, Test: 0.8040
Epoch: 41, Loss: 0.1033, Train: 1.0000, Val: 0.7640, Test: 0.8020
Epoch: 42, Loss: 0.0984, Train: 1.0000, Val: 0.7640, Test: 0.8030
Epoch: 43, Loss: 0.0808, Train: 1.0000, Val: 0.7660, Test: 0.8020
Epoch: 44, Loss: 0.0778, Train: 1.0000, Val: 0.7620, Test: 0.8010
Epoch: 45, Loss: 0.0956, Train: 1.0000, Val: 0.7560, Test: 0.7960
Epoch: 46, Loss: 0.0662, Train: 1.0000, Val: 0.7520, Test: 0.7910
Epoch: 47, Loss: 0.0675, Train: 1.0000, Val: 0.7500, Test: 0.7860
Epoch: 48, Loss: 0.0553, Train: 1.0000, Val: 0.7480, Test: 0.7860
Epoch: 49, Loss: 0.0523, Train: 1.0000, Val: 0.7540, Test: 0.7860
Epoch: 50, Loss: 0.0507, Train: 1.0000, Val: 0.7540, Test: 0.7850
MAD:  0.7084
Best Test Accuracy: 0.8090, Val Accuracy: 0.7740, Train Accuracy: 0.9929
Training completed.
Seed:  2
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0079, Train: 0.1500, Val: 0.1560, Test: 0.1660
Epoch: 2, Loss: 2.0256, Train: 0.2214, Val: 0.2080, Test: 0.2220
Epoch: 3, Loss: 1.8996, Train: 0.3857, Val: 0.2660, Test: 0.2810
Epoch: 4, Loss: 1.8801, Train: 0.5714, Val: 0.3480, Test: 0.3630
Epoch: 5, Loss: 1.8115, Train: 0.6357, Val: 0.3640, Test: 0.3880
Epoch: 6, Loss: 1.7407, Train: 0.7214, Val: 0.3820, Test: 0.4140
Epoch: 7, Loss: 1.7911, Train: 0.7857, Val: 0.4480, Test: 0.4470
Epoch: 8, Loss: 1.6794, Train: 0.8214, Val: 0.4540, Test: 0.4690
Epoch: 9, Loss: 1.6320, Train: 0.8571, Val: 0.4820, Test: 0.5040
Epoch: 10, Loss: 1.5854, Train: 0.8500, Val: 0.5280, Test: 0.5420
Epoch: 11, Loss: 1.4860, Train: 0.8857, Val: 0.5740, Test: 0.5850
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 12, Loss: 1.4952, Train: 0.9000, Val: 0.6140, Test: 0.6450
Epoch: 13, Loss: 1.4438, Train: 0.9214, Val: 0.6460, Test: 0.6740
Epoch: 14, Loss: 1.3019, Train: 0.9286, Val: 0.6680, Test: 0.6840
Epoch: 15, Loss: 1.2570, Train: 0.9429, Val: 0.6760, Test: 0.6950
Epoch: 16, Loss: 1.2183, Train: 0.9429, Val: 0.6780, Test: 0.7000
Epoch: 17, Loss: 1.1345, Train: 0.9429, Val: 0.6840, Test: 0.7070
Epoch: 18, Loss: 1.0329, Train: 0.9429, Val: 0.6960, Test: 0.7140
Epoch: 19, Loss: 0.9761, Train: 0.9429, Val: 0.7080, Test: 0.7210
Epoch: 20, Loss: 0.8712, Train: 0.9500, Val: 0.7260, Test: 0.7310
Epoch: 21, Loss: 0.8202, Train: 0.9500, Val: 0.7320, Test: 0.7340
Epoch: 22, Loss: 0.7961, Train: 0.9571, Val: 0.7360, Test: 0.7480
Epoch: 23, Loss: 0.7169, Train: 0.9357, Val: 0.7360, Test: 0.7430
Epoch: 24, Loss: 0.5625, Train: 0.9429, Val: 0.7300, Test: 0.7360
Epoch: 25, Loss: 0.5638, Train: 0.9500, Val: 0.7320, Test: 0.7410
Epoch: 26, Loss: 0.5563, Train: 0.9500, Val: 0.7340, Test: 0.7370
Epoch: 27, Loss: 0.4562, Train: 0.9571, Val: 0.7380, Test: 0.7380
Epoch: 28, Loss: 0.4367, Train: 0.9643, Val: 0.7440, Test: 0.7430
Epoch: 29, Loss: 0.4244, Train: 0.9714, Val: 0.7460, Test: 0.7460
Epoch: 30, Loss: 0.3793, Train: 0.9857, Val: 0.7460, Test: 0.7570
Epoch: 31, Loss: 0.3548, Train: 0.9857, Val: 0.7500, Test: 0.7660
Epoch: 32, Loss: 0.2516, Train: 0.9857, Val: 0.7580, Test: 0.7730
Epoch: 33, Loss: 0.3676, Train: 0.9857, Val: 0.7600, Test: 0.7750
Epoch: 34, Loss: 0.3100, Train: 0.9857, Val: 0.7560, Test: 0.7630
Epoch: 35, Loss: 0.2322, Train: 0.9857, Val: 0.7600, Test: 0.7520
Epoch: 36, Loss: 0.2169, Train: 0.9786, Val: 0.7500, Test: 0.7440
Epoch: 37, Loss: 0.2395, Train: 0.9857, Val: 0.7380, Test: 0.7410
Epoch: 38, Loss: 0.2454, Train: 0.9857, Val: 0.7320, Test: 0.7300
Epoch: 39, Loss: 0.2053, Train: 0.9857, Val: 0.7260, Test: 0.7250
Epoch: 40, Loss: 0.1962, Train: 0.9929, Val: 0.7240, Test: 0.7320
Epoch: 41, Loss: 0.1746, Train: 0.9929, Val: 0.7200, Test: 0.7410
Epoch: 42, Loss: 0.1604, Train: 0.9929, Val: 0.7420, Test: 0.7450
Epoch: 43, Loss: 0.1296, Train: 0.9929, Val: 0.7500, Test: 0.7420
Epoch: 44, Loss: 0.1243, Train: 0.9929, Val: 0.7600, Test: 0.7480
Epoch: 45, Loss: 0.1331, Train: 0.9929, Val: 0.7600, Test: 0.7520
Epoch: 46, Loss: 0.1476, Train: 0.9929, Val: 0.7560, Test: 0.7530
Epoch: 47, Loss: 0.1133, Train: 0.9929, Val: 0.7540, Test: 0.7530
Epoch: 48, Loss: 0.1104, Train: 0.9929, Val: 0.7460, Test: 0.7550
Epoch: 49, Loss: 0.1582, Train: 0.9929, Val: 0.7500, Test: 0.7500
Epoch: 50, Loss: 0.0992, Train: 0.9857, Val: 0.7400, Test: 0.7420
MAD:  0.8151
Best Test Accuracy: 0.7750, Val Accuracy: 0.7600, Train Accuracy: 0.9857
Training completed.
Seed:  3
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1158, Train: 0.1500, Val: 0.1380, Test: 0.1480
Epoch: 2, Loss: 2.0182, Train: 0.4357, Val: 0.2500, Test: 0.2710
Epoch: 3, Loss: 1.8802, Train: 0.5643, Val: 0.3580, Test: 0.4050
Epoch: 4, Loss: 1.8840, Train: 0.6571, Val: 0.4200, Test: 0.4570
Epoch: 5, Loss: 1.8051, Train: 0.7500, Val: 0.4740, Test: 0.5280
Epoch: 6, Loss: 1.7254, Train: 0.8143, Val: 0.5240, Test: 0.5890
Epoch: 7, Loss: 1.6734, Train: 0.8714, Val: 0.5780, Test: 0.6300
Epoch: 8, Loss: 1.6564, Train: 0.9071, Val: 0.6380, Test: 0.6760
Epoch: 9, Loss: 1.5454, Train: 0.9357, Val: 0.6660, Test: 0.7180
Epoch: 10, Loss: 1.4392, Train: 0.9500, Val: 0.7000, Test: 0.7490
Epoch: 11, Loss: 1.4496, Train: 0.9500, Val: 0.7260, Test: 0.7620
Epoch: 12, Loss: 1.3038, Train: 0.9571, Val: 0.7360, Test: 0.7660
Epoch: 13, Loss: 1.2905, Train: 0.9643, Val: 0.7520, Test: 0.7650
Epoch: 14, Loss: 1.1703, Train: 0.9643, Val: 0.7620, Test: 0.7640
Epoch: 15, Loss: 1.1335, Train: 0.9643, Val: 0.7720, Test: 0.7750
Epoch: 16, Loss: 1.0669, Train: 0.9643, Val: 0.7760, Test: 0.7820
Epoch: 17, Loss: 0.9674, Train: 0.9643, Val: 0.7780, Test: 0.7810
Epoch: 18, Loss: 0.8502, Train: 0.9714, Val: 0.7860, Test: 0.7850
Epoch: 19, Loss: 0.7419, Train: 0.9643, Val: 0.7860, Test: 0.7870
Epoch: 20, Loss: 0.7522, Train: 0.9714, Val: 0.7820, Test: 0.7850
Epoch: 21, Loss: 0.6164, Train: 0.9643, Val: 0.7700, Test: 0.7840
Epoch: 22, Loss: 0.5512, Train: 0.9643, Val: 0.7500, Test: 0.7680
Epoch: 23, Loss: 0.5762, Train: 0.9714, Val: 0.7400, Test: 0.7510
Epoch: 24, Loss: 0.4719, Train: 0.9714, Val: 0.7360, Test: 0.7460
Epoch: 25, Loss: 0.4457, Train: 0.9643, Val: 0.7300, Test: 0.7400
Epoch: 26, Loss: 0.4284, Train: 0.9643, Val: 0.7300, Test: 0.7450
Epoch: 27, Loss: 0.4782, Train: 0.9786, Val: 0.7360, Test: 0.7560
Epoch: 28, Loss: 0.3553, Train: 0.9929, Val: 0.7440, Test: 0.7630
Epoch: 29, Loss: 0.3249, Train: 0.9929, Val: 0.7520, Test: 0.7720
Epoch: 30, Loss: 0.3360, Train: 0.9929, Val: 0.7580, Test: 0.7770
Epoch: 31, Loss: 0.3825, Train: 0.9929, Val: 0.7540, Test: 0.7740
Epoch: 32, Loss: 0.2550, Train: 0.9929, Val: 0.7600, Test: 0.7730
Epoch: 33, Loss: 0.2346, Train: 0.9929, Val: 0.7580, Test: 0.7730
Epoch: 34, Loss: 0.1723, Train: 0.9929, Val: 0.7660, Test: 0.7730
Epoch: 35, Loss: 0.1972, Train: 0.9929, Val: 0.7620, Test: 0.7650
Epoch: 36, Loss: 0.2375, Train: 0.9929, Val: 0.7600, Test: 0.7620
Epoch: 37, Loss: 0.2078, Train: 0.9929, Val: 0.7580, Test: 0.7610
Epoch: 38, Loss: 0.1754, Train: 0.9929, Val: 0.7580, Test: 0.7610
Epoch: 39, Loss: 0.2121, Train: 0.9929, Val: 0.7580, Test: 0.7660
Epoch: 40, Loss: 0.1371, Train: 0.9929, Val: 0.7620, Test: 0.7730
Epoch: 41, Loss: 0.1189, Train: 0.9929, Val: 0.7620, Test: 0.7720
Epoch: 42, Loss: 0.1288, Train: 0.9929, Val: 0.7640, Test: 0.7730
Epoch: 43, Loss: 0.1143, Train: 0.9929, Val: 0.7640, Test: 0.7790
Epoch: 44, Loss: 0.1192, Train: 0.9929, Val: 0.7720, Test: 0.7850
Epoch: 45, Loss: 0.0899, Train: 0.9929, Val: 0.7740, Test: 0.7930
Epoch: 46, Loss: 0.0791, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 47, Loss: 0.1482, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 48, Loss: 0.0829, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 49, Loss: 0.0565, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 50, Loss: 0.0606, Train: 1.0000, Val: 0.7620, Test: 0.7850
MAD:  0.748
Best Test Accuracy: 0.7930, Val Accuracy: 0.7740, Train Accuracy: 0.9929
Training completed.
Seed:  4
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0272, Train: 0.2214, Val: 0.1880, Test: 0.1790
Epoch: 2, Loss: 1.9650, Train: 0.4786, Val: 0.3320, Test: 0.3470
Epoch: 3, Loss: 1.8772, Train: 0.6643, Val: 0.4380, Test: 0.4400
Epoch: 4, Loss: 1.8716, Train: 0.7857, Val: 0.4920, Test: 0.4930
Epoch: 5, Loss: 1.7642, Train: 0.8000, Val: 0.5180, Test: 0.5010
Epoch: 6, Loss: 1.7631, Train: 0.8214, Val: 0.5220, Test: 0.5360
Epoch: 7, Loss: 1.6901, Train: 0.8357, Val: 0.5440, Test: 0.5580
Epoch: 8, Loss: 1.6329, Train: 0.8714, Val: 0.5900, Test: 0.5950
Epoch: 9, Loss: 1.5786, Train: 0.8929, Val: 0.6340, Test: 0.6400
Epoch: 10, Loss: 1.5050, Train: 0.9000, Val: 0.6520, Test: 0.6820
Epoch: 11, Loss: 1.4275, Train: 0.9071, Val: 0.6880, Test: 0.7080
Epoch: 12, Loss: 1.2978, Train: 0.9214, Val: 0.7160, Test: 0.7230
Epoch: 13, Loss: 1.2167, Train: 0.9286, Val: 0.7240, Test: 0.7430
Epoch: 14, Loss: 1.1411, Train: 0.9357, Val: 0.7340, Test: 0.7560
Epoch: 15, Loss: 1.0712, Train: 0.9429, Val: 0.7380, Test: 0.7590
Epoch: 16, Loss: 1.0189, Train: 0.9500, Val: 0.7440, Test: 0.7580
Epoch: 17, Loss: 0.9578, Train: 0.9571, Val: 0.7520, Test: 0.7590
Epoch: 18, Loss: 0.7870, Train: 0.9643, Val: 0.7540, Test: 0.7650
Epoch: 19, Loss: 0.7611, Train: 0.9714, Val: 0.7580, Test: 0.7660
Epoch: 20, Loss: 0.6433, Train: 0.9643, Val: 0.7560, Test: 0.7660
Epoch: 21, Loss: 0.6025, Train: 0.9643, Val: 0.7580, Test: 0.7640
Epoch: 22, Loss: 0.6612, Train: 0.9643, Val: 0.7620, Test: 0.7690
Epoch: 23, Loss: 0.5126, Train: 0.9643, Val: 0.7640, Test: 0.7750
Epoch: 24, Loss: 0.4220, Train: 0.9714, Val: 0.7760, Test: 0.7830
Epoch: 25, Loss: 0.4826, Train: 0.9714, Val: 0.7860, Test: 0.7880
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 26, Loss: 0.3506, Train: 0.9714, Val: 0.7920, Test: 0.7940
Epoch: 27, Loss: 0.3684, Train: 0.9786, Val: 0.7860, Test: 0.7930
Epoch: 28, Loss: 0.3206, Train: 0.9786, Val: 0.7740, Test: 0.7890
Epoch: 29, Loss: 0.2494, Train: 0.9786, Val: 0.7720, Test: 0.7830
Epoch: 30, Loss: 0.2499, Train: 0.9714, Val: 0.7580, Test: 0.7790
Epoch: 31, Loss: 0.1901, Train: 0.9786, Val: 0.7500, Test: 0.7790
Epoch: 32, Loss: 0.2392, Train: 0.9786, Val: 0.7440, Test: 0.7860
Epoch: 33, Loss: 0.2203, Train: 0.9857, Val: 0.7500, Test: 0.7870
Epoch: 34, Loss: 0.2565, Train: 0.9857, Val: 0.7460, Test: 0.7920
Epoch: 35, Loss: 0.1619, Train: 0.9857, Val: 0.7460, Test: 0.7890
Epoch: 36, Loss: 0.2131, Train: 0.9857, Val: 0.7520, Test: 0.7900
Epoch: 37, Loss: 0.2112, Train: 0.9929, Val: 0.7500, Test: 0.7870
Epoch: 38, Loss: 0.0955, Train: 1.0000, Val: 0.7480, Test: 0.7870
Epoch: 39, Loss: 0.1222, Train: 1.0000, Val: 0.7480, Test: 0.7820
Epoch: 40, Loss: 0.1174, Train: 1.0000, Val: 0.7480, Test: 0.7810
Epoch: 41, Loss: 0.1103, Train: 1.0000, Val: 0.7460, Test: 0.7840
Epoch: 42, Loss: 0.1115, Train: 1.0000, Val: 0.7560, Test: 0.7850
Epoch: 43, Loss: 0.0966, Train: 1.0000, Val: 0.7540, Test: 0.7900
Epoch: 44, Loss: 0.1272, Train: 0.9929, Val: 0.7580, Test: 0.7890
Epoch: 45, Loss: 0.1468, Train: 0.9929, Val: 0.7660, Test: 0.7870
Epoch: 46, Loss: 0.1012, Train: 0.9929, Val: 0.7660, Test: 0.7880
Epoch: 47, Loss: 0.0651, Train: 0.9929, Val: 0.7640, Test: 0.7860
Epoch: 48, Loss: 0.1195, Train: 1.0000, Val: 0.7540, Test: 0.7850
Epoch: 49, Loss: 0.0581, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 50, Loss: 0.0644, Train: 1.0000, Val: 0.7500, Test: 0.7790
MAD:  0.6972
Best Test Accuracy: 0.7940, Val Accuracy: 0.7920, Train Accuracy: 0.9714
Training completed.
Seed:  5
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0140, Train: 0.3357, Val: 0.2640, Test: 0.2530
Epoch: 2, Loss: 1.9452, Train: 0.5786, Val: 0.3860, Test: 0.4030
Epoch: 3, Loss: 1.8976, Train: 0.7571, Val: 0.4380, Test: 0.4910
Epoch: 4, Loss: 1.7792, Train: 0.8357, Val: 0.5220, Test: 0.5750
Epoch: 5, Loss: 1.7284, Train: 0.8643, Val: 0.5740, Test: 0.6330
Epoch: 6, Loss: 1.7021, Train: 0.8786, Val: 0.6300, Test: 0.6730
Epoch: 7, Loss: 1.6093, Train: 0.9286, Val: 0.6580, Test: 0.6960
Epoch: 8, Loss: 1.6054, Train: 0.9429, Val: 0.6840, Test: 0.7270
Epoch: 9, Loss: 1.4671, Train: 0.9500, Val: 0.7180, Test: 0.7490
Epoch: 10, Loss: 1.4472, Train: 0.9643, Val: 0.7340, Test: 0.7630
Epoch: 11, Loss: 1.3438, Train: 0.9571, Val: 0.7580, Test: 0.7780
Epoch: 12, Loss: 1.2786, Train: 0.9643, Val: 0.7640, Test: 0.7740
Epoch: 13, Loss: 1.1958, Train: 0.9500, Val: 0.7640, Test: 0.7680
Epoch: 14, Loss: 1.1473, Train: 0.9357, Val: 0.7680, Test: 0.7650
Epoch: 15, Loss: 1.0566, Train: 0.9286, Val: 0.7420, Test: 0.7390
Epoch: 16, Loss: 0.9877, Train: 0.9357, Val: 0.7400, Test: 0.7360
Epoch: 17, Loss: 0.9124, Train: 0.9429, Val: 0.7300, Test: 0.7350
Epoch: 18, Loss: 0.8158, Train: 0.9429, Val: 0.7260, Test: 0.7340
Epoch: 19, Loss: 0.7360, Train: 0.9429, Val: 0.7220, Test: 0.7400
Epoch: 20, Loss: 0.6260, Train: 0.9571, Val: 0.7260, Test: 0.7380
Epoch: 21, Loss: 0.5673, Train: 0.9643, Val: 0.7320, Test: 0.7490
Epoch: 22, Loss: 0.5628, Train: 0.9643, Val: 0.7420, Test: 0.7570
Epoch: 23, Loss: 0.4478, Train: 0.9643, Val: 0.7500, Test: 0.7570
Epoch: 24, Loss: 0.5221, Train: 0.9714, Val: 0.7400, Test: 0.7540
Epoch: 25, Loss: 0.4644, Train: 0.9714, Val: 0.7340, Test: 0.7520
Epoch: 26, Loss: 0.4082, Train: 0.9714, Val: 0.7280, Test: 0.7500
Epoch: 27, Loss: 0.3634, Train: 0.9714, Val: 0.7300, Test: 0.7480
Epoch: 28, Loss: 0.3538, Train: 0.9786, Val: 0.7360, Test: 0.7500
Epoch: 29, Loss: 0.3107, Train: 0.9786, Val: 0.7440, Test: 0.7530
Epoch: 30, Loss: 0.3089, Train: 0.9786, Val: 0.7380, Test: 0.7480
Epoch: 31, Loss: 0.2563, Train: 0.9857, Val: 0.7380, Test: 0.7490
Epoch: 32, Loss: 0.2527, Train: 0.9857, Val: 0.7320, Test: 0.7370
Epoch: 33, Loss: 0.2643, Train: 0.9929, Val: 0.7180, Test: 0.7300
Epoch: 34, Loss: 0.1474, Train: 0.9929, Val: 0.7060, Test: 0.7190
Epoch: 35, Loss: 0.2403, Train: 0.9929, Val: 0.7000, Test: 0.7150
Epoch: 36, Loss: 0.1801, Train: 0.9929, Val: 0.7060, Test: 0.7140
Epoch: 37, Loss: 0.1273, Train: 0.9929, Val: 0.7100, Test: 0.7210
Epoch: 38, Loss: 0.1942, Train: 0.9929, Val: 0.7180, Test: 0.7250
Epoch: 39, Loss: 0.2023, Train: 0.9929, Val: 0.7180, Test: 0.7270
Epoch: 40, Loss: 0.1904, Train: 0.9929, Val: 0.7220, Test: 0.7270
Epoch: 41, Loss: 0.2037, Train: 0.9929, Val: 0.7200, Test: 0.7280
Epoch: 42, Loss: 0.1766, Train: 0.9929, Val: 0.7220, Test: 0.7300
Epoch: 43, Loss: 0.1464, Train: 0.9929, Val: 0.7120, Test: 0.7330
Epoch: 44, Loss: 0.0963, Train: 0.9929, Val: 0.7080, Test: 0.7360
Epoch: 45, Loss: 0.1351, Train: 0.9929, Val: 0.7060, Test: 0.7360
Epoch: 46, Loss: 0.1878, Train: 0.9929, Val: 0.7040, Test: 0.7260
Epoch: 47, Loss: 0.1165, Train: 0.9929, Val: 0.7020, Test: 0.7240
Epoch: 48, Loss: 0.1185, Train: 0.9929, Val: 0.6980, Test: 0.7190
Epoch: 49, Loss: 0.1356, Train: 0.9929, Val: 0.6980, Test: 0.7150
Epoch: 50, Loss: 0.0735, Train: 0.9929, Val: 0.6960, Test: 0.7130
MAD:  0.7857
Best Test Accuracy: 0.7780, Val Accuracy: 0.7580, Train Accuracy: 0.9571
Training completed.
Seed:  6
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0952, Train: 0.3143, Val: 0.2140, Test: 0.2320
Epoch: 2, Loss: 1.9315, Train: 0.4571, Val: 0.3120, Test: 0.3200
Epoch: 3, Loss: 1.8978, Train: 0.6286, Val: 0.4180, Test: 0.4500
Epoch: 4, Loss: 1.7915, Train: 0.7357, Val: 0.5420, Test: 0.5670
Epoch: 5, Loss: 1.8290, Train: 0.8071, Val: 0.5880, Test: 0.6020
Epoch: 6, Loss: 1.6814, Train: 0.8571, Val: 0.6140, Test: 0.6290
Epoch: 7, Loss: 1.7294, Train: 0.8643, Val: 0.6460, Test: 0.6490
Epoch: 8, Loss: 1.6163, Train: 0.8857, Val: 0.6500, Test: 0.6730
Epoch: 9, Loss: 1.5264, Train: 0.9000, Val: 0.6760, Test: 0.6850
Epoch: 10, Loss: 1.4345, Train: 0.9071, Val: 0.6980, Test: 0.7010
Epoch: 11, Loss: 1.3883, Train: 0.9286, Val: 0.7000, Test: 0.7230
Epoch: 12, Loss: 1.3262, Train: 0.9429, Val: 0.7200, Test: 0.7480
Epoch: 13, Loss: 1.2519, Train: 0.9500, Val: 0.7320, Test: 0.7650
Epoch: 14, Loss: 1.2023, Train: 0.9500, Val: 0.7380, Test: 0.7730
Epoch: 15, Loss: 1.1301, Train: 0.9571, Val: 0.7420, Test: 0.7730
Epoch: 16, Loss: 0.9741, Train: 0.9571, Val: 0.7440, Test: 0.7760
Epoch: 17, Loss: 0.9325, Train: 0.9571, Val: 0.7500, Test: 0.7840
Epoch: 18, Loss: 0.8524, Train: 0.9643, Val: 0.7580, Test: 0.7870
Epoch: 19, Loss: 0.7556, Train: 0.9643, Val: 0.7680, Test: 0.7900
Epoch: 20, Loss: 0.7033, Train: 0.9714, Val: 0.7780, Test: 0.7900
Epoch: 21, Loss: 0.6293, Train: 0.9786, Val: 0.7800, Test: 0.7930
Epoch: 22, Loss: 0.5996, Train: 0.9786, Val: 0.7860, Test: 0.7970
Epoch: 23, Loss: 0.5642, Train: 0.9786, Val: 0.7820, Test: 0.7960
Epoch: 24, Loss: 0.4507, Train: 0.9786, Val: 0.7740, Test: 0.7920
Epoch: 25, Loss: 0.4709, Train: 0.9714, Val: 0.7760, Test: 0.7900
Epoch: 26, Loss: 0.4168, Train: 0.9786, Val: 0.7780, Test: 0.7900
Epoch: 27, Loss: 0.2865, Train: 0.9786, Val: 0.7780, Test: 0.7910
Epoch: 28, Loss: 0.3642, Train: 0.9786, Val: 0.7780, Test: 0.7930
Epoch: 29, Loss: 0.3263, Train: 0.9786, Val: 0.7820, Test: 0.7960
Epoch: 30, Loss: 0.3384, Train: 0.9786, Val: 0.7860, Test: 0.7960
Epoch: 31, Loss: 0.2098, Train: 0.9857, Val: 0.7900, Test: 0.8050
Epoch: 32, Loss: 0.2445, Train: 0.9857, Val: 0.7920, Test: 0.8100
Epoch: 33, Loss: 0.2109, Train: 0.9929, Val: 0.7900, Test: 0.8100
Epoch: 34, Loss: 0.2106, Train: 0.9857, Val: 0.7900, Test: 0.8110
Epoch: 35, Loss: 0.1604, Train: 0.9929, Val: 0.7880, Test: 0.8100
Epoch: 36, Loss: 0.2130, Train: 0.9929, Val: 0.7880, Test: 0.8060
Epoch: 37, Loss: 0.2295, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 38, Loss: 0.2083, Train: 1.0000, Val: 0.7720, Test: 0.7980
Epoch: 39, Loss: 0.1185, Train: 1.0000, Val: 0.7760, Test: 0.7910
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 40, Loss: 0.1657, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 41, Loss: 0.1549, Train: 1.0000, Val: 0.7580, Test: 0.7860
Epoch: 42, Loss: 0.1410, Train: 1.0000, Val: 0.7560, Test: 0.7860
Epoch: 43, Loss: 0.1139, Train: 1.0000, Val: 0.7600, Test: 0.7840
Epoch: 44, Loss: 0.1000, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 45, Loss: 0.1163, Train: 0.9929, Val: 0.7760, Test: 0.7900
Epoch: 46, Loss: 0.1459, Train: 0.9929, Val: 0.7720, Test: 0.7950
Epoch: 47, Loss: 0.0956, Train: 0.9929, Val: 0.7740, Test: 0.7990
Epoch: 48, Loss: 0.1252, Train: 0.9929, Val: 0.7740, Test: 0.7990
Epoch: 49, Loss: 0.0947, Train: 0.9929, Val: 0.7660, Test: 0.7940
Epoch: 50, Loss: 0.1436, Train: 0.9929, Val: 0.7640, Test: 0.7940
MAD:  0.8118
Best Test Accuracy: 0.8110, Val Accuracy: 0.7900, Train Accuracy: 0.9857
Training completed.
Seed:  7
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9956, Train: 0.2286, Val: 0.2000, Test: 0.1900
Epoch: 2, Loss: 1.9082, Train: 0.3786, Val: 0.3280, Test: 0.3250
Epoch: 3, Loss: 1.8453, Train: 0.5500, Val: 0.4500, Test: 0.4660
Epoch: 4, Loss: 1.7833, Train: 0.7643, Val: 0.5320, Test: 0.5400
Epoch: 5, Loss: 1.7324, Train: 0.8000, Val: 0.5560, Test: 0.5860
Epoch: 6, Loss: 1.6606, Train: 0.8500, Val: 0.5820, Test: 0.6280
Epoch: 7, Loss: 1.5635, Train: 0.8643, Val: 0.6020, Test: 0.6580
Epoch: 8, Loss: 1.5367, Train: 0.8929, Val: 0.6300, Test: 0.6830
Epoch: 9, Loss: 1.4264, Train: 0.9000, Val: 0.6460, Test: 0.6990
Epoch: 10, Loss: 1.3467, Train: 0.9000, Val: 0.6620, Test: 0.7190
Epoch: 11, Loss: 1.2794, Train: 0.9071, Val: 0.6840, Test: 0.7350
Epoch: 12, Loss: 1.2243, Train: 0.9214, Val: 0.6980, Test: 0.7370
Epoch: 13, Loss: 1.1569, Train: 0.9286, Val: 0.7060, Test: 0.7450
Epoch: 14, Loss: 1.0834, Train: 0.9429, Val: 0.7200, Test: 0.7450
Epoch: 15, Loss: 0.9942, Train: 0.9429, Val: 0.7200, Test: 0.7440
Epoch: 16, Loss: 0.8748, Train: 0.9429, Val: 0.7280, Test: 0.7540
Epoch: 17, Loss: 0.8574, Train: 0.9429, Val: 0.7340, Test: 0.7560
Epoch: 18, Loss: 0.7229, Train: 0.9429, Val: 0.7300, Test: 0.7610
Epoch: 19, Loss: 0.7019, Train: 0.9429, Val: 0.7260, Test: 0.7610
Epoch: 20, Loss: 0.6582, Train: 0.9429, Val: 0.7140, Test: 0.7550
Epoch: 21, Loss: 0.5391, Train: 0.9429, Val: 0.7000, Test: 0.7460
Epoch: 22, Loss: 0.5420, Train: 0.9429, Val: 0.7100, Test: 0.7440
Epoch: 23, Loss: 0.5184, Train: 0.9571, Val: 0.7320, Test: 0.7510
Epoch: 24, Loss: 0.4876, Train: 0.9643, Val: 0.7440, Test: 0.7560
Epoch: 25, Loss: 0.4675, Train: 0.9714, Val: 0.7600, Test: 0.7640
Epoch: 26, Loss: 0.3630, Train: 0.9786, Val: 0.7680, Test: 0.7760
Epoch: 27, Loss: 0.3758, Train: 0.9857, Val: 0.7780, Test: 0.7810
Epoch: 28, Loss: 0.3962, Train: 0.9857, Val: 0.7740, Test: 0.7830
Epoch: 29, Loss: 0.3044, Train: 0.9857, Val: 0.7700, Test: 0.7850
Epoch: 30, Loss: 0.2651, Train: 0.9857, Val: 0.7640, Test: 0.7840
Epoch: 31, Loss: 0.3457, Train: 0.9857, Val: 0.7600, Test: 0.7820
Epoch: 32, Loss: 0.2435, Train: 0.9857, Val: 0.7560, Test: 0.7750
Epoch: 33, Loss: 0.2098, Train: 0.9857, Val: 0.7560, Test: 0.7740
Epoch: 34, Loss: 0.2182, Train: 0.9929, Val: 0.7520, Test: 0.7750
Epoch: 35, Loss: 0.2624, Train: 0.9929, Val: 0.7520, Test: 0.7770
Epoch: 36, Loss: 0.1852, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 37, Loss: 0.1281, Train: 1.0000, Val: 0.7500, Test: 0.7770
Epoch: 38, Loss: 0.1523, Train: 1.0000, Val: 0.7460, Test: 0.7820
Epoch: 39, Loss: 0.2131, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 40, Loss: 0.2022, Train: 1.0000, Val: 0.7520, Test: 0.7820
Epoch: 41, Loss: 0.2147, Train: 0.9929, Val: 0.7480, Test: 0.7860
Epoch: 42, Loss: 0.1065, Train: 0.9929, Val: 0.7460, Test: 0.7830
Epoch: 43, Loss: 0.0998, Train: 0.9929, Val: 0.7500, Test: 0.7840
Epoch: 44, Loss: 0.1305, Train: 0.9929, Val: 0.7500, Test: 0.7810
Epoch: 45, Loss: 0.0896, Train: 0.9929, Val: 0.7520, Test: 0.7800
Epoch: 46, Loss: 0.1176, Train: 0.9929, Val: 0.7500, Test: 0.7750
Epoch: 47, Loss: 0.1181, Train: 0.9929, Val: 0.7500, Test: 0.7730
Epoch: 48, Loss: 0.0981, Train: 0.9929, Val: 0.7460, Test: 0.7720
Epoch: 49, Loss: 0.1187, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 50, Loss: 0.1977, Train: 1.0000, Val: 0.7420, Test: 0.7780
MAD:  0.7957
Best Test Accuracy: 0.7860, Val Accuracy: 0.7480, Train Accuracy: 0.9929
Training completed.
Seed:  8
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9933, Train: 0.2714, Val: 0.2040, Test: 0.1920
Epoch: 2, Loss: 1.9983, Train: 0.4714, Val: 0.2840, Test: 0.2940
Epoch: 3, Loss: 1.9242, Train: 0.6214, Val: 0.3480, Test: 0.3620
Epoch: 4, Loss: 1.8048, Train: 0.7286, Val: 0.4180, Test: 0.4500
Epoch: 5, Loss: 1.8053, Train: 0.7714, Val: 0.4900, Test: 0.5240
Epoch: 6, Loss: 1.6880, Train: 0.8357, Val: 0.5440, Test: 0.5700
Epoch: 7, Loss: 1.6472, Train: 0.8500, Val: 0.5820, Test: 0.6120
Epoch: 8, Loss: 1.5728, Train: 0.8857, Val: 0.6020, Test: 0.6310
Epoch: 9, Loss: 1.5566, Train: 0.8857, Val: 0.6340, Test: 0.6600
Epoch: 10, Loss: 1.3844, Train: 0.9071, Val: 0.6620, Test: 0.6840
Epoch: 11, Loss: 1.3767, Train: 0.9214, Val: 0.6860, Test: 0.7130
Epoch: 12, Loss: 1.2892, Train: 0.9286, Val: 0.7100, Test: 0.7340
Epoch: 13, Loss: 1.1197, Train: 0.9286, Val: 0.7280, Test: 0.7400
Epoch: 14, Loss: 1.0683, Train: 0.9429, Val: 0.7400, Test: 0.7530
Epoch: 15, Loss: 1.0196, Train: 0.9571, Val: 0.7480, Test: 0.7590
Epoch: 16, Loss: 0.7804, Train: 0.9571, Val: 0.7560, Test: 0.7610
Epoch: 17, Loss: 0.8157, Train: 0.9500, Val: 0.7660, Test: 0.7590
Epoch: 18, Loss: 0.7090, Train: 0.9500, Val: 0.7700, Test: 0.7670
Epoch: 19, Loss: 0.6527, Train: 0.9571, Val: 0.7700, Test: 0.7660
Epoch: 20, Loss: 0.5372, Train: 0.9571, Val: 0.7640, Test: 0.7670
Epoch: 21, Loss: 0.5359, Train: 0.9643, Val: 0.7600, Test: 0.7640
Epoch: 22, Loss: 0.4810, Train: 0.9643, Val: 0.7680, Test: 0.7670
Epoch: 23, Loss: 0.4858, Train: 0.9714, Val: 0.7740, Test: 0.7720
Epoch: 24, Loss: 0.3584, Train: 0.9714, Val: 0.7820, Test: 0.7820
Epoch: 25, Loss: 0.4654, Train: 0.9786, Val: 0.7840, Test: 0.7890
Epoch: 26, Loss: 0.3462, Train: 0.9786, Val: 0.7900, Test: 0.7940
Epoch: 27, Loss: 0.4141, Train: 0.9786, Val: 0.7880, Test: 0.7950
Epoch: 28, Loss: 0.2793, Train: 0.9786, Val: 0.7860, Test: 0.7960
Epoch: 29, Loss: 0.2043, Train: 0.9786, Val: 0.7860, Test: 0.7980
Epoch: 30, Loss: 0.3367, Train: 0.9786, Val: 0.7860, Test: 0.7930
Epoch: 31, Loss: 0.2127, Train: 0.9786, Val: 0.7860, Test: 0.7940
Epoch: 32, Loss: 0.2274, Train: 0.9786, Val: 0.7760, Test: 0.7870
Epoch: 33, Loss: 0.2140, Train: 0.9857, Val: 0.7700, Test: 0.7840
Epoch: 34, Loss: 0.1873, Train: 0.9857, Val: 0.7660, Test: 0.7820
Epoch: 35, Loss: 0.1314, Train: 0.9929, Val: 0.7660, Test: 0.7880
Epoch: 36, Loss: 0.1304, Train: 0.9929, Val: 0.7720, Test: 0.7890
Epoch: 37, Loss: 0.1482, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 38, Loss: 0.1571, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 39, Loss: 0.1547, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 40, Loss: 0.2104, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 41, Loss: 0.1654, Train: 0.9929, Val: 0.7680, Test: 0.7860
Epoch: 42, Loss: 0.1418, Train: 1.0000, Val: 0.7700, Test: 0.7800
Epoch: 43, Loss: 0.1172, Train: 1.0000, Val: 0.7720, Test: 0.7830
Epoch: 44, Loss: 0.1087, Train: 1.0000, Val: 0.7760, Test: 0.7860
Epoch: 45, Loss: 0.1293, Train: 1.0000, Val: 0.7780, Test: 0.7860
Epoch: 46, Loss: 0.1022, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 47, Loss: 0.0909, Train: 1.0000, Val: 0.7700, Test: 0.7890
Epoch: 48, Loss: 0.1634, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 49, Loss: 0.1170, Train: 1.0000, Val: 0.7640, Test: 0.7960
Epoch: 50, Loss: 0.0922, Train: 1.0000, Val: 0.7580, Test: 0.7890
MAD:  0.7909
Best Test Accuracy: 0.7980, Val Accuracy: 0.7860, Train Accuracy: 0.9786
Training completed.
Seed:  9
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-3): 3 x GATConv(128, 128, heads=1)
    (4): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9762, Train: 0.2786, Val: 0.2380, Test: 0.2520
Epoch: 2, Loss: 1.8910, Train: 0.4714, Val: 0.3340, Test: 0.3610
Epoch: 3, Loss: 1.9064, Train: 0.6571, Val: 0.4600, Test: 0.5160
Epoch: 4, Loss: 1.7772, Train: 0.8071, Val: 0.5680, Test: 0.6060
Epoch: 5, Loss: 1.6631, Train: 0.8643, Val: 0.6200, Test: 0.6630
Epoch: 6, Loss: 1.6148, Train: 0.8857, Val: 0.6520, Test: 0.7000
Epoch: 7, Loss: 1.5689, Train: 0.9000, Val: 0.7000, Test: 0.7310
Epoch: 8, Loss: 1.5024, Train: 0.9286, Val: 0.7160, Test: 0.7480
Epoch: 9, Loss: 1.4239, Train: 0.9357, Val: 0.7320, Test: 0.7500
Epoch: 10, Loss: 1.4108, Train: 0.9571, Val: 0.7420, Test: 0.7590
Epoch: 11, Loss: 1.2853, Train: 0.9643, Val: 0.7560, Test: 0.7750
Epoch: 12, Loss: 1.1700, Train: 0.9500, Val: 0.7640, Test: 0.7840
Epoch: 13, Loss: 1.1445, Train: 0.9500, Val: 0.7660, Test: 0.7940
Epoch: 14, Loss: 1.0165, Train: 0.9286, Val: 0.7820, Test: 0.7940
Epoch: 15, Loss: 0.9023, Train: 0.9286, Val: 0.7820, Test: 0.7960
Epoch: 16, Loss: 0.8540, Train: 0.9071, Val: 0.7800, Test: 0.8040
Epoch: 17, Loss: 0.7798, Train: 0.9071, Val: 0.7720, Test: 0.8030
Epoch: 18, Loss: 0.7428, Train: 0.9286, Val: 0.7640, Test: 0.8040
Epoch: 19, Loss: 0.6053, Train: 0.9286, Val: 0.7680, Test: 0.7970
Epoch: 20, Loss: 0.5243, Train: 0.9357, Val: 0.7660, Test: 0.7870
Epoch: 21, Loss: 0.4529, Train: 0.9429, Val: 0.7600, Test: 0.7860
Epoch: 22, Loss: 0.4646, Train: 0.9500, Val: 0.7600, Test: 0.7900
Epoch: 23, Loss: 0.3889, Train: 0.9571, Val: 0.7640, Test: 0.7930
Epoch: 24, Loss: 0.3554, Train: 0.9571, Val: 0.7620, Test: 0.7920
Epoch: 25, Loss: 0.3696, Train: 0.9643, Val: 0.7700, Test: 0.7930
Epoch: 26, Loss: 0.2993, Train: 0.9643, Val: 0.7660, Test: 0.7940
Epoch: 27, Loss: 0.2791, Train: 0.9643, Val: 0.7680, Test: 0.7960
Epoch: 28, Loss: 0.2606, Train: 0.9643, Val: 0.7600, Test: 0.7920
Epoch: 29, Loss: 0.1779, Train: 0.9643, Val: 0.7580, Test: 0.7900
Epoch: 30, Loss: 0.2146, Train: 0.9643, Val: 0.7560, Test: 0.7930
Epoch: 31, Loss: 0.2972, Train: 0.9643, Val: 0.7620, Test: 0.7970
Epoch: 32, Loss: 0.2726, Train: 0.9643, Val: 0.7600, Test: 0.7970
Epoch: 33, Loss: 0.2505, Train: 0.9643, Val: 0.7540, Test: 0.7970
Epoch: 34, Loss: 0.1166, Train: 0.9857, Val: 0.7580, Test: 0.7890
Epoch: 35, Loss: 0.2777, Train: 0.9857, Val: 0.7580, Test: 0.7800
Epoch: 36, Loss: 0.1774, Train: 0.9929, Val: 0.7600, Test: 0.7800
Epoch: 37, Loss: 0.1364, Train: 0.9929, Val: 0.7580, Test: 0.7750
Epoch: 38, Loss: 0.1565, Train: 0.9929, Val: 0.7620, Test: 0.7770
Epoch: 39, Loss: 0.1641, Train: 0.9857, Val: 0.7700, Test: 0.7790
Epoch: 40, Loss: 0.0887, Train: 0.9857, Val: 0.7660, Test: 0.7800
Epoch: 41, Loss: 0.1378, Train: 0.9857, Val: 0.7580, Test: 0.7810
Epoch: 42, Loss: 0.1237, Train: 0.9857, Val: 0.7620, Test: 0.7830
Epoch: 43, Loss: 0.1218, Train: 0.9857, Val: 0.7600, Test: 0.7860
Epoch: 44, Loss: 0.0721, Train: 0.9857, Val: 0.7640, Test: 0.7920
Epoch: 45, Loss: 0.1311, Train: 0.9929, Val: 0.7640, Test: 0.7990
Epoch: 46, Loss: 0.0890, Train: 1.0000, Val: 0.7640, Test: 0.8010
Epoch: 47, Loss: 0.1211, Train: 0.9929, Val: 0.7780, Test: 0.8050
Epoch: 48, Loss: 0.0889, Train: 0.9929, Val: 0.7840, Test: 0.8030
Epoch: 49, Loss: 0.0635, Train: 0.9929, Val: 0.7780, Test: 0.7990
Epoch: 50, Loss: 0.0467, Train: 1.0000, Val: 0.7800, Test: 0.7930
MAD:  0.7664
Best Test Accuracy: 0.8050, Val Accuracy: 0.7780, Train Accuracy: 0.9929
Training completed.
Average Test Accuracy:  0.7946 ± 0.011568923891183667
Average MAD:  0.77096 ± 0.038888743872745477

Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8337, Train: 0.3571, Val: 0.1800, Test: 0.1750
Epoch: 2, Loss: 4.7900, Train: 0.4714, Val: 0.2640, Test: 0.2810
Epoch: 3, Loss: 4.7121, Train: 0.5286, Val: 0.3240, Test: 0.3410
Epoch: 4, Loss: 4.6820, Train: 0.5857, Val: 0.3940, Test: 0.4060
Epoch: 5, Loss: 4.6045, Train: 0.6571, Val: 0.4520, Test: 0.4560
Epoch: 6, Loss: 4.5614, Train: 0.7000, Val: 0.4900, Test: 0.5090
Epoch: 7, Loss: 4.5875, Train: 0.7214, Val: 0.5320, Test: 0.5300
Epoch: 8, Loss: 4.2452, Train: 0.7357, Val: 0.5460, Test: 0.5560
Epoch: 9, Loss: 4.4231, Train: 0.7786, Val: 0.5680, Test: 0.5830
Epoch: 10, Loss: 4.3824, Train: 0.7857, Val: 0.5860, Test: 0.6110
Epoch: 11, Loss: 4.1419, Train: 0.8000, Val: 0.6020, Test: 0.6380
Epoch: 12, Loss: 4.0641, Train: 0.8286, Val: 0.6160, Test: 0.6470
Epoch: 13, Loss: 4.1296, Train: 0.8500, Val: 0.6240, Test: 0.6560
Epoch: 14, Loss: 4.0617, Train: 0.8643, Val: 0.6360, Test: 0.6700
Epoch: 15, Loss: 4.1331, Train: 0.8857, Val: 0.6440, Test: 0.6820
Epoch: 16, Loss: 3.9342, Train: 0.9143, Val: 0.6560, Test: 0.6920
Epoch: 17, Loss: 3.8151, Train: 0.9357, Val: 0.6680, Test: 0.7020
Epoch: 18, Loss: 4.1198, Train: 0.9571, Val: 0.6660, Test: 0.7140
Epoch: 19, Loss: 4.2196, Train: 0.9714, Val: 0.6800, Test: 0.7270
Epoch: 20, Loss: 4.1488, Train: 0.9857, Val: 0.6940, Test: 0.7370
Epoch: 21, Loss: 3.6297, Train: 0.9857, Val: 0.7020, Test: 0.7480
Epoch: 22, Loss: 3.7263, Train: 0.9929, Val: 0.7060, Test: 0.7550
Epoch: 23, Loss: 3.9348, Train: 0.9929, Val: 0.7080, Test: 0.7560
Epoch: 24, Loss: 4.0407, Train: 0.9929, Val: 0.7180, Test: 0.7580
Epoch: 25, Loss: 4.1766, Train: 0.9929, Val: 0.7100, Test: 0.7610
Epoch: 26, Loss: 3.9187, Train: 0.9929, Val: 0.7140, Test: 0.7660
Epoch: 27, Loss: 3.8251, Train: 0.9929, Val: 0.7240, Test: 0.7650
Epoch: 28, Loss: 3.8024, Train: 0.9857, Val: 0.7200, Test: 0.7610
Epoch: 29, Loss: 3.5711, Train: 0.9857, Val: 0.7200, Test: 0.7550
Epoch: 30, Loss: 3.8312, Train: 0.9929, Val: 0.7220, Test: 0.7530
Epoch: 31, Loss: 3.7682, Train: 0.9929, Val: 0.7220, Test: 0.7500
Epoch: 32, Loss: 3.5548, Train: 1.0000, Val: 0.7240, Test: 0.7490
Epoch: 33, Loss: 3.8746, Train: 1.0000, Val: 0.7260, Test: 0.7500
Epoch: 34, Loss: 3.5567, Train: 1.0000, Val: 0.7260, Test: 0.7540
Epoch: 35, Loss: 3.6020, Train: 1.0000, Val: 0.7240, Test: 0.7580
Epoch: 36, Loss: 3.6739, Train: 1.0000, Val: 0.7260, Test: 0.7610
Epoch: 37, Loss: 3.6432, Train: 0.9929, Val: 0.7300, Test: 0.7620
Epoch: 38, Loss: 3.4310, Train: 0.9929, Val: 0.7300, Test: 0.7650
Epoch: 39, Loss: 3.6779, Train: 0.9929, Val: 0.7300, Test: 0.7640
Epoch: 40, Loss: 3.5719, Train: 0.9929, Val: 0.7360, Test: 0.7660
Epoch: 41, Loss: 3.5862, Train: 0.9929, Val: 0.7380, Test: 0.7680
Epoch: 42, Loss: 3.9411, Train: 0.9929, Val: 0.7380, Test: 0.7680
Epoch: 43, Loss: 3.5798, Train: 0.9929, Val: 0.7380, Test: 0.7740
Epoch: 44, Loss: 3.7619, Train: 0.9929, Val: 0.7420, Test: 0.7750
Epoch: 45, Loss: 3.6472, Train: 0.9929, Val: 0.7400, Test: 0.7750
Epoch: 46, Loss: 3.9195, Train: 0.9929, Val: 0.7420, Test: 0.7770
Epoch: 47, Loss: 3.6810, Train: 0.9929, Val: 0.7460, Test: 0.7780
Epoch: 48, Loss: 3.5056, Train: 0.9929, Val: 0.7480, Test: 0.7780
Epoch: 49, Loss: 3.6926, Train: 0.9929, Val: 0.7460, Test: 0.7790
Epoch: 50, Loss: 3.8057, Train: 1.0000, Val: 0.7480, Test: 0.7810
Epoch: 51, Loss: 3.7290, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 52, Loss: 3.7075, Train: 1.0000, Val: 0.7480, Test: 0.7790
Epoch: 53, Loss: 3.5288, Train: 1.0000, Val: 0.7500, Test: 0.7810
Epoch: 54, Loss: 3.6187, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 55, Loss: 3.5441, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 56, Loss: 3.6535, Train: 1.0000, Val: 0.7480, Test: 0.7850
Epoch: 57, Loss: 3.7530, Train: 1.0000, Val: 0.7480, Test: 0.7850
Epoch: 58, Loss: 3.5125, Train: 1.0000, Val: 0.7460, Test: 0.7850
Epoch: 59, Loss: 3.7896, Train: 1.0000, Val: 0.7440, Test: 0.7840
Epoch: 60, Loss: 3.5849, Train: 1.0000, Val: 0.7440, Test: 0.7850
Epoch: 61, Loss: 3.6846, Train: 1.0000, Val: 0.7440, Test: 0.7850
Epoch: 62, Loss: 4.0267, Train: 1.0000, Val: 0.7460, Test: 0.7860
Epoch: 63, Loss: 3.5763, Train: 1.0000, Val: 0.7460, Test: 0.7850
Epoch: 64, Loss: 3.7119, Train: 1.0000, Val: 0.7440, Test: 0.7840
Epoch: 65, Loss: 3.8458, Train: 1.0000, Val: 0.7440, Test: 0.7820
Epoch: 66, Loss: 3.8155, Train: 1.0000, Val: 0.7440, Test: 0.7820
Epoch: 67, Loss: 3.6738, Train: 1.0000, Val: 0.7440, Test: 0.7800
Epoch: 68, Loss: 3.2379, Train: 1.0000, Val: 0.7440, Test: 0.7800
Epoch: 69, Loss: 3.6052, Train: 1.0000, Val: 0.7380, Test: 0.7780
Epoch: 70, Loss: 3.9086, Train: 1.0000, Val: 0.7380, Test: 0.7770
Epoch: 71, Loss: 3.7381, Train: 1.0000, Val: 0.7400, Test: 0.7770
Epoch: 72, Loss: 3.5778, Train: 1.0000, Val: 0.7400, Test: 0.7750
Epoch: 73, Loss: 3.5399, Train: 1.0000, Val: 0.7400, Test: 0.7750
Epoch: 74, Loss: 3.8333, Train: 1.0000, Val: 0.7420, Test: 0.7750
Epoch: 75, Loss: 3.4944, Train: 1.0000, Val: 0.7420, Test: 0.7750
Epoch: 76, Loss: 3.4933, Train: 1.0000, Val: 0.7400, Test: 0.7720
Epoch: 77, Loss: 3.6375, Train: 1.0000, Val: 0.7380, Test: 0.7730
Epoch: 78, Loss: 3.8298, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 79, Loss: 3.3155, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 80, Loss: 3.6655, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 81, Loss: 3.4938, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 82, Loss: 3.6333, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 83, Loss: 3.3867, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 84, Loss: 3.9059, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 85, Loss: 3.5237, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 86, Loss: 3.4187, Train: 1.0000, Val: 0.7320, Test: 0.7670
Epoch: 87, Loss: 3.6916, Train: 1.0000, Val: 0.7320, Test: 0.7680
Epoch: 88, Loss: 3.6921, Train: 1.0000, Val: 0.7320, Test: 0.7690
Epoch: 89, Loss: 3.6255, Train: 1.0000, Val: 0.7300, Test: 0.7700
Epoch: 90, Loss: 3.8316, Train: 1.0000, Val: 0.7300, Test: 0.7710
Epoch: 91, Loss: 3.4847, Train: 1.0000, Val: 0.7300, Test: 0.7720
Epoch: 92, Loss: 3.6238, Train: 1.0000, Val: 0.7300, Test: 0.7710
Epoch: 93, Loss: 3.7224, Train: 1.0000, Val: 0.7300, Test: 0.7690
Epoch: 94, Loss: 3.7259, Train: 1.0000, Val: 0.7340, Test: 0.7690
Epoch: 95, Loss: 3.5865, Train: 1.0000, Val: 0.7360, Test: 0.7700
Epoch: 96, Loss: 3.2427, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 97, Loss: 3.4839, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 98, Loss: 3.5858, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 99, Loss: 3.6871, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 100, Loss: 3.9305, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 101, Loss: 3.5152, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 102, Loss: 3.5864, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 103, Loss: 3.3436, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 104, Loss: 3.8954, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 105, Loss: 3.4459, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 106, Loss: 3.4453, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 107, Loss: 3.6848, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 108, Loss: 3.6197, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 109, Loss: 3.8270, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 110, Loss: 3.8578, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 111, Loss: 3.5126, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 112, Loss: 3.7205, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 113, Loss: 3.6519, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 114, Loss: 3.7552, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 115, Loss: 3.5839, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 116, Loss: 3.8609, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 117, Loss: 3.7200, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 118, Loss: 3.4786, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 119, Loss: 4.0302, Train: 1.0000, Val: 0.7480, Test: 0.7710
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 120, Loss: 3.7543, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 121, Loss: 3.7562, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 122, Loss: 3.3773, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 123, Loss: 3.5804, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 124, Loss: 3.8896, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 125, Loss: 3.7902, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 126, Loss: 3.4107, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 127, Loss: 3.9261, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 128, Loss: 3.6517, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 129, Loss: 3.5818, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 130, Loss: 3.3745, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 131, Loss: 3.7216, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 132, Loss: 3.4075, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 133, Loss: 3.4426, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 134, Loss: 3.7542, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 135, Loss: 3.7860, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 136, Loss: 3.8578, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 137, Loss: 3.5120, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 138, Loss: 3.6149, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 139, Loss: 3.3038, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 140, Loss: 3.7190, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 141, Loss: 3.7870, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 142, Loss: 3.8224, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 143, Loss: 3.8208, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 144, Loss: 3.8228, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 145, Loss: 3.6151, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 146, Loss: 3.5813, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 147, Loss: 3.7515, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 148, Loss: 3.8901, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 149, Loss: 3.4760, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 150, Loss: 3.7860, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 151, Loss: 3.3031, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 152, Loss: 3.6834, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 153, Loss: 3.8188, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 154, Loss: 3.4417, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 155, Loss: 3.5794, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 156, Loss: 3.8211, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 157, Loss: 3.7165, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 158, Loss: 3.5440, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 159, Loss: 3.4780, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 160, Loss: 3.8902, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 161, Loss: 3.5111, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 162, Loss: 3.9244, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 163, Loss: 3.4753, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 164, Loss: 3.6819, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 165, Loss: 3.4744, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 166, Loss: 3.8552, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 167, Loss: 3.7166, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 168, Loss: 3.8207, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 169, Loss: 3.8203, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 170, Loss: 3.5437, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 171, Loss: 3.4760, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 172, Loss: 3.5781, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 173, Loss: 3.5454, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 174, Loss: 3.7509, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 175, Loss: 3.4382, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 176, Loss: 3.7158, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 177, Loss: 3.7160, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 178, Loss: 3.7162, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 179, Loss: 3.4763, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 180, Loss: 3.7508, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 181, Loss: 3.9233, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 182, Loss: 3.6128, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 183, Loss: 3.5417, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 184, Loss: 3.6820, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 185, Loss: 3.4746, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 186, Loss: 3.9916, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 187, Loss: 3.3374, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 188, Loss: 3.3717, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 189, Loss: 3.4756, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 190, Loss: 3.5104, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 191, Loss: 3.5789, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 192, Loss: 3.7851, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 193, Loss: 3.5089, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 194, Loss: 3.6472, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 195, Loss: 3.4056, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 196, Loss: 3.3009, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 197, Loss: 3.6131, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 198, Loss: 3.3003, Train: 1.0000, Val: 0.7520, Test: 0.7780
Epoch: 199, Loss: 3.8883, Train: 1.0000, Val: 0.7520, Test: 0.7790
Epoch: 200, Loss: 3.6811, Train: 1.0000, Val: 0.7520, Test: 0.7790
MAD:  0.4953
Best Test Accuracy: 0.7860, Val Accuracy: 0.7460, Train Accuracy: 1.0000
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8608, Train: 0.1857, Val: 0.0900, Test: 0.0860
Epoch: 2, Loss: 4.8311, Train: 0.3357, Val: 0.2400, Test: 0.2440
Epoch: 3, Loss: 4.7856, Train: 0.4357, Val: 0.3340, Test: 0.3260
Epoch: 4, Loss: 4.7145, Train: 0.4500, Val: 0.3500, Test: 0.3490
Epoch: 5, Loss: 4.6927, Train: 0.5000, Val: 0.3580, Test: 0.3790
Epoch: 6, Loss: 4.6236, Train: 0.5071, Val: 0.3760, Test: 0.4000
Epoch: 7, Loss: 4.5713, Train: 0.5357, Val: 0.4200, Test: 0.4340
Epoch: 8, Loss: 4.4994, Train: 0.5786, Val: 0.4580, Test: 0.4670
Epoch: 9, Loss: 4.3335, Train: 0.6286, Val: 0.4860, Test: 0.5040
Epoch: 10, Loss: 4.3701, Train: 0.6786, Val: 0.5160, Test: 0.5300
Epoch: 11, Loss: 4.3894, Train: 0.7286, Val: 0.5460, Test: 0.5550
Epoch: 12, Loss: 4.3741, Train: 0.7357, Val: 0.5560, Test: 0.5690
Epoch: 13, Loss: 4.2064, Train: 0.7571, Val: 0.5660, Test: 0.5850
Epoch: 14, Loss: 4.4407, Train: 0.7714, Val: 0.5680, Test: 0.5930
Epoch: 15, Loss: 4.0010, Train: 0.7786, Val: 0.5720, Test: 0.5960
Epoch: 16, Loss: 4.0574, Train: 0.8000, Val: 0.5740, Test: 0.6020
Epoch: 17, Loss: 4.1751, Train: 0.8643, Val: 0.5840, Test: 0.6140
Epoch: 18, Loss: 4.1472, Train: 0.8786, Val: 0.5980, Test: 0.6270
Epoch: 19, Loss: 4.2668, Train: 0.9214, Val: 0.6120, Test: 0.6460
Epoch: 20, Loss: 4.0554, Train: 0.9500, Val: 0.6340, Test: 0.6700
Epoch: 21, Loss: 4.0045, Train: 0.9571, Val: 0.6720, Test: 0.6870
Epoch: 22, Loss: 3.9936, Train: 0.9571, Val: 0.6800, Test: 0.7150
Epoch: 23, Loss: 3.7928, Train: 0.9786, Val: 0.7080, Test: 0.7360
Epoch: 24, Loss: 4.0617, Train: 0.9786, Val: 0.7300, Test: 0.7460
Epoch: 25, Loss: 3.7406, Train: 0.9786, Val: 0.7360, Test: 0.7590
Epoch: 26, Loss: 4.0114, Train: 0.9786, Val: 0.7460, Test: 0.7720
Epoch: 27, Loss: 3.9682, Train: 0.9786, Val: 0.7520, Test: 0.7710
Epoch: 28, Loss: 3.6467, Train: 0.9786, Val: 0.7400, Test: 0.7740
Epoch: 29, Loss: 3.8559, Train: 0.9857, Val: 0.7380, Test: 0.7740
Epoch: 30, Loss: 4.1997, Train: 0.9929, Val: 0.7400, Test: 0.7730
Epoch: 31, Loss: 3.7356, Train: 0.9929, Val: 0.7440, Test: 0.7740
Epoch: 32, Loss: 3.8046, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 33, Loss: 3.7113, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 34, Loss: 3.5886, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 35, Loss: 3.6013, Train: 1.0000, Val: 0.7380, Test: 0.7730
Epoch: 36, Loss: 3.9876, Train: 0.9929, Val: 0.7420, Test: 0.7750
Epoch: 37, Loss: 3.4581, Train: 0.9929, Val: 0.7420, Test: 0.7740
Epoch: 38, Loss: 3.7716, Train: 0.9929, Val: 0.7380, Test: 0.7730
Epoch: 39, Loss: 3.8267, Train: 0.9929, Val: 0.7420, Test: 0.7700
Epoch: 40, Loss: 3.6180, Train: 0.9929, Val: 0.7400, Test: 0.7700
Epoch: 41, Loss: 3.8162, Train: 0.9929, Val: 0.7420, Test: 0.7710
Epoch: 42, Loss: 3.6831, Train: 0.9929, Val: 0.7400, Test: 0.7710
Epoch: 43, Loss: 3.6642, Train: 0.9929, Val: 0.7420, Test: 0.7680
Epoch: 44, Loss: 3.5173, Train: 0.9929, Val: 0.7440, Test: 0.7730
Epoch: 45, Loss: 3.5848, Train: 0.9929, Val: 0.7420, Test: 0.7710
Epoch: 46, Loss: 3.6450, Train: 0.9929, Val: 0.7400, Test: 0.7690
Epoch: 47, Loss: 4.0652, Train: 0.9929, Val: 0.7380, Test: 0.7700
Epoch: 48, Loss: 3.7145, Train: 0.9929, Val: 0.7380, Test: 0.7690
Epoch: 49, Loss: 3.7399, Train: 0.9929, Val: 0.7380, Test: 0.7640
Epoch: 50, Loss: 3.4599, Train: 0.9929, Val: 0.7440, Test: 0.7630
Epoch: 51, Loss: 3.7343, Train: 0.9929, Val: 0.7520, Test: 0.7640
Epoch: 52, Loss: 3.4976, Train: 0.9929, Val: 0.7540, Test: 0.7630
Epoch: 53, Loss: 3.5036, Train: 0.9929, Val: 0.7540, Test: 0.7620
Epoch: 54, Loss: 3.3715, Train: 0.9929, Val: 0.7500, Test: 0.7610
Epoch: 55, Loss: 3.5665, Train: 0.9929, Val: 0.7440, Test: 0.7660
Epoch: 56, Loss: 3.7482, Train: 0.9929, Val: 0.7440, Test: 0.7680
Epoch: 57, Loss: 3.6477, Train: 0.9929, Val: 0.7460, Test: 0.7700
Epoch: 58, Loss: 3.4739, Train: 0.9929, Val: 0.7460, Test: 0.7700
Epoch: 59, Loss: 3.6184, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 60, Loss: 3.3540, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 61, Loss: 3.8565, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 62, Loss: 3.6056, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 63, Loss: 3.7124, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 64, Loss: 3.8519, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 65, Loss: 3.8594, Train: 1.0000, Val: 0.7380, Test: 0.7630
Epoch: 66, Loss: 3.5073, Train: 1.0000, Val: 0.7400, Test: 0.7620
Epoch: 67, Loss: 3.5672, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 68, Loss: 3.6528, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 69, Loss: 3.6049, Train: 1.0000, Val: 0.7380, Test: 0.7650
Epoch: 70, Loss: 3.2972, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 71, Loss: 3.8073, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 72, Loss: 3.6063, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 73, Loss: 3.7006, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 74, Loss: 3.8365, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 75, Loss: 3.8019, Train: 1.0000, Val: 0.7400, Test: 0.7620
Epoch: 76, Loss: 3.7002, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 77, Loss: 3.4191, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 78, Loss: 3.5334, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 79, Loss: 3.4243, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 80, Loss: 3.8317, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 81, Loss: 3.9976, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 82, Loss: 3.7634, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 83, Loss: 3.5861, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 84, Loss: 3.1450, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 85, Loss: 3.5901, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 86, Loss: 3.5249, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 87, Loss: 3.6599, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 88, Loss: 3.9664, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 89, Loss: 3.5275, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 90, Loss: 3.3882, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 91, Loss: 3.8326, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 92, Loss: 4.0033, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 93, Loss: 3.5893, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 94, Loss: 3.3813, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 95, Loss: 3.5591, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 96, Loss: 3.7954, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 97, Loss: 3.6200, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 98, Loss: 3.4512, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 99, Loss: 3.5511, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 100, Loss: 3.2742, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 101, Loss: 3.8243, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 102, Loss: 3.3442, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 103, Loss: 3.5198, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 104, Loss: 3.6215, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 105, Loss: 3.8576, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 106, Loss: 3.3808, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 107, Loss: 3.5496, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 108, Loss: 3.5513, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 109, Loss: 3.5820, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 110, Loss: 3.8590, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 111, Loss: 3.6864, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 112, Loss: 3.4832, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 113, Loss: 3.5812, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 114, Loss: 3.5482, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 115, Loss: 3.8219, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 116, Loss: 3.8589, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 117, Loss: 3.4833, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 118, Loss: 3.6160, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 119, Loss: 3.7206, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 120, Loss: 3.4468, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 121, Loss: 3.8965, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 122, Loss: 3.8900, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 123, Loss: 3.5457, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 124, Loss: 3.5844, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 125, Loss: 3.3763, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 126, Loss: 3.5804, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 127, Loss: 3.5133, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 128, Loss: 3.5839, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 129, Loss: 3.7184, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 130, Loss: 3.9243, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 131, Loss: 3.8558, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 132, Loss: 3.7528, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 133, Loss: 3.7520, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 134, Loss: 3.8223, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 135, Loss: 4.0645, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 136, Loss: 3.7885, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 137, Loss: 3.6158, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 138, Loss: 3.8555, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 139, Loss: 3.8240, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 140, Loss: 3.7867, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 141, Loss: 3.8889, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 142, Loss: 3.5807, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 143, Loss: 3.7868, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 144, Loss: 3.7187, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 145, Loss: 3.8210, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 146, Loss: 3.5109, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 147, Loss: 3.8222, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 148, Loss: 3.3362, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 149, Loss: 3.5439, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 150, Loss: 3.7163, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 151, Loss: 3.3396, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 152, Loss: 4.0970, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 153, Loss: 3.6829, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 154, Loss: 3.8551, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 155, Loss: 3.4750, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 156, Loss: 3.9233, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 157, Loss: 3.5781, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 158, Loss: 3.3022, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 159, Loss: 3.6864, Train: 1.0000, Val: 0.7440, Test: 0.7730
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 160, Loss: 3.3389, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 161, Loss: 3.8540, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 162, Loss: 3.8891, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 163, Loss: 2.9241, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 164, Loss: 3.4738, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 165, Loss: 3.6808, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 166, Loss: 3.6146, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 167, Loss: 3.5787, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 168, Loss: 3.4753, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 169, Loss: 3.9580, Train: 1.0000, Val: 0.7520, Test: 0.7730
Epoch: 170, Loss: 3.6153, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 171, Loss: 3.8903, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 172, Loss: 3.5101, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 173, Loss: 3.7183, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 174, Loss: 3.8214, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 175, Loss: 3.7503, Train: 1.0000, Val: 0.7560, Test: 0.7720
Epoch: 176, Loss: 3.5784, Train: 1.0000, Val: 0.7560, Test: 0.7720
Epoch: 177, Loss: 3.8194, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 178, Loss: 3.6828, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 179, Loss: 3.6476, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 180, Loss: 3.4748, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 181, Loss: 3.6113, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 182, Loss: 3.4052, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 183, Loss: 3.8200, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 184, Loss: 3.4759, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 185, Loss: 3.8886, Train: 1.0000, Val: 0.7540, Test: 0.7710
Epoch: 186, Loss: 3.6482, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 187, Loss: 3.4733, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 188, Loss: 4.0953, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 189, Loss: 3.6125, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 190, Loss: 3.9580, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 191, Loss: 3.5767, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 192, Loss: 3.6126, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 193, Loss: 3.7850, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 194, Loss: 3.8556, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 195, Loss: 3.5082, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 196, Loss: 3.6821, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 197, Loss: 3.6471, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 198, Loss: 3.5460, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 199, Loss: 3.7158, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 200, Loss: 3.8548, Train: 1.0000, Val: 0.7480, Test: 0.7760
MAD:  0.6513
Best Test Accuracy: 0.7760, Val Accuracy: 0.7500, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8589, Train: 0.1429, Val: 0.0760, Test: 0.0780
Epoch: 2, Loss: 4.8157, Train: 0.4357, Val: 0.2680, Test: 0.2490
Epoch: 3, Loss: 4.7685, Train: 0.5000, Val: 0.3300, Test: 0.3110
Epoch: 4, Loss: 4.7113, Train: 0.5429, Val: 0.3380, Test: 0.3200
Epoch: 5, Loss: 4.6373, Train: 0.5429, Val: 0.3440, Test: 0.3230
Epoch: 6, Loss: 4.6512, Train: 0.5714, Val: 0.3400, Test: 0.3250
Epoch: 7, Loss: 4.5016, Train: 0.5857, Val: 0.3540, Test: 0.3290
Epoch: 8, Loss: 4.3555, Train: 0.6071, Val: 0.3520, Test: 0.3360
Epoch: 9, Loss: 4.2924, Train: 0.6286, Val: 0.3600, Test: 0.3460
Epoch: 10, Loss: 4.1987, Train: 0.6500, Val: 0.3640, Test: 0.3530
Epoch: 11, Loss: 4.3185, Train: 0.6571, Val: 0.3720, Test: 0.3580
Epoch: 12, Loss: 4.1405, Train: 0.6643, Val: 0.3720, Test: 0.3610
Epoch: 13, Loss: 4.3622, Train: 0.7071, Val: 0.3840, Test: 0.3750
Epoch: 14, Loss: 4.3118, Train: 0.7286, Val: 0.3960, Test: 0.3860
Epoch: 15, Loss: 4.1614, Train: 0.7500, Val: 0.4180, Test: 0.4040
Epoch: 16, Loss: 4.1047, Train: 0.8000, Val: 0.4400, Test: 0.4430
Epoch: 17, Loss: 4.1238, Train: 0.8357, Val: 0.4780, Test: 0.4950
Epoch: 18, Loss: 3.8319, Train: 0.8929, Val: 0.5180, Test: 0.5510
Epoch: 19, Loss: 4.0455, Train: 0.9214, Val: 0.5760, Test: 0.5950
Epoch: 20, Loss: 3.8299, Train: 0.9429, Val: 0.6040, Test: 0.6370
Epoch: 21, Loss: 4.1596, Train: 0.9500, Val: 0.6380, Test: 0.6710
Epoch: 22, Loss: 4.3482, Train: 0.9500, Val: 0.6740, Test: 0.7070
Epoch: 23, Loss: 3.7713, Train: 0.9786, Val: 0.7040, Test: 0.7310
Epoch: 24, Loss: 3.7977, Train: 0.9786, Val: 0.7120, Test: 0.7400
Epoch: 25, Loss: 3.9842, Train: 0.9857, Val: 0.7180, Test: 0.7480
Epoch: 26, Loss: 3.9678, Train: 0.9857, Val: 0.7180, Test: 0.7460
Epoch: 27, Loss: 3.8449, Train: 0.9857, Val: 0.7200, Test: 0.7490
Epoch: 28, Loss: 4.1507, Train: 0.9929, Val: 0.7280, Test: 0.7490
Epoch: 29, Loss: 4.0391, Train: 1.0000, Val: 0.7240, Test: 0.7530
Epoch: 30, Loss: 3.7179, Train: 1.0000, Val: 0.7180, Test: 0.7530
Epoch: 31, Loss: 3.8221, Train: 1.0000, Val: 0.7240, Test: 0.7520
Epoch: 32, Loss: 3.6751, Train: 1.0000, Val: 0.7280, Test: 0.7540
Epoch: 33, Loss: 3.6363, Train: 1.0000, Val: 0.7420, Test: 0.7570
Epoch: 34, Loss: 3.6291, Train: 1.0000, Val: 0.7460, Test: 0.7560
Epoch: 35, Loss: 3.6862, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 36, Loss: 3.8973, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 37, Loss: 3.7548, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 38, Loss: 3.8341, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 39, Loss: 3.7869, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 40, Loss: 3.4321, Train: 0.9929, Val: 0.7440, Test: 0.7620
Epoch: 41, Loss: 3.5115, Train: 0.9929, Val: 0.7460, Test: 0.7660
Epoch: 42, Loss: 3.6479, Train: 0.9929, Val: 0.7520, Test: 0.7690
Epoch: 43, Loss: 3.7894, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 44, Loss: 3.5289, Train: 1.0000, Val: 0.7580, Test: 0.7730
Epoch: 45, Loss: 3.6564, Train: 1.0000, Val: 0.7600, Test: 0.7730
Epoch: 46, Loss: 3.9120, Train: 0.9929, Val: 0.7580, Test: 0.7720
Epoch: 47, Loss: 3.8998, Train: 0.9929, Val: 0.7580, Test: 0.7720
Epoch: 48, Loss: 3.7029, Train: 0.9929, Val: 0.7580, Test: 0.7720
Epoch: 49, Loss: 3.7003, Train: 0.9929, Val: 0.7580, Test: 0.7720
Epoch: 50, Loss: 3.6026, Train: 0.9929, Val: 0.7600, Test: 0.7700
Epoch: 51, Loss: 3.8656, Train: 0.9929, Val: 0.7560, Test: 0.7680
Epoch: 52, Loss: 3.5585, Train: 1.0000, Val: 0.7540, Test: 0.7670
Epoch: 53, Loss: 3.7984, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 54, Loss: 3.4282, Train: 1.0000, Val: 0.7520, Test: 0.7670
Epoch: 55, Loss: 3.6686, Train: 1.0000, Val: 0.7520, Test: 0.7700
Epoch: 56, Loss: 3.8998, Train: 1.0000, Val: 0.7560, Test: 0.7700
Epoch: 57, Loss: 3.9539, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 58, Loss: 3.6521, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 59, Loss: 3.9172, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 60, Loss: 3.6025, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 61, Loss: 3.6806, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 62, Loss: 3.6551, Train: 1.0000, Val: 0.7400, Test: 0.7730
Epoch: 63, Loss: 3.4055, Train: 1.0000, Val: 0.7400, Test: 0.7710
Epoch: 64, Loss: 3.7386, Train: 1.0000, Val: 0.7400, Test: 0.7730
Epoch: 65, Loss: 3.9771, Train: 1.0000, Val: 0.7400, Test: 0.7720
Epoch: 66, Loss: 3.6640, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 67, Loss: 3.6399, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 68, Loss: 3.6086, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 69, Loss: 3.3205, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 70, Loss: 3.7411, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 71, Loss: 3.4663, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 72, Loss: 3.7355, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 73, Loss: 3.5019, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 74, Loss: 3.7734, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 75, Loss: 3.8053, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 76, Loss: 3.6947, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 77, Loss: 3.6685, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 78, Loss: 3.8400, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 79, Loss: 3.5586, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 80, Loss: 3.4549, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 81, Loss: 3.8648, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 82, Loss: 3.6312, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 83, Loss: 3.9005, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 84, Loss: 3.5258, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 85, Loss: 3.9680, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 86, Loss: 3.6688, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 87, Loss: 3.6294, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 88, Loss: 3.9321, Train: 1.0000, Val: 0.7420, Test: 0.7600
Epoch: 89, Loss: 3.4491, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 90, Loss: 3.7653, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 91, Loss: 3.6178, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 92, Loss: 3.7547, Train: 1.0000, Val: 0.7440, Test: 0.7580
Epoch: 93, Loss: 3.6894, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 94, Loss: 3.5889, Train: 1.0000, Val: 0.7420, Test: 0.7600
Epoch: 95, Loss: 3.5903, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 96, Loss: 3.7955, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 97, Loss: 3.6542, Train: 1.0000, Val: 0.7380, Test: 0.7630
Epoch: 98, Loss: 3.5834, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 99, Loss: 3.5497, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 100, Loss: 3.6938, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 101, Loss: 3.4862, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 102, Loss: 3.9963, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 103, Loss: 3.8603, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 104, Loss: 3.5176, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 105, Loss: 3.6190, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 106, Loss: 3.6227, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 107, Loss: 3.7591, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 108, Loss: 3.7599, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 109, Loss: 3.8939, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 110, Loss: 3.3781, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 111, Loss: 3.2737, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 112, Loss: 3.9651, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 113, Loss: 3.7533, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 114, Loss: 3.8266, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 115, Loss: 3.5838, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 116, Loss: 3.4448, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 117, Loss: 3.7548, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 118, Loss: 3.8236, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 119, Loss: 3.7231, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 120, Loss: 3.6863, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 121, Loss: 3.4113, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 122, Loss: 3.4113, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 123, Loss: 3.6839, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 124, Loss: 3.6530, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 125, Loss: 3.6170, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 126, Loss: 3.5801, Train: 1.0000, Val: 0.7420, Test: 0.7600
Epoch: 127, Loss: 3.5454, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 128, Loss: 3.3759, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 129, Loss: 3.8569, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 130, Loss: 3.9612, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 131, Loss: 3.6519, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 132, Loss: 3.5466, Train: 1.0000, Val: 0.7380, Test: 0.7580
Epoch: 133, Loss: 3.7184, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 134, Loss: 3.7538, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 135, Loss: 3.6485, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 136, Loss: 3.6496, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 137, Loss: 3.8567, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 138, Loss: 3.8223, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 139, Loss: 3.7900, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 140, Loss: 3.7200, Train: 1.0000, Val: 0.7380, Test: 0.7580
Epoch: 141, Loss: 3.5796, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 142, Loss: 3.8560, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 143, Loss: 3.6485, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 144, Loss: 3.8583, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 145, Loss: 3.7877, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 146, Loss: 3.3053, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 147, Loss: 3.7857, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 148, Loss: 3.3731, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 149, Loss: 3.5460, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 150, Loss: 3.5459, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 151, Loss: 4.0275, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 152, Loss: 3.5119, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 153, Loss: 3.4403, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 154, Loss: 3.1667, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 155, Loss: 3.1315, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 156, Loss: 3.5781, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 157, Loss: 3.4076, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 158, Loss: 3.6139, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 159, Loss: 3.5782, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 160, Loss: 3.5803, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 161, Loss: 3.7179, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 162, Loss: 4.0278, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 163, Loss: 3.5437, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 164, Loss: 3.6467, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 165, Loss: 3.7515, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 166, Loss: 3.5447, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 167, Loss: 3.6825, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 168, Loss: 3.7500, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 169, Loss: 3.8533, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 170, Loss: 3.7517, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 171, Loss: 3.5095, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 172, Loss: 3.4750, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 173, Loss: 3.8544, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 174, Loss: 3.6477, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 175, Loss: 3.7165, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 176, Loss: 3.6479, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 177, Loss: 3.7510, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 178, Loss: 3.5800, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 179, Loss: 3.7163, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 180, Loss: 3.5091, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 181, Loss: 3.6139, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 182, Loss: 3.5784, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 183, Loss: 3.7867, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 184, Loss: 3.4744, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 185, Loss: 3.4403, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 186, Loss: 3.8888, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 187, Loss: 3.5420, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 188, Loss: 3.5788, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 189, Loss: 3.3364, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 190, Loss: 3.7487, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 191, Loss: 3.7855, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 192, Loss: 3.4062, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 193, Loss: 3.6484, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 194, Loss: 3.7839, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 195, Loss: 3.4395, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 196, Loss: 4.0272, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 197, Loss: 3.4741, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 198, Loss: 3.7166, Train: 1.0000, Val: 0.7420, Test: 0.7670
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 199, Loss: 3.6108, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 200, Loss: 3.5777, Train: 1.0000, Val: 0.7420, Test: 0.7670
MAD:  0.357
Best Test Accuracy: 0.7740, Val Accuracy: 0.7500, Train Accuracy: 1.0000
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8449, Train: 0.2571, Val: 0.1040, Test: 0.1290
Epoch: 2, Loss: 4.8058, Train: 0.3714, Val: 0.2180, Test: 0.2500
Epoch: 3, Loss: 4.7335, Train: 0.4500, Val: 0.2920, Test: 0.3330
Epoch: 4, Loss: 4.6535, Train: 0.5571, Val: 0.3660, Test: 0.3960
Epoch: 5, Loss: 4.6351, Train: 0.6071, Val: 0.4140, Test: 0.4490
Epoch: 6, Loss: 4.6268, Train: 0.6643, Val: 0.4500, Test: 0.4850
Epoch: 7, Loss: 4.4916, Train: 0.6786, Val: 0.4740, Test: 0.5090
Epoch: 8, Loss: 4.5295, Train: 0.7071, Val: 0.5100, Test: 0.5440
Epoch: 9, Loss: 4.3020, Train: 0.7143, Val: 0.5460, Test: 0.5720
Epoch: 10, Loss: 4.4566, Train: 0.7643, Val: 0.5720, Test: 0.5910
Epoch: 11, Loss: 4.4101, Train: 0.7929, Val: 0.5840, Test: 0.6080
Epoch: 12, Loss: 4.0805, Train: 0.8214, Val: 0.5900, Test: 0.6250
Epoch: 13, Loss: 4.1704, Train: 0.8500, Val: 0.6060, Test: 0.6370
Epoch: 14, Loss: 4.1005, Train: 0.8643, Val: 0.6260, Test: 0.6500
Epoch: 15, Loss: 4.0859, Train: 0.8857, Val: 0.6360, Test: 0.6560
Epoch: 16, Loss: 3.8515, Train: 0.9000, Val: 0.6600, Test: 0.6730
Epoch: 17, Loss: 4.0183, Train: 0.9071, Val: 0.6760, Test: 0.6810
Epoch: 18, Loss: 4.0021, Train: 0.9357, Val: 0.6840, Test: 0.7050
Epoch: 19, Loss: 4.1533, Train: 0.9500, Val: 0.6920, Test: 0.7220
Epoch: 20, Loss: 4.1132, Train: 0.9571, Val: 0.7080, Test: 0.7370
Epoch: 21, Loss: 3.7106, Train: 0.9643, Val: 0.7000, Test: 0.7370
Epoch: 22, Loss: 3.8914, Train: 0.9786, Val: 0.7060, Test: 0.7470
Epoch: 23, Loss: 3.7707, Train: 0.9786, Val: 0.7080, Test: 0.7480
Epoch: 24, Loss: 4.0417, Train: 0.9786, Val: 0.7100, Test: 0.7400
Epoch: 25, Loss: 4.1826, Train: 0.9857, Val: 0.7100, Test: 0.7390
Epoch: 26, Loss: 3.7037, Train: 0.9786, Val: 0.7080, Test: 0.7370
Epoch: 27, Loss: 4.0201, Train: 0.9786, Val: 0.7120, Test: 0.7330
Epoch: 28, Loss: 4.1777, Train: 0.9857, Val: 0.7180, Test: 0.7350
Epoch: 29, Loss: 3.7959, Train: 0.9857, Val: 0.7100, Test: 0.7300
Epoch: 30, Loss: 3.3253, Train: 0.9857, Val: 0.7080, Test: 0.7300
Epoch: 31, Loss: 3.6926, Train: 0.9857, Val: 0.7080, Test: 0.7290
Epoch: 32, Loss: 3.8763, Train: 0.9857, Val: 0.7120, Test: 0.7270
Epoch: 33, Loss: 3.7089, Train: 0.9929, Val: 0.7120, Test: 0.7300
Epoch: 34, Loss: 3.9392, Train: 0.9929, Val: 0.7100, Test: 0.7300
Epoch: 35, Loss: 3.7203, Train: 0.9929, Val: 0.7120, Test: 0.7310
Epoch: 36, Loss: 3.7349, Train: 0.9929, Val: 0.7160, Test: 0.7400
Epoch: 37, Loss: 3.4849, Train: 0.9929, Val: 0.7200, Test: 0.7450
Epoch: 38, Loss: 3.7149, Train: 0.9929, Val: 0.7320, Test: 0.7560
Epoch: 39, Loss: 3.6136, Train: 0.9929, Val: 0.7320, Test: 0.7590
Epoch: 40, Loss: 3.5876, Train: 0.9929, Val: 0.7340, Test: 0.7640
Epoch: 41, Loss: 3.3656, Train: 0.9929, Val: 0.7360, Test: 0.7640
Epoch: 42, Loss: 3.9397, Train: 0.9929, Val: 0.7360, Test: 0.7670
Epoch: 43, Loss: 3.9848, Train: 0.9929, Val: 0.7340, Test: 0.7680
Epoch: 44, Loss: 3.7953, Train: 0.9929, Val: 0.7380, Test: 0.7660
Epoch: 45, Loss: 3.6152, Train: 0.9929, Val: 0.7360, Test: 0.7660
Epoch: 46, Loss: 3.5783, Train: 0.9929, Val: 0.7320, Test: 0.7660
Epoch: 47, Loss: 3.8386, Train: 1.0000, Val: 0.7340, Test: 0.7700
Epoch: 48, Loss: 4.0678, Train: 1.0000, Val: 0.7380, Test: 0.7730
Epoch: 49, Loss: 4.1242, Train: 1.0000, Val: 0.7360, Test: 0.7790
Epoch: 50, Loss: 3.8710, Train: 1.0000, Val: 0.7460, Test: 0.7830
Epoch: 51, Loss: 3.7259, Train: 1.0000, Val: 0.7440, Test: 0.7830
Epoch: 52, Loss: 3.2163, Train: 1.0000, Val: 0.7420, Test: 0.7880
Epoch: 53, Loss: 3.5841, Train: 1.0000, Val: 0.7400, Test: 0.7870
Epoch: 54, Loss: 3.7853, Train: 1.0000, Val: 0.7360, Test: 0.7890
Epoch: 55, Loss: 3.6881, Train: 1.0000, Val: 0.7340, Test: 0.7880
Epoch: 56, Loss: 3.6200, Train: 1.0000, Val: 0.7340, Test: 0.7860
Epoch: 57, Loss: 3.4460, Train: 1.0000, Val: 0.7360, Test: 0.7860
Epoch: 58, Loss: 3.3614, Train: 1.0000, Val: 0.7380, Test: 0.7850
Epoch: 59, Loss: 3.5419, Train: 1.0000, Val: 0.7380, Test: 0.7840
Epoch: 60, Loss: 3.7808, Train: 1.0000, Val: 0.7380, Test: 0.7830
Epoch: 61, Loss: 3.5789, Train: 1.0000, Val: 0.7400, Test: 0.7820
Epoch: 62, Loss: 3.6779, Train: 1.0000, Val: 0.7360, Test: 0.7800
Epoch: 63, Loss: 3.9191, Train: 1.0000, Val: 0.7340, Test: 0.7750
Epoch: 64, Loss: 3.8476, Train: 1.0000, Val: 0.7360, Test: 0.7750
Epoch: 65, Loss: 3.4991, Train: 1.0000, Val: 0.7380, Test: 0.7780
Epoch: 66, Loss: 3.6712, Train: 1.0000, Val: 0.7400, Test: 0.7760
Epoch: 67, Loss: 3.9757, Train: 1.0000, Val: 0.7380, Test: 0.7750
Epoch: 68, Loss: 3.7671, Train: 1.0000, Val: 0.7400, Test: 0.7740
Epoch: 69, Loss: 3.4317, Train: 1.0000, Val: 0.7400, Test: 0.7740
Epoch: 70, Loss: 3.7715, Train: 1.0000, Val: 0.7400, Test: 0.7760
Epoch: 71, Loss: 3.9054, Train: 1.0000, Val: 0.7420, Test: 0.7770
Epoch: 72, Loss: 3.6663, Train: 1.0000, Val: 0.7420, Test: 0.7780
Epoch: 73, Loss: 3.4906, Train: 1.0000, Val: 0.7460, Test: 0.7780
Epoch: 74, Loss: 3.7650, Train: 1.0000, Val: 0.7420, Test: 0.7780
Epoch: 75, Loss: 3.4953, Train: 1.0000, Val: 0.7420, Test: 0.7780
Epoch: 76, Loss: 3.8690, Train: 1.0000, Val: 0.7420, Test: 0.7760
Epoch: 77, Loss: 3.4623, Train: 1.0000, Val: 0.7420, Test: 0.7760
Epoch: 78, Loss: 3.8695, Train: 1.0000, Val: 0.7400, Test: 0.7760
Epoch: 79, Loss: 4.0421, Train: 1.0000, Val: 0.7420, Test: 0.7750
Epoch: 80, Loss: 3.6638, Train: 1.0000, Val: 0.7420, Test: 0.7750
Epoch: 81, Loss: 3.5906, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 82, Loss: 3.7257, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 83, Loss: 3.3894, Train: 1.0000, Val: 0.7400, Test: 0.7740
Epoch: 84, Loss: 3.6985, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 85, Loss: 3.9336, Train: 1.0000, Val: 0.7400, Test: 0.7730
Epoch: 86, Loss: 3.5864, Train: 1.0000, Val: 0.7400, Test: 0.7750
Epoch: 87, Loss: 3.5184, Train: 1.0000, Val: 0.7400, Test: 0.7750
Epoch: 88, Loss: 3.3832, Train: 1.0000, Val: 0.7400, Test: 0.7750
Epoch: 89, Loss: 3.8693, Train: 1.0000, Val: 0.7400, Test: 0.7740
Epoch: 90, Loss: 3.8633, Train: 1.0000, Val: 0.7400, Test: 0.7740
Epoch: 91, Loss: 3.6913, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 92, Loss: 3.8286, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 93, Loss: 3.5517, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 94, Loss: 3.5887, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 95, Loss: 3.6548, Train: 1.0000, Val: 0.7400, Test: 0.7720
Epoch: 96, Loss: 3.6562, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 97, Loss: 3.5570, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 98, Loss: 3.6897, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 99, Loss: 3.7256, Train: 1.0000, Val: 0.7360, Test: 0.7720
Epoch: 100, Loss: 3.4488, Train: 1.0000, Val: 0.7360, Test: 0.7720
Epoch: 101, Loss: 3.6537, Train: 1.0000, Val: 0.7360, Test: 0.7720
Epoch: 102, Loss: 3.6567, Train: 1.0000, Val: 0.7360, Test: 0.7710
Epoch: 103, Loss: 3.4836, Train: 1.0000, Val: 0.7360, Test: 0.7710
Epoch: 104, Loss: 3.4783, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 105, Loss: 3.7919, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 106, Loss: 3.7200, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 107, Loss: 3.8249, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 108, Loss: 3.6517, Train: 1.0000, Val: 0.7400, Test: 0.7710
Epoch: 109, Loss: 3.7887, Train: 1.0000, Val: 0.7400, Test: 0.7710
Epoch: 110, Loss: 3.7260, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 111, Loss: 3.6144, Train: 1.0000, Val: 0.7380, Test: 0.7730
Epoch: 112, Loss: 3.6193, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 113, Loss: 3.6859, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 114, Loss: 3.3416, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 115, Loss: 3.5126, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 116, Loss: 3.5826, Train: 1.0000, Val: 0.7380, Test: 0.7700
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 117, Loss: 3.5848, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 118, Loss: 3.8593, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 119, Loss: 3.9584, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 120, Loss: 3.6154, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 121, Loss: 3.6846, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 122, Loss: 3.5134, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 123, Loss: 3.7894, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 124, Loss: 3.6887, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 125, Loss: 3.8593, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 126, Loss: 3.8585, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 127, Loss: 3.3394, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 128, Loss: 3.5147, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 129, Loss: 3.5843, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 130, Loss: 3.4093, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 131, Loss: 3.8586, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 132, Loss: 3.7203, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 133, Loss: 3.5838, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 134, Loss: 3.7201, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 135, Loss: 3.7181, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 136, Loss: 3.8560, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 137, Loss: 3.7188, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 138, Loss: 3.8931, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 139, Loss: 3.8223, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 140, Loss: 3.4080, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 141, Loss: 3.5122, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 142, Loss: 3.6467, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 143, Loss: 3.9250, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 144, Loss: 3.6835, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 145, Loss: 3.6837, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 146, Loss: 3.3723, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 147, Loss: 3.5097, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 148, Loss: 4.1310, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 149, Loss: 3.4062, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 150, Loss: 3.5109, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 151, Loss: 3.7515, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 152, Loss: 3.5806, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 153, Loss: 3.6837, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 154, Loss: 3.6128, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 155, Loss: 3.8898, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 156, Loss: 3.4777, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 157, Loss: 3.4773, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 158, Loss: 3.7524, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 159, Loss: 3.7522, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 160, Loss: 3.3387, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 161, Loss: 3.4759, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 162, Loss: 3.4052, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 163, Loss: 3.7508, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 164, Loss: 3.7182, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 165, Loss: 3.7512, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 166, Loss: 3.6469, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 167, Loss: 3.8559, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 168, Loss: 3.4754, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 169, Loss: 3.8218, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 170, Loss: 3.6824, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 171, Loss: 3.5440, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 172, Loss: 3.6842, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 173, Loss: 3.7163, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 174, Loss: 3.4745, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 175, Loss: 3.5779, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 176, Loss: 3.6839, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 177, Loss: 3.3032, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 178, Loss: 3.7155, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 179, Loss: 3.6825, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 180, Loss: 3.4755, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 181, Loss: 3.7496, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 182, Loss: 3.6809, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 183, Loss: 3.3713, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 184, Loss: 3.6128, Train: 1.0000, Val: 0.7360, Test: 0.7670
Epoch: 185, Loss: 3.7844, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 186, Loss: 3.7505, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 187, Loss: 3.8198, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 188, Loss: 3.7176, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 189, Loss: 3.6157, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 190, Loss: 3.6137, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 191, Loss: 3.8530, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 192, Loss: 3.9239, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 193, Loss: 3.8195, Train: 1.0000, Val: 0.7380, Test: 0.7650
Epoch: 194, Loss: 3.6126, Train: 1.0000, Val: 0.7380, Test: 0.7650
Epoch: 195, Loss: 3.6800, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 196, Loss: 3.7150, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 197, Loss: 3.5770, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 198, Loss: 3.7498, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 199, Loss: 3.6472, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 200, Loss: 3.4387, Train: 1.0000, Val: 0.7460, Test: 0.7700
MAD:  0.4557
Best Test Accuracy: 0.7890, Val Accuracy: 0.7360, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8519, Train: 0.2143, Val: 0.0600, Test: 0.0700
Epoch: 2, Loss: 4.7928, Train: 0.2286, Val: 0.0740, Test: 0.0810
Epoch: 3, Loss: 4.7937, Train: 0.2714, Val: 0.0860, Test: 0.0900
Epoch: 4, Loss: 4.7064, Train: 0.3214, Val: 0.1100, Test: 0.1000
Epoch: 5, Loss: 4.6679, Train: 0.3500, Val: 0.1300, Test: 0.1190
Epoch: 6, Loss: 4.6070, Train: 0.4000, Val: 0.1580, Test: 0.1500
Epoch: 7, Loss: 4.6506, Train: 0.4429, Val: 0.1920, Test: 0.1860
Epoch: 8, Loss: 4.5814, Train: 0.4857, Val: 0.2260, Test: 0.2060
Epoch: 9, Loss: 4.4041, Train: 0.5071, Val: 0.2600, Test: 0.2370
Epoch: 10, Loss: 4.3653, Train: 0.5429, Val: 0.2720, Test: 0.2780
Epoch: 11, Loss: 4.3737, Train: 0.5714, Val: 0.3000, Test: 0.2990
Epoch: 12, Loss: 4.2953, Train: 0.6286, Val: 0.3180, Test: 0.3160
Epoch: 13, Loss: 4.4803, Train: 0.6571, Val: 0.3400, Test: 0.3520
Epoch: 14, Loss: 4.0087, Train: 0.6857, Val: 0.3700, Test: 0.3740
Epoch: 15, Loss: 4.3296, Train: 0.7143, Val: 0.3920, Test: 0.4040
Epoch: 16, Loss: 4.0599, Train: 0.7143, Val: 0.4040, Test: 0.4170
Epoch: 17, Loss: 4.2636, Train: 0.7214, Val: 0.4180, Test: 0.4390
Epoch: 18, Loss: 4.0209, Train: 0.7357, Val: 0.4320, Test: 0.4500
Epoch: 19, Loss: 4.2214, Train: 0.7643, Val: 0.4460, Test: 0.4710
Epoch: 20, Loss: 4.1016, Train: 0.7714, Val: 0.4820, Test: 0.5070
Epoch: 21, Loss: 4.1922, Train: 0.8000, Val: 0.5220, Test: 0.5370
Epoch: 22, Loss: 4.1556, Train: 0.8214, Val: 0.5460, Test: 0.5700
Epoch: 23, Loss: 4.0317, Train: 0.8286, Val: 0.5740, Test: 0.5900
Epoch: 24, Loss: 4.1063, Train: 0.8286, Val: 0.5920, Test: 0.6210
Epoch: 25, Loss: 4.1560, Train: 0.8357, Val: 0.6080, Test: 0.6360
Epoch: 26, Loss: 3.8649, Train: 0.8429, Val: 0.6200, Test: 0.6480
Epoch: 27, Loss: 4.1284, Train: 0.8500, Val: 0.6300, Test: 0.6570
Epoch: 28, Loss: 3.8439, Train: 0.8500, Val: 0.6280, Test: 0.6620
Epoch: 29, Loss: 3.6960, Train: 0.8500, Val: 0.6380, Test: 0.6640
Epoch: 30, Loss: 3.9059, Train: 0.8500, Val: 0.6420, Test: 0.6660
Epoch: 31, Loss: 3.8514, Train: 0.8500, Val: 0.6440, Test: 0.6670
Epoch: 32, Loss: 4.0884, Train: 0.8500, Val: 0.6440, Test: 0.6690
Epoch: 33, Loss: 3.8806, Train: 0.8500, Val: 0.6480, Test: 0.6700
Epoch: 34, Loss: 3.9523, Train: 0.8500, Val: 0.6340, Test: 0.6690
Epoch: 35, Loss: 4.0787, Train: 0.8500, Val: 0.6380, Test: 0.6700
Epoch: 36, Loss: 3.9706, Train: 0.8500, Val: 0.6320, Test: 0.6630
Epoch: 37, Loss: 4.0571, Train: 0.8500, Val: 0.6260, Test: 0.6640
Epoch: 38, Loss: 3.7642, Train: 0.8500, Val: 0.6260, Test: 0.6600
Epoch: 39, Loss: 3.9423, Train: 0.8500, Val: 0.6200, Test: 0.6580
Epoch: 40, Loss: 4.0492, Train: 0.8500, Val: 0.6220, Test: 0.6590
Epoch: 41, Loss: 3.9875, Train: 0.8500, Val: 0.6200, Test: 0.6540
Epoch: 42, Loss: 4.1302, Train: 0.8500, Val: 0.6200, Test: 0.6550
Epoch: 43, Loss: 3.8619, Train: 0.8500, Val: 0.6180, Test: 0.6520
Epoch: 44, Loss: 3.8991, Train: 0.8500, Val: 0.6160, Test: 0.6510
Epoch: 45, Loss: 4.0173, Train: 0.8500, Val: 0.6160, Test: 0.6490
Epoch: 46, Loss: 3.9205, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 47, Loss: 3.7119, Train: 0.8571, Val: 0.6120, Test: 0.6480
Epoch: 48, Loss: 3.7528, Train: 0.8571, Val: 0.6120, Test: 0.6500
Epoch: 49, Loss: 3.7640, Train: 0.8571, Val: 0.6140, Test: 0.6500
Epoch: 50, Loss: 3.9082, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 51, Loss: 3.6298, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 52, Loss: 3.9012, Train: 0.8571, Val: 0.6100, Test: 0.6490
Epoch: 53, Loss: 3.8111, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 54, Loss: 4.1267, Train: 0.8571, Val: 0.6120, Test: 0.6480
Epoch: 55, Loss: 3.7589, Train: 0.8571, Val: 0.6140, Test: 0.6500
Epoch: 56, Loss: 3.7149, Train: 0.8571, Val: 0.6140, Test: 0.6500
Epoch: 57, Loss: 3.7290, Train: 0.8571, Val: 0.6140, Test: 0.6510
Epoch: 58, Loss: 3.5111, Train: 0.8571, Val: 0.6160, Test: 0.6480
Epoch: 59, Loss: 3.9539, Train: 0.8571, Val: 0.6140, Test: 0.6480
Epoch: 60, Loss: 3.6760, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 61, Loss: 3.7784, Train: 0.8571, Val: 0.6120, Test: 0.6550
Epoch: 62, Loss: 3.9163, Train: 0.8571, Val: 0.6160, Test: 0.6550
Epoch: 63, Loss: 3.7420, Train: 0.8571, Val: 0.6140, Test: 0.6560
Epoch: 64, Loss: 3.9431, Train: 0.8571, Val: 0.6160, Test: 0.6580
Epoch: 65, Loss: 3.8729, Train: 0.8571, Val: 0.6160, Test: 0.6590
Epoch: 66, Loss: 3.6061, Train: 0.8571, Val: 0.6140, Test: 0.6610
Epoch: 67, Loss: 3.6988, Train: 0.8571, Val: 0.6140, Test: 0.6580
Epoch: 68, Loss: 3.8678, Train: 0.8571, Val: 0.6140, Test: 0.6590
Epoch: 69, Loss: 3.9811, Train: 0.8571, Val: 0.6140, Test: 0.6570
Epoch: 70, Loss: 3.7696, Train: 0.8571, Val: 0.6120, Test: 0.6570
Epoch: 71, Loss: 3.9776, Train: 0.8571, Val: 0.6120, Test: 0.6560
Epoch: 72, Loss: 3.7990, Train: 0.8571, Val: 0.6100, Test: 0.6590
Epoch: 73, Loss: 3.8326, Train: 0.8571, Val: 0.6100, Test: 0.6590
Epoch: 74, Loss: 3.9700, Train: 0.8571, Val: 0.6100, Test: 0.6600
Epoch: 75, Loss: 3.8679, Train: 0.8571, Val: 0.6100, Test: 0.6600
Epoch: 76, Loss: 3.8990, Train: 0.8571, Val: 0.6120, Test: 0.6590
Epoch: 77, Loss: 3.6614, Train: 0.8571, Val: 0.6120, Test: 0.6580
Epoch: 78, Loss: 3.4925, Train: 0.8571, Val: 0.6120, Test: 0.6570
Epoch: 79, Loss: 3.9732, Train: 0.8571, Val: 0.6140, Test: 0.6590
Epoch: 80, Loss: 3.5550, Train: 0.8571, Val: 0.6140, Test: 0.6580
Epoch: 81, Loss: 3.8285, Train: 0.8571, Val: 0.6120, Test: 0.6560
Epoch: 82, Loss: 3.8306, Train: 0.8571, Val: 0.6120, Test: 0.6570
Epoch: 83, Loss: 4.0695, Train: 0.8571, Val: 0.6140, Test: 0.6530
Epoch: 84, Loss: 3.9675, Train: 0.8571, Val: 0.6120, Test: 0.6500
Epoch: 85, Loss: 3.8299, Train: 0.8571, Val: 0.6120, Test: 0.6530
Epoch: 86, Loss: 3.9658, Train: 0.8571, Val: 0.6120, Test: 0.6540
Epoch: 87, Loss: 4.0377, Train: 0.8571, Val: 0.6120, Test: 0.6510
Epoch: 88, Loss: 4.2090, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 89, Loss: 3.7958, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 90, Loss: 3.9974, Train: 0.8571, Val: 0.6120, Test: 0.6510
Epoch: 91, Loss: 3.7919, Train: 0.8571, Val: 0.6140, Test: 0.6510
Epoch: 92, Loss: 3.7258, Train: 0.8571, Val: 0.6120, Test: 0.6510
Epoch: 93, Loss: 3.8972, Train: 0.8571, Val: 0.6120, Test: 0.6510
Epoch: 94, Loss: 3.8967, Train: 0.8571, Val: 0.6120, Test: 0.6510
Epoch: 95, Loss: 4.1349, Train: 0.8571, Val: 0.6120, Test: 0.6500
Epoch: 96, Loss: 3.8257, Train: 0.8571, Val: 0.6120, Test: 0.6500
Epoch: 97, Loss: 3.9996, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 98, Loss: 3.9286, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 99, Loss: 3.6215, Train: 0.8571, Val: 0.6100, Test: 0.6490
Epoch: 100, Loss: 3.6174, Train: 0.8571, Val: 0.6100, Test: 0.6490
Epoch: 101, Loss: 3.7906, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 102, Loss: 3.9986, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 103, Loss: 3.8257, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 104, Loss: 3.8570, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 105, Loss: 3.5146, Train: 0.8571, Val: 0.6100, Test: 0.6500
Epoch: 106, Loss: 3.8239, Train: 0.8571, Val: 0.6120, Test: 0.6500
Epoch: 107, Loss: 3.9638, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 108, Loss: 3.7905, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 109, Loss: 4.0296, Train: 0.8571, Val: 0.6120, Test: 0.6490
Epoch: 110, Loss: 3.6154, Train: 0.8571, Val: 0.6140, Test: 0.6480
Epoch: 111, Loss: 3.4088, Train: 0.8571, Val: 0.6140, Test: 0.6480
Epoch: 112, Loss: 3.8925, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 113, Loss: 3.7881, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 114, Loss: 3.8218, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 115, Loss: 3.8241, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 116, Loss: 3.9624, Train: 0.8571, Val: 0.6140, Test: 0.6480
Epoch: 117, Loss: 3.7881, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 118, Loss: 3.5482, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 119, Loss: 4.0645, Train: 0.8571, Val: 0.6140, Test: 0.6470
Epoch: 120, Loss: 3.8564, Train: 0.8571, Val: 0.6160, Test: 0.6470
Epoch: 121, Loss: 3.4424, Train: 0.8571, Val: 0.6160, Test: 0.6480
Epoch: 122, Loss: 3.9264, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 123, Loss: 3.8561, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 124, Loss: 3.6157, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 125, Loss: 3.8563, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 126, Loss: 3.6848, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 127, Loss: 3.6499, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 128, Loss: 4.0975, Train: 0.8571, Val: 0.6180, Test: 0.6500
Epoch: 129, Loss: 3.8212, Train: 0.8571, Val: 0.6180, Test: 0.6500
Epoch: 130, Loss: 3.6826, Train: 0.8571, Val: 0.6180, Test: 0.6500
Epoch: 131, Loss: 3.8896, Train: 0.8571, Val: 0.6180, Test: 0.6490
Epoch: 132, Loss: 3.9934, Train: 0.8571, Val: 0.6180, Test: 0.6490
Epoch: 133, Loss: 3.7871, Train: 0.8571, Val: 0.6220, Test: 0.6490
Epoch: 134, Loss: 3.8213, Train: 0.8571, Val: 0.6220, Test: 0.6500
Epoch: 135, Loss: 4.0631, Train: 0.8571, Val: 0.6220, Test: 0.6500
Epoch: 136, Loss: 3.7514, Train: 0.8571, Val: 0.6220, Test: 0.6490
Epoch: 137, Loss: 4.0295, Train: 0.8571, Val: 0.6220, Test: 0.6490
Epoch: 138, Loss: 3.6864, Train: 0.8571, Val: 0.6220, Test: 0.6500
Epoch: 139, Loss: 3.7536, Train: 0.8571, Val: 0.6220, Test: 0.6510
Epoch: 140, Loss: 3.6818, Train: 0.8571, Val: 0.6220, Test: 0.6510
Epoch: 141, Loss: 3.9219, Train: 0.8571, Val: 0.6220, Test: 0.6510
Epoch: 142, Loss: 3.8192, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 143, Loss: 3.6129, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 144, Loss: 3.8904, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 145, Loss: 3.8188, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 146, Loss: 4.0261, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 147, Loss: 3.7522, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 148, Loss: 3.6842, Train: 0.8571, Val: 0.6200, Test: 0.6550
Epoch: 149, Loss: 3.6479, Train: 0.8571, Val: 0.6200, Test: 0.6550
Epoch: 150, Loss: 3.8914, Train: 0.8571, Val: 0.6200, Test: 0.6550
Epoch: 151, Loss: 3.7527, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 152, Loss: 3.3368, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 153, Loss: 3.5792, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 154, Loss: 4.1656, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 155, Loss: 3.4077, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 156, Loss: 3.8213, Train: 0.8571, Val: 0.6200, Test: 0.6540
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 157, Loss: 3.7174, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 158, Loss: 3.9918, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 159, Loss: 3.7513, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 160, Loss: 3.7857, Train: 0.8571, Val: 0.6180, Test: 0.6530
Epoch: 161, Loss: 4.0271, Train: 0.8571, Val: 0.6180, Test: 0.6530
Epoch: 162, Loss: 4.0272, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 163, Loss: 3.7514, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 164, Loss: 3.7860, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 165, Loss: 3.6469, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 166, Loss: 3.8556, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 167, Loss: 3.6127, Train: 0.8571, Val: 0.6180, Test: 0.6520
Epoch: 168, Loss: 3.7510, Train: 0.8571, Val: 0.6180, Test: 0.6520
Epoch: 169, Loss: 3.6120, Train: 0.8571, Val: 0.6180, Test: 0.6510
Epoch: 170, Loss: 3.9248, Train: 0.8571, Val: 0.6180, Test: 0.6510
Epoch: 171, Loss: 3.7846, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 172, Loss: 3.8537, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 173, Loss: 3.9919, Train: 0.8571, Val: 0.6200, Test: 0.6500
Epoch: 174, Loss: 3.8184, Train: 0.8571, Val: 0.6200, Test: 0.6500
Epoch: 175, Loss: 3.9584, Train: 0.8571, Val: 0.6200, Test: 0.6500
Epoch: 176, Loss: 3.7856, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 177, Loss: 3.7858, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 178, Loss: 3.7862, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 179, Loss: 3.9570, Train: 0.8571, Val: 0.6200, Test: 0.6510
Epoch: 180, Loss: 3.7848, Train: 0.8571, Val: 0.6180, Test: 0.6520
Epoch: 181, Loss: 3.8188, Train: 0.8571, Val: 0.6160, Test: 0.6520
Epoch: 182, Loss: 3.6818, Train: 0.8571, Val: 0.6160, Test: 0.6520
Epoch: 183, Loss: 3.7486, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 184, Loss: 3.8897, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 185, Loss: 3.5447, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 186, Loss: 3.6112, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 187, Loss: 3.3022, Train: 0.8571, Val: 0.6180, Test: 0.6550
Epoch: 188, Loss: 4.0959, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 189, Loss: 3.9923, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 190, Loss: 3.8200, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 191, Loss: 3.9213, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 192, Loss: 3.9232, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 193, Loss: 3.6464, Train: 0.8571, Val: 0.6160, Test: 0.6550
Epoch: 194, Loss: 3.8880, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 195, Loss: 3.8192, Train: 0.8571, Val: 0.6160, Test: 0.6550
Epoch: 196, Loss: 4.1995, Train: 0.8571, Val: 0.6160, Test: 0.6550
Epoch: 197, Loss: 4.0251, Train: 0.8571, Val: 0.6160, Test: 0.6550
Epoch: 198, Loss: 3.7845, Train: 0.8571, Val: 0.6160, Test: 0.6550
Epoch: 199, Loss: 3.6821, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 200, Loss: 3.6118, Train: 0.8571, Val: 0.6160, Test: 0.6540
MAD:  0.209
Best Test Accuracy: 0.6700, Val Accuracy: 0.6480, Train Accuracy: 0.8500
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8628, Train: 0.1357, Val: 0.0820, Test: 0.0880
Epoch: 2, Loss: 4.8371, Train: 0.2357, Val: 0.2120, Test: 0.1890
Epoch: 3, Loss: 4.8075, Train: 0.2929, Val: 0.2240, Test: 0.2250
Epoch: 4, Loss: 4.7562, Train: 0.3357, Val: 0.2300, Test: 0.2360
Epoch: 5, Loss: 4.7477, Train: 0.4000, Val: 0.2400, Test: 0.2560
Epoch: 6, Loss: 4.7073, Train: 0.4714, Val: 0.2520, Test: 0.2740
Epoch: 7, Loss: 4.4655, Train: 0.5071, Val: 0.2740, Test: 0.2940
Epoch: 8, Loss: 4.5036, Train: 0.5143, Val: 0.2960, Test: 0.3140
Epoch: 9, Loss: 4.5045, Train: 0.5429, Val: 0.2980, Test: 0.3260
Epoch: 10, Loss: 4.4830, Train: 0.5357, Val: 0.3160, Test: 0.3360
Epoch: 11, Loss: 4.3879, Train: 0.5429, Val: 0.3260, Test: 0.3440
Epoch: 12, Loss: 4.1683, Train: 0.5429, Val: 0.3360, Test: 0.3510
Epoch: 13, Loss: 4.1774, Train: 0.5429, Val: 0.3480, Test: 0.3560
Epoch: 14, Loss: 4.3804, Train: 0.5571, Val: 0.3500, Test: 0.3630
Epoch: 15, Loss: 4.2094, Train: 0.5857, Val: 0.3720, Test: 0.3710
Epoch: 16, Loss: 4.5094, Train: 0.6286, Val: 0.3800, Test: 0.3850
Epoch: 17, Loss: 4.4589, Train: 0.6786, Val: 0.4080, Test: 0.4140
Epoch: 18, Loss: 4.0545, Train: 0.7286, Val: 0.4540, Test: 0.4560
Epoch: 19, Loss: 4.3291, Train: 0.7714, Val: 0.4720, Test: 0.4890
Epoch: 20, Loss: 4.0965, Train: 0.7929, Val: 0.5200, Test: 0.5340
Epoch: 21, Loss: 4.2738, Train: 0.7929, Val: 0.5460, Test: 0.5780
Epoch: 22, Loss: 4.2822, Train: 0.8143, Val: 0.5920, Test: 0.6160
Epoch: 23, Loss: 4.0043, Train: 0.8286, Val: 0.6180, Test: 0.6420
Epoch: 24, Loss: 4.3406, Train: 0.8429, Val: 0.6260, Test: 0.6600
Epoch: 25, Loss: 3.8883, Train: 0.8429, Val: 0.6400, Test: 0.6700
Epoch: 26, Loss: 4.0332, Train: 0.8429, Val: 0.6380, Test: 0.6720
Epoch: 27, Loss: 3.7351, Train: 0.8429, Val: 0.6500, Test: 0.6730
Epoch: 28, Loss: 4.1243, Train: 0.8500, Val: 0.6500, Test: 0.6730
Epoch: 29, Loss: 4.1593, Train: 0.8500, Val: 0.6440, Test: 0.6740
Epoch: 30, Loss: 3.8773, Train: 0.8500, Val: 0.6380, Test: 0.6700
Epoch: 31, Loss: 3.6864, Train: 0.8571, Val: 0.6420, Test: 0.6670
Epoch: 32, Loss: 3.9831, Train: 0.8571, Val: 0.6400, Test: 0.6680
Epoch: 33, Loss: 4.1067, Train: 0.8571, Val: 0.6440, Test: 0.6670
Epoch: 34, Loss: 3.8638, Train: 0.8571, Val: 0.6480, Test: 0.6640
Epoch: 35, Loss: 4.1703, Train: 0.8571, Val: 0.6480, Test: 0.6720
Epoch: 36, Loss: 3.8297, Train: 0.8571, Val: 0.6460, Test: 0.6710
Epoch: 37, Loss: 3.8380, Train: 0.8571, Val: 0.6480, Test: 0.6710
Epoch: 38, Loss: 3.9683, Train: 0.8571, Val: 0.6440, Test: 0.6700
Epoch: 39, Loss: 3.7732, Train: 0.8571, Val: 0.6440, Test: 0.6720
Epoch: 40, Loss: 4.0726, Train: 0.8571, Val: 0.6400, Test: 0.6720
Epoch: 41, Loss: 4.0051, Train: 0.8571, Val: 0.6320, Test: 0.6710
Epoch: 42, Loss: 3.8803, Train: 0.8571, Val: 0.6240, Test: 0.6680
Epoch: 43, Loss: 3.7065, Train: 0.8571, Val: 0.6240, Test: 0.6700
Epoch: 44, Loss: 3.9415, Train: 0.8571, Val: 0.6220, Test: 0.6680
Epoch: 45, Loss: 4.1955, Train: 0.8571, Val: 0.6240, Test: 0.6630
Epoch: 46, Loss: 3.8305, Train: 0.8571, Val: 0.6200, Test: 0.6600
Epoch: 47, Loss: 3.7661, Train: 0.8571, Val: 0.6200, Test: 0.6570
Epoch: 48, Loss: 4.0594, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 49, Loss: 3.9166, Train: 0.8500, Val: 0.6220, Test: 0.6540
Epoch: 50, Loss: 3.9991, Train: 0.8571, Val: 0.6200, Test: 0.6560
Epoch: 51, Loss: 3.9633, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 52, Loss: 3.8836, Train: 0.8571, Val: 0.6140, Test: 0.6550
Epoch: 53, Loss: 3.6739, Train: 0.8571, Val: 0.6140, Test: 0.6560
Epoch: 54, Loss: 3.9942, Train: 0.8571, Val: 0.6140, Test: 0.6560
Epoch: 55, Loss: 3.9995, Train: 0.8571, Val: 0.6160, Test: 0.6580
Epoch: 56, Loss: 3.9808, Train: 0.8571, Val: 0.6180, Test: 0.6570
Epoch: 57, Loss: 3.6061, Train: 0.8571, Val: 0.6160, Test: 0.6560
Epoch: 58, Loss: 3.7096, Train: 0.8571, Val: 0.6180, Test: 0.6570
Epoch: 59, Loss: 3.9133, Train: 0.8571, Val: 0.6180, Test: 0.6580
Epoch: 60, Loss: 4.0255, Train: 0.8571, Val: 0.6180, Test: 0.6590
Epoch: 61, Loss: 3.8734, Train: 0.8571, Val: 0.6200, Test: 0.6610
Epoch: 62, Loss: 3.8098, Train: 0.8571, Val: 0.6220, Test: 0.6620
Epoch: 63, Loss: 3.7374, Train: 0.8571, Val: 0.6200, Test: 0.6610
Epoch: 64, Loss: 4.0125, Train: 0.8571, Val: 0.6180, Test: 0.6590
Epoch: 65, Loss: 4.1166, Train: 0.8571, Val: 0.6200, Test: 0.6600
Epoch: 66, Loss: 3.8667, Train: 0.8571, Val: 0.6220, Test: 0.6630
Epoch: 67, Loss: 4.1870, Train: 0.8571, Val: 0.6240, Test: 0.6640
Epoch: 68, Loss: 3.9747, Train: 0.8571, Val: 0.6240, Test: 0.6640
Epoch: 69, Loss: 3.9365, Train: 0.8571, Val: 0.6240, Test: 0.6630
Epoch: 70, Loss: 3.6946, Train: 0.8571, Val: 0.6280, Test: 0.6630
Epoch: 71, Loss: 3.8468, Train: 0.8571, Val: 0.6300, Test: 0.6630
Epoch: 72, Loss: 4.0426, Train: 0.8571, Val: 0.6320, Test: 0.6650
Epoch: 73, Loss: 4.0430, Train: 0.8571, Val: 0.6280, Test: 0.6650
Epoch: 74, Loss: 4.0470, Train: 0.8571, Val: 0.6280, Test: 0.6640
Epoch: 75, Loss: 3.6991, Train: 0.8571, Val: 0.6280, Test: 0.6640
Epoch: 76, Loss: 3.8347, Train: 0.8571, Val: 0.6280, Test: 0.6660
Epoch: 77, Loss: 4.1435, Train: 0.8571, Val: 0.6280, Test: 0.6660
Epoch: 78, Loss: 3.7985, Train: 0.8571, Val: 0.6260, Test: 0.6650
Epoch: 79, Loss: 3.5583, Train: 0.8571, Val: 0.6260, Test: 0.6660
Epoch: 80, Loss: 3.6957, Train: 0.8571, Val: 0.6280, Test: 0.6670
Epoch: 81, Loss: 4.2099, Train: 0.8571, Val: 0.6260, Test: 0.6670
Epoch: 82, Loss: 3.6252, Train: 0.8571, Val: 0.6260, Test: 0.6660
Epoch: 83, Loss: 3.9340, Train: 0.8571, Val: 0.6280, Test: 0.6660
Epoch: 84, Loss: 3.9696, Train: 0.8571, Val: 0.6260, Test: 0.6640
Epoch: 85, Loss: 3.8998, Train: 0.8571, Val: 0.6260, Test: 0.6640
Epoch: 86, Loss: 3.8292, Train: 0.8571, Val: 0.6300, Test: 0.6620
Epoch: 87, Loss: 3.7649, Train: 0.8571, Val: 0.6300, Test: 0.6620
Epoch: 88, Loss: 3.5926, Train: 0.8571, Val: 0.6280, Test: 0.6610
Epoch: 89, Loss: 3.7992, Train: 0.8571, Val: 0.6220, Test: 0.6580
Epoch: 90, Loss: 3.8298, Train: 0.8571, Val: 0.6220, Test: 0.6560
Epoch: 91, Loss: 3.8287, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 92, Loss: 3.8977, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 93, Loss: 3.8286, Train: 0.8571, Val: 0.6180, Test: 0.6530
Epoch: 94, Loss: 3.8291, Train: 0.8571, Val: 0.6180, Test: 0.6530
Epoch: 95, Loss: 3.9325, Train: 0.8571, Val: 0.6160, Test: 0.6530
Epoch: 96, Loss: 3.9651, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 97, Loss: 4.0333, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 98, Loss: 4.2713, Train: 0.8571, Val: 0.6160, Test: 0.6530
Epoch: 99, Loss: 3.9611, Train: 0.8571, Val: 0.6160, Test: 0.6520
Epoch: 100, Loss: 3.6872, Train: 0.8571, Val: 0.6160, Test: 0.6530
Epoch: 101, Loss: 3.6181, Train: 0.8571, Val: 0.6160, Test: 0.6510
Epoch: 102, Loss: 3.8248, Train: 0.8571, Val: 0.6160, Test: 0.6510
Epoch: 103, Loss: 3.9288, Train: 0.8571, Val: 0.6140, Test: 0.6520
Epoch: 104, Loss: 3.7229, Train: 0.8571, Val: 0.6160, Test: 0.6530
Epoch: 105, Loss: 3.7933, Train: 0.8571, Val: 0.6180, Test: 0.6530
Epoch: 106, Loss: 3.9633, Train: 0.8571, Val: 0.6180, Test: 0.6540
Epoch: 107, Loss: 3.9978, Train: 0.8571, Val: 0.6200, Test: 0.6540
Epoch: 108, Loss: 4.0335, Train: 0.8571, Val: 0.6200, Test: 0.6550
Epoch: 109, Loss: 3.7889, Train: 0.8571, Val: 0.6200, Test: 0.6560
Epoch: 110, Loss: 3.7193, Train: 0.8571, Val: 0.6200, Test: 0.6550
Epoch: 111, Loss: 3.7565, Train: 0.8571, Val: 0.6240, Test: 0.6550
Epoch: 112, Loss: 3.7923, Train: 0.8571, Val: 0.6260, Test: 0.6560
Epoch: 113, Loss: 3.5842, Train: 0.8571, Val: 0.6260, Test: 0.6540
Epoch: 114, Loss: 3.4117, Train: 0.8571, Val: 0.6260, Test: 0.6540
Epoch: 115, Loss: 3.8241, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 116, Loss: 3.9975, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 117, Loss: 3.7538, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 118, Loss: 3.6874, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 119, Loss: 3.7543, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 120, Loss: 3.8241, Train: 0.8571, Val: 0.6280, Test: 0.6530
Epoch: 121, Loss: 4.0296, Train: 0.8571, Val: 0.6280, Test: 0.6530
Epoch: 122, Loss: 3.4768, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 123, Loss: 4.0614, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 124, Loss: 3.7882, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 125, Loss: 3.9599, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 126, Loss: 3.7533, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 127, Loss: 3.8236, Train: 0.8571, Val: 0.6280, Test: 0.6530
Epoch: 128, Loss: 3.8565, Train: 0.8571, Val: 0.6280, Test: 0.6530
Epoch: 129, Loss: 3.9264, Train: 0.8571, Val: 0.6280, Test: 0.6530
Epoch: 130, Loss: 3.7560, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 131, Loss: 3.8935, Train: 0.8571, Val: 0.6280, Test: 0.6540
Epoch: 132, Loss: 3.6842, Train: 0.8571, Val: 0.6280, Test: 0.6550
Epoch: 133, Loss: 3.8218, Train: 0.8571, Val: 0.6280, Test: 0.6550
Epoch: 134, Loss: 3.8231, Train: 0.8571, Val: 0.6260, Test: 0.6550
Epoch: 135, Loss: 3.8584, Train: 0.8571, Val: 0.6260, Test: 0.6550
Epoch: 136, Loss: 3.9578, Train: 0.8571, Val: 0.6260, Test: 0.6550
Epoch: 137, Loss: 3.7867, Train: 0.8571, Val: 0.6260, Test: 0.6550
Epoch: 138, Loss: 3.8568, Train: 0.8571, Val: 0.6260, Test: 0.6540
Epoch: 139, Loss: 3.9595, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 140, Loss: 3.8899, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 141, Loss: 3.8223, Train: 0.8571, Val: 0.6280, Test: 0.6570
Epoch: 142, Loss: 3.7214, Train: 0.8571, Val: 0.6280, Test: 0.6570
Epoch: 143, Loss: 3.5798, Train: 0.8571, Val: 0.6280, Test: 0.6580
Epoch: 144, Loss: 3.6496, Train: 0.8571, Val: 0.6280, Test: 0.6580
Epoch: 145, Loss: 3.6149, Train: 0.8571, Val: 0.6280, Test: 0.6590
Epoch: 146, Loss: 3.8547, Train: 0.8571, Val: 0.6280, Test: 0.6590
Epoch: 147, Loss: 3.9261, Train: 0.8571, Val: 0.6280, Test: 0.6590
Epoch: 148, Loss: 4.0276, Train: 0.8571, Val: 0.6280, Test: 0.6580
Epoch: 149, Loss: 3.7870, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 150, Loss: 3.6829, Train: 0.8571, Val: 0.6260, Test: 0.6590
Epoch: 151, Loss: 3.9583, Train: 0.8571, Val: 0.6260, Test: 0.6590
Epoch: 152, Loss: 3.4427, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 153, Loss: 3.6839, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 154, Loss: 3.6472, Train: 0.8571, Val: 0.6260, Test: 0.6590
Epoch: 155, Loss: 3.9253, Train: 0.8571, Val: 0.6260, Test: 0.6590
Epoch: 156, Loss: 3.8190, Train: 0.8571, Val: 0.6260, Test: 0.6590
Epoch: 157, Loss: 3.6819, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 158, Loss: 3.8878, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 159, Loss: 3.7171, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 160, Loss: 3.7506, Train: 0.8571, Val: 0.6260, Test: 0.6570
Epoch: 161, Loss: 3.4755, Train: 0.8571, Val: 0.6260, Test: 0.6570
Epoch: 162, Loss: 4.1639, Train: 0.8571, Val: 0.6260, Test: 0.6560
Epoch: 163, Loss: 3.7173, Train: 0.8571, Val: 0.6260, Test: 0.6550
Epoch: 164, Loss: 4.1648, Train: 0.8571, Val: 0.6280, Test: 0.6550
Epoch: 165, Loss: 3.9226, Train: 0.8571, Val: 0.6280, Test: 0.6550
Epoch: 166, Loss: 3.7529, Train: 0.8571, Val: 0.6280, Test: 0.6550
Epoch: 167, Loss: 3.7857, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 168, Loss: 3.7156, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 169, Loss: 3.9935, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 170, Loss: 3.8890, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 171, Loss: 3.7522, Train: 0.8571, Val: 0.6260, Test: 0.6560
Epoch: 172, Loss: 3.7842, Train: 0.8571, Val: 0.6260, Test: 0.6560
Epoch: 173, Loss: 3.8548, Train: 0.8571, Val: 0.6260, Test: 0.6560
Epoch: 174, Loss: 3.7515, Train: 0.8571, Val: 0.6260, Test: 0.6560
Epoch: 175, Loss: 3.7169, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 176, Loss: 3.5434, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 177, Loss: 3.5777, Train: 0.8571, Val: 0.6280, Test: 0.6560
Epoch: 178, Loss: 3.6829, Train: 0.8571, Val: 0.6280, Test: 0.6580
Epoch: 179, Loss: 3.9224, Train: 0.8571, Val: 0.6280, Test: 0.6580
Epoch: 180, Loss: 3.7850, Train: 0.8571, Val: 0.6300, Test: 0.6580
Epoch: 181, Loss: 3.6470, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 182, Loss: 3.7507, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 183, Loss: 3.9576, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 184, Loss: 3.7853, Train: 0.8571, Val: 0.6260, Test: 0.6580
Epoch: 185, Loss: 3.9240, Train: 0.8571, Val: 0.6240, Test: 0.6580
Epoch: 186, Loss: 3.7850, Train: 0.8571, Val: 0.6260, Test: 0.6570
Epoch: 187, Loss: 3.8889, Train: 0.8571, Val: 0.6260, Test: 0.6570
Epoch: 188, Loss: 3.8530, Train: 0.8571, Val: 0.6240, Test: 0.6570
Epoch: 189, Loss: 3.7846, Train: 0.8571, Val: 0.6240, Test: 0.6570
Epoch: 190, Loss: 3.9585, Train: 0.8571, Val: 0.6240, Test: 0.6570
Epoch: 191, Loss: 3.9224, Train: 0.8571, Val: 0.6240, Test: 0.6570
Epoch: 192, Loss: 3.6812, Train: 0.8571, Val: 0.6240, Test: 0.6580
Epoch: 193, Loss: 3.6129, Train: 0.8571, Val: 0.6240, Test: 0.6580
Epoch: 194, Loss: 3.9568, Train: 0.8571, Val: 0.6240, Test: 0.6580
Epoch: 195, Loss: 3.9918, Train: 0.8571, Val: 0.6280, Test: 0.6580
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 196, Loss: 3.9232, Train: 0.8571, Val: 0.6280, Test: 0.6590
Epoch: 197, Loss: 3.8198, Train: 0.8571, Val: 0.6280, Test: 0.6590
Epoch: 198, Loss: 3.9905, Train: 0.8571, Val: 0.6280, Test: 0.6580
Epoch: 199, Loss: 3.8540, Train: 0.8571, Val: 0.6280, Test: 0.6590
Epoch: 200, Loss: 3.7499, Train: 0.8571, Val: 0.6280, Test: 0.6580
MAD:  0.1419
Best Test Accuracy: 0.6740, Val Accuracy: 0.6440, Train Accuracy: 0.8500
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8395, Train: 0.2786, Val: 0.1600, Test: 0.2030
Epoch: 2, Loss: 4.7706, Train: 0.4000, Val: 0.2340, Test: 0.2740
Epoch: 3, Loss: 4.7478, Train: 0.4714, Val: 0.2380, Test: 0.2810
Epoch: 4, Loss: 4.6585, Train: 0.5071, Val: 0.2220, Test: 0.2860
Epoch: 5, Loss: 4.5968, Train: 0.5286, Val: 0.2300, Test: 0.2900
Epoch: 6, Loss: 4.5213, Train: 0.5786, Val: 0.2480, Test: 0.2960
Epoch: 7, Loss: 4.5016, Train: 0.6500, Val: 0.2860, Test: 0.3250
Epoch: 8, Loss: 4.3773, Train: 0.6714, Val: 0.3280, Test: 0.3560
Epoch: 9, Loss: 4.4827, Train: 0.7286, Val: 0.3660, Test: 0.3960
Epoch: 10, Loss: 4.4004, Train: 0.7929, Val: 0.4040, Test: 0.4240
Epoch: 11, Loss: 4.2353, Train: 0.8214, Val: 0.4680, Test: 0.4750
Epoch: 12, Loss: 4.2414, Train: 0.8571, Val: 0.5080, Test: 0.5290
Epoch: 13, Loss: 4.1883, Train: 0.9000, Val: 0.5420, Test: 0.5830
Epoch: 14, Loss: 4.1104, Train: 0.9143, Val: 0.6040, Test: 0.6260
Epoch: 15, Loss: 4.2272, Train: 0.9286, Val: 0.6220, Test: 0.6720
Epoch: 16, Loss: 3.9970, Train: 0.9357, Val: 0.6540, Test: 0.7020
Epoch: 17, Loss: 3.9246, Train: 0.9429, Val: 0.6800, Test: 0.7180
Epoch: 18, Loss: 3.9258, Train: 0.9643, Val: 0.6980, Test: 0.7410
Epoch: 19, Loss: 3.6877, Train: 0.9643, Val: 0.7060, Test: 0.7520
Epoch: 20, Loss: 4.0974, Train: 0.9857, Val: 0.7200, Test: 0.7630
Epoch: 21, Loss: 3.9344, Train: 0.9929, Val: 0.7360, Test: 0.7680
Epoch: 22, Loss: 4.0702, Train: 0.9929, Val: 0.7440, Test: 0.7730
Epoch: 23, Loss: 3.7876, Train: 0.9929, Val: 0.7500, Test: 0.7730
Epoch: 24, Loss: 3.9205, Train: 0.9929, Val: 0.7460, Test: 0.7750
Epoch: 25, Loss: 3.7566, Train: 0.9929, Val: 0.7440, Test: 0.7730
Epoch: 26, Loss: 3.7646, Train: 0.9929, Val: 0.7440, Test: 0.7690
Epoch: 27, Loss: 3.6159, Train: 0.9929, Val: 0.7460, Test: 0.7660
Epoch: 28, Loss: 3.6201, Train: 0.9929, Val: 0.7320, Test: 0.7630
Epoch: 29, Loss: 3.9100, Train: 0.9929, Val: 0.7340, Test: 0.7650
Epoch: 30, Loss: 4.0322, Train: 0.9929, Val: 0.7340, Test: 0.7630
Epoch: 31, Loss: 3.6564, Train: 0.9929, Val: 0.7300, Test: 0.7660
Epoch: 32, Loss: 3.9921, Train: 0.9929, Val: 0.7340, Test: 0.7640
Epoch: 33, Loss: 4.1046, Train: 0.9929, Val: 0.7320, Test: 0.7650
Epoch: 34, Loss: 3.7287, Train: 0.9929, Val: 0.7420, Test: 0.7630
Epoch: 35, Loss: 3.8098, Train: 1.0000, Val: 0.7380, Test: 0.7650
Epoch: 36, Loss: 3.7102, Train: 1.0000, Val: 0.7360, Test: 0.7650
Epoch: 37, Loss: 3.7395, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 38, Loss: 3.5536, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 39, Loss: 3.7479, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 40, Loss: 3.5084, Train: 1.0000, Val: 0.7420, Test: 0.7580
Epoch: 41, Loss: 3.8999, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 42, Loss: 3.8647, Train: 1.0000, Val: 0.7380, Test: 0.7580
Epoch: 43, Loss: 3.2615, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 44, Loss: 3.7575, Train: 1.0000, Val: 0.7340, Test: 0.7550
Epoch: 45, Loss: 3.6858, Train: 1.0000, Val: 0.7340, Test: 0.7550
Epoch: 46, Loss: 3.9177, Train: 1.0000, Val: 0.7280, Test: 0.7550
Epoch: 47, Loss: 3.3272, Train: 1.0000, Val: 0.7300, Test: 0.7590
Epoch: 48, Loss: 3.6382, Train: 1.0000, Val: 0.7340, Test: 0.7590
Epoch: 49, Loss: 4.0601, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 50, Loss: 3.5605, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 51, Loss: 3.6854, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 52, Loss: 3.9530, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 53, Loss: 3.5126, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 54, Loss: 3.9148, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 55, Loss: 3.7236, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 56, Loss: 3.8872, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 57, Loss: 4.0246, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 58, Loss: 3.8651, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 59, Loss: 3.5654, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 60, Loss: 3.9151, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 61, Loss: 3.9230, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 62, Loss: 3.5403, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 63, Loss: 3.5753, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 64, Loss: 3.9242, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 65, Loss: 3.9453, Train: 1.0000, Val: 0.7320, Test: 0.7710
Epoch: 66, Loss: 3.4361, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 67, Loss: 3.5018, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 68, Loss: 3.7468, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 69, Loss: 3.4282, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 70, Loss: 3.7011, Train: 1.0000, Val: 0.7400, Test: 0.7710
Epoch: 71, Loss: 3.7669, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 72, Loss: 3.6730, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 73, Loss: 3.5957, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 74, Loss: 3.7597, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 75, Loss: 3.9067, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 76, Loss: 3.5696, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 77, Loss: 3.4210, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 78, Loss: 3.5984, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 79, Loss: 3.5899, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 80, Loss: 3.6962, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 81, Loss: 3.6604, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 82, Loss: 3.5637, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 83, Loss: 3.8256, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 84, Loss: 3.6606, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 85, Loss: 3.6535, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 86, Loss: 3.9297, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 87, Loss: 3.6211, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 88, Loss: 3.8610, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 89, Loss: 3.6923, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 90, Loss: 3.5523, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 91, Loss: 3.8263, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 92, Loss: 3.5519, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 93, Loss: 3.8629, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 94, Loss: 3.7622, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 95, Loss: 3.8313, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 96, Loss: 3.8984, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 97, Loss: 3.6907, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 98, Loss: 3.9968, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 99, Loss: 3.4831, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 100, Loss: 3.5500, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 101, Loss: 3.7896, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 102, Loss: 4.0310, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 103, Loss: 3.5874, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 104, Loss: 3.7219, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 105, Loss: 3.6194, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 106, Loss: 3.5515, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 107, Loss: 3.7540, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 108, Loss: 3.5128, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 109, Loss: 3.5844, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 110, Loss: 3.6851, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 111, Loss: 4.0990, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 112, Loss: 3.5842, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 113, Loss: 3.6861, Train: 1.0000, Val: 0.7440, Test: 0.7680
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 114, Loss: 3.5129, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 115, Loss: 3.6517, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 116, Loss: 3.6525, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 117, Loss: 3.3078, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 118, Loss: 3.4438, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 119, Loss: 3.6181, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 120, Loss: 3.2389, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 121, Loss: 3.6848, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 122, Loss: 3.7532, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 123, Loss: 3.7890, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 124, Loss: 3.4091, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 125, Loss: 3.4433, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 126, Loss: 3.6507, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 127, Loss: 3.6486, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 128, Loss: 3.6161, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 129, Loss: 3.8910, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 130, Loss: 3.5133, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 131, Loss: 3.3387, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 132, Loss: 3.4756, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 133, Loss: 3.4417, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 134, Loss: 3.4804, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 135, Loss: 3.6174, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 136, Loss: 3.7871, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 137, Loss: 3.8216, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 138, Loss: 3.4783, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 139, Loss: 3.7170, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 140, Loss: 3.6491, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 141, Loss: 3.7509, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 142, Loss: 3.5478, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 143, Loss: 3.4079, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 144, Loss: 3.3040, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 145, Loss: 3.4074, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 146, Loss: 3.6833, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 147, Loss: 3.5790, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 148, Loss: 3.3413, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 149, Loss: 3.7530, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 150, Loss: 3.3726, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 151, Loss: 3.7171, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 152, Loss: 3.4064, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 153, Loss: 3.4062, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 154, Loss: 3.7533, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 155, Loss: 3.8894, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 156, Loss: 3.7507, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 157, Loss: 3.6471, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 158, Loss: 3.7521, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 159, Loss: 3.6832, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 160, Loss: 3.7520, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 161, Loss: 3.3363, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 162, Loss: 3.7164, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 163, Loss: 3.7146, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 164, Loss: 3.5089, Train: 1.0000, Val: 0.7360, Test: 0.7670
Epoch: 165, Loss: 3.8198, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 166, Loss: 3.6138, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 167, Loss: 3.6820, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 168, Loss: 3.7858, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 169, Loss: 3.6118, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 170, Loss: 3.3688, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 171, Loss: 3.5783, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 172, Loss: 3.5787, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 173, Loss: 3.9224, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 174, Loss: 3.7158, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 175, Loss: 3.6473, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 176, Loss: 3.5081, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 177, Loss: 3.9589, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 178, Loss: 3.8197, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 179, Loss: 3.5762, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 180, Loss: 3.8542, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 181, Loss: 3.9227, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 182, Loss: 3.8540, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 183, Loss: 3.5435, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 184, Loss: 3.3008, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 185, Loss: 3.6120, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 186, Loss: 3.8884, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 187, Loss: 3.4399, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 188, Loss: 3.7856, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 189, Loss: 3.4049, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 190, Loss: 3.6464, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 191, Loss: 3.7158, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 192, Loss: 3.7148, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 193, Loss: 3.6131, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 194, Loss: 3.4390, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 195, Loss: 3.5077, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 196, Loss: 3.9228, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 197, Loss: 3.6803, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 198, Loss: 3.5094, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 199, Loss: 3.7162, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 200, Loss: 3.7167, Train: 1.0000, Val: 0.7400, Test: 0.7680
MAD:  0.1077
Best Test Accuracy: 0.7750, Val Accuracy: 0.7460, Train Accuracy: 0.9929
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8562, Train: 0.1714, Val: 0.0860, Test: 0.0930
Epoch: 2, Loss: 4.8081, Train: 0.3357, Val: 0.1980, Test: 0.2130
Epoch: 3, Loss: 4.7724, Train: 0.4286, Val: 0.2360, Test: 0.2500
Epoch: 4, Loss: 4.7126, Train: 0.5000, Val: 0.2720, Test: 0.2910
Epoch: 5, Loss: 4.6321, Train: 0.5429, Val: 0.3080, Test: 0.3080
Epoch: 6, Loss: 4.6193, Train: 0.5786, Val: 0.3160, Test: 0.3340
Epoch: 7, Loss: 4.5148, Train: 0.6214, Val: 0.3220, Test: 0.3450
Epoch: 8, Loss: 4.5052, Train: 0.6571, Val: 0.3320, Test: 0.3650
Epoch: 9, Loss: 4.4907, Train: 0.6857, Val: 0.3440, Test: 0.3790
Epoch: 10, Loss: 4.3597, Train: 0.7000, Val: 0.3580, Test: 0.3920
Epoch: 11, Loss: 4.3269, Train: 0.7571, Val: 0.3800, Test: 0.4130
Epoch: 12, Loss: 4.2859, Train: 0.7643, Val: 0.3960, Test: 0.4380
Epoch: 13, Loss: 3.8823, Train: 0.7786, Val: 0.4280, Test: 0.4740
Epoch: 14, Loss: 4.1561, Train: 0.8071, Val: 0.4560, Test: 0.5040
Epoch: 15, Loss: 4.0796, Train: 0.8286, Val: 0.4940, Test: 0.5450
Epoch: 16, Loss: 3.8760, Train: 0.8786, Val: 0.5340, Test: 0.5980
Epoch: 17, Loss: 3.7572, Train: 0.9286, Val: 0.5800, Test: 0.6390
Epoch: 18, Loss: 3.8610, Train: 0.9500, Val: 0.6260, Test: 0.6780
Epoch: 19, Loss: 4.0379, Train: 0.9571, Val: 0.6720, Test: 0.7060
Epoch: 20, Loss: 4.1261, Train: 0.9714, Val: 0.6900, Test: 0.7310
Epoch: 21, Loss: 3.9330, Train: 0.9786, Val: 0.6960, Test: 0.7530
Epoch: 22, Loss: 4.0787, Train: 0.9857, Val: 0.7100, Test: 0.7540
Epoch: 23, Loss: 3.7851, Train: 0.9929, Val: 0.7180, Test: 0.7590
Epoch: 24, Loss: 4.1141, Train: 0.9929, Val: 0.7180, Test: 0.7570
Epoch: 25, Loss: 3.8908, Train: 0.9929, Val: 0.7080, Test: 0.7520
Epoch: 26, Loss: 3.9307, Train: 1.0000, Val: 0.7000, Test: 0.7410
Epoch: 27, Loss: 3.8398, Train: 1.0000, Val: 0.7020, Test: 0.7460
Epoch: 28, Loss: 3.9532, Train: 1.0000, Val: 0.7080, Test: 0.7530
Epoch: 29, Loss: 3.6769, Train: 1.0000, Val: 0.7200, Test: 0.7620
Epoch: 30, Loss: 3.7543, Train: 1.0000, Val: 0.7280, Test: 0.7660
Epoch: 31, Loss: 3.7082, Train: 1.0000, Val: 0.7280, Test: 0.7700
Epoch: 32, Loss: 3.7062, Train: 1.0000, Val: 0.7300, Test: 0.7740
Epoch: 33, Loss: 3.7807, Train: 1.0000, Val: 0.7400, Test: 0.7790
Epoch: 34, Loss: 3.9060, Train: 1.0000, Val: 0.7380, Test: 0.7780
Epoch: 35, Loss: 3.4277, Train: 1.0000, Val: 0.7380, Test: 0.7800
Epoch: 36, Loss: 3.9556, Train: 1.0000, Val: 0.7440, Test: 0.7800
Epoch: 37, Loss: 3.7705, Train: 1.0000, Val: 0.7460, Test: 0.7790
Epoch: 38, Loss: 3.8543, Train: 1.0000, Val: 0.7460, Test: 0.7790
Epoch: 39, Loss: 3.8607, Train: 1.0000, Val: 0.7440, Test: 0.7830
Epoch: 40, Loss: 3.7634, Train: 1.0000, Val: 0.7440, Test: 0.7800
Epoch: 41, Loss: 3.6117, Train: 1.0000, Val: 0.7440, Test: 0.7790
Epoch: 42, Loss: 3.6842, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 43, Loss: 3.7160, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 44, Loss: 3.4807, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 45, Loss: 3.6515, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 46, Loss: 3.4784, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 47, Loss: 3.7579, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 48, Loss: 3.8493, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 49, Loss: 3.9042, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 50, Loss: 3.8682, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 51, Loss: 3.4230, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 52, Loss: 3.7579, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 53, Loss: 3.6966, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 54, Loss: 3.4940, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 55, Loss: 3.7530, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 56, Loss: 3.8570, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 57, Loss: 3.8515, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 58, Loss: 3.8513, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 59, Loss: 3.6224, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 60, Loss: 3.5467, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 61, Loss: 3.3716, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 62, Loss: 3.6496, Train: 1.0000, Val: 0.7460, Test: 0.7770
Epoch: 63, Loss: 3.5051, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 64, Loss: 3.6105, Train: 1.0000, Val: 0.7480, Test: 0.7770
Epoch: 65, Loss: 3.5037, Train: 1.0000, Val: 0.7480, Test: 0.7780
Epoch: 66, Loss: 3.7657, Train: 1.0000, Val: 0.7460, Test: 0.7790
Epoch: 67, Loss: 3.4040, Train: 1.0000, Val: 0.7460, Test: 0.7790
Epoch: 68, Loss: 3.5324, Train: 1.0000, Val: 0.7460, Test: 0.7780
Epoch: 69, Loss: 3.7005, Train: 1.0000, Val: 0.7460, Test: 0.7770
Epoch: 70, Loss: 3.8353, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 71, Loss: 3.8653, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 72, Loss: 3.4210, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 73, Loss: 3.6890, Train: 1.0000, Val: 0.7540, Test: 0.7770
Epoch: 74, Loss: 3.4577, Train: 1.0000, Val: 0.7540, Test: 0.7780
Epoch: 75, Loss: 3.5963, Train: 1.0000, Val: 0.7540, Test: 0.7770
Epoch: 76, Loss: 3.6645, Train: 1.0000, Val: 0.7540, Test: 0.7770
Epoch: 77, Loss: 3.3197, Train: 1.0000, Val: 0.7540, Test: 0.7780
Epoch: 78, Loss: 3.9380, Train: 1.0000, Val: 0.7520, Test: 0.7780
Epoch: 79, Loss: 3.7252, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 80, Loss: 3.6595, Train: 1.0000, Val: 0.7520, Test: 0.7780
Epoch: 81, Loss: 3.6238, Train: 1.0000, Val: 0.7520, Test: 0.7790
Epoch: 82, Loss: 3.4946, Train: 1.0000, Val: 0.7500, Test: 0.7790
Epoch: 83, Loss: 3.4930, Train: 1.0000, Val: 0.7500, Test: 0.7790
Epoch: 84, Loss: 3.6913, Train: 1.0000, Val: 0.7480, Test: 0.7780
Epoch: 85, Loss: 3.8659, Train: 1.0000, Val: 0.7460, Test: 0.7770
Epoch: 86, Loss: 3.5566, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 87, Loss: 3.2797, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 88, Loss: 3.6540, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 89, Loss: 3.5894, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 90, Loss: 3.6853, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 91, Loss: 3.5864, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 92, Loss: 3.6195, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 93, Loss: 3.7583, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 94, Loss: 3.6869, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 95, Loss: 3.8945, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 96, Loss: 3.8622, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 97, Loss: 3.7973, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 98, Loss: 3.4439, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 99, Loss: 3.8972, Train: 1.0000, Val: 0.7420, Test: 0.7760
Epoch: 100, Loss: 3.5888, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 101, Loss: 3.7536, Train: 1.0000, Val: 0.7400, Test: 0.7770
Epoch: 102, Loss: 3.5822, Train: 1.0000, Val: 0.7400, Test: 0.7760
Epoch: 103, Loss: 3.3112, Train: 1.0000, Val: 0.7360, Test: 0.7760
Epoch: 104, Loss: 3.7209, Train: 1.0000, Val: 0.7360, Test: 0.7760
Epoch: 105, Loss: 3.7555, Train: 1.0000, Val: 0.7360, Test: 0.7770
Epoch: 106, Loss: 3.5859, Train: 1.0000, Val: 0.7360, Test: 0.7770
Epoch: 107, Loss: 3.5481, Train: 1.0000, Val: 0.7360, Test: 0.7770
Epoch: 108, Loss: 3.7937, Train: 1.0000, Val: 0.7360, Test: 0.7770
Epoch: 109, Loss: 3.6874, Train: 1.0000, Val: 0.7360, Test: 0.7770
Epoch: 110, Loss: 3.6879, Train: 1.0000, Val: 0.7360, Test: 0.7770
Epoch: 111, Loss: 3.7542, Train: 1.0000, Val: 0.7380, Test: 0.7770
Epoch: 112, Loss: 3.6877, Train: 1.0000, Val: 0.7380, Test: 0.7770
Epoch: 113, Loss: 3.6167, Train: 1.0000, Val: 0.7380, Test: 0.7770
Epoch: 114, Loss: 3.7217, Train: 1.0000, Val: 0.7380, Test: 0.7770
Epoch: 115, Loss: 3.8570, Train: 1.0000, Val: 0.7400, Test: 0.7770
Epoch: 116, Loss: 3.4443, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 117, Loss: 3.5122, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 118, Loss: 3.5801, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 119, Loss: 3.6155, Train: 1.0000, Val: 0.7420, Test: 0.7760
Epoch: 120, Loss: 3.5128, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 121, Loss: 3.8592, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 122, Loss: 3.6849, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 123, Loss: 3.4467, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 124, Loss: 3.8912, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 125, Loss: 3.6866, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 126, Loss: 3.8588, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 127, Loss: 3.3769, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 128, Loss: 3.7174, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 129, Loss: 3.7566, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 130, Loss: 3.6837, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 131, Loss: 3.5114, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 132, Loss: 3.7525, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 133, Loss: 3.6485, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 134, Loss: 3.6841, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 135, Loss: 3.9246, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 136, Loss: 3.5793, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 137, Loss: 3.8938, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 138, Loss: 3.6836, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 139, Loss: 3.8570, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 140, Loss: 3.5108, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 141, Loss: 3.7553, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 142, Loss: 3.8200, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 143, Loss: 3.7879, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 144, Loss: 3.7869, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 145, Loss: 3.6480, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 146, Loss: 3.4403, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 147, Loss: 3.9600, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 148, Loss: 3.5433, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 149, Loss: 3.6828, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 150, Loss: 3.4743, Train: 1.0000, Val: 0.7420, Test: 0.7750
Epoch: 151, Loss: 3.5452, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 152, Loss: 3.7509, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 153, Loss: 3.6484, Train: 1.0000, Val: 0.7440, Test: 0.7740
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 154, Loss: 3.6495, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 155, Loss: 3.8219, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 156, Loss: 3.7187, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 157, Loss: 3.6492, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 158, Loss: 3.4419, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 159, Loss: 3.9230, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 160, Loss: 3.6137, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 161, Loss: 3.9608, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 162, Loss: 3.7843, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 163, Loss: 4.0289, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 164, Loss: 3.7174, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 165, Loss: 3.7502, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 166, Loss: 3.9584, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 167, Loss: 3.5106, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 168, Loss: 3.6474, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 169, Loss: 3.3736, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 170, Loss: 3.8223, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 171, Loss: 3.5805, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 172, Loss: 3.4046, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 173, Loss: 3.4403, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 174, Loss: 3.7869, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 175, Loss: 3.5099, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 176, Loss: 4.0619, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 177, Loss: 3.4739, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 178, Loss: 3.5100, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 179, Loss: 3.6809, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 180, Loss: 3.6119, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 181, Loss: 3.8542, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 182, Loss: 3.5083, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 183, Loss: 3.8891, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 184, Loss: 3.5444, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 185, Loss: 3.7165, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 186, Loss: 3.8542, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 187, Loss: 3.5761, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 188, Loss: 3.1988, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 189, Loss: 3.6814, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 190, Loss: 3.6484, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 191, Loss: 4.0595, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 192, Loss: 3.7849, Train: 1.0000, Val: 0.7420, Test: 0.7690
Epoch: 193, Loss: 3.8560, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 194, Loss: 3.7841, Train: 1.0000, Val: 0.7400, Test: 0.7680
Epoch: 195, Loss: 3.6469, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 196, Loss: 3.7179, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 197, Loss: 3.6115, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 198, Loss: 3.8190, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 199, Loss: 3.6816, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 200, Loss: 3.4395, Train: 1.0000, Val: 0.7460, Test: 0.7700
MAD:  0.3076
Best Test Accuracy: 0.7830, Val Accuracy: 0.7440, Train Accuracy: 1.0000
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8500, Train: 0.2500, Val: 0.1720, Test: 0.1600
Epoch: 2, Loss: 4.8119, Train: 0.4357, Val: 0.3840, Test: 0.3630
Epoch: 3, Loss: 4.7209, Train: 0.5429, Val: 0.4760, Test: 0.4590
Epoch: 4, Loss: 4.6990, Train: 0.6071, Val: 0.5080, Test: 0.4940
Epoch: 5, Loss: 4.5839, Train: 0.6500, Val: 0.5300, Test: 0.5260
Epoch: 6, Loss: 4.5735, Train: 0.7000, Val: 0.5440, Test: 0.5550
Epoch: 7, Loss: 4.5567, Train: 0.7214, Val: 0.5660, Test: 0.5640
Epoch: 8, Loss: 4.4066, Train: 0.7429, Val: 0.5640, Test: 0.5820
Epoch: 9, Loss: 4.5223, Train: 0.7714, Val: 0.5700, Test: 0.5930
Epoch: 10, Loss: 4.2535, Train: 0.7714, Val: 0.5740, Test: 0.6060
Epoch: 11, Loss: 4.3559, Train: 0.7857, Val: 0.5780, Test: 0.6070
Epoch: 12, Loss: 4.3291, Train: 0.7929, Val: 0.5740, Test: 0.6040
Epoch: 13, Loss: 4.1734, Train: 0.8143, Val: 0.5800, Test: 0.6060
Epoch: 14, Loss: 4.1657, Train: 0.8214, Val: 0.5880, Test: 0.6060
Epoch: 15, Loss: 4.0175, Train: 0.8214, Val: 0.5860, Test: 0.6050
Epoch: 16, Loss: 4.0605, Train: 0.8286, Val: 0.5840, Test: 0.6100
Epoch: 17, Loss: 4.0580, Train: 0.8286, Val: 0.5960, Test: 0.6210
Epoch: 18, Loss: 3.9957, Train: 0.8286, Val: 0.6040, Test: 0.6330
Epoch: 19, Loss: 4.1140, Train: 0.8286, Val: 0.6100, Test: 0.6400
Epoch: 20, Loss: 3.9646, Train: 0.8429, Val: 0.6160, Test: 0.6480
Epoch: 21, Loss: 4.1277, Train: 0.8571, Val: 0.6200, Test: 0.6550
Epoch: 22, Loss: 3.8875, Train: 0.8571, Val: 0.6220, Test: 0.6560
Epoch: 23, Loss: 3.8436, Train: 0.8571, Val: 0.6240, Test: 0.6600
Epoch: 24, Loss: 4.1996, Train: 0.8571, Val: 0.6320, Test: 0.6590
Epoch: 25, Loss: 4.0240, Train: 0.8571, Val: 0.6320, Test: 0.6620
Epoch: 26, Loss: 3.9496, Train: 0.8571, Val: 0.6340, Test: 0.6630
Epoch: 27, Loss: 4.1891, Train: 0.8571, Val: 0.6420, Test: 0.6690
Epoch: 28, Loss: 4.2885, Train: 0.8571, Val: 0.6380, Test: 0.6770
Epoch: 29, Loss: 3.9487, Train: 0.8571, Val: 0.6380, Test: 0.6800
Epoch: 30, Loss: 4.0077, Train: 0.8571, Val: 0.6400, Test: 0.6800
Epoch: 31, Loss: 4.1375, Train: 0.8571, Val: 0.6420, Test: 0.6830
Epoch: 32, Loss: 4.3492, Train: 0.8571, Val: 0.6440, Test: 0.6810
Epoch: 33, Loss: 3.9927, Train: 0.8571, Val: 0.6480, Test: 0.6820
Epoch: 34, Loss: 4.1607, Train: 0.8571, Val: 0.6480, Test: 0.6810
Epoch: 35, Loss: 3.8918, Train: 0.8571, Val: 0.6540, Test: 0.6810
Epoch: 36, Loss: 3.9234, Train: 0.8571, Val: 0.6560, Test: 0.6810
Epoch: 37, Loss: 3.9160, Train: 0.8571, Val: 0.6600, Test: 0.6810
Epoch: 38, Loss: 4.0996, Train: 0.8571, Val: 0.6600, Test: 0.6790
Epoch: 39, Loss: 3.8394, Train: 0.8571, Val: 0.6560, Test: 0.6810
Epoch: 40, Loss: 3.9670, Train: 0.8571, Val: 0.6540, Test: 0.6790
Epoch: 41, Loss: 3.7842, Train: 0.8571, Val: 0.6520, Test: 0.6790
Epoch: 42, Loss: 4.1144, Train: 0.8571, Val: 0.6520, Test: 0.6780
Epoch: 43, Loss: 3.9853, Train: 0.8571, Val: 0.6480, Test: 0.6760
Epoch: 44, Loss: 4.1108, Train: 0.8571, Val: 0.6480, Test: 0.6740
Epoch: 45, Loss: 3.9641, Train: 0.8571, Val: 0.6480, Test: 0.6750
Epoch: 46, Loss: 4.0338, Train: 0.8571, Val: 0.6500, Test: 0.6750
Epoch: 47, Loss: 4.0034, Train: 0.8571, Val: 0.6500, Test: 0.6750
Epoch: 48, Loss: 3.7696, Train: 0.8571, Val: 0.6460, Test: 0.6740
Epoch: 49, Loss: 3.9967, Train: 0.8571, Val: 0.6460, Test: 0.6740
Epoch: 50, Loss: 3.9841, Train: 0.8571, Val: 0.6460, Test: 0.6770
Epoch: 51, Loss: 3.8154, Train: 0.8571, Val: 0.6460, Test: 0.6770
Epoch: 52, Loss: 3.8533, Train: 0.8571, Val: 0.6460, Test: 0.6770
Epoch: 53, Loss: 4.1786, Train: 0.8571, Val: 0.6420, Test: 0.6770
Epoch: 54, Loss: 4.0572, Train: 0.8571, Val: 0.6400, Test: 0.6780
Epoch: 55, Loss: 3.9588, Train: 0.8571, Val: 0.6400, Test: 0.6770
Epoch: 56, Loss: 3.6448, Train: 0.8571, Val: 0.6400, Test: 0.6780
Epoch: 57, Loss: 3.5093, Train: 0.8571, Val: 0.6400, Test: 0.6780
Epoch: 58, Loss: 4.0126, Train: 0.8571, Val: 0.6420, Test: 0.6780
Epoch: 59, Loss: 3.6042, Train: 0.8571, Val: 0.6420, Test: 0.6770
Epoch: 60, Loss: 3.7762, Train: 0.8571, Val: 0.6440, Test: 0.6780
Epoch: 61, Loss: 3.5808, Train: 0.8571, Val: 0.6440, Test: 0.6780
Epoch: 62, Loss: 3.8009, Train: 0.8571, Val: 0.6380, Test: 0.6770
Epoch: 63, Loss: 3.8414, Train: 0.8571, Val: 0.6380, Test: 0.6770
Epoch: 64, Loss: 3.5924, Train: 0.8571, Val: 0.6440, Test: 0.6770
Epoch: 65, Loss: 3.7351, Train: 0.8571, Val: 0.6420, Test: 0.6760
Epoch: 66, Loss: 3.8379, Train: 0.8571, Val: 0.6420, Test: 0.6750
Epoch: 67, Loss: 3.6344, Train: 0.8571, Val: 0.6420, Test: 0.6750
Epoch: 68, Loss: 3.6631, Train: 0.8571, Val: 0.6440, Test: 0.6750
Epoch: 69, Loss: 3.8354, Train: 0.8571, Val: 0.6420, Test: 0.6750
Epoch: 70, Loss: 3.8028, Train: 0.8571, Val: 0.6400, Test: 0.6750
Epoch: 71, Loss: 3.9000, Train: 0.8571, Val: 0.6380, Test: 0.6750
Epoch: 72, Loss: 3.8663, Train: 0.8571, Val: 0.6380, Test: 0.6750
Epoch: 73, Loss: 3.8005, Train: 0.8571, Val: 0.6380, Test: 0.6740
Epoch: 74, Loss: 4.1433, Train: 0.8571, Val: 0.6420, Test: 0.6740
Epoch: 75, Loss: 3.7987, Train: 0.8571, Val: 0.6380, Test: 0.6740
Epoch: 76, Loss: 4.1397, Train: 0.8571, Val: 0.6400, Test: 0.6740
Epoch: 77, Loss: 3.6219, Train: 0.8571, Val: 0.6400, Test: 0.6760
Epoch: 78, Loss: 3.8328, Train: 0.8571, Val: 0.6420, Test: 0.6760
Epoch: 79, Loss: 3.8618, Train: 0.8571, Val: 0.6420, Test: 0.6740
Epoch: 80, Loss: 3.8965, Train: 0.8571, Val: 0.6420, Test: 0.6740
Epoch: 81, Loss: 3.8640, Train: 0.8571, Val: 0.6420, Test: 0.6740
Epoch: 82, Loss: 4.1360, Train: 0.8571, Val: 0.6420, Test: 0.6750
Epoch: 83, Loss: 3.7978, Train: 0.8571, Val: 0.6420, Test: 0.6750
Epoch: 84, Loss: 3.8622, Train: 0.8571, Val: 0.6440, Test: 0.6740
Epoch: 85, Loss: 3.9331, Train: 0.8571, Val: 0.6440, Test: 0.6730
Epoch: 86, Loss: 3.7232, Train: 0.8571, Val: 0.6440, Test: 0.6740
Epoch: 87, Loss: 3.7907, Train: 0.8571, Val: 0.6440, Test: 0.6740
Epoch: 88, Loss: 3.9650, Train: 0.8571, Val: 0.6440, Test: 0.6730
Epoch: 89, Loss: 3.9332, Train: 0.8571, Val: 0.6440, Test: 0.6720
Epoch: 90, Loss: 3.8242, Train: 0.8571, Val: 0.6440, Test: 0.6720
Epoch: 91, Loss: 3.7595, Train: 0.8571, Val: 0.6420, Test: 0.6700
Epoch: 92, Loss: 3.8595, Train: 0.8571, Val: 0.6460, Test: 0.6700
Epoch: 93, Loss: 3.4801, Train: 0.8571, Val: 0.6440, Test: 0.6690
Epoch: 94, Loss: 3.7259, Train: 0.8571, Val: 0.6440, Test: 0.6670
Epoch: 95, Loss: 3.8596, Train: 0.8571, Val: 0.6420, Test: 0.6690
Epoch: 96, Loss: 4.0662, Train: 0.8571, Val: 0.6440, Test: 0.6700
Epoch: 97, Loss: 3.6847, Train: 0.8571, Val: 0.6440, Test: 0.6710
Epoch: 98, Loss: 3.8928, Train: 0.8571, Val: 0.6440, Test: 0.6710
Epoch: 99, Loss: 3.7552, Train: 0.8571, Val: 0.6440, Test: 0.6700
Epoch: 100, Loss: 3.8957, Train: 0.8571, Val: 0.6420, Test: 0.6700
Epoch: 101, Loss: 3.7215, Train: 0.8571, Val: 0.6420, Test: 0.6690
Epoch: 102, Loss: 3.8942, Train: 0.8571, Val: 0.6420, Test: 0.6690
Epoch: 103, Loss: 3.5159, Train: 0.8571, Val: 0.6400, Test: 0.6690
Epoch: 104, Loss: 3.5493, Train: 0.8571, Val: 0.6400, Test: 0.6680
Epoch: 105, Loss: 3.6847, Train: 0.8571, Val: 0.6400, Test: 0.6680
Epoch: 106, Loss: 4.2047, Train: 0.8571, Val: 0.6400, Test: 0.6680
Epoch: 107, Loss: 3.9616, Train: 0.8571, Val: 0.6400, Test: 0.6680
Epoch: 108, Loss: 4.0651, Train: 0.8571, Val: 0.6400, Test: 0.6690
Epoch: 109, Loss: 3.5827, Train: 0.8571, Val: 0.6400, Test: 0.6690
Epoch: 110, Loss: 3.8591, Train: 0.8571, Val: 0.6400, Test: 0.6710
Epoch: 111, Loss: 3.9272, Train: 0.8571, Val: 0.6380, Test: 0.6710
Epoch: 112, Loss: 3.8243, Train: 0.8571, Val: 0.6380, Test: 0.6710
Epoch: 113, Loss: 3.7179, Train: 0.8571, Val: 0.6380, Test: 0.6700
Epoch: 114, Loss: 3.8587, Train: 0.8571, Val: 0.6380, Test: 0.6690
Epoch: 115, Loss: 3.4793, Train: 0.8571, Val: 0.6380, Test: 0.6680
Epoch: 116, Loss: 3.8938, Train: 0.8571, Val: 0.6380, Test: 0.6680
Epoch: 117, Loss: 3.9251, Train: 0.8571, Val: 0.6400, Test: 0.6690
Epoch: 118, Loss: 3.7542, Train: 0.8571, Val: 0.6400, Test: 0.6690
Epoch: 119, Loss: 3.5145, Train: 0.8571, Val: 0.6400, Test: 0.6690
Epoch: 120, Loss: 3.8594, Train: 0.8571, Val: 0.6380, Test: 0.6690
Epoch: 121, Loss: 4.0643, Train: 0.8571, Val: 0.6380, Test: 0.6690
Epoch: 122, Loss: 3.5493, Train: 0.8571, Val: 0.6380, Test: 0.6680
Epoch: 123, Loss: 3.9955, Train: 0.8571, Val: 0.6380, Test: 0.6670
Epoch: 124, Loss: 3.8574, Train: 0.8571, Val: 0.6380, Test: 0.6670
Epoch: 125, Loss: 3.8208, Train: 0.8571, Val: 0.6380, Test: 0.6670
Epoch: 126, Loss: 3.7172, Train: 0.8571, Val: 0.6380, Test: 0.6670
Epoch: 127, Loss: 3.8898, Train: 0.8571, Val: 0.6360, Test: 0.6680
Epoch: 128, Loss: 3.9598, Train: 0.8571, Val: 0.6360, Test: 0.6690
Epoch: 129, Loss: 3.7526, Train: 0.8571, Val: 0.6360, Test: 0.6690
Epoch: 130, Loss: 3.6517, Train: 0.8571, Val: 0.6360, Test: 0.6690
Epoch: 131, Loss: 3.8922, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 132, Loss: 3.6843, Train: 0.8571, Val: 0.6360, Test: 0.6690
Epoch: 133, Loss: 3.8216, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 134, Loss: 4.0266, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 135, Loss: 3.7179, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 136, Loss: 3.7176, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 137, Loss: 3.9589, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 138, Loss: 3.8190, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 139, Loss: 3.9262, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 140, Loss: 3.8910, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 141, Loss: 3.4078, Train: 0.8571, Val: 0.6360, Test: 0.6700
Epoch: 142, Loss: 3.9257, Train: 0.8571, Val: 0.6360, Test: 0.6690
Epoch: 143, Loss: 3.7191, Train: 0.8571, Val: 0.6360, Test: 0.6680
Epoch: 144, Loss: 3.9242, Train: 0.8571, Val: 0.6360, Test: 0.6670
Epoch: 145, Loss: 3.7174, Train: 0.8571, Val: 0.6360, Test: 0.6670
Epoch: 146, Loss: 3.8223, Train: 0.8571, Val: 0.6360, Test: 0.6680
Epoch: 147, Loss: 3.9914, Train: 0.8571, Val: 0.6360, Test: 0.6690
Epoch: 148, Loss: 3.8881, Train: 0.8571, Val: 0.6340, Test: 0.6690
Epoch: 149, Loss: 3.7521, Train: 0.8571, Val: 0.6340, Test: 0.6690
Epoch: 150, Loss: 3.8205, Train: 0.8571, Val: 0.6340, Test: 0.6710
Epoch: 151, Loss: 3.9243, Train: 0.8571, Val: 0.6340, Test: 0.6720
Epoch: 152, Loss: 3.6835, Train: 0.8571, Val: 0.6360, Test: 0.6720
Epoch: 153, Loss: 3.8895, Train: 0.8571, Val: 0.6360, Test: 0.6730
Epoch: 154, Loss: 4.0284, Train: 0.8571, Val: 0.6360, Test: 0.6730
Epoch: 155, Loss: 4.0276, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 156, Loss: 3.7518, Train: 0.8571, Val: 0.6360, Test: 0.6750
Epoch: 157, Loss: 3.7160, Train: 0.8571, Val: 0.6360, Test: 0.6750
Epoch: 158, Loss: 3.7518, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 159, Loss: 3.9232, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 160, Loss: 3.7864, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 161, Loss: 3.7163, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 162, Loss: 4.0966, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 163, Loss: 3.6477, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 164, Loss: 3.9927, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 165, Loss: 3.8208, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 166, Loss: 3.8224, Train: 0.8571, Val: 0.6360, Test: 0.6750
Epoch: 167, Loss: 3.8198, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 168, Loss: 3.6827, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 169, Loss: 3.6820, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 170, Loss: 3.6831, Train: 0.8571, Val: 0.6340, Test: 0.6740
Epoch: 171, Loss: 3.6819, Train: 0.8571, Val: 0.6340, Test: 0.6740
Epoch: 172, Loss: 3.6114, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 173, Loss: 4.1636, Train: 0.8571, Val: 0.6360, Test: 0.6730
Epoch: 174, Loss: 3.8533, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 175, Loss: 3.9930, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 176, Loss: 3.7155, Train: 0.8571, Val: 0.6360, Test: 0.6740
Epoch: 177, Loss: 3.6469, Train: 0.8571, Val: 0.6360, Test: 0.6730
Epoch: 178, Loss: 3.8545, Train: 0.8571, Val: 0.6360, Test: 0.6750
Epoch: 179, Loss: 4.1648, Train: 0.8571, Val: 0.6360, Test: 0.6750
Epoch: 180, Loss: 3.6816, Train: 0.8571, Val: 0.6360, Test: 0.6750
Epoch: 181, Loss: 3.8881, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 182, Loss: 4.0249, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 183, Loss: 3.8536, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 184, Loss: 3.4751, Train: 0.8571, Val: 0.6360, Test: 0.6760
Epoch: 185, Loss: 3.6807, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 186, Loss: 3.9570, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 187, Loss: 3.7498, Train: 0.8571, Val: 0.6340, Test: 0.6760
Epoch: 188, Loss: 3.9214, Train: 0.8571, Val: 0.6320, Test: 0.6760
Epoch: 189, Loss: 3.8529, Train: 0.8571, Val: 0.6320, Test: 0.6760
Epoch: 190, Loss: 4.0607, Train: 0.8571, Val: 0.6320, Test: 0.6740
Epoch: 191, Loss: 3.7504, Train: 0.8571, Val: 0.6320, Test: 0.6740
Epoch: 192, Loss: 3.7842, Train: 0.8571, Val: 0.6320, Test: 0.6740
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 193, Loss: 3.8874, Train: 0.8571, Val: 0.6320, Test: 0.6740
Epoch: 194, Loss: 3.6801, Train: 0.8571, Val: 0.6320, Test: 0.6740
Epoch: 195, Loss: 3.9564, Train: 0.8571, Val: 0.6300, Test: 0.6750
Epoch: 196, Loss: 3.9565, Train: 0.8571, Val: 0.6300, Test: 0.6760
Epoch: 197, Loss: 3.7500, Train: 0.8571, Val: 0.6300, Test: 0.6740
Epoch: 198, Loss: 3.7509, Train: 0.8571, Val: 0.6300, Test: 0.6740
Epoch: 199, Loss: 3.8189, Train: 0.8571, Val: 0.6300, Test: 0.6740
Epoch: 200, Loss: 3.8862, Train: 0.8571, Val: 0.6300, Test: 0.6750
MAD:  0.2283
Best Test Accuracy: 0.6830, Val Accuracy: 0.6420, Train Accuracy: 0.8571
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8520, Train: 0.1714, Val: 0.1580, Test: 0.1420
Epoch: 2, Loss: 4.8349, Train: 0.2357, Val: 0.2000, Test: 0.1890
Epoch: 3, Loss: 4.7248, Train: 0.3143, Val: 0.2180, Test: 0.2170
Epoch: 4, Loss: 4.7618, Train: 0.3857, Val: 0.2600, Test: 0.2490
Epoch: 5, Loss: 4.6851, Train: 0.4286, Val: 0.2880, Test: 0.2800
Epoch: 6, Loss: 4.5160, Train: 0.4714, Val: 0.3280, Test: 0.3210
Epoch: 7, Loss: 4.4505, Train: 0.5429, Val: 0.3520, Test: 0.3420
Epoch: 8, Loss: 4.6175, Train: 0.5643, Val: 0.3800, Test: 0.3730
Epoch: 9, Loss: 4.4769, Train: 0.6143, Val: 0.4140, Test: 0.3960
Epoch: 10, Loss: 4.4251, Train: 0.6357, Val: 0.4460, Test: 0.4200
Epoch: 11, Loss: 4.2759, Train: 0.6786, Val: 0.4800, Test: 0.4600
Epoch: 12, Loss: 4.2124, Train: 0.7214, Val: 0.5080, Test: 0.4970
Epoch: 13, Loss: 4.2671, Train: 0.7500, Val: 0.5360, Test: 0.5430
Epoch: 14, Loss: 4.2639, Train: 0.7857, Val: 0.5640, Test: 0.5890
Epoch: 15, Loss: 4.0391, Train: 0.7929, Val: 0.5820, Test: 0.6040
Epoch: 16, Loss: 3.9315, Train: 0.8143, Val: 0.6020, Test: 0.6300
Epoch: 17, Loss: 3.7325, Train: 0.8571, Val: 0.6160, Test: 0.6540
Epoch: 18, Loss: 4.1463, Train: 0.9143, Val: 0.6440, Test: 0.6630
Epoch: 19, Loss: 3.8980, Train: 0.9214, Val: 0.6600, Test: 0.6900
Epoch: 20, Loss: 4.0311, Train: 0.9357, Val: 0.6780, Test: 0.7210
Epoch: 21, Loss: 4.0129, Train: 0.9643, Val: 0.7040, Test: 0.7310
Epoch: 22, Loss: 3.8149, Train: 0.9643, Val: 0.7020, Test: 0.7490
Epoch: 23, Loss: 3.9910, Train: 0.9857, Val: 0.7120, Test: 0.7580
Epoch: 24, Loss: 3.9481, Train: 0.9857, Val: 0.7200, Test: 0.7680
Epoch: 25, Loss: 3.9913, Train: 0.9857, Val: 0.7200, Test: 0.7700
Epoch: 26, Loss: 3.8836, Train: 0.9857, Val: 0.7160, Test: 0.7670
Epoch: 27, Loss: 3.7727, Train: 0.9857, Val: 0.7080, Test: 0.7650
Epoch: 28, Loss: 3.5179, Train: 0.9929, Val: 0.7160, Test: 0.7620
Epoch: 29, Loss: 3.6758, Train: 0.9929, Val: 0.7180, Test: 0.7640
Epoch: 30, Loss: 3.8269, Train: 0.9929, Val: 0.7140, Test: 0.7630
Epoch: 31, Loss: 4.2983, Train: 0.9929, Val: 0.7240, Test: 0.7620
Epoch: 32, Loss: 3.8361, Train: 0.9929, Val: 0.7360, Test: 0.7720
Epoch: 33, Loss: 4.2653, Train: 0.9929, Val: 0.7380, Test: 0.7760
Epoch: 34, Loss: 3.8407, Train: 1.0000, Val: 0.7460, Test: 0.7780
Epoch: 35, Loss: 3.8633, Train: 1.0000, Val: 0.7480, Test: 0.7880
Epoch: 36, Loss: 3.9977, Train: 1.0000, Val: 0.7500, Test: 0.7920
Epoch: 37, Loss: 3.8252, Train: 1.0000, Val: 0.7520, Test: 0.7950
Epoch: 38, Loss: 3.7101, Train: 1.0000, Val: 0.7500, Test: 0.7960
Epoch: 39, Loss: 3.9399, Train: 1.0000, Val: 0.7460, Test: 0.7960
Epoch: 40, Loss: 3.7281, Train: 1.0000, Val: 0.7480, Test: 0.7970
Epoch: 41, Loss: 3.8689, Train: 1.0000, Val: 0.7500, Test: 0.7970
Epoch: 42, Loss: 3.9478, Train: 1.0000, Val: 0.7520, Test: 0.7980
Epoch: 43, Loss: 3.6818, Train: 1.0000, Val: 0.7520, Test: 0.7960
Epoch: 44, Loss: 3.6466, Train: 1.0000, Val: 0.7500, Test: 0.7920
Epoch: 45, Loss: 3.4711, Train: 1.0000, Val: 0.7520, Test: 0.7880
Epoch: 46, Loss: 3.5528, Train: 1.0000, Val: 0.7460, Test: 0.7890
Epoch: 47, Loss: 3.7423, Train: 1.0000, Val: 0.7440, Test: 0.7880
Epoch: 48, Loss: 3.6879, Train: 1.0000, Val: 0.7440, Test: 0.7860
Epoch: 49, Loss: 3.7684, Train: 1.0000, Val: 0.7440, Test: 0.7860
Epoch: 50, Loss: 3.9475, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 51, Loss: 3.7533, Train: 1.0000, Val: 0.7440, Test: 0.7830
Epoch: 52, Loss: 3.6363, Train: 1.0000, Val: 0.7440, Test: 0.7850
Epoch: 53, Loss: 3.8363, Train: 1.0000, Val: 0.7440, Test: 0.7840
Epoch: 54, Loss: 3.7263, Train: 1.0000, Val: 0.7480, Test: 0.7820
Epoch: 55, Loss: 3.7306, Train: 1.0000, Val: 0.7460, Test: 0.7790
Epoch: 56, Loss: 3.3217, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 57, Loss: 3.7496, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 58, Loss: 3.9590, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 59, Loss: 3.8886, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 60, Loss: 3.7072, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 61, Loss: 3.7090, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 62, Loss: 3.7772, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 63, Loss: 3.7085, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 64, Loss: 3.8928, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 65, Loss: 3.5450, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 66, Loss: 3.6728, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 67, Loss: 3.4741, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 68, Loss: 3.8715, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 69, Loss: 3.6048, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 70, Loss: 3.7704, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 71, Loss: 3.4962, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 72, Loss: 3.5668, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 73, Loss: 3.8337, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 74, Loss: 3.5611, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 75, Loss: 3.8348, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 76, Loss: 3.8668, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 77, Loss: 3.7680, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 78, Loss: 3.6710, Train: 1.0000, Val: 0.7480, Test: 0.7770
Epoch: 79, Loss: 3.4184, Train: 1.0000, Val: 0.7460, Test: 0.7780
Epoch: 80, Loss: 3.3179, Train: 1.0000, Val: 0.7440, Test: 0.7810
Epoch: 81, Loss: 3.4557, Train: 1.0000, Val: 0.7440, Test: 0.7840
Epoch: 82, Loss: 4.1061, Train: 1.0000, Val: 0.7460, Test: 0.7850
Epoch: 83, Loss: 3.7934, Train: 1.0000, Val: 0.7480, Test: 0.7850
Epoch: 84, Loss: 4.0069, Train: 1.0000, Val: 0.7500, Test: 0.7840
Epoch: 85, Loss: 3.5253, Train: 1.0000, Val: 0.7500, Test: 0.7840
Epoch: 86, Loss: 4.0796, Train: 1.0000, Val: 0.7500, Test: 0.7830
Epoch: 87, Loss: 3.8646, Train: 1.0000, Val: 0.7520, Test: 0.7830
Epoch: 88, Loss: 3.4901, Train: 1.0000, Val: 0.7500, Test: 0.7850
Epoch: 89, Loss: 4.1446, Train: 1.0000, Val: 0.7500, Test: 0.7840
Epoch: 90, Loss: 3.5227, Train: 1.0000, Val: 0.7520, Test: 0.7850
Epoch: 91, Loss: 3.5260, Train: 1.0000, Val: 0.7480, Test: 0.7840
Epoch: 92, Loss: 3.7655, Train: 1.0000, Val: 0.7480, Test: 0.7840
Epoch: 93, Loss: 3.6940, Train: 1.0000, Val: 0.7500, Test: 0.7830
Epoch: 94, Loss: 3.7605, Train: 1.0000, Val: 0.7480, Test: 0.7840
Epoch: 95, Loss: 3.6272, Train: 1.0000, Val: 0.7480, Test: 0.7810
Epoch: 96, Loss: 3.5166, Train: 1.0000, Val: 0.7500, Test: 0.7820
Epoch: 97, Loss: 3.4832, Train: 1.0000, Val: 0.7520, Test: 0.7820
Epoch: 98, Loss: 3.4148, Train: 1.0000, Val: 0.7520, Test: 0.7820
Epoch: 99, Loss: 3.8264, Train: 1.0000, Val: 0.7540, Test: 0.7820
Epoch: 100, Loss: 3.6552, Train: 1.0000, Val: 0.7540, Test: 0.7810
Epoch: 101, Loss: 3.5216, Train: 1.0000, Val: 0.7540, Test: 0.7810
Epoch: 102, Loss: 3.7615, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 103, Loss: 3.6237, Train: 1.0000, Val: 0.7540, Test: 0.7780
Epoch: 104, Loss: 3.4439, Train: 1.0000, Val: 0.7540, Test: 0.7770
Epoch: 105, Loss: 3.6905, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 106, Loss: 3.8257, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 107, Loss: 3.5172, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 108, Loss: 3.8246, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 109, Loss: 3.8598, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 110, Loss: 3.7561, Train: 1.0000, Val: 0.7480, Test: 0.7750
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 111, Loss: 3.7921, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 112, Loss: 3.4443, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 113, Loss: 3.6173, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 114, Loss: 3.8601, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 115, Loss: 3.7184, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 116, Loss: 3.6529, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 117, Loss: 3.2702, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 118, Loss: 3.6521, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 119, Loss: 3.3758, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 120, Loss: 3.5148, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 121, Loss: 3.7869, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 122, Loss: 3.2720, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 123, Loss: 3.5823, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 124, Loss: 3.7539, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 125, Loss: 3.6832, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 126, Loss: 3.6173, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 127, Loss: 3.6856, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 128, Loss: 3.2343, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 129, Loss: 4.0323, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 130, Loss: 3.6521, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 131, Loss: 3.5821, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 132, Loss: 3.4443, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 133, Loss: 3.6155, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 134, Loss: 3.2740, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 135, Loss: 3.6151, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 136, Loss: 3.9250, Train: 1.0000, Val: 0.7440, Test: 0.7720
Epoch: 137, Loss: 3.8556, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 138, Loss: 3.5147, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 139, Loss: 3.5125, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 140, Loss: 3.7192, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 141, Loss: 3.8918, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 142, Loss: 3.6853, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 143, Loss: 3.5836, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 144, Loss: 3.5121, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 145, Loss: 3.4416, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 146, Loss: 3.5444, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 147, Loss: 3.5813, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 148, Loss: 3.8194, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 149, Loss: 3.5782, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 150, Loss: 3.6145, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 151, Loss: 3.6129, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 152, Loss: 3.6135, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 153, Loss: 3.6136, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 154, Loss: 3.9579, Train: 1.0000, Val: 0.7420, Test: 0.7740
Epoch: 155, Loss: 3.6847, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 156, Loss: 3.6494, Train: 1.0000, Val: 0.7420, Test: 0.7710
Epoch: 157, Loss: 3.6126, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 158, Loss: 3.7517, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 159, Loss: 3.6833, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 160, Loss: 3.7863, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 161, Loss: 3.7508, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 162, Loss: 3.7169, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 163, Loss: 3.6463, Train: 1.0000, Val: 0.7420, Test: 0.7730
Epoch: 164, Loss: 3.5800, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 165, Loss: 3.6128, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 166, Loss: 3.6485, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 167, Loss: 3.7517, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 168, Loss: 3.7513, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 169, Loss: 3.7863, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 170, Loss: 3.5771, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 171, Loss: 3.4762, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 172, Loss: 3.8904, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 173, Loss: 3.8557, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 174, Loss: 3.8562, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 175, Loss: 3.5790, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 176, Loss: 3.4745, Train: 1.0000, Val: 0.7480, Test: 0.7740
Epoch: 177, Loss: 3.6134, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 178, Loss: 3.7504, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 179, Loss: 3.8206, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 180, Loss: 3.5435, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 181, Loss: 3.5105, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 182, Loss: 3.5446, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 183, Loss: 3.6832, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 184, Loss: 3.5109, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 185, Loss: 3.6823, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 186, Loss: 3.6470, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 187, Loss: 3.7498, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 188, Loss: 3.7157, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 189, Loss: 3.6474, Train: 1.0000, Val: 0.7460, Test: 0.7740
Epoch: 190, Loss: 3.5765, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 191, Loss: 3.7859, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 192, Loss: 3.6821, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 193, Loss: 3.4727, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 194, Loss: 3.8212, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 195, Loss: 3.5801, Train: 1.0000, Val: 0.7460, Test: 0.7750
Epoch: 196, Loss: 3.9568, Train: 1.0000, Val: 0.7440, Test: 0.7760
Epoch: 197, Loss: 3.5426, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 198, Loss: 3.6110, Train: 1.0000, Val: 0.7440, Test: 0.7750
Epoch: 199, Loss: 3.7858, Train: 1.0000, Val: 0.7440, Test: 0.7740
Epoch: 200, Loss: 3.5423, Train: 1.0000, Val: 0.7440, Test: 0.7750
MAD:  0.3547
Best Test Accuracy: 0.7980, Val Accuracy: 0.7520, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.7508 ± 0.04974494949238565
Average MAD:  0.33085000000000003 ± 0.16061590363348208

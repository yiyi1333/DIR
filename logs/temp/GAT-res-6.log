/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1087, Train: 0.2071, Val: 0.1520, Test: 0.1600
Epoch: 2, Loss: 2.0889, Train: 0.2500, Val: 0.1660, Test: 0.1790
Epoch: 3, Loss: 1.9726, Train: 0.3929, Val: 0.2560, Test: 0.2680
Epoch: 4, Loss: 2.0568, Train: 0.6000, Val: 0.3620, Test: 0.3680
Epoch: 5, Loss: 1.8291, Train: 0.6500, Val: 0.4020, Test: 0.4210
Epoch: 6, Loss: 1.9003, Train: 0.7071, Val: 0.4280, Test: 0.4550
Epoch: 7, Loss: 1.8605, Train: 0.7357, Val: 0.4800, Test: 0.4960
Epoch: 8, Loss: 1.7296, Train: 0.7643, Val: 0.5020, Test: 0.5240
Epoch: 9, Loss: 1.7303, Train: 0.7857, Val: 0.5480, Test: 0.5530
Epoch: 10, Loss: 1.7751, Train: 0.8286, Val: 0.5900, Test: 0.5890
Epoch: 11, Loss: 1.6730, Train: 0.8500, Val: 0.6320, Test: 0.6220
Epoch: 12, Loss: 1.6436, Train: 0.8429, Val: 0.6460, Test: 0.6430
Epoch: 13, Loss: 1.5322, Train: 0.8500, Val: 0.6560, Test: 0.6610
Epoch: 14, Loss: 1.5532, Train: 0.8714, Val: 0.6540, Test: 0.6700
Epoch: 15, Loss: 1.5525, Train: 0.8786, Val: 0.6580, Test: 0.6730
Epoch: 16, Loss: 1.4414, Train: 0.8929, Val: 0.6720, Test: 0.6830
Epoch: 17, Loss: 1.3795, Train: 0.8929, Val: 0.6740, Test: 0.6880
Epoch: 18, Loss: 1.2689, Train: 0.8857, Val: 0.6820, Test: 0.6880
Epoch: 19, Loss: 1.3368, Train: 0.8857, Val: 0.6840, Test: 0.6890
Epoch: 20, Loss: 1.2435, Train: 0.8929, Val: 0.6780, Test: 0.6860
Epoch: 21, Loss: 1.1282, Train: 0.8929, Val: 0.6780, Test: 0.6780
Epoch: 22, Loss: 1.2237, Train: 0.8857, Val: 0.6720, Test: 0.6810
Epoch: 23, Loss: 1.0552, Train: 0.8857, Val: 0.6660, Test: 0.6790
Epoch: 24, Loss: 1.0594, Train: 0.8857, Val: 0.6640, Test: 0.6790
Epoch: 25, Loss: 0.9269, Train: 0.8857, Val: 0.6720, Test: 0.6850
Epoch: 26, Loss: 0.9178, Train: 0.8857, Val: 0.6720, Test: 0.6860
Epoch: 27, Loss: 1.0107, Train: 0.8857, Val: 0.6860, Test: 0.6960
Epoch: 28, Loss: 0.8208, Train: 0.9000, Val: 0.7020, Test: 0.7140
Epoch: 29, Loss: 0.7547, Train: 0.9286, Val: 0.7220, Test: 0.7370
Epoch: 30, Loss: 0.7841, Train: 0.9357, Val: 0.7520, Test: 0.7530
Epoch: 31, Loss: 0.7342, Train: 0.9571, Val: 0.7680, Test: 0.7630
Epoch: 32, Loss: 0.5965, Train: 0.9643, Val: 0.7760, Test: 0.7720
Epoch: 33, Loss: 0.5648, Train: 0.9714, Val: 0.7780, Test: 0.7780
Epoch: 34, Loss: 0.5326, Train: 0.9714, Val: 0.7820, Test: 0.7840
Epoch: 35, Loss: 0.5361, Train: 0.9786, Val: 0.7820, Test: 0.7820
Epoch: 36, Loss: 0.4894, Train: 0.9786, Val: 0.7860, Test: 0.7750
Epoch: 37, Loss: 0.5968, Train: 0.9714, Val: 0.7860, Test: 0.7690
Epoch: 38, Loss: 0.4258, Train: 0.9643, Val: 0.7620, Test: 0.7600
Epoch: 39, Loss: 0.3973, Train: 0.9643, Val: 0.7440, Test: 0.7530
Epoch: 40, Loss: 0.4224, Train: 0.9643, Val: 0.7340, Test: 0.7380
Epoch: 41, Loss: 0.4078, Train: 0.9643, Val: 0.7300, Test: 0.7320
Epoch: 42, Loss: 0.4521, Train: 0.9714, Val: 0.7280, Test: 0.7330
Epoch: 43, Loss: 0.3338, Train: 0.9786, Val: 0.7400, Test: 0.7320
Epoch: 44, Loss: 0.3328, Train: 0.9857, Val: 0.7480, Test: 0.7350
Epoch: 45, Loss: 0.2573, Train: 0.9857, Val: 0.7480, Test: 0.7330
Epoch: 46, Loss: 0.2907, Train: 0.9857, Val: 0.7540, Test: 0.7390
Epoch: 47, Loss: 0.3404, Train: 0.9857, Val: 0.7540, Test: 0.7460
Epoch: 48, Loss: 0.2133, Train: 0.9857, Val: 0.7620, Test: 0.7520
Epoch: 49, Loss: 0.3616, Train: 0.9857, Val: 0.7540, Test: 0.7480
Epoch: 50, Loss: 0.2802, Train: 0.9857, Val: 0.7420, Test: 0.7450
MAD:  0.9082
Best Test Accuracy: 0.7840, Val Accuracy: 0.7820, Train Accuracy: 0.9714
Training completed.
Seed:  1
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.4220, Train: 0.1500, Val: 0.1260, Test: 0.1380
Epoch: 2, Loss: 2.1743, Train: 0.2571, Val: 0.1600, Test: 0.1560
Epoch: 3, Loss: 2.0105, Train: 0.2214, Val: 0.1000, Test: 0.1150
Epoch: 4, Loss: 2.0450, Train: 0.2286, Val: 0.0960, Test: 0.1060
Epoch: 5, Loss: 1.9245, Train: 0.2357, Val: 0.1040, Test: 0.1190
Epoch: 6, Loss: 1.8563, Train: 0.2714, Val: 0.1180, Test: 0.1460
Epoch: 7, Loss: 1.8093, Train: 0.3143, Val: 0.1500, Test: 0.1930
Epoch: 8, Loss: 1.8181, Train: 0.4000, Val: 0.2280, Test: 0.2490
Epoch: 9, Loss: 1.7969, Train: 0.4643, Val: 0.3040, Test: 0.3130
Epoch: 10, Loss: 1.7924, Train: 0.5143, Val: 0.3880, Test: 0.3970
Epoch: 11, Loss: 1.7582, Train: 0.5857, Val: 0.4560, Test: 0.4640
Epoch: 12, Loss: 1.7591, Train: 0.6714, Val: 0.5080, Test: 0.5170
Epoch: 13, Loss: 1.7527, Train: 0.7214, Val: 0.5480, Test: 0.5760
Epoch: 14, Loss: 1.6322, Train: 0.7571, Val: 0.5940, Test: 0.6110
Epoch: 15, Loss: 1.6347, Train: 0.7714, Val: 0.6060, Test: 0.6290
Epoch: 16, Loss: 1.5690, Train: 0.8000, Val: 0.6380, Test: 0.6390
Epoch: 17, Loss: 1.4518, Train: 0.8143, Val: 0.6660, Test: 0.6620
Epoch: 18, Loss: 1.4715, Train: 0.8357, Val: 0.6580, Test: 0.6710
Epoch: 19, Loss: 1.3907, Train: 0.8429, Val: 0.6720, Test: 0.6800
Epoch: 20, Loss: 1.3638, Train: 0.8500, Val: 0.6680, Test: 0.6850
Epoch: 21, Loss: 1.3272, Train: 0.8643, Val: 0.6660, Test: 0.6950
Epoch: 22, Loss: 1.3422, Train: 0.8786, Val: 0.6660, Test: 0.6990
Epoch: 23, Loss: 1.2063, Train: 0.8786, Val: 0.6680, Test: 0.7080
Epoch: 24, Loss: 1.0786, Train: 0.8786, Val: 0.6820, Test: 0.7120
Epoch: 25, Loss: 1.0762, Train: 0.9000, Val: 0.6900, Test: 0.7160
Epoch: 26, Loss: 0.9779, Train: 0.9000, Val: 0.6980, Test: 0.7290
Epoch: 27, Loss: 0.9654, Train: 0.9071, Val: 0.7080, Test: 0.7440
Epoch: 28, Loss: 0.9602, Train: 0.9000, Val: 0.7220, Test: 0.7610
Epoch: 29, Loss: 0.8130, Train: 0.8929, Val: 0.7320, Test: 0.7710
Epoch: 30, Loss: 0.7736, Train: 0.9071, Val: 0.7420, Test: 0.7800
Epoch: 31, Loss: 0.7008, Train: 0.9071, Val: 0.7440, Test: 0.7830
Epoch: 32, Loss: 0.6273, Train: 0.9214, Val: 0.7580, Test: 0.7970
Epoch: 33, Loss: 0.6019, Train: 0.9286, Val: 0.7620, Test: 0.7950
Epoch: 34, Loss: 0.5742, Train: 0.9500, Val: 0.7640, Test: 0.7890
Epoch: 35, Loss: 0.6226, Train: 0.9500, Val: 0.7600, Test: 0.7940
Epoch: 36, Loss: 0.5845, Train: 0.9571, Val: 0.7640, Test: 0.7910
Epoch: 37, Loss: 0.4886, Train: 0.9571, Val: 0.7700, Test: 0.7900
Epoch: 38, Loss: 0.5286, Train: 0.9571, Val: 0.7720, Test: 0.7890
Epoch: 39, Loss: 0.3892, Train: 0.9571, Val: 0.7740, Test: 0.7890
Epoch: 40, Loss: 0.3791, Train: 0.9643, Val: 0.7780, Test: 0.7950
Epoch: 41, Loss: 0.3666, Train: 0.9714, Val: 0.7820, Test: 0.8060
Epoch: 42, Loss: 0.3567, Train: 0.9714, Val: 0.7860, Test: 0.8140
Epoch: 43, Loss: 0.4219, Train: 0.9714, Val: 0.7880, Test: 0.8140
Epoch: 44, Loss: 0.3702, Train: 0.9714, Val: 0.7880, Test: 0.8120
Epoch: 45, Loss: 0.3096, Train: 0.9714, Val: 0.7820, Test: 0.8110
Epoch: 46, Loss: 0.3374, Train: 0.9786, Val: 0.7800, Test: 0.8090
Epoch: 47, Loss: 0.3215, Train: 0.9714, Val: 0.7820, Test: 0.8060
Epoch: 48, Loss: 0.2972, Train: 0.9786, Val: 0.7760, Test: 0.8000
Epoch: 49, Loss: 0.2714, Train: 0.9786, Val: 0.7740, Test: 0.7940
Epoch: 50, Loss: 0.1838, Train: 0.9786, Val: 0.7860, Test: 0.7950
MAD:  0.8969
Best Test Accuracy: 0.8140, Val Accuracy: 0.7860, Train Accuracy: 0.9714
Training completed.
Seed:  2
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1621, Train: 0.2286, Val: 0.1120, Test: 0.1120
Epoch: 2, Loss: 2.0232, Train: 0.3214, Val: 0.1540, Test: 0.1660
Epoch: 3, Loss: 1.9699, Train: 0.4500, Val: 0.3180, Test: 0.3440
Epoch: 4, Loss: 2.0458, Train: 0.4643, Val: 0.4240, Test: 0.4440
Epoch: 5, Loss: 1.9039, Train: 0.5357, Val: 0.4720, Test: 0.4950
Epoch: 6, Loss: 1.7996, Train: 0.5571, Val: 0.5080, Test: 0.5200
Epoch: 7, Loss: 1.7934, Train: 0.6357, Val: 0.5600, Test: 0.5700
Epoch: 8, Loss: 1.7272, Train: 0.6929, Val: 0.6020, Test: 0.6120
Epoch: 9, Loss: 1.6910, Train: 0.7500, Val: 0.6260, Test: 0.6460
Epoch: 10, Loss: 1.6915, Train: 0.7714, Val: 0.6700, Test: 0.6750
Epoch: 11, Loss: 1.5553, Train: 0.8071, Val: 0.6840, Test: 0.6980
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 12, Loss: 1.6524, Train: 0.8571, Val: 0.7020, Test: 0.7180
Epoch: 13, Loss: 1.4929, Train: 0.8714, Val: 0.7260, Test: 0.7370
Epoch: 14, Loss: 1.5210, Train: 0.8929, Val: 0.7320, Test: 0.7590
Epoch: 15, Loss: 1.3877, Train: 0.9214, Val: 0.7360, Test: 0.7710
Epoch: 16, Loss: 1.3061, Train: 0.9429, Val: 0.7400, Test: 0.7690
Epoch: 17, Loss: 1.3220, Train: 0.9429, Val: 0.7440, Test: 0.7630
Epoch: 18, Loss: 1.1248, Train: 0.9571, Val: 0.7480, Test: 0.7690
Epoch: 19, Loss: 1.0449, Train: 0.9500, Val: 0.7520, Test: 0.7800
Epoch: 20, Loss: 0.9607, Train: 0.9357, Val: 0.7640, Test: 0.7940
Epoch: 21, Loss: 0.9216, Train: 0.9429, Val: 0.7800, Test: 0.8100
Epoch: 22, Loss: 0.8990, Train: 0.9429, Val: 0.7840, Test: 0.8130
Epoch: 23, Loss: 0.8407, Train: 0.9429, Val: 0.7760, Test: 0.8130
Epoch: 24, Loss: 0.8216, Train: 0.9500, Val: 0.7780, Test: 0.8110
Epoch: 25, Loss: 0.7554, Train: 0.9714, Val: 0.7840, Test: 0.8040
Epoch: 26, Loss: 0.6522, Train: 0.9857, Val: 0.7840, Test: 0.8000
Epoch: 27, Loss: 0.6634, Train: 0.9857, Val: 0.7740, Test: 0.7880
Epoch: 28, Loss: 0.5295, Train: 0.9857, Val: 0.7700, Test: 0.7820
Epoch: 29, Loss: 0.5290, Train: 0.9857, Val: 0.7620, Test: 0.7840
Epoch: 30, Loss: 0.5681, Train: 0.9786, Val: 0.7660, Test: 0.7850
Epoch: 31, Loss: 0.5589, Train: 0.9786, Val: 0.7720, Test: 0.7940
Epoch: 32, Loss: 0.4691, Train: 0.9786, Val: 0.7820, Test: 0.8030
Epoch: 33, Loss: 0.3958, Train: 0.9786, Val: 0.7860, Test: 0.8070
Epoch: 34, Loss: 0.3798, Train: 0.9786, Val: 0.7900, Test: 0.8080
Epoch: 35, Loss: 0.3546, Train: 0.9786, Val: 0.8060, Test: 0.8170
Epoch: 36, Loss: 0.3971, Train: 0.9786, Val: 0.8100, Test: 0.8180
Epoch: 37, Loss: 0.3117, Train: 0.9714, Val: 0.8100, Test: 0.8160
Epoch: 38, Loss: 0.3053, Train: 0.9786, Val: 0.8060, Test: 0.8090
Epoch: 39, Loss: 0.2611, Train: 0.9786, Val: 0.8000, Test: 0.8050
Epoch: 40, Loss: 0.2146, Train: 0.9786, Val: 0.7900, Test: 0.7970
Epoch: 41, Loss: 0.1831, Train: 0.9786, Val: 0.7800, Test: 0.7910
Epoch: 42, Loss: 0.2363, Train: 0.9786, Val: 0.7760, Test: 0.7820
Epoch: 43, Loss: 0.2564, Train: 0.9857, Val: 0.7760, Test: 0.7790
Epoch: 44, Loss: 0.1555, Train: 0.9857, Val: 0.7680, Test: 0.7780
Epoch: 45, Loss: 0.2628, Train: 0.9857, Val: 0.7640, Test: 0.7790
Epoch: 46, Loss: 0.1650, Train: 1.0000, Val: 0.7620, Test: 0.7810
Epoch: 47, Loss: 0.1377, Train: 1.0000, Val: 0.7640, Test: 0.7820
Epoch: 48, Loss: 0.1501, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 49, Loss: 0.1726, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 50, Loss: 0.1398, Train: 1.0000, Val: 0.7680, Test: 0.7910
MAD:  0.8055
Best Test Accuracy: 0.8180, Val Accuracy: 0.8100, Train Accuracy: 0.9786
Training completed.
Seed:  3
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.2511, Train: 0.2571, Val: 0.1760, Test: 0.1960
Epoch: 2, Loss: 2.0382, Train: 0.3214, Val: 0.2400, Test: 0.2540
Epoch: 3, Loss: 1.9523, Train: 0.4143, Val: 0.2720, Test: 0.2610
Epoch: 4, Loss: 1.9463, Train: 0.4571, Val: 0.3080, Test: 0.2870
Epoch: 5, Loss: 1.8924, Train: 0.5000, Val: 0.3280, Test: 0.3150
Epoch: 6, Loss: 1.8315, Train: 0.5429, Val: 0.3800, Test: 0.3460
Epoch: 7, Loss: 1.7789, Train: 0.6143, Val: 0.4080, Test: 0.3920
Epoch: 8, Loss: 1.6922, Train: 0.6857, Val: 0.4400, Test: 0.4420
Epoch: 9, Loss: 1.6891, Train: 0.7500, Val: 0.4880, Test: 0.4870
Epoch: 10, Loss: 1.6805, Train: 0.7857, Val: 0.5300, Test: 0.5360
Epoch: 11, Loss: 1.5903, Train: 0.8357, Val: 0.5420, Test: 0.5610
Epoch: 12, Loss: 1.5476, Train: 0.8857, Val: 0.5860, Test: 0.5960
Epoch: 13, Loss: 1.5490, Train: 0.9143, Val: 0.6260, Test: 0.6330
Epoch: 14, Loss: 1.4526, Train: 0.9143, Val: 0.6600, Test: 0.6550
Epoch: 15, Loss: 1.3421, Train: 0.9357, Val: 0.6780, Test: 0.6810
Epoch: 16, Loss: 1.3015, Train: 0.9357, Val: 0.6920, Test: 0.6920
Epoch: 17, Loss: 1.2966, Train: 0.9357, Val: 0.7080, Test: 0.6980
Epoch: 18, Loss: 1.1642, Train: 0.9357, Val: 0.7060, Test: 0.7020
Epoch: 19, Loss: 1.1738, Train: 0.9286, Val: 0.7020, Test: 0.6920
Epoch: 20, Loss: 0.9977, Train: 0.9214, Val: 0.7000, Test: 0.6890
Epoch: 21, Loss: 0.9147, Train: 0.9214, Val: 0.7060, Test: 0.7030
Epoch: 22, Loss: 0.8752, Train: 0.9214, Val: 0.7360, Test: 0.7190
Epoch: 23, Loss: 0.7640, Train: 0.9500, Val: 0.7680, Test: 0.7350
Epoch: 24, Loss: 0.8285, Train: 0.9500, Val: 0.7780, Test: 0.7540
Epoch: 25, Loss: 0.7561, Train: 0.9571, Val: 0.7940, Test: 0.7690
Epoch: 26, Loss: 0.5953, Train: 0.9571, Val: 0.7960, Test: 0.7820
Epoch: 27, Loss: 0.6875, Train: 0.9571, Val: 0.8000, Test: 0.7850
Epoch: 28, Loss: 0.5602, Train: 0.9571, Val: 0.8000, Test: 0.7850
Epoch: 29, Loss: 0.5184, Train: 0.9429, Val: 0.8000, Test: 0.7790
Epoch: 30, Loss: 0.5771, Train: 0.9500, Val: 0.8000, Test: 0.7750
Epoch: 31, Loss: 0.4092, Train: 0.9500, Val: 0.7920, Test: 0.7710
Epoch: 32, Loss: 0.3896, Train: 0.9500, Val: 0.7820, Test: 0.7710
Epoch: 33, Loss: 0.4204, Train: 0.9643, Val: 0.7840, Test: 0.7690
Epoch: 34, Loss: 0.3646, Train: 0.9643, Val: 0.7840, Test: 0.7730
Epoch: 35, Loss: 0.3084, Train: 0.9643, Val: 0.7840, Test: 0.7800
Epoch: 36, Loss: 0.3373, Train: 0.9643, Val: 0.7820, Test: 0.7790
Epoch: 37, Loss: 0.3604, Train: 0.9643, Val: 0.7780, Test: 0.7780
Epoch: 38, Loss: 0.4599, Train: 0.9643, Val: 0.7800, Test: 0.7830
Epoch: 39, Loss: 0.3496, Train: 0.9714, Val: 0.7780, Test: 0.7820
Epoch: 40, Loss: 0.1947, Train: 0.9714, Val: 0.7780, Test: 0.7850
Epoch: 41, Loss: 0.1931, Train: 0.9786, Val: 0.7780, Test: 0.7880
Epoch: 42, Loss: 0.2119, Train: 0.9786, Val: 0.7800, Test: 0.7830
Epoch: 43, Loss: 0.1969, Train: 0.9786, Val: 0.7820, Test: 0.7820
Epoch: 44, Loss: 0.2476, Train: 0.9857, Val: 0.7800, Test: 0.7840
Epoch: 45, Loss: 0.1879, Train: 0.9929, Val: 0.7800, Test: 0.7830
Epoch: 46, Loss: 0.1630, Train: 0.9929, Val: 0.7780, Test: 0.7850
Epoch: 47, Loss: 0.2587, Train: 0.9929, Val: 0.7820, Test: 0.7850
Epoch: 48, Loss: 0.1420, Train: 0.9929, Val: 0.7760, Test: 0.7850
Epoch: 49, Loss: 0.1962, Train: 0.9929, Val: 0.7780, Test: 0.7920
Epoch: 50, Loss: 0.1401, Train: 0.9857, Val: 0.7740, Test: 0.7940
MAD:  0.8104
Best Test Accuracy: 0.7940, Val Accuracy: 0.7740, Train Accuracy: 0.9857
Training completed.
Seed:  4
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1211, Train: 0.2571, Val: 0.2320, Test: 0.2400
Epoch: 2, Loss: 2.0178, Train: 0.3214, Val: 0.2560, Test: 0.2670
Epoch: 3, Loss: 1.9910, Train: 0.4214, Val: 0.3260, Test: 0.3020
Epoch: 4, Loss: 2.0010, Train: 0.5071, Val: 0.3640, Test: 0.3690
Epoch: 5, Loss: 1.9204, Train: 0.6500, Val: 0.4380, Test: 0.4640
Epoch: 6, Loss: 1.7972, Train: 0.7286, Val: 0.5340, Test: 0.5410
Epoch: 7, Loss: 1.8025, Train: 0.8071, Val: 0.6040, Test: 0.6140
Epoch: 8, Loss: 1.7333, Train: 0.8429, Val: 0.6460, Test: 0.6490
Epoch: 9, Loss: 1.7090, Train: 0.8643, Val: 0.6640, Test: 0.6760
Epoch: 10, Loss: 1.6750, Train: 0.8786, Val: 0.6740, Test: 0.6880
Epoch: 11, Loss: 1.6025, Train: 0.9000, Val: 0.6740, Test: 0.6950
Epoch: 12, Loss: 1.5384, Train: 0.8929, Val: 0.6760, Test: 0.6870
Epoch: 13, Loss: 1.5135, Train: 0.8857, Val: 0.6800, Test: 0.6830
Epoch: 14, Loss: 1.5397, Train: 0.9000, Val: 0.6880, Test: 0.6780
Epoch: 15, Loss: 1.3507, Train: 0.8857, Val: 0.6900, Test: 0.6740
Epoch: 16, Loss: 1.2756, Train: 0.9000, Val: 0.6940, Test: 0.6820
Epoch: 17, Loss: 1.2112, Train: 0.9071, Val: 0.7060, Test: 0.6930
Epoch: 18, Loss: 1.1981, Train: 0.9143, Val: 0.7100, Test: 0.7160
Epoch: 19, Loss: 1.1700, Train: 0.9286, Val: 0.7200, Test: 0.7260
Epoch: 20, Loss: 1.1417, Train: 0.9429, Val: 0.7260, Test: 0.7410
Epoch: 21, Loss: 1.0057, Train: 0.9500, Val: 0.7340, Test: 0.7480
Epoch: 22, Loss: 0.9450, Train: 0.9500, Val: 0.7560, Test: 0.7540
Epoch: 23, Loss: 0.8812, Train: 0.9571, Val: 0.7540, Test: 0.7550
Epoch: 24, Loss: 0.8748, Train: 0.9571, Val: 0.7660, Test: 0.7640
Epoch: 25, Loss: 0.7696, Train: 0.9571, Val: 0.7820, Test: 0.7790
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 26, Loss: 0.6410, Train: 0.9571, Val: 0.7840, Test: 0.7910
Epoch: 27, Loss: 0.6301, Train: 0.9571, Val: 0.7820, Test: 0.7890
Epoch: 28, Loss: 0.7482, Train: 0.9643, Val: 0.7840, Test: 0.7930
Epoch: 29, Loss: 0.4890, Train: 0.9643, Val: 0.7920, Test: 0.7960
Epoch: 30, Loss: 0.5830, Train: 0.9714, Val: 0.7960, Test: 0.7980
Epoch: 31, Loss: 0.6459, Train: 0.9714, Val: 0.7980, Test: 0.8030
Epoch: 32, Loss: 0.4956, Train: 0.9714, Val: 0.7960, Test: 0.8030
Epoch: 33, Loss: 0.4706, Train: 0.9714, Val: 0.7940, Test: 0.8060
Epoch: 34, Loss: 0.3921, Train: 0.9714, Val: 0.7940, Test: 0.8060
Epoch: 35, Loss: 0.3881, Train: 0.9714, Val: 0.7920, Test: 0.8050
Epoch: 36, Loss: 0.4363, Train: 0.9714, Val: 0.7900, Test: 0.8070
Epoch: 37, Loss: 0.3183, Train: 0.9571, Val: 0.7880, Test: 0.8060
Epoch: 38, Loss: 0.3267, Train: 0.9643, Val: 0.7920, Test: 0.8070
Epoch: 39, Loss: 0.3221, Train: 0.9714, Val: 0.7900, Test: 0.8010
Epoch: 40, Loss: 0.3417, Train: 0.9714, Val: 0.7920, Test: 0.8010
Epoch: 41, Loss: 0.2545, Train: 0.9714, Val: 0.7880, Test: 0.7940
Epoch: 42, Loss: 0.2975, Train: 0.9714, Val: 0.7940, Test: 0.7990
Epoch: 43, Loss: 0.3406, Train: 0.9786, Val: 0.7920, Test: 0.8050
Epoch: 44, Loss: 0.2404, Train: 0.9786, Val: 0.7900, Test: 0.8060
Epoch: 45, Loss: 0.2548, Train: 0.9857, Val: 0.7860, Test: 0.8050
Epoch: 46, Loss: 0.1813, Train: 0.9929, Val: 0.7900, Test: 0.8050
Epoch: 47, Loss: 0.1842, Train: 0.9929, Val: 0.7940, Test: 0.8110
Epoch: 48, Loss: 0.2062, Train: 0.9929, Val: 0.7880, Test: 0.8090
Epoch: 49, Loss: 0.1667, Train: 0.9857, Val: 0.7880, Test: 0.8080
Epoch: 50, Loss: 0.3184, Train: 0.9857, Val: 0.7840, Test: 0.8060
MAD:  0.7136
Best Test Accuracy: 0.8110, Val Accuracy: 0.7940, Train Accuracy: 0.9929
Training completed.
Seed:  5
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1197, Train: 0.1857, Val: 0.1680, Test: 0.1750
Epoch: 2, Loss: 1.9985, Train: 0.3286, Val: 0.2220, Test: 0.2550
Epoch: 3, Loss: 1.9347, Train: 0.3786, Val: 0.1900, Test: 0.2430
Epoch: 4, Loss: 1.9832, Train: 0.4000, Val: 0.2020, Test: 0.2340
Epoch: 5, Loss: 1.9022, Train: 0.4500, Val: 0.2960, Test: 0.2970
Epoch: 6, Loss: 1.8375, Train: 0.5929, Val: 0.3780, Test: 0.3900
Epoch: 7, Loss: 1.7981, Train: 0.6500, Val: 0.4460, Test: 0.4630
Epoch: 8, Loss: 1.7340, Train: 0.7786, Val: 0.5240, Test: 0.5690
Epoch: 9, Loss: 1.6805, Train: 0.8571, Val: 0.6080, Test: 0.6360
Epoch: 10, Loss: 1.6274, Train: 0.8714, Val: 0.6780, Test: 0.6860
Epoch: 11, Loss: 1.6524, Train: 0.8929, Val: 0.7100, Test: 0.7270
Epoch: 12, Loss: 1.6007, Train: 0.9000, Val: 0.7300, Test: 0.7550
Epoch: 13, Loss: 1.4873, Train: 0.9143, Val: 0.7380, Test: 0.7700
Epoch: 14, Loss: 1.4815, Train: 0.9143, Val: 0.7460, Test: 0.7770
Epoch: 15, Loss: 1.3769, Train: 0.9214, Val: 0.7460, Test: 0.7790
Epoch: 16, Loss: 1.3777, Train: 0.9286, Val: 0.7480, Test: 0.7730
Epoch: 17, Loss: 1.2760, Train: 0.9429, Val: 0.7520, Test: 0.7750
Epoch: 18, Loss: 1.3243, Train: 0.9500, Val: 0.7620, Test: 0.7800
Epoch: 19, Loss: 1.1497, Train: 0.9500, Val: 0.7680, Test: 0.7840
Epoch: 20, Loss: 1.1835, Train: 0.9357, Val: 0.7700, Test: 0.7840
Epoch: 21, Loss: 1.0865, Train: 0.9429, Val: 0.7720, Test: 0.7860
Epoch: 22, Loss: 0.9921, Train: 0.9500, Val: 0.7760, Test: 0.7880
Epoch: 23, Loss: 0.9208, Train: 0.9571, Val: 0.7760, Test: 0.7870
Epoch: 24, Loss: 0.8652, Train: 0.9571, Val: 0.7860, Test: 0.7900
Epoch: 25, Loss: 0.6906, Train: 0.9500, Val: 0.7820, Test: 0.7860
Epoch: 26, Loss: 0.8292, Train: 0.9429, Val: 0.7840, Test: 0.7870
Epoch: 27, Loss: 0.8123, Train: 0.9429, Val: 0.7880, Test: 0.7850
Epoch: 28, Loss: 0.6554, Train: 0.9429, Val: 0.7880, Test: 0.7830
Epoch: 29, Loss: 0.7064, Train: 0.9429, Val: 0.7880, Test: 0.7830
Epoch: 30, Loss: 0.6459, Train: 0.9500, Val: 0.7800, Test: 0.7820
Epoch: 31, Loss: 0.6025, Train: 0.9500, Val: 0.7820, Test: 0.7780
Epoch: 32, Loss: 0.4773, Train: 0.9643, Val: 0.7840, Test: 0.7840
Epoch: 33, Loss: 0.4931, Train: 0.9571, Val: 0.7900, Test: 0.7940
Epoch: 34, Loss: 0.4372, Train: 0.9571, Val: 0.7880, Test: 0.8000
Epoch: 35, Loss: 0.5588, Train: 0.9714, Val: 0.7920, Test: 0.8100
Epoch: 36, Loss: 0.3802, Train: 0.9786, Val: 0.7920, Test: 0.8160
Epoch: 37, Loss: 0.3597, Train: 0.9857, Val: 0.7920, Test: 0.8120
Epoch: 38, Loss: 0.4103, Train: 0.9857, Val: 0.7960, Test: 0.8090
Epoch: 39, Loss: 0.3237, Train: 0.9857, Val: 0.7920, Test: 0.8090
Epoch: 40, Loss: 0.2969, Train: 0.9857, Val: 0.7960, Test: 0.8090
Epoch: 41, Loss: 0.2854, Train: 0.9857, Val: 0.8000, Test: 0.8070
Epoch: 42, Loss: 0.3067, Train: 0.9857, Val: 0.7980, Test: 0.8030
Epoch: 43, Loss: 0.2246, Train: 0.9786, Val: 0.7960, Test: 0.7990
Epoch: 44, Loss: 0.2084, Train: 0.9786, Val: 0.7960, Test: 0.7990
Epoch: 45, Loss: 0.2177, Train: 0.9786, Val: 0.7920, Test: 0.7980
Epoch: 46, Loss: 0.2226, Train: 0.9857, Val: 0.7840, Test: 0.7990
Epoch: 47, Loss: 0.2381, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 48, Loss: 0.2421, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 49, Loss: 0.1756, Train: 0.9929, Val: 0.7760, Test: 0.7970
Epoch: 50, Loss: 0.2021, Train: 0.9929, Val: 0.7740, Test: 0.8040
MAD:  0.8005
Best Test Accuracy: 0.8160, Val Accuracy: 0.7920, Train Accuracy: 0.9786
Training completed.
Seed:  6
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1752, Train: 0.2214, Val: 0.2800, Test: 0.3020
Epoch: 2, Loss: 2.0496, Train: 0.3071, Val: 0.3000, Test: 0.3620
Epoch: 3, Loss: 1.9177, Train: 0.4500, Val: 0.2900, Test: 0.3270
Epoch: 4, Loss: 1.9537, Train: 0.4929, Val: 0.2760, Test: 0.2940
Epoch: 5, Loss: 1.8481, Train: 0.5071, Val: 0.2620, Test: 0.2960
Epoch: 6, Loss: 1.8236, Train: 0.5571, Val: 0.3020, Test: 0.3330
Epoch: 7, Loss: 1.8338, Train: 0.6143, Val: 0.3360, Test: 0.3560
Epoch: 8, Loss: 1.7336, Train: 0.6571, Val: 0.3740, Test: 0.4000
Epoch: 9, Loss: 1.6883, Train: 0.7143, Val: 0.4180, Test: 0.4290
Epoch: 10, Loss: 1.6940, Train: 0.7571, Val: 0.4700, Test: 0.4840
Epoch: 11, Loss: 1.7169, Train: 0.7857, Val: 0.5300, Test: 0.5380
Epoch: 12, Loss: 1.5982, Train: 0.8286, Val: 0.5800, Test: 0.5920
Epoch: 13, Loss: 1.5238, Train: 0.8500, Val: 0.6220, Test: 0.6330
Epoch: 14, Loss: 1.5803, Train: 0.8643, Val: 0.6420, Test: 0.6640
Epoch: 15, Loss: 1.3966, Train: 0.8857, Val: 0.6680, Test: 0.7030
Epoch: 16, Loss: 1.3865, Train: 0.8857, Val: 0.6980, Test: 0.7140
Epoch: 17, Loss: 1.3091, Train: 0.9000, Val: 0.7120, Test: 0.7270
Epoch: 18, Loss: 1.2778, Train: 0.9071, Val: 0.7320, Test: 0.7410
Epoch: 19, Loss: 1.2333, Train: 0.9214, Val: 0.7360, Test: 0.7450
Epoch: 20, Loss: 1.1922, Train: 0.9357, Val: 0.7380, Test: 0.7560
Epoch: 21, Loss: 1.1158, Train: 0.9357, Val: 0.7500, Test: 0.7670
Epoch: 22, Loss: 1.0109, Train: 0.9500, Val: 0.7780, Test: 0.7830
Epoch: 23, Loss: 0.9253, Train: 0.9643, Val: 0.7940, Test: 0.7960
Epoch: 24, Loss: 0.7579, Train: 0.9643, Val: 0.8020, Test: 0.7970
Epoch: 25, Loss: 0.8548, Train: 0.9714, Val: 0.7980, Test: 0.7960
Epoch: 26, Loss: 0.8664, Train: 0.9643, Val: 0.7980, Test: 0.8020
Epoch: 27, Loss: 0.8640, Train: 0.9643, Val: 0.7900, Test: 0.7980
Epoch: 28, Loss: 0.7050, Train: 0.9714, Val: 0.7880, Test: 0.7950
Epoch: 29, Loss: 0.5116, Train: 0.9786, Val: 0.7840, Test: 0.7930
Epoch: 30, Loss: 0.5315, Train: 0.9786, Val: 0.7880, Test: 0.7890
Epoch: 31, Loss: 0.5289, Train: 0.9714, Val: 0.7840, Test: 0.7850
Epoch: 32, Loss: 0.4585, Train: 0.9786, Val: 0.7800, Test: 0.7850
Epoch: 33, Loss: 0.5229, Train: 0.9786, Val: 0.7820, Test: 0.7840
Epoch: 34, Loss: 0.5285, Train: 0.9786, Val: 0.7820, Test: 0.7810
Epoch: 35, Loss: 0.3603, Train: 0.9786, Val: 0.7860, Test: 0.7870
Epoch: 36, Loss: 0.4702, Train: 0.9857, Val: 0.7860, Test: 0.7870
Epoch: 37, Loss: 0.3491, Train: 0.9929, Val: 0.7860, Test: 0.7890
Epoch: 38, Loss: 0.3431, Train: 0.9929, Val: 0.7960, Test: 0.7940
Epoch: 39, Loss: 0.2724, Train: 0.9929, Val: 0.7940, Test: 0.7990
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 40, Loss: 0.2699, Train: 0.9929, Val: 0.7980, Test: 0.8010
Epoch: 41, Loss: 0.3610, Train: 0.9929, Val: 0.7960, Test: 0.7990
Epoch: 42, Loss: 0.2617, Train: 0.9857, Val: 0.8020, Test: 0.7960
Epoch: 43, Loss: 0.2041, Train: 0.9857, Val: 0.7980, Test: 0.7940
Epoch: 44, Loss: 0.2309, Train: 0.9857, Val: 0.7980, Test: 0.7880
Epoch: 45, Loss: 0.1927, Train: 0.9857, Val: 0.7940, Test: 0.7870
Epoch: 46, Loss: 0.2316, Train: 0.9929, Val: 0.7900, Test: 0.7860
Epoch: 47, Loss: 0.2627, Train: 0.9929, Val: 0.7860, Test: 0.7870
Epoch: 48, Loss: 0.0961, Train: 0.9929, Val: 0.7840, Test: 0.7870
Epoch: 49, Loss: 0.1305, Train: 0.9929, Val: 0.7840, Test: 0.7830
Epoch: 50, Loss: 0.1553, Train: 0.9929, Val: 0.7860, Test: 0.7860
MAD:  0.8863
Best Test Accuracy: 0.8020, Val Accuracy: 0.7980, Train Accuracy: 0.9643
Training completed.
Seed:  7
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.4021, Train: 0.1929, Val: 0.1220, Test: 0.1270
Epoch: 2, Loss: 2.1548, Train: 0.2500, Val: 0.1980, Test: 0.1880
Epoch: 3, Loss: 2.0501, Train: 0.3071, Val: 0.2520, Test: 0.2440
Epoch: 4, Loss: 2.0019, Train: 0.4286, Val: 0.3100, Test: 0.2950
Epoch: 5, Loss: 2.0035, Train: 0.4571, Val: 0.3080, Test: 0.3110
Epoch: 6, Loss: 1.9620, Train: 0.4786, Val: 0.3120, Test: 0.3210
Epoch: 7, Loss: 1.8237, Train: 0.5500, Val: 0.3460, Test: 0.3540
Epoch: 8, Loss: 1.7515, Train: 0.6071, Val: 0.4060, Test: 0.3940
Epoch: 9, Loss: 1.8191, Train: 0.6857, Val: 0.4360, Test: 0.4250
Epoch: 10, Loss: 1.7543, Train: 0.7429, Val: 0.4840, Test: 0.4790
Epoch: 11, Loss: 1.7702, Train: 0.7857, Val: 0.5340, Test: 0.5280
Epoch: 12, Loss: 1.6419, Train: 0.8214, Val: 0.5760, Test: 0.5770
Epoch: 13, Loss: 1.6098, Train: 0.8286, Val: 0.6300, Test: 0.6160
Epoch: 14, Loss: 1.5752, Train: 0.8357, Val: 0.6660, Test: 0.6330
Epoch: 15, Loss: 1.4896, Train: 0.8357, Val: 0.6660, Test: 0.6440
Epoch: 16, Loss: 1.4534, Train: 0.8429, Val: 0.6900, Test: 0.6700
Epoch: 17, Loss: 1.4150, Train: 0.8643, Val: 0.6980, Test: 0.6860
Epoch: 18, Loss: 1.3559, Train: 0.8643, Val: 0.6920, Test: 0.6860
Epoch: 19, Loss: 1.3424, Train: 0.8714, Val: 0.6880, Test: 0.6910
Epoch: 20, Loss: 1.2957, Train: 0.8929, Val: 0.6960, Test: 0.7070
Epoch: 21, Loss: 1.3119, Train: 0.8929, Val: 0.7140, Test: 0.7160
Epoch: 22, Loss: 1.1749, Train: 0.9071, Val: 0.7280, Test: 0.7330
Epoch: 23, Loss: 1.0818, Train: 0.9286, Val: 0.7420, Test: 0.7400
Epoch: 24, Loss: 1.0208, Train: 0.9286, Val: 0.7480, Test: 0.7480
Epoch: 25, Loss: 0.9579, Train: 0.9571, Val: 0.7520, Test: 0.7560
Epoch: 26, Loss: 0.8370, Train: 0.9571, Val: 0.7540, Test: 0.7630
Epoch: 27, Loss: 0.8245, Train: 0.9571, Val: 0.7540, Test: 0.7720
Epoch: 28, Loss: 0.8458, Train: 0.9571, Val: 0.7600, Test: 0.7670
Epoch: 29, Loss: 0.7530, Train: 0.9643, Val: 0.7640, Test: 0.7770
Epoch: 30, Loss: 0.6647, Train: 0.9714, Val: 0.7700, Test: 0.7820
Epoch: 31, Loss: 0.7174, Train: 0.9714, Val: 0.7760, Test: 0.7830
Epoch: 32, Loss: 0.6292, Train: 0.9714, Val: 0.7800, Test: 0.7860
Epoch: 33, Loss: 0.5851, Train: 0.9714, Val: 0.7860, Test: 0.7880
Epoch: 34, Loss: 0.5264, Train: 0.9571, Val: 0.7840, Test: 0.7870
Epoch: 35, Loss: 0.4602, Train: 0.9643, Val: 0.7820, Test: 0.7810
Epoch: 36, Loss: 0.3662, Train: 0.9643, Val: 0.7800, Test: 0.7800
Epoch: 37, Loss: 0.4038, Train: 0.9643, Val: 0.7820, Test: 0.7800
Epoch: 38, Loss: 0.3937, Train: 0.9714, Val: 0.7840, Test: 0.7820
Epoch: 39, Loss: 0.4449, Train: 0.9714, Val: 0.7840, Test: 0.7810
Epoch: 40, Loss: 0.3886, Train: 0.9714, Val: 0.7860, Test: 0.7840
Epoch: 41, Loss: 0.3426, Train: 0.9714, Val: 0.7820, Test: 0.7830
Epoch: 42, Loss: 0.3056, Train: 0.9714, Val: 0.7760, Test: 0.7810
Epoch: 43, Loss: 0.3294, Train: 0.9714, Val: 0.7740, Test: 0.7790
Epoch: 44, Loss: 0.2002, Train: 0.9643, Val: 0.7720, Test: 0.7740
Epoch: 45, Loss: 0.2809, Train: 0.9643, Val: 0.7700, Test: 0.7710
Epoch: 46, Loss: 0.2205, Train: 0.9786, Val: 0.7660, Test: 0.7710
Epoch: 47, Loss: 0.2064, Train: 0.9786, Val: 0.7680, Test: 0.7760
Epoch: 48, Loss: 0.2554, Train: 0.9786, Val: 0.7640, Test: 0.7730
Epoch: 49, Loss: 0.3023, Train: 0.9786, Val: 0.7640, Test: 0.7710
Epoch: 50, Loss: 0.1788, Train: 0.9714, Val: 0.7680, Test: 0.7710
MAD:  0.8582
Best Test Accuracy: 0.7880, Val Accuracy: 0.7860, Train Accuracy: 0.9714
Training completed.
Seed:  8
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.2867, Train: 0.2643, Val: 0.2140, Test: 0.2000
Epoch: 2, Loss: 2.0019, Train: 0.2857, Val: 0.2280, Test: 0.2220
Epoch: 3, Loss: 2.0517, Train: 0.4286, Val: 0.2900, Test: 0.2870
Epoch: 4, Loss: 1.8686, Train: 0.4857, Val: 0.3460, Test: 0.3800
Epoch: 5, Loss: 1.8546, Train: 0.5357, Val: 0.4060, Test: 0.4710
Epoch: 6, Loss: 1.8194, Train: 0.6214, Val: 0.4480, Test: 0.5100
Epoch: 7, Loss: 1.8201, Train: 0.6857, Val: 0.5140, Test: 0.5470
Epoch: 8, Loss: 1.7857, Train: 0.7786, Val: 0.5480, Test: 0.5830
Epoch: 9, Loss: 1.7323, Train: 0.8214, Val: 0.5920, Test: 0.6370
Epoch: 10, Loss: 1.6733, Train: 0.8500, Val: 0.6260, Test: 0.6510
Epoch: 11, Loss: 1.6855, Train: 0.8857, Val: 0.6540, Test: 0.6820
Epoch: 12, Loss: 1.5824, Train: 0.9000, Val: 0.6820, Test: 0.6910
Epoch: 13, Loss: 1.5051, Train: 0.8857, Val: 0.6860, Test: 0.6960
Epoch: 14, Loss: 1.5447, Train: 0.8857, Val: 0.6860, Test: 0.6930
Epoch: 15, Loss: 1.4100, Train: 0.8929, Val: 0.6820, Test: 0.6990
Epoch: 16, Loss: 1.4545, Train: 0.8929, Val: 0.6800, Test: 0.7030
Epoch: 17, Loss: 1.3362, Train: 0.9000, Val: 0.6840, Test: 0.7150
Epoch: 18, Loss: 1.3503, Train: 0.9000, Val: 0.6980, Test: 0.7280
Epoch: 19, Loss: 1.1934, Train: 0.9000, Val: 0.7100, Test: 0.7310
Epoch: 20, Loss: 1.0917, Train: 0.9286, Val: 0.7280, Test: 0.7490
Epoch: 21, Loss: 1.0581, Train: 0.9357, Val: 0.7440, Test: 0.7600
Epoch: 22, Loss: 0.9904, Train: 0.9357, Val: 0.7720, Test: 0.7680
Epoch: 23, Loss: 0.9502, Train: 0.9357, Val: 0.7840, Test: 0.7730
Epoch: 24, Loss: 0.8294, Train: 0.9500, Val: 0.7840, Test: 0.7790
Epoch: 25, Loss: 0.8413, Train: 0.9643, Val: 0.7780, Test: 0.7770
Epoch: 26, Loss: 0.7512, Train: 0.9643, Val: 0.7760, Test: 0.7790
Epoch: 27, Loss: 0.6904, Train: 0.9643, Val: 0.7740, Test: 0.7700
Epoch: 28, Loss: 0.6183, Train: 0.9571, Val: 0.7660, Test: 0.7690
Epoch: 29, Loss: 0.6094, Train: 0.9571, Val: 0.7640, Test: 0.7700
Epoch: 30, Loss: 0.4997, Train: 0.9571, Val: 0.7680, Test: 0.7800
Epoch: 31, Loss: 0.5228, Train: 0.9643, Val: 0.7720, Test: 0.7880
Epoch: 32, Loss: 0.4067, Train: 0.9643, Val: 0.7680, Test: 0.7870
Epoch: 33, Loss: 0.4688, Train: 0.9786, Val: 0.7800, Test: 0.7930
Epoch: 34, Loss: 0.3151, Train: 0.9786, Val: 0.7840, Test: 0.8010
Epoch: 35, Loss: 0.3079, Train: 0.9786, Val: 0.7800, Test: 0.8030
Epoch: 36, Loss: 0.4116, Train: 0.9786, Val: 0.7940, Test: 0.8070
Epoch: 37, Loss: 0.5264, Train: 0.9714, Val: 0.7980, Test: 0.8020
Epoch: 38, Loss: 0.3425, Train: 0.9786, Val: 0.7900, Test: 0.8020
Epoch: 39, Loss: 0.3475, Train: 0.9786, Val: 0.7840, Test: 0.7940
Epoch: 40, Loss: 0.2890, Train: 0.9786, Val: 0.7840, Test: 0.7950
Epoch: 41, Loss: 0.2687, Train: 0.9786, Val: 0.7780, Test: 0.7940
Epoch: 42, Loss: 0.1666, Train: 0.9786, Val: 0.7740, Test: 0.7900
Epoch: 43, Loss: 0.1708, Train: 0.9786, Val: 0.7740, Test: 0.7910
Epoch: 44, Loss: 0.1751, Train: 0.9857, Val: 0.7680, Test: 0.7890
Epoch: 45, Loss: 0.1573, Train: 0.9857, Val: 0.7620, Test: 0.7870
Epoch: 46, Loss: 0.1676, Train: 0.9929, Val: 0.7540, Test: 0.7850
Epoch: 47, Loss: 0.1630, Train: 0.9929, Val: 0.7540, Test: 0.7850
Epoch: 48, Loss: 0.1078, Train: 0.9929, Val: 0.7540, Test: 0.7850
Epoch: 49, Loss: 0.1265, Train: 0.9929, Val: 0.7520, Test: 0.7850
Epoch: 50, Loss: 0.1304, Train: 0.9929, Val: 0.7500, Test: 0.7850
MAD:  0.8676
Best Test Accuracy: 0.8070, Val Accuracy: 0.7940, Train Accuracy: 0.9786
Training completed.
Seed:  9
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-4): 4 x GATConv(128, 128, heads=1)
    (5): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0655, Train: 0.1643, Val: 0.1320, Test: 0.1560
Epoch: 2, Loss: 2.0297, Train: 0.1571, Val: 0.1300, Test: 0.1430
Epoch: 3, Loss: 2.0076, Train: 0.2500, Val: 0.1840, Test: 0.2000
Epoch: 4, Loss: 1.9154, Train: 0.3786, Val: 0.2500, Test: 0.2660
Epoch: 5, Loss: 1.9179, Train: 0.4786, Val: 0.3400, Test: 0.3220
Epoch: 6, Loss: 1.7708, Train: 0.5929, Val: 0.3780, Test: 0.3670
Epoch: 7, Loss: 1.7999, Train: 0.6643, Val: 0.4120, Test: 0.4100
Epoch: 8, Loss: 1.8182, Train: 0.7214, Val: 0.4720, Test: 0.4520
Epoch: 9, Loss: 1.8068, Train: 0.7429, Val: 0.5500, Test: 0.5450
Epoch: 10, Loss: 1.7565, Train: 0.8071, Val: 0.6280, Test: 0.6300
Epoch: 11, Loss: 1.6822, Train: 0.8929, Val: 0.6780, Test: 0.6840
Epoch: 12, Loss: 1.6358, Train: 0.9071, Val: 0.6980, Test: 0.7240
Epoch: 13, Loss: 1.5798, Train: 0.9000, Val: 0.7240, Test: 0.7440
Epoch: 14, Loss: 1.5483, Train: 0.9214, Val: 0.7340, Test: 0.7670
Epoch: 15, Loss: 1.4995, Train: 0.9143, Val: 0.7600, Test: 0.7670
Epoch: 16, Loss: 1.3907, Train: 0.9286, Val: 0.7600, Test: 0.7740
Epoch: 17, Loss: 1.3587, Train: 0.9286, Val: 0.7760, Test: 0.7830
Epoch: 18, Loss: 1.3320, Train: 0.9286, Val: 0.7680, Test: 0.7720
Epoch: 19, Loss: 1.2462, Train: 0.9429, Val: 0.7560, Test: 0.7660
Epoch: 20, Loss: 1.2241, Train: 0.9214, Val: 0.7520, Test: 0.7530
Epoch: 21, Loss: 1.0738, Train: 0.9143, Val: 0.7500, Test: 0.7580
Epoch: 22, Loss: 1.1353, Train: 0.9214, Val: 0.7500, Test: 0.7600
Epoch: 23, Loss: 0.9393, Train: 0.9214, Val: 0.7520, Test: 0.7630
Epoch: 24, Loss: 0.8765, Train: 0.9286, Val: 0.7580, Test: 0.7770
Epoch: 25, Loss: 0.8903, Train: 0.9286, Val: 0.7600, Test: 0.7870
Epoch: 26, Loss: 0.9370, Train: 0.9357, Val: 0.7640, Test: 0.7890
Epoch: 27, Loss: 0.8290, Train: 0.9429, Val: 0.7660, Test: 0.7910
Epoch: 28, Loss: 0.7530, Train: 0.9500, Val: 0.7760, Test: 0.7970
Epoch: 29, Loss: 0.6289, Train: 0.9500, Val: 0.7800, Test: 0.7900
Epoch: 30, Loss: 0.5990, Train: 0.9500, Val: 0.7780, Test: 0.7960
Epoch: 31, Loss: 0.5811, Train: 0.9429, Val: 0.7740, Test: 0.8000
Epoch: 32, Loss: 0.6452, Train: 0.9571, Val: 0.7760, Test: 0.8040
Epoch: 33, Loss: 0.5564, Train: 0.9643, Val: 0.7740, Test: 0.8010
Epoch: 34, Loss: 0.5337, Train: 0.9714, Val: 0.7760, Test: 0.7960
Epoch: 35, Loss: 0.3918, Train: 0.9643, Val: 0.7740, Test: 0.7920
Epoch: 36, Loss: 0.3890, Train: 0.9643, Val: 0.7700, Test: 0.7950
Epoch: 37, Loss: 0.3318, Train: 0.9714, Val: 0.7640, Test: 0.7930
Epoch: 38, Loss: 0.3311, Train: 0.9714, Val: 0.7660, Test: 0.7930
Epoch: 39, Loss: 0.4168, Train: 0.9714, Val: 0.7640, Test: 0.7930
Epoch: 40, Loss: 0.2768, Train: 0.9714, Val: 0.7640, Test: 0.7940
Epoch: 41, Loss: 0.3367, Train: 0.9714, Val: 0.7680, Test: 0.7980
Epoch: 42, Loss: 0.2559, Train: 0.9786, Val: 0.7760, Test: 0.8030
Epoch: 43, Loss: 0.2417, Train: 0.9786, Val: 0.7780, Test: 0.8020
Epoch: 44, Loss: 0.2947, Train: 0.9929, Val: 0.7820, Test: 0.7990
Epoch: 45, Loss: 0.2686, Train: 0.9929, Val: 0.7840, Test: 0.8000
Epoch: 46, Loss: 0.2447, Train: 0.9929, Val: 0.7780, Test: 0.8020
Epoch: 47, Loss: 0.2464, Train: 0.9929, Val: 0.7760, Test: 0.8020
Epoch: 48, Loss: 0.1859, Train: 1.0000, Val: 0.7820, Test: 0.8010
Epoch: 49, Loss: 0.1928, Train: 1.0000, Val: 0.7860, Test: 0.8060
Epoch: 50, Loss: 0.1822, Train: 1.0000, Val: 0.7820, Test: 0.8060
MAD:  0.8879
Best Test Accuracy: 0.8060, Val Accuracy: 0.7860, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.8039999999999999 ± 0.011233877335986868
Average MAD:  0.84351 ± 0.057358843258908214

/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0132, Train: 0.2357, Val: 0.2400, Test: 0.2190
Epoch: 2, Loss: 1.9414, Train: 0.4714, Val: 0.3220, Test: 0.3040
Epoch: 3, Loss: 1.8906, Train: 0.6357, Val: 0.3540, Test: 0.3890
Epoch: 4, Loss: 1.8004, Train: 0.7643, Val: 0.4440, Test: 0.4750
Epoch: 5, Loss: 1.7764, Train: 0.8571, Val: 0.5000, Test: 0.5360
Epoch: 6, Loss: 1.7787, Train: 0.9000, Val: 0.5700, Test: 0.5810
Epoch: 7, Loss: 1.6668, Train: 0.9214, Val: 0.6060, Test: 0.6130
Epoch: 8, Loss: 1.5800, Train: 0.9214, Val: 0.6260, Test: 0.6400
Epoch: 9, Loss: 1.5387, Train: 0.9286, Val: 0.6740, Test: 0.6700
Epoch: 10, Loss: 1.4270, Train: 0.9500, Val: 0.6880, Test: 0.6910
Epoch: 11, Loss: 1.2806, Train: 0.9571, Val: 0.7020, Test: 0.7090
Epoch: 12, Loss: 1.3068, Train: 0.9643, Val: 0.7160, Test: 0.7270
Epoch: 13, Loss: 1.1794, Train: 0.9643, Val: 0.7360, Test: 0.7460
Epoch: 14, Loss: 1.0359, Train: 0.9643, Val: 0.7620, Test: 0.7600
Epoch: 15, Loss: 0.9836, Train: 0.9643, Val: 0.7720, Test: 0.7800
Epoch: 16, Loss: 0.8724, Train: 0.9714, Val: 0.7820, Test: 0.7870
Epoch: 17, Loss: 0.7527, Train: 0.9643, Val: 0.7880, Test: 0.7890
Epoch: 18, Loss: 0.7404, Train: 0.9714, Val: 0.7880, Test: 0.7920
Epoch: 19, Loss: 0.5982, Train: 0.9714, Val: 0.7900, Test: 0.7920
Epoch: 20, Loss: 0.5593, Train: 0.9714, Val: 0.7960, Test: 0.8010
Epoch: 21, Loss: 0.4880, Train: 0.9786, Val: 0.7960, Test: 0.8010
Epoch: 22, Loss: 0.4534, Train: 0.9786, Val: 0.7960, Test: 0.7990
Epoch: 23, Loss: 0.3547, Train: 0.9786, Val: 0.7900, Test: 0.7970
Epoch: 24, Loss: 0.3618, Train: 0.9857, Val: 0.7780, Test: 0.7900
Epoch: 25, Loss: 0.2846, Train: 0.9857, Val: 0.7800, Test: 0.7870
Epoch: 26, Loss: 0.2209, Train: 0.9857, Val: 0.7760, Test: 0.7840
Epoch: 27, Loss: 0.2047, Train: 0.9929, Val: 0.7700, Test: 0.7880
Epoch: 28, Loss: 0.2428, Train: 1.0000, Val: 0.7780, Test: 0.7900
Epoch: 29, Loss: 0.1746, Train: 1.0000, Val: 0.7780, Test: 0.7930
Epoch: 30, Loss: 0.1713, Train: 0.9929, Val: 0.7860, Test: 0.7970
Epoch: 31, Loss: 0.1388, Train: 0.9929, Val: 0.7900, Test: 0.7990
Epoch: 32, Loss: 0.1576, Train: 0.9929, Val: 0.7920, Test: 0.7990
Epoch: 33, Loss: 0.1212, Train: 1.0000, Val: 0.7920, Test: 0.7990
Epoch: 34, Loss: 0.0621, Train: 1.0000, Val: 0.7940, Test: 0.8010
Epoch: 35, Loss: 0.0506, Train: 0.9857, Val: 0.7980, Test: 0.7980
Epoch: 36, Loss: 0.0869, Train: 0.9857, Val: 0.8020, Test: 0.7980
Epoch: 37, Loss: 0.0946, Train: 0.9929, Val: 0.7960, Test: 0.8040
Epoch: 38, Loss: 0.0716, Train: 1.0000, Val: 0.7840, Test: 0.8040
Epoch: 39, Loss: 0.0594, Train: 1.0000, Val: 0.7780, Test: 0.7950
Epoch: 40, Loss: 0.0453, Train: 0.9929, Val: 0.7720, Test: 0.7970
Epoch: 41, Loss: 0.0679, Train: 0.9929, Val: 0.7740, Test: 0.7940
Epoch: 42, Loss: 0.0498, Train: 0.9929, Val: 0.7720, Test: 0.7890
Epoch: 43, Loss: 0.0546, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 44, Loss: 0.0476, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 45, Loss: 0.0370, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 46, Loss: 0.0378, Train: 1.0000, Val: 0.7840, Test: 0.7980
Epoch: 47, Loss: 0.0258, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 48, Loss: 0.0179, Train: 1.0000, Val: 0.7820, Test: 0.7920
Epoch: 49, Loss: 0.0241, Train: 1.0000, Val: 0.7840, Test: 0.7870
Epoch: 50, Loss: 0.0258, Train: 1.0000, Val: 0.7800, Test: 0.7840
MAD:  0.8978
Best Test Accuracy: 0.8040, Val Accuracy: 0.7960, Train Accuracy: 0.9929
Training completed.
Seed:  1
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0366, Train: 0.2857, Val: 0.2060, Test: 0.2100
Epoch: 2, Loss: 1.9164, Train: 0.4429, Val: 0.3020, Test: 0.3180
Epoch: 3, Loss: 1.8951, Train: 0.5714, Val: 0.3720, Test: 0.3660
Epoch: 4, Loss: 1.8534, Train: 0.6643, Val: 0.3840, Test: 0.3950
Epoch: 5, Loss: 1.7425, Train: 0.7143, Val: 0.3940, Test: 0.4110
Epoch: 6, Loss: 1.6815, Train: 0.7571, Val: 0.4340, Test: 0.4390
Epoch: 7, Loss: 1.6373, Train: 0.8000, Val: 0.4720, Test: 0.4960
Epoch: 8, Loss: 1.6111, Train: 0.8214, Val: 0.5300, Test: 0.5510
Epoch: 9, Loss: 1.5190, Train: 0.8500, Val: 0.5800, Test: 0.6020
Epoch: 10, Loss: 1.4224, Train: 0.8643, Val: 0.6340, Test: 0.6480
Epoch: 11, Loss: 1.3404, Train: 0.9071, Val: 0.6800, Test: 0.7000
Epoch: 12, Loss: 1.2898, Train: 0.9143, Val: 0.7100, Test: 0.7370
Epoch: 13, Loss: 1.2042, Train: 0.9143, Val: 0.7360, Test: 0.7550
Epoch: 14, Loss: 1.0139, Train: 0.9214, Val: 0.7440, Test: 0.7720
Epoch: 15, Loss: 1.0035, Train: 0.9286, Val: 0.7500, Test: 0.7790
Epoch: 16, Loss: 0.9035, Train: 0.9357, Val: 0.7580, Test: 0.7940
Epoch: 17, Loss: 0.7805, Train: 0.9500, Val: 0.7640, Test: 0.7960
Epoch: 18, Loss: 0.6784, Train: 0.9500, Val: 0.7720, Test: 0.7980
Epoch: 19, Loss: 0.7404, Train: 0.9571, Val: 0.7780, Test: 0.8040
Epoch: 20, Loss: 0.5176, Train: 0.9714, Val: 0.7740, Test: 0.7980
Epoch: 21, Loss: 0.4876, Train: 0.9714, Val: 0.7680, Test: 0.7970
Epoch: 22, Loss: 0.4229, Train: 0.9714, Val: 0.7680, Test: 0.7920
Epoch: 23, Loss: 0.3660, Train: 0.9714, Val: 0.7720, Test: 0.7910
Epoch: 24, Loss: 0.3141, Train: 0.9786, Val: 0.7680, Test: 0.8000
Epoch: 25, Loss: 0.3131, Train: 0.9786, Val: 0.7740, Test: 0.8060
Epoch: 26, Loss: 0.2867, Train: 0.9786, Val: 0.7740, Test: 0.8070
Epoch: 27, Loss: 0.2602, Train: 0.9857, Val: 0.7860, Test: 0.8060
Epoch: 28, Loss: 0.1970, Train: 0.9857, Val: 0.7840, Test: 0.8070
Epoch: 29, Loss: 0.1880, Train: 0.9786, Val: 0.7820, Test: 0.8080
Epoch: 30, Loss: 0.1542, Train: 0.9857, Val: 0.7820, Test: 0.8070
Epoch: 31, Loss: 0.1866, Train: 1.0000, Val: 0.7760, Test: 0.8120
Epoch: 32, Loss: 0.1640, Train: 0.9929, Val: 0.7720, Test: 0.8150
Epoch: 33, Loss: 0.1206, Train: 1.0000, Val: 0.7700, Test: 0.8090
Epoch: 34, Loss: 0.1192, Train: 1.0000, Val: 0.7700, Test: 0.8110
Epoch: 35, Loss: 0.1072, Train: 1.0000, Val: 0.7760, Test: 0.8100
Epoch: 36, Loss: 0.0971, Train: 1.0000, Val: 0.7780, Test: 0.8090
Epoch: 37, Loss: 0.0732, Train: 0.9857, Val: 0.7860, Test: 0.8010
Epoch: 38, Loss: 0.0739, Train: 0.9929, Val: 0.7860, Test: 0.8030
Epoch: 39, Loss: 0.0724, Train: 0.9929, Val: 0.7860, Test: 0.8040
Epoch: 40, Loss: 0.0961, Train: 1.0000, Val: 0.7820, Test: 0.8030
Epoch: 41, Loss: 0.0691, Train: 1.0000, Val: 0.7760, Test: 0.8010
Epoch: 42, Loss: 0.0534, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 43, Loss: 0.0410, Train: 1.0000, Val: 0.7700, Test: 0.7970
Epoch: 44, Loss: 0.0382, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 45, Loss: 0.0572, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 46, Loss: 0.0274, Train: 1.0000, Val: 0.7740, Test: 0.7890
Epoch: 47, Loss: 0.0395, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 48, Loss: 0.0575, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 49, Loss: 0.0441, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 50, Loss: 0.0258, Train: 1.0000, Val: 0.7680, Test: 0.7860
MAD:  0.8837
Best Test Accuracy: 0.8150, Val Accuracy: 0.7720, Train Accuracy: 0.9929
Training completed.
Seed:  2
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0237, Train: 0.3571, Val: 0.2880, Test: 0.2680
Epoch: 2, Loss: 1.9572, Train: 0.5714, Val: 0.3880, Test: 0.3910
Epoch: 3, Loss: 1.8695, Train: 0.7000, Val: 0.5060, Test: 0.5100
Epoch: 4, Loss: 1.8092, Train: 0.7714, Val: 0.5700, Test: 0.5780
Epoch: 5, Loss: 1.7592, Train: 0.8357, Val: 0.6180, Test: 0.6300
Epoch: 6, Loss: 1.6703, Train: 0.8429, Val: 0.6380, Test: 0.6700
Epoch: 7, Loss: 1.6146, Train: 0.8857, Val: 0.6640, Test: 0.6930
Epoch: 8, Loss: 1.5467, Train: 0.9143, Val: 0.6720, Test: 0.7040
Epoch: 9, Loss: 1.4760, Train: 0.9214, Val: 0.6920, Test: 0.7080
Epoch: 10, Loss: 1.3857, Train: 0.9357, Val: 0.7000, Test: 0.7270
Epoch: 11, Loss: 1.2917, Train: 0.9429, Val: 0.7060, Test: 0.7270
Epoch: 12, Loss: 1.2076, Train: 0.9429, Val: 0.7240, Test: 0.7370
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 13, Loss: 1.1221, Train: 0.9429, Val: 0.7280, Test: 0.7480
Epoch: 14, Loss: 1.0660, Train: 0.9429, Val: 0.7460, Test: 0.7680
Epoch: 15, Loss: 0.9858, Train: 0.9429, Val: 0.7600, Test: 0.7780
Epoch: 16, Loss: 0.9353, Train: 0.9500, Val: 0.7720, Test: 0.7850
Epoch: 17, Loss: 0.7577, Train: 0.9571, Val: 0.7700, Test: 0.7920
Epoch: 18, Loss: 0.7076, Train: 0.9714, Val: 0.7740, Test: 0.7920
Epoch: 19, Loss: 0.5824, Train: 0.9714, Val: 0.7720, Test: 0.7970
Epoch: 20, Loss: 0.6002, Train: 0.9714, Val: 0.7740, Test: 0.7960
Epoch: 21, Loss: 0.5284, Train: 0.9714, Val: 0.7720, Test: 0.7960
Epoch: 22, Loss: 0.3801, Train: 0.9786, Val: 0.7840, Test: 0.7920
Epoch: 23, Loss: 0.3171, Train: 0.9643, Val: 0.7800, Test: 0.7940
Epoch: 24, Loss: 0.3129, Train: 0.9643, Val: 0.7740, Test: 0.7950
Epoch: 25, Loss: 0.3068, Train: 0.9714, Val: 0.7680, Test: 0.7970
Epoch: 26, Loss: 0.2270, Train: 0.9857, Val: 0.7760, Test: 0.7960
Epoch: 27, Loss: 0.2515, Train: 0.9857, Val: 0.7820, Test: 0.7960
Epoch: 28, Loss: 0.2626, Train: 0.9857, Val: 0.7800, Test: 0.7960
Epoch: 29, Loss: 0.2156, Train: 0.9857, Val: 0.7780, Test: 0.7970
Epoch: 30, Loss: 0.1425, Train: 0.9929, Val: 0.7820, Test: 0.7970
Epoch: 31, Loss: 0.1544, Train: 0.9929, Val: 0.7760, Test: 0.7930
Epoch: 32, Loss: 0.1441, Train: 0.9929, Val: 0.7740, Test: 0.7950
Epoch: 33, Loss: 0.1355, Train: 0.9929, Val: 0.7740, Test: 0.7940
Epoch: 34, Loss: 0.1304, Train: 1.0000, Val: 0.7760, Test: 0.7960
Epoch: 35, Loss: 0.0976, Train: 1.0000, Val: 0.7820, Test: 0.8000
Epoch: 36, Loss: 0.0839, Train: 1.0000, Val: 0.7760, Test: 0.8030
Epoch: 37, Loss: 0.0734, Train: 1.0000, Val: 0.7940, Test: 0.8080
Epoch: 38, Loss: 0.0589, Train: 1.0000, Val: 0.7860, Test: 0.8120
Epoch: 39, Loss: 0.0681, Train: 1.0000, Val: 0.7940, Test: 0.8100
Epoch: 40, Loss: 0.0610, Train: 0.9929, Val: 0.7960, Test: 0.8070
Epoch: 41, Loss: 0.0904, Train: 1.0000, Val: 0.7880, Test: 0.8030
Epoch: 42, Loss: 0.0410, Train: 1.0000, Val: 0.7840, Test: 0.8020
Epoch: 43, Loss: 0.0696, Train: 1.0000, Val: 0.7740, Test: 0.8010
Epoch: 44, Loss: 0.0518, Train: 1.0000, Val: 0.7600, Test: 0.7930
Epoch: 45, Loss: 0.0372, Train: 1.0000, Val: 0.7580, Test: 0.7890
Epoch: 46, Loss: 0.0440, Train: 1.0000, Val: 0.7560, Test: 0.7910
Epoch: 47, Loss: 0.0550, Train: 1.0000, Val: 0.7580, Test: 0.7900
Epoch: 48, Loss: 0.0340, Train: 1.0000, Val: 0.7580, Test: 0.7870
Epoch: 49, Loss: 0.0627, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 50, Loss: 0.0130, Train: 1.0000, Val: 0.7700, Test: 0.7880
MAD:  0.8986
Best Test Accuracy: 0.8120, Val Accuracy: 0.7860, Train Accuracy: 1.0000
Training completed.
Seed:  3
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0294, Train: 0.2643, Val: 0.2040, Test: 0.1920
Epoch: 2, Loss: 1.9306, Train: 0.5286, Val: 0.3860, Test: 0.3870
Epoch: 3, Loss: 1.8792, Train: 0.6357, Val: 0.4880, Test: 0.4840
Epoch: 4, Loss: 1.8165, Train: 0.7000, Val: 0.5440, Test: 0.5350
Epoch: 5, Loss: 1.7583, Train: 0.8143, Val: 0.5840, Test: 0.6050
Epoch: 6, Loss: 1.6945, Train: 0.8429, Val: 0.6420, Test: 0.6620
Epoch: 7, Loss: 1.6520, Train: 0.8857, Val: 0.6720, Test: 0.6900
Epoch: 8, Loss: 1.5382, Train: 0.9071, Val: 0.7000, Test: 0.7080
Epoch: 9, Loss: 1.4547, Train: 0.9214, Val: 0.6960, Test: 0.7130
Epoch: 10, Loss: 1.3889, Train: 0.9357, Val: 0.7000, Test: 0.7250
Epoch: 11, Loss: 1.3166, Train: 0.9429, Val: 0.7080, Test: 0.7370
Epoch: 12, Loss: 1.1901, Train: 0.9500, Val: 0.7160, Test: 0.7530
Epoch: 13, Loss: 1.1018, Train: 0.9500, Val: 0.7340, Test: 0.7610
Epoch: 14, Loss: 0.9311, Train: 0.9500, Val: 0.7460, Test: 0.7680
Epoch: 15, Loss: 0.8975, Train: 0.9500, Val: 0.7620, Test: 0.7740
Epoch: 16, Loss: 0.8532, Train: 0.9500, Val: 0.7660, Test: 0.7770
Epoch: 17, Loss: 0.7016, Train: 0.9643, Val: 0.7720, Test: 0.7840
Epoch: 18, Loss: 0.6519, Train: 0.9714, Val: 0.7740, Test: 0.7940
Epoch: 19, Loss: 0.5369, Train: 0.9643, Val: 0.7880, Test: 0.8100
Epoch: 20, Loss: 0.5197, Train: 0.9714, Val: 0.7900, Test: 0.8100
Epoch: 21, Loss: 0.4180, Train: 0.9786, Val: 0.7840, Test: 0.8040
Epoch: 22, Loss: 0.3526, Train: 0.9786, Val: 0.7740, Test: 0.8010
Epoch: 23, Loss: 0.3510, Train: 0.9786, Val: 0.7740, Test: 0.8010
Epoch: 24, Loss: 0.2774, Train: 0.9786, Val: 0.7780, Test: 0.7940
Epoch: 25, Loss: 0.2740, Train: 0.9857, Val: 0.7800, Test: 0.7870
Epoch: 26, Loss: 0.2031, Train: 0.9786, Val: 0.7840, Test: 0.7900
Epoch: 27, Loss: 0.2049, Train: 0.9857, Val: 0.7820, Test: 0.8020
Epoch: 28, Loss: 0.1825, Train: 0.9857, Val: 0.7800, Test: 0.8010
Epoch: 29, Loss: 0.1954, Train: 0.9857, Val: 0.7700, Test: 0.7950
Epoch: 30, Loss: 0.1479, Train: 0.9857, Val: 0.7620, Test: 0.7900
Epoch: 31, Loss: 0.1251, Train: 0.9786, Val: 0.7600, Test: 0.7880
Epoch: 32, Loss: 0.1006, Train: 0.9857, Val: 0.7640, Test: 0.7850
Epoch: 33, Loss: 0.0925, Train: 1.0000, Val: 0.7640, Test: 0.7880
Epoch: 34, Loss: 0.1313, Train: 1.0000, Val: 0.7700, Test: 0.7850
Epoch: 35, Loss: 0.0839, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 36, Loss: 0.1077, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 37, Loss: 0.0415, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 38, Loss: 0.0616, Train: 1.0000, Val: 0.7700, Test: 0.7920
Epoch: 39, Loss: 0.0606, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 40, Loss: 0.0464, Train: 1.0000, Val: 0.7740, Test: 0.7990
Epoch: 41, Loss: 0.0568, Train: 1.0000, Val: 0.7640, Test: 0.7940
Epoch: 42, Loss: 0.0391, Train: 0.9929, Val: 0.7660, Test: 0.7920
Epoch: 43, Loss: 0.0657, Train: 0.9929, Val: 0.7660, Test: 0.7890
Epoch: 44, Loss: 0.0386, Train: 0.9929, Val: 0.7700, Test: 0.7850
Epoch: 45, Loss: 0.0675, Train: 0.9929, Val: 0.7660, Test: 0.7850
Epoch: 46, Loss: 0.0214, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 47, Loss: 0.0365, Train: 1.0000, Val: 0.7700, Test: 0.7890
Epoch: 48, Loss: 0.0212, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 49, Loss: 0.0257, Train: 1.0000, Val: 0.7720, Test: 0.7990
Epoch: 50, Loss: 0.0210, Train: 1.0000, Val: 0.7740, Test: 0.7990
MAD:  0.7589
Best Test Accuracy: 0.8100, Val Accuracy: 0.7880, Train Accuracy: 0.9643
Training completed.
Seed:  4
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9744, Train: 0.3429, Val: 0.2040, Test: 0.2210
Epoch: 2, Loss: 1.9056, Train: 0.6071, Val: 0.3320, Test: 0.3580
Epoch: 3, Loss: 1.7974, Train: 0.6214, Val: 0.3720, Test: 0.4370
Epoch: 4, Loss: 1.7913, Train: 0.6500, Val: 0.3720, Test: 0.4250
Epoch: 5, Loss: 1.6984, Train: 0.6857, Val: 0.3980, Test: 0.4570
Epoch: 6, Loss: 1.6470, Train: 0.7357, Val: 0.4380, Test: 0.4830
Epoch: 7, Loss: 1.6417, Train: 0.8214, Val: 0.4900, Test: 0.5380
Epoch: 8, Loss: 1.5593, Train: 0.8286, Val: 0.5640, Test: 0.5920
Epoch: 9, Loss: 1.4945, Train: 0.8571, Val: 0.6160, Test: 0.6500
Epoch: 10, Loss: 1.3842, Train: 0.8786, Val: 0.6560, Test: 0.6960
Epoch: 11, Loss: 1.3195, Train: 0.9214, Val: 0.6840, Test: 0.7240
Epoch: 12, Loss: 1.1986, Train: 0.9286, Val: 0.7100, Test: 0.7380
Epoch: 13, Loss: 1.1122, Train: 0.9357, Val: 0.7160, Test: 0.7400
Epoch: 14, Loss: 1.0503, Train: 0.9429, Val: 0.7220, Test: 0.7460
Epoch: 15, Loss: 0.9291, Train: 0.9429, Val: 0.7200, Test: 0.7540
Epoch: 16, Loss: 0.7990, Train: 0.9429, Val: 0.7440, Test: 0.7730
Epoch: 17, Loss: 0.8322, Train: 0.9429, Val: 0.7480, Test: 0.7850
Epoch: 18, Loss: 0.6815, Train: 0.9500, Val: 0.7740, Test: 0.7950
Epoch: 19, Loss: 0.6048, Train: 0.9571, Val: 0.7720, Test: 0.8050
Epoch: 20, Loss: 0.5602, Train: 0.9714, Val: 0.7720, Test: 0.8060
Epoch: 21, Loss: 0.4844, Train: 0.9714, Val: 0.7760, Test: 0.8050
Epoch: 22, Loss: 0.4467, Train: 0.9714, Val: 0.7740, Test: 0.8040
Epoch: 23, Loss: 0.4154, Train: 0.9786, Val: 0.7700, Test: 0.8000
Epoch: 24, Loss: 0.3789, Train: 0.9786, Val: 0.7720, Test: 0.8070
Epoch: 25, Loss: 0.3310, Train: 0.9786, Val: 0.7820, Test: 0.8120
Epoch: 26, Loss: 0.2979, Train: 0.9786, Val: 0.7980, Test: 0.8140
Epoch: 27, Loss: 0.2693, Train: 0.9857, Val: 0.7980, Test: 0.8180
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 28, Loss: 0.2264, Train: 0.9857, Val: 0.8020, Test: 0.8150
Epoch: 29, Loss: 0.1975, Train: 0.9857, Val: 0.8040, Test: 0.8180
Epoch: 30, Loss: 0.1783, Train: 0.9857, Val: 0.7980, Test: 0.8190
Epoch: 31, Loss: 0.1362, Train: 0.9857, Val: 0.7920, Test: 0.8140
Epoch: 32, Loss: 0.1057, Train: 0.9857, Val: 0.7860, Test: 0.8170
Epoch: 33, Loss: 0.1412, Train: 1.0000, Val: 0.7940, Test: 0.8170
Epoch: 34, Loss: 0.1152, Train: 1.0000, Val: 0.7880, Test: 0.8140
Epoch: 35, Loss: 0.1358, Train: 1.0000, Val: 0.7860, Test: 0.8130
Epoch: 36, Loss: 0.0905, Train: 1.0000, Val: 0.7900, Test: 0.8120
Epoch: 37, Loss: 0.1025, Train: 1.0000, Val: 0.7920, Test: 0.8110
Epoch: 38, Loss: 0.0854, Train: 1.0000, Val: 0.7880, Test: 0.8060
Epoch: 39, Loss: 0.0601, Train: 1.0000, Val: 0.7740, Test: 0.8000
Epoch: 40, Loss: 0.0610, Train: 1.0000, Val: 0.7660, Test: 0.7960
Epoch: 41, Loss: 0.0789, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 42, Loss: 0.0356, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 43, Loss: 0.0333, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 44, Loss: 0.0639, Train: 1.0000, Val: 0.7760, Test: 0.7940
Epoch: 45, Loss: 0.0412, Train: 1.0000, Val: 0.7760, Test: 0.7940
Epoch: 46, Loss: 0.0441, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 47, Loss: 0.0414, Train: 1.0000, Val: 0.7800, Test: 0.7940
Epoch: 48, Loss: 0.0231, Train: 1.0000, Val: 0.7760, Test: 0.7940
Epoch: 49, Loss: 0.0198, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 50, Loss: 0.0672, Train: 1.0000, Val: 0.7900, Test: 0.7960
MAD:  0.8211
Best Test Accuracy: 0.8190, Val Accuracy: 0.7980, Train Accuracy: 0.9857
Training completed.
Seed:  5
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0328, Train: 0.2714, Val: 0.1620, Test: 0.1630
Epoch: 2, Loss: 1.9762, Train: 0.4571, Val: 0.3000, Test: 0.3230
Epoch: 3, Loss: 1.9064, Train: 0.6214, Val: 0.4500, Test: 0.4600
Epoch: 4, Loss: 1.8375, Train: 0.7071, Val: 0.4840, Test: 0.5140
Epoch: 5, Loss: 1.7934, Train: 0.7500, Val: 0.5380, Test: 0.5540
Epoch: 6, Loss: 1.7504, Train: 0.8071, Val: 0.5900, Test: 0.5890
Epoch: 7, Loss: 1.6637, Train: 0.8286, Val: 0.6420, Test: 0.6500
Epoch: 8, Loss: 1.5961, Train: 0.8429, Val: 0.6860, Test: 0.6970
Epoch: 9, Loss: 1.5535, Train: 0.8643, Val: 0.7160, Test: 0.7270
Epoch: 10, Loss: 1.4496, Train: 0.9000, Val: 0.7320, Test: 0.7410
Epoch: 11, Loss: 1.3553, Train: 0.9286, Val: 0.7440, Test: 0.7580
Epoch: 12, Loss: 1.2905, Train: 0.9214, Val: 0.7460, Test: 0.7680
Epoch: 13, Loss: 1.2128, Train: 0.9143, Val: 0.7580, Test: 0.7780
Epoch: 14, Loss: 1.0754, Train: 0.9214, Val: 0.7680, Test: 0.7770
Epoch: 15, Loss: 1.0387, Train: 0.9286, Val: 0.7680, Test: 0.7810
Epoch: 16, Loss: 0.9488, Train: 0.9357, Val: 0.7780, Test: 0.7820
Epoch: 17, Loss: 0.8275, Train: 0.9429, Val: 0.7760, Test: 0.7880
Epoch: 18, Loss: 0.6987, Train: 0.9500, Val: 0.7760, Test: 0.7900
Epoch: 19, Loss: 0.6394, Train: 0.9500, Val: 0.7840, Test: 0.7970
Epoch: 20, Loss: 0.6405, Train: 0.9571, Val: 0.7920, Test: 0.8090
Epoch: 21, Loss: 0.5224, Train: 0.9714, Val: 0.7960, Test: 0.8120
Epoch: 22, Loss: 0.4665, Train: 0.9714, Val: 0.8080, Test: 0.8110
Epoch: 23, Loss: 0.3950, Train: 0.9786, Val: 0.8020, Test: 0.8190
Epoch: 24, Loss: 0.3023, Train: 0.9857, Val: 0.7940, Test: 0.8140
Epoch: 25, Loss: 0.2940, Train: 0.9857, Val: 0.7880, Test: 0.8100
Epoch: 26, Loss: 0.3107, Train: 0.9857, Val: 0.7820, Test: 0.8110
Epoch: 27, Loss: 0.2834, Train: 0.9857, Val: 0.7840, Test: 0.8060
Epoch: 28, Loss: 0.2223, Train: 0.9857, Val: 0.7840, Test: 0.8010
Epoch: 29, Loss: 0.1599, Train: 0.9857, Val: 0.7820, Test: 0.8030
Epoch: 30, Loss: 0.1327, Train: 0.9857, Val: 0.7820, Test: 0.8050
Epoch: 31, Loss: 0.1749, Train: 1.0000, Val: 0.7840, Test: 0.8040
Epoch: 32, Loss: 0.1187, Train: 1.0000, Val: 0.7780, Test: 0.8050
Epoch: 33, Loss: 0.1211, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 34, Loss: 0.1091, Train: 1.0000, Val: 0.7920, Test: 0.8040
Epoch: 35, Loss: 0.1173, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 36, Loss: 0.0714, Train: 1.0000, Val: 0.7840, Test: 0.7990
Epoch: 37, Loss: 0.1053, Train: 1.0000, Val: 0.7940, Test: 0.8010
Epoch: 38, Loss: 0.0797, Train: 1.0000, Val: 0.7980, Test: 0.8050
Epoch: 39, Loss: 0.0616, Train: 1.0000, Val: 0.8100, Test: 0.8130
Epoch: 40, Loss: 0.0510, Train: 1.0000, Val: 0.8040, Test: 0.8170
Epoch: 41, Loss: 0.0837, Train: 1.0000, Val: 0.8020, Test: 0.8170
Epoch: 42, Loss: 0.0705, Train: 1.0000, Val: 0.7980, Test: 0.8140
Epoch: 43, Loss: 0.0637, Train: 1.0000, Val: 0.7940, Test: 0.8100
Epoch: 44, Loss: 0.0394, Train: 1.0000, Val: 0.7920, Test: 0.8080
Epoch: 45, Loss: 0.0403, Train: 1.0000, Val: 0.7880, Test: 0.8040
Epoch: 46, Loss: 0.0595, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 47, Loss: 0.0803, Train: 1.0000, Val: 0.7800, Test: 0.7940
Epoch: 48, Loss: 0.0458, Train: 1.0000, Val: 0.7600, Test: 0.7900
Epoch: 49, Loss: 0.0232, Train: 1.0000, Val: 0.7600, Test: 0.7900
Epoch: 50, Loss: 0.0465, Train: 1.0000, Val: 0.7600, Test: 0.7870
MAD:  0.875
Best Test Accuracy: 0.8190, Val Accuracy: 0.8020, Train Accuracy: 0.9786
Training completed.
Seed:  6
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9733, Train: 0.2214, Val: 0.1640, Test: 0.1660
Epoch: 2, Loss: 1.8785, Train: 0.5214, Val: 0.3460, Test: 0.3200
Epoch: 3, Loss: 1.8485, Train: 0.7357, Val: 0.4960, Test: 0.4760
Epoch: 4, Loss: 1.7873, Train: 0.7857, Val: 0.5780, Test: 0.5720
Epoch: 5, Loss: 1.7752, Train: 0.8286, Val: 0.6220, Test: 0.6200
Epoch: 6, Loss: 1.7055, Train: 0.8643, Val: 0.6360, Test: 0.6430
Epoch: 7, Loss: 1.5701, Train: 0.9000, Val: 0.6500, Test: 0.6530
Epoch: 8, Loss: 1.5112, Train: 0.9071, Val: 0.6500, Test: 0.6610
Epoch: 9, Loss: 1.5089, Train: 0.9214, Val: 0.6580, Test: 0.6660
Epoch: 10, Loss: 1.3883, Train: 0.9214, Val: 0.6700, Test: 0.6750
Epoch: 11, Loss: 1.2639, Train: 0.9429, Val: 0.6940, Test: 0.6950
Epoch: 12, Loss: 1.2149, Train: 0.9500, Val: 0.7260, Test: 0.7190
Epoch: 13, Loss: 1.1742, Train: 0.9571, Val: 0.7460, Test: 0.7550
Epoch: 14, Loss: 0.9829, Train: 0.9571, Val: 0.7740, Test: 0.7820
Epoch: 15, Loss: 0.8916, Train: 0.9571, Val: 0.7900, Test: 0.7980
Epoch: 16, Loss: 0.7833, Train: 0.9643, Val: 0.8000, Test: 0.8090
Epoch: 17, Loss: 0.7330, Train: 0.9786, Val: 0.8000, Test: 0.8170
Epoch: 18, Loss: 0.6557, Train: 0.9786, Val: 0.8000, Test: 0.8200
Epoch: 19, Loss: 0.5847, Train: 0.9786, Val: 0.8060, Test: 0.8230
Epoch: 20, Loss: 0.5107, Train: 0.9857, Val: 0.8100, Test: 0.8240
Epoch: 21, Loss: 0.4473, Train: 0.9786, Val: 0.8140, Test: 0.8250
Epoch: 22, Loss: 0.3577, Train: 0.9714, Val: 0.8120, Test: 0.8230
Epoch: 23, Loss: 0.2494, Train: 0.9714, Val: 0.8100, Test: 0.8190
Epoch: 24, Loss: 0.2759, Train: 0.9786, Val: 0.8120, Test: 0.8170
Epoch: 25, Loss: 0.2826, Train: 0.9857, Val: 0.8140, Test: 0.8120
Epoch: 26, Loss: 0.2669, Train: 1.0000, Val: 0.8040, Test: 0.8110
Epoch: 27, Loss: 0.1688, Train: 0.9929, Val: 0.7980, Test: 0.8040
Epoch: 28, Loss: 0.1638, Train: 0.9857, Val: 0.7920, Test: 0.8030
Epoch: 29, Loss: 0.1295, Train: 0.9857, Val: 0.7960, Test: 0.8000
Epoch: 30, Loss: 0.1556, Train: 0.9929, Val: 0.7920, Test: 0.8060
Epoch: 31, Loss: 0.1214, Train: 0.9929, Val: 0.8000, Test: 0.8160
Epoch: 32, Loss: 0.1286, Train: 1.0000, Val: 0.8000, Test: 0.8180
Epoch: 33, Loss: 0.1013, Train: 1.0000, Val: 0.8000, Test: 0.8150
Epoch: 34, Loss: 0.0893, Train: 1.0000, Val: 0.8020, Test: 0.8160
Epoch: 35, Loss: 0.0909, Train: 1.0000, Val: 0.8080, Test: 0.8200
Epoch: 36, Loss: 0.0696, Train: 1.0000, Val: 0.8020, Test: 0.8170
Epoch: 37, Loss: 0.0580, Train: 1.0000, Val: 0.8020, Test: 0.8160
Epoch: 38, Loss: 0.0649, Train: 1.0000, Val: 0.8020, Test: 0.8170
Epoch: 39, Loss: 0.0523, Train: 1.0000, Val: 0.8080, Test: 0.8140
Epoch: 40, Loss: 0.0604, Train: 1.0000, Val: 0.8040, Test: 0.8140
Epoch: 41, Loss: 0.0671, Train: 1.0000, Val: 0.7940, Test: 0.8050
Epoch: 42, Loss: 0.0448, Train: 1.0000, Val: 0.7860, Test: 0.8010
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 43, Loss: 0.0446, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 44, Loss: 0.0310, Train: 1.0000, Val: 0.7760, Test: 0.7920
Epoch: 45, Loss: 0.0554, Train: 1.0000, Val: 0.7680, Test: 0.7830
Epoch: 46, Loss: 0.0376, Train: 1.0000, Val: 0.7700, Test: 0.7840
Epoch: 47, Loss: 0.0334, Train: 1.0000, Val: 0.7720, Test: 0.7850
Epoch: 48, Loss: 0.0398, Train: 1.0000, Val: 0.7780, Test: 0.7900
Epoch: 49, Loss: 0.0124, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 50, Loss: 0.0258, Train: 1.0000, Val: 0.7880, Test: 0.7970
MAD:  0.871
Best Test Accuracy: 0.8250, Val Accuracy: 0.8140, Train Accuracy: 0.9786
Training completed.
Seed:  7
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0383, Train: 0.3000, Val: 0.1340, Test: 0.1770
Epoch: 2, Loss: 1.9310, Train: 0.5071, Val: 0.2840, Test: 0.3190
Epoch: 3, Loss: 1.8465, Train: 0.6786, Val: 0.4020, Test: 0.4360
Epoch: 4, Loss: 1.7824, Train: 0.7786, Val: 0.4880, Test: 0.5480
Epoch: 5, Loss: 1.7477, Train: 0.8286, Val: 0.5140, Test: 0.6010
Epoch: 6, Loss: 1.6716, Train: 0.8857, Val: 0.5760, Test: 0.6470
Epoch: 7, Loss: 1.5281, Train: 0.9000, Val: 0.6200, Test: 0.6830
Epoch: 8, Loss: 1.5169, Train: 0.9071, Val: 0.6540, Test: 0.6990
Epoch: 9, Loss: 1.4139, Train: 0.9286, Val: 0.6740, Test: 0.7200
Epoch: 10, Loss: 1.3621, Train: 0.9357, Val: 0.6900, Test: 0.7370
Epoch: 11, Loss: 1.2947, Train: 0.9357, Val: 0.7220, Test: 0.7550
Epoch: 12, Loss: 1.1409, Train: 0.9429, Val: 0.7320, Test: 0.7670
Epoch: 13, Loss: 1.0570, Train: 0.9500, Val: 0.7440, Test: 0.7780
Epoch: 14, Loss: 1.0493, Train: 0.9571, Val: 0.7560, Test: 0.7920
Epoch: 15, Loss: 0.9043, Train: 0.9643, Val: 0.7700, Test: 0.8060
Epoch: 16, Loss: 0.7753, Train: 0.9643, Val: 0.7740, Test: 0.8080
Epoch: 17, Loss: 0.7180, Train: 0.9714, Val: 0.7780, Test: 0.8070
Epoch: 18, Loss: 0.5905, Train: 0.9714, Val: 0.7920, Test: 0.8190
Epoch: 19, Loss: 0.5316, Train: 0.9857, Val: 0.7940, Test: 0.8190
Epoch: 20, Loss: 0.4636, Train: 0.9857, Val: 0.7960, Test: 0.8190
Epoch: 21, Loss: 0.3636, Train: 0.9857, Val: 0.7940, Test: 0.8150
Epoch: 22, Loss: 0.3516, Train: 0.9929, Val: 0.7900, Test: 0.8110
Epoch: 23, Loss: 0.3234, Train: 0.9929, Val: 0.7860, Test: 0.8120
Epoch: 24, Loss: 0.2709, Train: 0.9929, Val: 0.7800, Test: 0.8070
Epoch: 25, Loss: 0.2162, Train: 0.9857, Val: 0.7760, Test: 0.8030
Epoch: 26, Loss: 0.2100, Train: 0.9929, Val: 0.7820, Test: 0.8010
Epoch: 27, Loss: 0.2217, Train: 0.9929, Val: 0.7840, Test: 0.8020
Epoch: 28, Loss: 0.1898, Train: 0.9929, Val: 0.7940, Test: 0.8120
Epoch: 29, Loss: 0.1575, Train: 0.9857, Val: 0.7960, Test: 0.8110
Epoch: 30, Loss: 0.1455, Train: 1.0000, Val: 0.7920, Test: 0.8120
Epoch: 31, Loss: 0.1599, Train: 1.0000, Val: 0.7920, Test: 0.8100
Epoch: 32, Loss: 0.0966, Train: 1.0000, Val: 0.7860, Test: 0.8050
Epoch: 33, Loss: 0.1215, Train: 1.0000, Val: 0.7760, Test: 0.7920
Epoch: 34, Loss: 0.0722, Train: 1.0000, Val: 0.7720, Test: 0.7840
Epoch: 35, Loss: 0.1321, Train: 1.0000, Val: 0.7700, Test: 0.7870
Epoch: 36, Loss: 0.0679, Train: 1.0000, Val: 0.7760, Test: 0.7900
Epoch: 37, Loss: 0.0673, Train: 1.0000, Val: 0.7860, Test: 0.7970
Epoch: 38, Loss: 0.0883, Train: 1.0000, Val: 0.7900, Test: 0.7940
Epoch: 39, Loss: 0.0883, Train: 1.0000, Val: 0.7860, Test: 0.7960
Epoch: 40, Loss: 0.1031, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 41, Loss: 0.0884, Train: 1.0000, Val: 0.7840, Test: 0.7990
Epoch: 42, Loss: 0.0515, Train: 1.0000, Val: 0.7860, Test: 0.8020
Epoch: 43, Loss: 0.0754, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 44, Loss: 0.0612, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 45, Loss: 0.0405, Train: 1.0000, Val: 0.7820, Test: 0.7980
Epoch: 46, Loss: 0.0191, Train: 0.9929, Val: 0.7860, Test: 0.7960
Epoch: 47, Loss: 0.0309, Train: 0.9857, Val: 0.7780, Test: 0.7880
Epoch: 48, Loss: 0.0319, Train: 0.9857, Val: 0.7680, Test: 0.7880
Epoch: 49, Loss: 0.0574, Train: 0.9857, Val: 0.7740, Test: 0.7770
Epoch: 50, Loss: 0.0335, Train: 1.0000, Val: 0.7620, Test: 0.7720
MAD:  0.8682
Best Test Accuracy: 0.8190, Val Accuracy: 0.7920, Train Accuracy: 0.9714
Training completed.
Seed:  8
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0122, Train: 0.3000, Val: 0.2260, Test: 0.2300
Epoch: 2, Loss: 1.9433, Train: 0.4571, Val: 0.3180, Test: 0.3220
Epoch: 3, Loss: 1.8863, Train: 0.6500, Val: 0.4460, Test: 0.4550
Epoch: 4, Loss: 1.8197, Train: 0.8071, Val: 0.5480, Test: 0.5660
Epoch: 5, Loss: 1.6924, Train: 0.8429, Val: 0.5760, Test: 0.5930
Epoch: 6, Loss: 1.6114, Train: 0.8643, Val: 0.5940, Test: 0.5950
Epoch: 7, Loss: 1.5927, Train: 0.8571, Val: 0.6020, Test: 0.6100
Epoch: 8, Loss: 1.5180, Train: 0.8857, Val: 0.6240, Test: 0.6350
Epoch: 9, Loss: 1.4004, Train: 0.9286, Val: 0.6640, Test: 0.6840
Epoch: 10, Loss: 1.3177, Train: 0.9429, Val: 0.6920, Test: 0.7150
Epoch: 11, Loss: 1.2944, Train: 0.9429, Val: 0.7160, Test: 0.7340
Epoch: 12, Loss: 1.1685, Train: 0.9357, Val: 0.7240, Test: 0.7450
Epoch: 13, Loss: 1.0971, Train: 0.9357, Val: 0.7420, Test: 0.7620
Epoch: 14, Loss: 1.0042, Train: 0.9500, Val: 0.7440, Test: 0.7700
Epoch: 15, Loss: 0.8022, Train: 0.9643, Val: 0.7600, Test: 0.7780
Epoch: 16, Loss: 0.7366, Train: 0.9643, Val: 0.7680, Test: 0.7840
Epoch: 17, Loss: 0.6157, Train: 0.9714, Val: 0.7720, Test: 0.7860
Epoch: 18, Loss: 0.5241, Train: 0.9786, Val: 0.7740, Test: 0.7920
Epoch: 19, Loss: 0.4235, Train: 0.9857, Val: 0.7800, Test: 0.7970
Epoch: 20, Loss: 0.5204, Train: 0.9929, Val: 0.7800, Test: 0.8080
Epoch: 21, Loss: 0.3894, Train: 0.9929, Val: 0.7740, Test: 0.8090
Epoch: 22, Loss: 0.4708, Train: 0.9929, Val: 0.7720, Test: 0.7990
Epoch: 23, Loss: 0.2793, Train: 0.9929, Val: 0.7720, Test: 0.7930
Epoch: 24, Loss: 0.2667, Train: 0.9929, Val: 0.7620, Test: 0.7900
Epoch: 25, Loss: 0.2187, Train: 0.9857, Val: 0.7640, Test: 0.7910
Epoch: 26, Loss: 0.2190, Train: 0.9929, Val: 0.7600, Test: 0.7870
Epoch: 27, Loss: 0.1677, Train: 0.9857, Val: 0.7600, Test: 0.7880
Epoch: 28, Loss: 0.2240, Train: 0.9857, Val: 0.7640, Test: 0.7890
Epoch: 29, Loss: 0.1374, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 30, Loss: 0.1411, Train: 1.0000, Val: 0.7800, Test: 0.7990
Epoch: 31, Loss: 0.1313, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 32, Loss: 0.0990, Train: 0.9929, Val: 0.7820, Test: 0.8030
Epoch: 33, Loss: 0.0900, Train: 0.9929, Val: 0.7780, Test: 0.8010
Epoch: 34, Loss: 0.1568, Train: 0.9929, Val: 0.7640, Test: 0.7950
Epoch: 35, Loss: 0.0846, Train: 0.9929, Val: 0.7640, Test: 0.7950
Epoch: 36, Loss: 0.0580, Train: 1.0000, Val: 0.7680, Test: 0.7940
Epoch: 37, Loss: 0.0606, Train: 1.0000, Val: 0.7700, Test: 0.7950
Epoch: 38, Loss: 0.0433, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 39, Loss: 0.0509, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 40, Loss: 0.0675, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 41, Loss: 0.0477, Train: 1.0000, Val: 0.7640, Test: 0.7920
Epoch: 42, Loss: 0.0669, Train: 1.0000, Val: 0.7660, Test: 0.7970
Epoch: 43, Loss: 0.0931, Train: 1.0000, Val: 0.7780, Test: 0.8040
Epoch: 44, Loss: 0.0492, Train: 1.0000, Val: 0.7920, Test: 0.8120
Epoch: 45, Loss: 0.0393, Train: 1.0000, Val: 0.7800, Test: 0.8130
Epoch: 46, Loss: 0.0364, Train: 1.0000, Val: 0.7820, Test: 0.8110
Epoch: 47, Loss: 0.0186, Train: 1.0000, Val: 0.7880, Test: 0.8090
Epoch: 48, Loss: 0.0578, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 49, Loss: 0.0530, Train: 1.0000, Val: 0.7800, Test: 0.7950
Epoch: 50, Loss: 0.0221, Train: 1.0000, Val: 0.7740, Test: 0.7920
MAD:  0.9107
Best Test Accuracy: 0.8130, Val Accuracy: 0.7800, Train Accuracy: 1.0000
Training completed.
Seed:  9
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-3): 3 x GCNConv(128, 128)
    (4): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9977, Train: 0.3000, Val: 0.2600, Test: 0.2300
Epoch: 2, Loss: 1.9234, Train: 0.4714, Val: 0.3620, Test: 0.3760
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 3, Loss: 1.8691, Train: 0.6143, Val: 0.4400, Test: 0.4650
Epoch: 4, Loss: 1.8389, Train: 0.7214, Val: 0.5020, Test: 0.5390
Epoch: 5, Loss: 1.7788, Train: 0.8000, Val: 0.5680, Test: 0.6000
Epoch: 6, Loss: 1.7185, Train: 0.8786, Val: 0.6080, Test: 0.6510
Epoch: 7, Loss: 1.6307, Train: 0.9143, Val: 0.6520, Test: 0.6760
Epoch: 8, Loss: 1.5665, Train: 0.9214, Val: 0.7000, Test: 0.7020
Epoch: 9, Loss: 1.5035, Train: 0.9143, Val: 0.7180, Test: 0.7230
Epoch: 10, Loss: 1.4675, Train: 0.9286, Val: 0.7300, Test: 0.7380
Epoch: 11, Loss: 1.3251, Train: 0.9357, Val: 0.7440, Test: 0.7510
Epoch: 12, Loss: 1.2698, Train: 0.9357, Val: 0.7400, Test: 0.7670
Epoch: 13, Loss: 1.1747, Train: 0.9429, Val: 0.7480, Test: 0.7750
Epoch: 14, Loss: 1.0894, Train: 0.9429, Val: 0.7540, Test: 0.7820
Epoch: 15, Loss: 1.0547, Train: 0.9429, Val: 0.7640, Test: 0.7940
Epoch: 16, Loss: 0.9759, Train: 0.9429, Val: 0.7680, Test: 0.8010
Epoch: 17, Loss: 0.7583, Train: 0.9429, Val: 0.7720, Test: 0.8040
Epoch: 18, Loss: 0.6870, Train: 0.9500, Val: 0.7740, Test: 0.7960
Epoch: 19, Loss: 0.6778, Train: 0.9571, Val: 0.7780, Test: 0.8020
Epoch: 20, Loss: 0.6612, Train: 0.9714, Val: 0.7800, Test: 0.8040
Epoch: 21, Loss: 0.5045, Train: 0.9714, Val: 0.7860, Test: 0.8010
Epoch: 22, Loss: 0.4476, Train: 0.9714, Val: 0.7800, Test: 0.8110
Epoch: 23, Loss: 0.3891, Train: 0.9714, Val: 0.7860, Test: 0.8100
Epoch: 24, Loss: 0.3158, Train: 0.9714, Val: 0.7840, Test: 0.8070
Epoch: 25, Loss: 0.3231, Train: 0.9857, Val: 0.7840, Test: 0.8120
Epoch: 26, Loss: 0.3101, Train: 0.9857, Val: 0.7860, Test: 0.8170
Epoch: 27, Loss: 0.2516, Train: 0.9929, Val: 0.7920, Test: 0.8150
Epoch: 28, Loss: 0.2382, Train: 0.9929, Val: 0.7940, Test: 0.8100
Epoch: 29, Loss: 0.2062, Train: 0.9929, Val: 0.7960, Test: 0.8050
Epoch: 30, Loss: 0.1504, Train: 0.9929, Val: 0.7940, Test: 0.8090
Epoch: 31, Loss: 0.1788, Train: 0.9929, Val: 0.8000, Test: 0.8120
Epoch: 32, Loss: 0.1712, Train: 0.9857, Val: 0.8000, Test: 0.8100
Epoch: 33, Loss: 0.1257, Train: 0.9929, Val: 0.7940, Test: 0.8060
Epoch: 34, Loss: 0.1536, Train: 1.0000, Val: 0.7820, Test: 0.8030
Epoch: 35, Loss: 0.1010, Train: 1.0000, Val: 0.7720, Test: 0.7980
Epoch: 36, Loss: 0.0932, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 37, Loss: 0.0898, Train: 0.9929, Val: 0.7700, Test: 0.7960
Epoch: 38, Loss: 0.1189, Train: 0.9929, Val: 0.7760, Test: 0.8020
Epoch: 39, Loss: 0.1169, Train: 1.0000, Val: 0.7800, Test: 0.8080
Epoch: 40, Loss: 0.0677, Train: 1.0000, Val: 0.7800, Test: 0.8150
Epoch: 41, Loss: 0.0718, Train: 1.0000, Val: 0.7840, Test: 0.8120
Epoch: 42, Loss: 0.0905, Train: 1.0000, Val: 0.7820, Test: 0.8100
Epoch: 43, Loss: 0.0567, Train: 0.9929, Val: 0.7800, Test: 0.8040
Epoch: 44, Loss: 0.0594, Train: 0.9929, Val: 0.7820, Test: 0.8010
Epoch: 45, Loss: 0.0661, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 46, Loss: 0.0300, Train: 1.0000, Val: 0.7720, Test: 0.7850
Epoch: 47, Loss: 0.0381, Train: 1.0000, Val: 0.7680, Test: 0.7790
Epoch: 48, Loss: 0.0394, Train: 1.0000, Val: 0.7600, Test: 0.7780
Epoch: 49, Loss: 0.0711, Train: 1.0000, Val: 0.7620, Test: 0.7810
Epoch: 50, Loss: 0.0464, Train: 1.0000, Val: 0.7640, Test: 0.7870
MAD:  0.8642
Best Test Accuracy: 0.8170, Val Accuracy: 0.7860, Train Accuracy: 0.9857
Training completed.
Average Test Accuracy:  0.8152999999999999 ± 0.005568662316930307
Average MAD:  0.86492 ± 0.042396009246154276

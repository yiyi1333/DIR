/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8599, Train: 0.0714, Val: 0.0500, Test: 0.0620
Epoch: 2, Loss: 4.8480, Train: 0.3214, Val: 0.2780, Test: 0.3030
Epoch: 3, Loss: 4.8338, Train: 0.5071, Val: 0.4120, Test: 0.4170
Epoch: 4, Loss: 4.7883, Train: 0.5571, Val: 0.4560, Test: 0.4830
Epoch: 5, Loss: 4.7856, Train: 0.5571, Val: 0.4700, Test: 0.5010
Epoch: 6, Loss: 4.7245, Train: 0.5571, Val: 0.4740, Test: 0.5120
Epoch: 7, Loss: 4.7358, Train: 0.5571, Val: 0.4860, Test: 0.5220
Epoch: 8, Loss: 4.6652, Train: 0.5500, Val: 0.4940, Test: 0.5340
Epoch: 9, Loss: 4.6889, Train: 0.5714, Val: 0.5020, Test: 0.5400
Epoch: 10, Loss: 4.6592, Train: 0.6071, Val: 0.5240, Test: 0.5530
Epoch: 11, Loss: 4.5005, Train: 0.6214, Val: 0.5300, Test: 0.5610
Epoch: 12, Loss: 4.5975, Train: 0.6500, Val: 0.5300, Test: 0.5660
Epoch: 13, Loss: 4.5191, Train: 0.6500, Val: 0.5320, Test: 0.5640
Epoch: 14, Loss: 4.4105, Train: 0.6571, Val: 0.5380, Test: 0.5670
Epoch: 15, Loss: 4.3748, Train: 0.6714, Val: 0.5360, Test: 0.5740
Epoch: 16, Loss: 4.2386, Train: 0.6857, Val: 0.5440, Test: 0.5760
Epoch: 17, Loss: 4.1238, Train: 0.7143, Val: 0.5580, Test: 0.5910
Epoch: 18, Loss: 4.2141, Train: 0.7500, Val: 0.5660, Test: 0.6290
Epoch: 19, Loss: 3.9600, Train: 0.7714, Val: 0.5860, Test: 0.6540
Epoch: 20, Loss: 4.1063, Train: 0.7929, Val: 0.6160, Test: 0.6740
Epoch: 21, Loss: 4.3872, Train: 0.8000, Val: 0.6340, Test: 0.6850
Epoch: 22, Loss: 4.2240, Train: 0.8571, Val: 0.6360, Test: 0.6940
Epoch: 23, Loss: 4.3131, Train: 0.9214, Val: 0.6760, Test: 0.7150
Epoch: 24, Loss: 3.9006, Train: 0.9357, Val: 0.6860, Test: 0.7370
Epoch: 25, Loss: 4.2153, Train: 0.9571, Val: 0.7320, Test: 0.7580
Epoch: 26, Loss: 4.0352, Train: 0.9571, Val: 0.7380, Test: 0.7620
Epoch: 27, Loss: 4.0299, Train: 0.9571, Val: 0.7360, Test: 0.7590
Epoch: 28, Loss: 4.0282, Train: 0.9571, Val: 0.7260, Test: 0.7470
Epoch: 29, Loss: 3.8782, Train: 0.9571, Val: 0.7180, Test: 0.7380
Epoch: 30, Loss: 3.9108, Train: 0.9571, Val: 0.7140, Test: 0.7450
Epoch: 31, Loss: 4.3135, Train: 0.9571, Val: 0.7260, Test: 0.7520
Epoch: 32, Loss: 3.9711, Train: 0.9643, Val: 0.7320, Test: 0.7670
Epoch: 33, Loss: 4.0603, Train: 0.9643, Val: 0.7400, Test: 0.7710
Epoch: 34, Loss: 3.6332, Train: 0.9786, Val: 0.7460, Test: 0.7840
Epoch: 35, Loss: 4.1742, Train: 0.9786, Val: 0.7620, Test: 0.7930
Epoch: 36, Loss: 3.8704, Train: 0.9857, Val: 0.7720, Test: 0.8000
Epoch: 37, Loss: 4.0359, Train: 0.9786, Val: 0.7880, Test: 0.8040
Epoch: 38, Loss: 3.7018, Train: 0.9786, Val: 0.7960, Test: 0.8080
Epoch: 39, Loss: 3.9792, Train: 0.9786, Val: 0.8000, Test: 0.8100
Epoch: 40, Loss: 3.8628, Train: 0.9786, Val: 0.7960, Test: 0.8160
Epoch: 41, Loss: 3.8327, Train: 0.9786, Val: 0.8000, Test: 0.8160
Epoch: 42, Loss: 4.0557, Train: 0.9786, Val: 0.8000, Test: 0.8170
Epoch: 43, Loss: 3.6023, Train: 0.9786, Val: 0.8020, Test: 0.8170
Epoch: 44, Loss: 3.8584, Train: 0.9786, Val: 0.8040, Test: 0.8160
Epoch: 45, Loss: 3.9580, Train: 0.9786, Val: 0.8040, Test: 0.8120
Epoch: 46, Loss: 3.7372, Train: 0.9857, Val: 0.8060, Test: 0.8150
Epoch: 47, Loss: 3.8833, Train: 0.9857, Val: 0.8040, Test: 0.8150
Epoch: 48, Loss: 3.4107, Train: 0.9857, Val: 0.8020, Test: 0.8200
Epoch: 49, Loss: 3.7029, Train: 0.9857, Val: 0.7980, Test: 0.8180
Epoch: 50, Loss: 4.0324, Train: 0.9929, Val: 0.8000, Test: 0.8190
MAD:  0.3978
Best Test Accuracy: 0.8200, Val Accuracy: 0.8020, Train Accuracy: 0.9857
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8517, Train: 0.1714, Val: 0.1020, Test: 0.1310
Epoch: 2, Loss: 4.8214, Train: 0.2786, Val: 0.1760, Test: 0.1860
Epoch: 3, Loss: 4.8199, Train: 0.3214, Val: 0.1960, Test: 0.1950
Epoch: 4, Loss: 4.7811, Train: 0.3643, Val: 0.2000, Test: 0.2110
Epoch: 5, Loss: 4.7530, Train: 0.3857, Val: 0.2160, Test: 0.2390
Epoch: 6, Loss: 4.7413, Train: 0.4000, Val: 0.2300, Test: 0.2560
Epoch: 7, Loss: 4.7517, Train: 0.4071, Val: 0.2320, Test: 0.2680
Epoch: 8, Loss: 4.6372, Train: 0.4143, Val: 0.2600, Test: 0.2780
Epoch: 9, Loss: 4.6393, Train: 0.4214, Val: 0.2940, Test: 0.3070
Epoch: 10, Loss: 4.6043, Train: 0.4714, Val: 0.3100, Test: 0.3380
Epoch: 11, Loss: 4.4679, Train: 0.5214, Val: 0.3180, Test: 0.3600
Epoch: 12, Loss: 4.5256, Train: 0.5857, Val: 0.3440, Test: 0.3880
Epoch: 13, Loss: 4.4870, Train: 0.6000, Val: 0.3740, Test: 0.4250
Epoch: 14, Loss: 4.2259, Train: 0.6429, Val: 0.4140, Test: 0.4590
Epoch: 15, Loss: 4.5074, Train: 0.6929, Val: 0.4740, Test: 0.5310
Epoch: 16, Loss: 4.2470, Train: 0.7357, Val: 0.5360, Test: 0.5840
Epoch: 17, Loss: 4.1198, Train: 0.7429, Val: 0.5960, Test: 0.6310
Epoch: 18, Loss: 4.3410, Train: 0.7929, Val: 0.6280, Test: 0.6620
Epoch: 19, Loss: 4.1883, Train: 0.8214, Val: 0.6520, Test: 0.6810
Epoch: 20, Loss: 4.0578, Train: 0.8357, Val: 0.6700, Test: 0.6980
Epoch: 21, Loss: 4.1400, Train: 0.8714, Val: 0.6720, Test: 0.7210
Epoch: 22, Loss: 3.9252, Train: 0.9214, Val: 0.7140, Test: 0.7610
Epoch: 23, Loss: 4.0212, Train: 0.9786, Val: 0.7260, Test: 0.7670
Epoch: 24, Loss: 4.0863, Train: 0.9786, Val: 0.7480, Test: 0.7750
Epoch: 25, Loss: 3.8222, Train: 0.9857, Val: 0.7680, Test: 0.7800
Epoch: 26, Loss: 3.8383, Train: 0.9929, Val: 0.7740, Test: 0.7930
Epoch: 27, Loss: 3.8142, Train: 0.9857, Val: 0.7800, Test: 0.7910
Epoch: 28, Loss: 4.0679, Train: 0.9857, Val: 0.7760, Test: 0.7810
Epoch: 29, Loss: 3.8006, Train: 0.9714, Val: 0.7720, Test: 0.7770
Epoch: 30, Loss: 3.6322, Train: 0.9643, Val: 0.7760, Test: 0.7770
Epoch: 31, Loss: 3.8494, Train: 0.9643, Val: 0.7740, Test: 0.7830
Epoch: 32, Loss: 4.0917, Train: 0.9714, Val: 0.7760, Test: 0.7860
Epoch: 33, Loss: 3.7826, Train: 0.9714, Val: 0.7780, Test: 0.7880
Epoch: 34, Loss: 3.8728, Train: 0.9857, Val: 0.7860, Test: 0.7940
Epoch: 35, Loss: 3.6267, Train: 0.9857, Val: 0.7960, Test: 0.7960
Epoch: 36, Loss: 3.8076, Train: 0.9786, Val: 0.7980, Test: 0.8080
Epoch: 37, Loss: 3.9757, Train: 0.9786, Val: 0.7960, Test: 0.8080
Epoch: 38, Loss: 3.8706, Train: 0.9786, Val: 0.7980, Test: 0.8070
Epoch: 39, Loss: 3.7134, Train: 0.9857, Val: 0.7940, Test: 0.8020
Epoch: 40, Loss: 3.9800, Train: 0.9857, Val: 0.7860, Test: 0.8010
Epoch: 41, Loss: 3.9144, Train: 0.9929, Val: 0.7820, Test: 0.8050
Epoch: 42, Loss: 3.3834, Train: 0.9929, Val: 0.7880, Test: 0.8090
Epoch: 43, Loss: 3.6937, Train: 0.9929, Val: 0.7800, Test: 0.8060
Epoch: 44, Loss: 4.0950, Train: 0.9929, Val: 0.7840, Test: 0.8100
Epoch: 45, Loss: 3.5810, Train: 0.9929, Val: 0.7900, Test: 0.8150
Epoch: 46, Loss: 4.1196, Train: 0.9929, Val: 0.7960, Test: 0.8180
Epoch: 47, Loss: 3.5132, Train: 1.0000, Val: 0.7960, Test: 0.8200
Epoch: 48, Loss: 3.9217, Train: 1.0000, Val: 0.7980, Test: 0.8220
Epoch: 49, Loss: 3.5979, Train: 1.0000, Val: 0.8000, Test: 0.8220
Epoch: 50, Loss: 3.3875, Train: 1.0000, Val: 0.8000, Test: 0.8220
MAD:  0.4442
Best Test Accuracy: 0.8220, Val Accuracy: 0.7980, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8560, Train: 0.1643, Val: 0.1140, Test: 0.1130
Epoch: 2, Loss: 4.8381, Train: 0.2786, Val: 0.2580, Test: 0.2540
Epoch: 3, Loss: 4.8250, Train: 0.3000, Val: 0.2840, Test: 0.2850
Epoch: 4, Loss: 4.7863, Train: 0.3429, Val: 0.2860, Test: 0.2910
Epoch: 5, Loss: 4.7539, Train: 0.3714, Val: 0.2660, Test: 0.2790
Epoch: 6, Loss: 4.7424, Train: 0.3786, Val: 0.2620, Test: 0.2720
Epoch: 7, Loss: 4.7409, Train: 0.4071, Val: 0.2620, Test: 0.2750
Epoch: 8, Loss: 4.6556, Train: 0.4071, Val: 0.2560, Test: 0.2680
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 9, Loss: 4.6053, Train: 0.4000, Val: 0.2540, Test: 0.2690
Epoch: 10, Loss: 4.5325, Train: 0.4000, Val: 0.2620, Test: 0.2650
Epoch: 11, Loss: 4.6678, Train: 0.4143, Val: 0.2700, Test: 0.2740
Epoch: 12, Loss: 4.4772, Train: 0.4429, Val: 0.2740, Test: 0.2820
Epoch: 13, Loss: 4.4811, Train: 0.4571, Val: 0.2800, Test: 0.2880
Epoch: 14, Loss: 4.5515, Train: 0.5071, Val: 0.3180, Test: 0.3300
Epoch: 15, Loss: 4.2219, Train: 0.5357, Val: 0.3480, Test: 0.3630
Epoch: 16, Loss: 4.3150, Train: 0.6071, Val: 0.4240, Test: 0.4410
Epoch: 17, Loss: 4.1355, Train: 0.6286, Val: 0.4920, Test: 0.5130
Epoch: 18, Loss: 4.3115, Train: 0.6643, Val: 0.5360, Test: 0.5600
Epoch: 19, Loss: 4.2300, Train: 0.6857, Val: 0.5660, Test: 0.5950
Epoch: 20, Loss: 3.9540, Train: 0.7429, Val: 0.6040, Test: 0.6400
Epoch: 21, Loss: 4.1169, Train: 0.8143, Val: 0.6380, Test: 0.6860
Epoch: 22, Loss: 3.9739, Train: 0.8786, Val: 0.6820, Test: 0.7260
Epoch: 23, Loss: 4.3850, Train: 0.9286, Val: 0.7120, Test: 0.7590
Epoch: 24, Loss: 4.0691, Train: 0.9571, Val: 0.7280, Test: 0.7540
Epoch: 25, Loss: 4.0209, Train: 0.9429, Val: 0.6980, Test: 0.7330
Epoch: 26, Loss: 4.0112, Train: 0.9429, Val: 0.6780, Test: 0.7110
Epoch: 27, Loss: 3.9534, Train: 0.9357, Val: 0.6780, Test: 0.7010
Epoch: 28, Loss: 4.2806, Train: 0.9500, Val: 0.6700, Test: 0.6920
Epoch: 29, Loss: 4.0026, Train: 0.9714, Val: 0.6760, Test: 0.6790
Epoch: 30, Loss: 3.9128, Train: 0.9714, Val: 0.6760, Test: 0.6820
Epoch: 31, Loss: 3.9475, Train: 0.9643, Val: 0.6780, Test: 0.6920
Epoch: 32, Loss: 4.0090, Train: 0.9714, Val: 0.6900, Test: 0.7180
Epoch: 33, Loss: 3.9646, Train: 0.9714, Val: 0.7120, Test: 0.7440
Epoch: 34, Loss: 3.9049, Train: 0.9786, Val: 0.7400, Test: 0.7630
Epoch: 35, Loss: 4.0348, Train: 0.9786, Val: 0.7540, Test: 0.7750
Epoch: 36, Loss: 3.9608, Train: 0.9786, Val: 0.7680, Test: 0.7790
Epoch: 37, Loss: 4.0162, Train: 0.9857, Val: 0.7820, Test: 0.7830
Epoch: 38, Loss: 3.8724, Train: 0.9857, Val: 0.7780, Test: 0.7820
Epoch: 39, Loss: 4.0925, Train: 0.9857, Val: 0.7760, Test: 0.7840
Epoch: 40, Loss: 3.6860, Train: 0.9857, Val: 0.7840, Test: 0.7910
Epoch: 41, Loss: 3.8716, Train: 0.9857, Val: 0.7880, Test: 0.7990
Epoch: 42, Loss: 3.7551, Train: 0.9857, Val: 0.7920, Test: 0.8030
Epoch: 43, Loss: 3.8704, Train: 0.9857, Val: 0.7860, Test: 0.8110
Epoch: 44, Loss: 4.0998, Train: 0.9857, Val: 0.7900, Test: 0.8150
Epoch: 45, Loss: 3.9306, Train: 0.9929, Val: 0.7940, Test: 0.8150
Epoch: 46, Loss: 3.9011, Train: 0.9929, Val: 0.7940, Test: 0.8160
Epoch: 47, Loss: 3.7701, Train: 0.9929, Val: 0.7920, Test: 0.8160
Epoch: 48, Loss: 3.9362, Train: 0.9857, Val: 0.7900, Test: 0.8180
Epoch: 49, Loss: 3.7386, Train: 0.9857, Val: 0.7860, Test: 0.8120
Epoch: 50, Loss: 3.8361, Train: 0.9857, Val: 0.7800, Test: 0.8070
MAD:  0.385
Best Test Accuracy: 0.8180, Val Accuracy: 0.7900, Train Accuracy: 0.9857
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8505, Train: 0.1500, Val: 0.1220, Test: 0.1090
Epoch: 2, Loss: 4.8394, Train: 0.3571, Val: 0.2480, Test: 0.2410
Epoch: 3, Loss: 4.8098, Train: 0.4071, Val: 0.2720, Test: 0.2710
Epoch: 4, Loss: 4.7955, Train: 0.4357, Val: 0.2780, Test: 0.2790
Epoch: 5, Loss: 4.7621, Train: 0.4500, Val: 0.2740, Test: 0.2780
Epoch: 6, Loss: 4.6712, Train: 0.4643, Val: 0.2820, Test: 0.2780
Epoch: 7, Loss: 4.6346, Train: 0.4857, Val: 0.3000, Test: 0.2930
Epoch: 8, Loss: 4.5719, Train: 0.4929, Val: 0.3120, Test: 0.3110
Epoch: 9, Loss: 4.5664, Train: 0.5000, Val: 0.3220, Test: 0.3180
Epoch: 10, Loss: 4.6000, Train: 0.5000, Val: 0.3260, Test: 0.3240
Epoch: 11, Loss: 4.4559, Train: 0.5000, Val: 0.3440, Test: 0.3350
Epoch: 12, Loss: 4.5245, Train: 0.5286, Val: 0.3500, Test: 0.3490
Epoch: 13, Loss: 4.2952, Train: 0.5500, Val: 0.3520, Test: 0.3580
Epoch: 14, Loss: 4.5389, Train: 0.5643, Val: 0.3780, Test: 0.3860
Epoch: 15, Loss: 4.0153, Train: 0.5929, Val: 0.3960, Test: 0.4040
Epoch: 16, Loss: 4.2463, Train: 0.6143, Val: 0.4200, Test: 0.4370
Epoch: 17, Loss: 4.3812, Train: 0.6786, Val: 0.4560, Test: 0.4670
Epoch: 18, Loss: 4.1624, Train: 0.7071, Val: 0.4820, Test: 0.5000
Epoch: 19, Loss: 4.1373, Train: 0.7429, Val: 0.5140, Test: 0.5140
Epoch: 20, Loss: 4.0506, Train: 0.8143, Val: 0.5440, Test: 0.5450
Epoch: 21, Loss: 4.3221, Train: 0.8429, Val: 0.5700, Test: 0.5680
Epoch: 22, Loss: 4.2016, Train: 0.8714, Val: 0.6080, Test: 0.6050
Epoch: 23, Loss: 3.9553, Train: 0.9214, Val: 0.6820, Test: 0.6680
Epoch: 24, Loss: 4.3703, Train: 0.9357, Val: 0.7160, Test: 0.7160
Epoch: 25, Loss: 4.1574, Train: 0.9429, Val: 0.7360, Test: 0.7410
Epoch: 26, Loss: 3.5976, Train: 0.9571, Val: 0.7520, Test: 0.7550
Epoch: 27, Loss: 4.0719, Train: 0.9571, Val: 0.7680, Test: 0.7700
Epoch: 28, Loss: 3.9669, Train: 0.9643, Val: 0.7680, Test: 0.7800
Epoch: 29, Loss: 3.6901, Train: 0.9714, Val: 0.7820, Test: 0.7870
Epoch: 30, Loss: 4.0310, Train: 0.9714, Val: 0.7820, Test: 0.7850
Epoch: 31, Loss: 3.9795, Train: 0.9714, Val: 0.7740, Test: 0.7840
Epoch: 32, Loss: 4.0376, Train: 0.9714, Val: 0.7720, Test: 0.7730
Epoch: 33, Loss: 3.8886, Train: 0.9714, Val: 0.7720, Test: 0.7710
Epoch: 34, Loss: 3.9743, Train: 0.9714, Val: 0.7740, Test: 0.7750
Epoch: 35, Loss: 4.0151, Train: 0.9714, Val: 0.7700, Test: 0.7800
Epoch: 36, Loss: 3.8564, Train: 0.9786, Val: 0.7700, Test: 0.7770
Epoch: 37, Loss: 3.9407, Train: 0.9786, Val: 0.7600, Test: 0.7720
Epoch: 38, Loss: 4.0765, Train: 0.9857, Val: 0.7600, Test: 0.7680
Epoch: 39, Loss: 4.0727, Train: 0.9857, Val: 0.7520, Test: 0.7660
Epoch: 40, Loss: 3.8471, Train: 0.9929, Val: 0.7420, Test: 0.7620
Epoch: 41, Loss: 3.8841, Train: 0.9929, Val: 0.7440, Test: 0.7600
Epoch: 42, Loss: 3.8782, Train: 0.9929, Val: 0.7440, Test: 0.7620
Epoch: 43, Loss: 3.7437, Train: 0.9929, Val: 0.7480, Test: 0.7660
Epoch: 44, Loss: 3.5984, Train: 0.9929, Val: 0.7500, Test: 0.7710
Epoch: 45, Loss: 3.9825, Train: 0.9857, Val: 0.7500, Test: 0.7730
Epoch: 46, Loss: 3.9279, Train: 0.9857, Val: 0.7680, Test: 0.7790
Epoch: 47, Loss: 3.7025, Train: 0.9929, Val: 0.7700, Test: 0.7810
Epoch: 48, Loss: 3.7669, Train: 0.9929, Val: 0.7740, Test: 0.7830
Epoch: 49, Loss: 3.8175, Train: 0.9929, Val: 0.7780, Test: 0.7830
Epoch: 50, Loss: 3.6031, Train: 1.0000, Val: 0.7780, Test: 0.7850
MAD:  0.1524
Best Test Accuracy: 0.7870, Val Accuracy: 0.7820, Train Accuracy: 0.9714
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8564, Train: 0.1357, Val: 0.1280, Test: 0.1240
Epoch: 2, Loss: 4.8234, Train: 0.2643, Val: 0.1660, Test: 0.1630
Epoch: 3, Loss: 4.8080, Train: 0.2714, Val: 0.1800, Test: 0.1720
Epoch: 4, Loss: 4.7778, Train: 0.2929, Val: 0.1840, Test: 0.1880
Epoch: 5, Loss: 4.7369, Train: 0.3286, Val: 0.1940, Test: 0.2100
Epoch: 6, Loss: 4.6827, Train: 0.3714, Val: 0.2200, Test: 0.2230
Epoch: 7, Loss: 4.5982, Train: 0.3857, Val: 0.2220, Test: 0.2280
Epoch: 8, Loss: 4.5753, Train: 0.3857, Val: 0.2280, Test: 0.2300
Epoch: 9, Loss: 4.5023, Train: 0.4000, Val: 0.2320, Test: 0.2340
Epoch: 10, Loss: 4.5046, Train: 0.4286, Val: 0.2600, Test: 0.2420
Epoch: 11, Loss: 4.5444, Train: 0.4643, Val: 0.2800, Test: 0.2740
Epoch: 12, Loss: 4.3377, Train: 0.5143, Val: 0.3140, Test: 0.3250
Epoch: 13, Loss: 4.2207, Train: 0.5643, Val: 0.3720, Test: 0.3730
Epoch: 14, Loss: 4.2473, Train: 0.6286, Val: 0.4300, Test: 0.4390
Epoch: 15, Loss: 4.2461, Train: 0.6714, Val: 0.5060, Test: 0.5160
Epoch: 16, Loss: 4.4136, Train: 0.7071, Val: 0.6040, Test: 0.5980
Epoch: 17, Loss: 4.1780, Train: 0.7929, Val: 0.6820, Test: 0.6880
Epoch: 18, Loss: 4.3262, Train: 0.9143, Val: 0.7420, Test: 0.7580
Epoch: 19, Loss: 4.1164, Train: 0.9500, Val: 0.7740, Test: 0.7850
Epoch: 20, Loss: 4.3387, Train: 0.9643, Val: 0.7760, Test: 0.7990
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 21, Loss: 4.3437, Train: 0.9571, Val: 0.7700, Test: 0.8040
Epoch: 22, Loss: 4.0825, Train: 0.9571, Val: 0.7600, Test: 0.7910
Epoch: 23, Loss: 4.0629, Train: 0.9571, Val: 0.7600, Test: 0.7760
Epoch: 24, Loss: 4.0220, Train: 0.9571, Val: 0.7620, Test: 0.7600
Epoch: 25, Loss: 4.1076, Train: 0.9500, Val: 0.7600, Test: 0.7500
Epoch: 26, Loss: 3.9832, Train: 0.9571, Val: 0.7540, Test: 0.7500
Epoch: 27, Loss: 4.2424, Train: 0.9571, Val: 0.7600, Test: 0.7580
Epoch: 28, Loss: 3.8583, Train: 0.9571, Val: 0.7640, Test: 0.7680
Epoch: 29, Loss: 3.7039, Train: 0.9571, Val: 0.7620, Test: 0.7870
Epoch: 30, Loss: 3.8980, Train: 0.9571, Val: 0.7720, Test: 0.7980
Epoch: 31, Loss: 4.1315, Train: 0.9643, Val: 0.7760, Test: 0.8060
Epoch: 32, Loss: 4.0599, Train: 0.9714, Val: 0.7840, Test: 0.8110
Epoch: 33, Loss: 3.8172, Train: 0.9786, Val: 0.7920, Test: 0.8160
Epoch: 34, Loss: 3.9822, Train: 0.9786, Val: 0.8060, Test: 0.8190
Epoch: 35, Loss: 3.8159, Train: 0.9786, Val: 0.8040, Test: 0.8210
Epoch: 36, Loss: 3.9015, Train: 0.9714, Val: 0.8080, Test: 0.8240
Epoch: 37, Loss: 3.9151, Train: 0.9714, Val: 0.8100, Test: 0.8260
Epoch: 38, Loss: 3.8800, Train: 0.9714, Val: 0.8060, Test: 0.8280
Epoch: 39, Loss: 3.6658, Train: 0.9714, Val: 0.8060, Test: 0.8270
Epoch: 40, Loss: 3.5649, Train: 0.9714, Val: 0.8080, Test: 0.8320
Epoch: 41, Loss: 3.9086, Train: 0.9857, Val: 0.8000, Test: 0.8240
Epoch: 42, Loss: 3.9652, Train: 0.9857, Val: 0.7920, Test: 0.8240
Epoch: 43, Loss: 4.0250, Train: 0.9857, Val: 0.7840, Test: 0.8170
Epoch: 44, Loss: 4.0251, Train: 0.9857, Val: 0.7800, Test: 0.8130
Epoch: 45, Loss: 3.9496, Train: 0.9857, Val: 0.7780, Test: 0.8080
Epoch: 46, Loss: 3.8121, Train: 0.9857, Val: 0.7800, Test: 0.8050
Epoch: 47, Loss: 3.9556, Train: 0.9857, Val: 0.7840, Test: 0.8060
Epoch: 48, Loss: 3.8237, Train: 0.9857, Val: 0.7820, Test: 0.8040
Epoch: 49, Loss: 3.8602, Train: 0.9929, Val: 0.7860, Test: 0.7970
Epoch: 50, Loss: 3.6252, Train: 0.9929, Val: 0.7860, Test: 0.7950
MAD:  0.2598
Best Test Accuracy: 0.8320, Val Accuracy: 0.8080, Train Accuracy: 0.9714
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8535, Train: 0.2143, Val: 0.0960, Test: 0.0950
Epoch: 2, Loss: 4.8365, Train: 0.2786, Val: 0.1580, Test: 0.1640
Epoch: 3, Loss: 4.8322, Train: 0.3000, Val: 0.1960, Test: 0.1910
Epoch: 4, Loss: 4.7795, Train: 0.3214, Val: 0.2140, Test: 0.2110
Epoch: 5, Loss: 4.7525, Train: 0.3357, Val: 0.2200, Test: 0.2140
Epoch: 6, Loss: 4.6658, Train: 0.3429, Val: 0.2200, Test: 0.2270
Epoch: 7, Loss: 4.7072, Train: 0.3857, Val: 0.2240, Test: 0.2340
Epoch: 8, Loss: 4.6863, Train: 0.4000, Val: 0.2360, Test: 0.2420
Epoch: 9, Loss: 4.5959, Train: 0.3929, Val: 0.2380, Test: 0.2420
Epoch: 10, Loss: 4.5800, Train: 0.4143, Val: 0.2440, Test: 0.2500
Epoch: 11, Loss: 4.5996, Train: 0.4286, Val: 0.2480, Test: 0.2560
Epoch: 12, Loss: 4.6231, Train: 0.4357, Val: 0.2560, Test: 0.2620
Epoch: 13, Loss: 4.4807, Train: 0.4357, Val: 0.2580, Test: 0.2690
Epoch: 14, Loss: 4.4775, Train: 0.4500, Val: 0.2720, Test: 0.2810
Epoch: 15, Loss: 4.2691, Train: 0.4500, Val: 0.2780, Test: 0.2920
Epoch: 16, Loss: 4.3300, Train: 0.4643, Val: 0.3060, Test: 0.3030
Epoch: 17, Loss: 4.2007, Train: 0.5286, Val: 0.3480, Test: 0.3350
Epoch: 18, Loss: 4.1101, Train: 0.6571, Val: 0.4400, Test: 0.4150
Epoch: 19, Loss: 4.3323, Train: 0.7786, Val: 0.5640, Test: 0.5540
Epoch: 20, Loss: 4.3789, Train: 0.8643, Val: 0.7040, Test: 0.6860
Epoch: 21, Loss: 4.0529, Train: 0.9071, Val: 0.7700, Test: 0.7770
Epoch: 22, Loss: 4.2113, Train: 0.9214, Val: 0.7860, Test: 0.7980
Epoch: 23, Loss: 4.1153, Train: 0.9214, Val: 0.7820, Test: 0.8070
Epoch: 24, Loss: 4.1619, Train: 0.9071, Val: 0.7700, Test: 0.7870
Epoch: 25, Loss: 4.2247, Train: 0.8929, Val: 0.7800, Test: 0.7860
Epoch: 26, Loss: 4.1283, Train: 0.9071, Val: 0.7860, Test: 0.7800
Epoch: 27, Loss: 4.1908, Train: 0.9071, Val: 0.7840, Test: 0.7800
Epoch: 28, Loss: 4.0858, Train: 0.9286, Val: 0.7720, Test: 0.7650
Epoch: 29, Loss: 3.8824, Train: 0.9286, Val: 0.7760, Test: 0.7770
Epoch: 30, Loss: 4.1385, Train: 0.9429, Val: 0.7700, Test: 0.7790
Epoch: 31, Loss: 3.9212, Train: 0.9429, Val: 0.7700, Test: 0.7830
Epoch: 32, Loss: 4.1170, Train: 0.9500, Val: 0.7760, Test: 0.7860
Epoch: 33, Loss: 3.9072, Train: 0.9429, Val: 0.7720, Test: 0.7790
Epoch: 34, Loss: 4.0093, Train: 0.9429, Val: 0.7820, Test: 0.7780
Epoch: 35, Loss: 3.7726, Train: 0.9500, Val: 0.7820, Test: 0.7790
Epoch: 36, Loss: 4.0444, Train: 0.9500, Val: 0.7720, Test: 0.7720
Epoch: 37, Loss: 4.1503, Train: 0.9643, Val: 0.7760, Test: 0.7740
Epoch: 38, Loss: 3.9325, Train: 0.9643, Val: 0.7720, Test: 0.7800
Epoch: 39, Loss: 3.9032, Train: 0.9714, Val: 0.7840, Test: 0.7840
Epoch: 40, Loss: 3.8261, Train: 0.9786, Val: 0.7860, Test: 0.7870
Epoch: 41, Loss: 3.6634, Train: 0.9857, Val: 0.7820, Test: 0.7900
Epoch: 42, Loss: 3.9682, Train: 0.9857, Val: 0.7840, Test: 0.7890
Epoch: 43, Loss: 3.8436, Train: 0.9929, Val: 0.7880, Test: 0.7940
Epoch: 44, Loss: 3.6560, Train: 0.9929, Val: 0.7900, Test: 0.7950
Epoch: 45, Loss: 3.8229, Train: 0.9857, Val: 0.7900, Test: 0.7990
Epoch: 46, Loss: 3.9360, Train: 0.9857, Val: 0.7880, Test: 0.7940
Epoch: 47, Loss: 3.7716, Train: 0.9857, Val: 0.7900, Test: 0.7950
Epoch: 48, Loss: 3.8988, Train: 0.9857, Val: 0.7860, Test: 0.7970
Epoch: 49, Loss: 4.3323, Train: 0.9857, Val: 0.7820, Test: 0.7980
Epoch: 50, Loss: 3.6951, Train: 0.9857, Val: 0.7840, Test: 0.7940
MAD:  0.0623
Best Test Accuracy: 0.8070, Val Accuracy: 0.7820, Train Accuracy: 0.9214
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8448, Train: 0.1714, Val: 0.0860, Test: 0.0930
Epoch: 2, Loss: 4.8246, Train: 0.2357, Val: 0.1100, Test: 0.1190
Epoch: 3, Loss: 4.8099, Train: 0.3143, Val: 0.1540, Test: 0.1660
Epoch: 4, Loss: 4.7452, Train: 0.4429, Val: 0.2000, Test: 0.2260
Epoch: 5, Loss: 4.7778, Train: 0.5357, Val: 0.2700, Test: 0.3090
Epoch: 6, Loss: 4.7099, Train: 0.6286, Val: 0.3620, Test: 0.4020
Epoch: 7, Loss: 4.6506, Train: 0.7000, Val: 0.4560, Test: 0.4790
Epoch: 8, Loss: 4.6445, Train: 0.7571, Val: 0.5140, Test: 0.5350
Epoch: 9, Loss: 4.5870, Train: 0.7786, Val: 0.5500, Test: 0.5670
Epoch: 10, Loss: 4.5811, Train: 0.8000, Val: 0.5660, Test: 0.5930
Epoch: 11, Loss: 4.5567, Train: 0.7857, Val: 0.5860, Test: 0.6140
Epoch: 12, Loss: 4.4204, Train: 0.7857, Val: 0.5860, Test: 0.6170
Epoch: 13, Loss: 4.3925, Train: 0.8000, Val: 0.5880, Test: 0.6190
Epoch: 14, Loss: 4.1913, Train: 0.8000, Val: 0.5820, Test: 0.6130
Epoch: 15, Loss: 4.3945, Train: 0.8214, Val: 0.5660, Test: 0.6120
Epoch: 16, Loss: 4.4170, Train: 0.8143, Val: 0.5660, Test: 0.6000
Epoch: 17, Loss: 4.2082, Train: 0.8214, Val: 0.5620, Test: 0.5890
Epoch: 18, Loss: 4.1452, Train: 0.8143, Val: 0.5560, Test: 0.5660
Epoch: 19, Loss: 4.0320, Train: 0.8214, Val: 0.5240, Test: 0.5500
Epoch: 20, Loss: 3.9749, Train: 0.8571, Val: 0.5400, Test: 0.5590
Epoch: 21, Loss: 4.2705, Train: 0.8929, Val: 0.6080, Test: 0.6150
Epoch: 22, Loss: 4.1097, Train: 0.9071, Val: 0.6720, Test: 0.6480
Epoch: 23, Loss: 4.2571, Train: 0.9357, Val: 0.6900, Test: 0.6660
Epoch: 24, Loss: 4.0133, Train: 0.9357, Val: 0.6840, Test: 0.6840
Epoch: 25, Loss: 3.9316, Train: 0.9357, Val: 0.6860, Test: 0.6890
Epoch: 26, Loss: 4.2164, Train: 0.9357, Val: 0.6680, Test: 0.6840
Epoch: 27, Loss: 4.1913, Train: 0.9357, Val: 0.6800, Test: 0.6940
Epoch: 28, Loss: 4.1666, Train: 0.9429, Val: 0.7040, Test: 0.7140
Epoch: 29, Loss: 4.1588, Train: 0.9500, Val: 0.7200, Test: 0.7370
Epoch: 30, Loss: 4.1727, Train: 0.9643, Val: 0.7420, Test: 0.7570
Epoch: 31, Loss: 3.8626, Train: 0.9643, Val: 0.7740, Test: 0.7760
Epoch: 32, Loss: 4.1709, Train: 0.9643, Val: 0.7760, Test: 0.7910
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 33, Loss: 3.7966, Train: 0.9643, Val: 0.7900, Test: 0.8040
Epoch: 34, Loss: 4.0190, Train: 0.9643, Val: 0.7940, Test: 0.8100
Epoch: 35, Loss: 3.9478, Train: 0.9714, Val: 0.8040, Test: 0.8090
Epoch: 36, Loss: 3.9115, Train: 0.9786, Val: 0.8020, Test: 0.8080
Epoch: 37, Loss: 3.9257, Train: 0.9786, Val: 0.8080, Test: 0.8150
Epoch: 38, Loss: 3.7927, Train: 0.9857, Val: 0.8100, Test: 0.8220
Epoch: 39, Loss: 3.8504, Train: 0.9857, Val: 0.8080, Test: 0.8170
Epoch: 40, Loss: 3.8898, Train: 0.9786, Val: 0.8040, Test: 0.8160
Epoch: 41, Loss: 3.7546, Train: 0.9786, Val: 0.8080, Test: 0.8150
Epoch: 42, Loss: 3.8158, Train: 0.9786, Val: 0.8080, Test: 0.8120
Epoch: 43, Loss: 4.0862, Train: 0.9857, Val: 0.8080, Test: 0.8070
Epoch: 44, Loss: 3.9992, Train: 0.9857, Val: 0.8100, Test: 0.8100
Epoch: 45, Loss: 3.6926, Train: 0.9857, Val: 0.8040, Test: 0.8030
Epoch: 46, Loss: 3.6894, Train: 0.9857, Val: 0.8020, Test: 0.7950
Epoch: 47, Loss: 3.8667, Train: 0.9857, Val: 0.7900, Test: 0.7980
Epoch: 48, Loss: 4.0524, Train: 0.9857, Val: 0.7820, Test: 0.7930
Epoch: 49, Loss: 4.0584, Train: 0.9857, Val: 0.7820, Test: 0.7880
Epoch: 50, Loss: 3.6615, Train: 0.9857, Val: 0.7800, Test: 0.7850
MAD:  0.2881
Best Test Accuracy: 0.8220, Val Accuracy: 0.8100, Train Accuracy: 0.9857
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8496, Train: 0.1429, Val: 0.1760, Test: 0.1960
Epoch: 2, Loss: 4.8346, Train: 0.2429, Val: 0.3000, Test: 0.3010
Epoch: 3, Loss: 4.8235, Train: 0.2857, Val: 0.3340, Test: 0.3450
Epoch: 4, Loss: 4.8029, Train: 0.2929, Val: 0.3400, Test: 0.3550
Epoch: 5, Loss: 4.7431, Train: 0.3071, Val: 0.3440, Test: 0.3590
Epoch: 6, Loss: 4.7377, Train: 0.3143, Val: 0.3580, Test: 0.3870
Epoch: 7, Loss: 4.7018, Train: 0.3786, Val: 0.3760, Test: 0.4010
Epoch: 8, Loss: 4.5872, Train: 0.3929, Val: 0.3620, Test: 0.4180
Epoch: 9, Loss: 4.5191, Train: 0.4000, Val: 0.3560, Test: 0.4080
Epoch: 10, Loss: 4.6144, Train: 0.4143, Val: 0.3460, Test: 0.3990
Epoch: 11, Loss: 4.5265, Train: 0.4071, Val: 0.3340, Test: 0.3970
Epoch: 12, Loss: 4.5448, Train: 0.4357, Val: 0.3400, Test: 0.3940
Epoch: 13, Loss: 4.3905, Train: 0.4357, Val: 0.3440, Test: 0.4010
Epoch: 14, Loss: 4.3920, Train: 0.4714, Val: 0.3560, Test: 0.4090
Epoch: 15, Loss: 4.2666, Train: 0.5429, Val: 0.3860, Test: 0.4280
Epoch: 16, Loss: 4.2366, Train: 0.5714, Val: 0.4120, Test: 0.4500
Epoch: 17, Loss: 4.3007, Train: 0.6429, Val: 0.4340, Test: 0.4900
Epoch: 18, Loss: 4.4420, Train: 0.6714, Val: 0.4840, Test: 0.5320
Epoch: 19, Loss: 4.3352, Train: 0.7286, Val: 0.5280, Test: 0.5850
Epoch: 20, Loss: 4.2108, Train: 0.8357, Val: 0.5840, Test: 0.6490
Epoch: 21, Loss: 4.0944, Train: 0.9071, Val: 0.6180, Test: 0.6980
Epoch: 22, Loss: 4.0470, Train: 0.9429, Val: 0.6660, Test: 0.7230
Epoch: 23, Loss: 3.9466, Train: 0.9643, Val: 0.6960, Test: 0.7570
Epoch: 24, Loss: 4.2295, Train: 0.9714, Val: 0.7120, Test: 0.7560
Epoch: 25, Loss: 4.2552, Train: 0.9714, Val: 0.7360, Test: 0.7660
Epoch: 26, Loss: 4.0579, Train: 0.9714, Val: 0.7400, Test: 0.7670
Epoch: 27, Loss: 3.9046, Train: 0.9714, Val: 0.7380, Test: 0.7680
Epoch: 28, Loss: 4.1746, Train: 0.9714, Val: 0.7400, Test: 0.7680
Epoch: 29, Loss: 4.1821, Train: 0.9786, Val: 0.7460, Test: 0.7660
Epoch: 30, Loss: 3.9157, Train: 0.9714, Val: 0.7420, Test: 0.7730
Epoch: 31, Loss: 4.0181, Train: 0.9786, Val: 0.7360, Test: 0.7750
Epoch: 32, Loss: 3.9437, Train: 0.9714, Val: 0.7380, Test: 0.7770
Epoch: 33, Loss: 3.9809, Train: 0.9714, Val: 0.7560, Test: 0.7840
Epoch: 34, Loss: 3.8610, Train: 0.9714, Val: 0.7580, Test: 0.7860
Epoch: 35, Loss: 4.0453, Train: 0.9714, Val: 0.7600, Test: 0.7940
Epoch: 36, Loss: 3.6959, Train: 0.9714, Val: 0.7640, Test: 0.8030
Epoch: 37, Loss: 3.7440, Train: 0.9714, Val: 0.7640, Test: 0.8050
Epoch: 38, Loss: 3.8699, Train: 0.9714, Val: 0.7620, Test: 0.8050
Epoch: 39, Loss: 4.1166, Train: 0.9786, Val: 0.7620, Test: 0.8070
Epoch: 40, Loss: 3.8914, Train: 0.9786, Val: 0.7660, Test: 0.8060
Epoch: 41, Loss: 3.7646, Train: 0.9786, Val: 0.7680, Test: 0.8100
Epoch: 42, Loss: 3.8720, Train: 0.9786, Val: 0.7760, Test: 0.8070
Epoch: 43, Loss: 3.7683, Train: 0.9786, Val: 0.7800, Test: 0.8060
Epoch: 44, Loss: 3.8186, Train: 0.9786, Val: 0.7760, Test: 0.8050
Epoch: 45, Loss: 3.8252, Train: 0.9786, Val: 0.7840, Test: 0.8020
Epoch: 46, Loss: 3.7783, Train: 0.9786, Val: 0.7780, Test: 0.7990
Epoch: 47, Loss: 3.8353, Train: 0.9857, Val: 0.7840, Test: 0.7970
Epoch: 48, Loss: 3.9837, Train: 0.9857, Val: 0.7820, Test: 0.7990
Epoch: 49, Loss: 3.5758, Train: 0.9857, Val: 0.7840, Test: 0.8010
Epoch: 50, Loss: 3.7459, Train: 0.9857, Val: 0.7740, Test: 0.8020
MAD:  0.2655
Best Test Accuracy: 0.8100, Val Accuracy: 0.7680, Train Accuracy: 0.9786
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8620, Train: 0.1000, Val: 0.0460, Test: 0.0570
Epoch: 2, Loss: 4.8440, Train: 0.2286, Val: 0.1980, Test: 0.2110
Epoch: 3, Loss: 4.8289, Train: 0.2000, Val: 0.1900, Test: 0.1950
Epoch: 4, Loss: 4.8034, Train: 0.2000, Val: 0.1840, Test: 0.1760
Epoch: 5, Loss: 4.7809, Train: 0.2000, Val: 0.1720, Test: 0.1740
Epoch: 6, Loss: 4.7519, Train: 0.2000, Val: 0.1640, Test: 0.1730
Epoch: 7, Loss: 4.7238, Train: 0.2000, Val: 0.1500, Test: 0.1560
Epoch: 8, Loss: 4.6706, Train: 0.2000, Val: 0.1500, Test: 0.1570
Epoch: 9, Loss: 4.6492, Train: 0.2071, Val: 0.1500, Test: 0.1580
Epoch: 10, Loss: 4.6430, Train: 0.2429, Val: 0.1620, Test: 0.1760
Epoch: 11, Loss: 4.5909, Train: 0.3000, Val: 0.1940, Test: 0.1990
Epoch: 12, Loss: 4.6310, Train: 0.3357, Val: 0.2320, Test: 0.2310
Epoch: 13, Loss: 4.4673, Train: 0.3786, Val: 0.2740, Test: 0.2850
Epoch: 14, Loss: 4.6412, Train: 0.4071, Val: 0.3120, Test: 0.3460
Epoch: 15, Loss: 4.4649, Train: 0.4214, Val: 0.3560, Test: 0.3930
Epoch: 16, Loss: 4.5849, Train: 0.4500, Val: 0.4160, Test: 0.4470
Epoch: 17, Loss: 4.4874, Train: 0.4857, Val: 0.4440, Test: 0.4930
Epoch: 18, Loss: 4.3511, Train: 0.5071, Val: 0.4740, Test: 0.5080
Epoch: 19, Loss: 4.3839, Train: 0.5571, Val: 0.5040, Test: 0.5360
Epoch: 20, Loss: 4.2980, Train: 0.6786, Val: 0.5500, Test: 0.5770
Epoch: 21, Loss: 4.3664, Train: 0.7857, Val: 0.6380, Test: 0.6360
Epoch: 22, Loss: 4.3418, Train: 0.8500, Val: 0.6960, Test: 0.7000
Epoch: 23, Loss: 4.2370, Train: 0.8857, Val: 0.7520, Test: 0.7600
Epoch: 24, Loss: 4.0321, Train: 0.9071, Val: 0.7480, Test: 0.7780
Epoch: 25, Loss: 4.2220, Train: 0.9357, Val: 0.7260, Test: 0.7670
Epoch: 26, Loss: 4.0020, Train: 0.9357, Val: 0.6940, Test: 0.7300
Epoch: 27, Loss: 4.3250, Train: 0.9429, Val: 0.6980, Test: 0.7210
Epoch: 28, Loss: 4.0293, Train: 0.9429, Val: 0.7000, Test: 0.7400
Epoch: 29, Loss: 4.1292, Train: 0.9429, Val: 0.7340, Test: 0.7600
Epoch: 30, Loss: 4.0787, Train: 0.9429, Val: 0.7360, Test: 0.7760
Epoch: 31, Loss: 3.9908, Train: 0.9500, Val: 0.7520, Test: 0.7930
Epoch: 32, Loss: 3.8031, Train: 0.9643, Val: 0.7680, Test: 0.8090
Epoch: 33, Loss: 3.8941, Train: 0.9571, Val: 0.7640, Test: 0.8070
Epoch: 34, Loss: 3.8975, Train: 0.9643, Val: 0.7600, Test: 0.8050
Epoch: 35, Loss: 3.8759, Train: 0.9643, Val: 0.7640, Test: 0.7990
Epoch: 36, Loss: 4.0196, Train: 0.9857, Val: 0.7640, Test: 0.7960
Epoch: 37, Loss: 4.1462, Train: 0.9857, Val: 0.7700, Test: 0.7980
Epoch: 38, Loss: 4.0738, Train: 0.9857, Val: 0.7700, Test: 0.8030
Epoch: 39, Loss: 3.9067, Train: 0.9857, Val: 0.7760, Test: 0.8060
Epoch: 40, Loss: 3.8919, Train: 0.9857, Val: 0.7860, Test: 0.8100
Epoch: 41, Loss: 4.1180, Train: 0.9857, Val: 0.7880, Test: 0.8110
Epoch: 42, Loss: 3.8986, Train: 0.9857, Val: 0.7920, Test: 0.8170
Epoch: 43, Loss: 3.8217, Train: 0.9786, Val: 0.7920, Test: 0.8200
Epoch: 44, Loss: 4.0133, Train: 0.9786, Val: 0.8000, Test: 0.8250
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 45, Loss: 3.6993, Train: 0.9786, Val: 0.8020, Test: 0.8210
Epoch: 46, Loss: 3.8476, Train: 0.9786, Val: 0.7920, Test: 0.8230
Epoch: 47, Loss: 3.7876, Train: 0.9857, Val: 0.7860, Test: 0.8250
Epoch: 48, Loss: 3.9933, Train: 0.9857, Val: 0.7800, Test: 0.8210
Epoch: 49, Loss: 3.9143, Train: 0.9857, Val: 0.7780, Test: 0.8130
Epoch: 50, Loss: 3.8536, Train: 0.9857, Val: 0.7740, Test: 0.8100
MAD:  0.3913
Best Test Accuracy: 0.8250, Val Accuracy: 0.8000, Train Accuracy: 0.9786
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8542, Train: 0.1214, Val: 0.0740, Test: 0.0780
Epoch: 2, Loss: 4.8358, Train: 0.2286, Val: 0.0960, Test: 0.1130
Epoch: 3, Loss: 4.8306, Train: 0.3000, Val: 0.1560, Test: 0.1720
Epoch: 4, Loss: 4.8135, Train: 0.4000, Val: 0.2440, Test: 0.2650
Epoch: 5, Loss: 4.7657, Train: 0.4643, Val: 0.2880, Test: 0.3090
Epoch: 6, Loss: 4.7018, Train: 0.4786, Val: 0.3060, Test: 0.3260
Epoch: 7, Loss: 4.7000, Train: 0.4786, Val: 0.3100, Test: 0.3320
Epoch: 8, Loss: 4.5937, Train: 0.4857, Val: 0.3080, Test: 0.3260
Epoch: 9, Loss: 4.5715, Train: 0.4786, Val: 0.3080, Test: 0.3260
Epoch: 10, Loss: 4.5521, Train: 0.4857, Val: 0.3060, Test: 0.3260
Epoch: 11, Loss: 4.4101, Train: 0.4786, Val: 0.3080, Test: 0.3270
Epoch: 12, Loss: 4.4354, Train: 0.4786, Val: 0.3100, Test: 0.3260
Epoch: 13, Loss: 4.4727, Train: 0.5000, Val: 0.3060, Test: 0.3280
Epoch: 14, Loss: 4.1683, Train: 0.5143, Val: 0.3120, Test: 0.3350
Epoch: 15, Loss: 4.3153, Train: 0.5571, Val: 0.3280, Test: 0.3490
Epoch: 16, Loss: 4.2643, Train: 0.6357, Val: 0.3620, Test: 0.3670
Epoch: 17, Loss: 4.3624, Train: 0.7143, Val: 0.4200, Test: 0.4210
Epoch: 18, Loss: 4.3479, Train: 0.8000, Val: 0.5220, Test: 0.5400
Epoch: 19, Loss: 4.1659, Train: 0.8857, Val: 0.6600, Test: 0.6520
Epoch: 20, Loss: 4.1979, Train: 0.9286, Val: 0.7380, Test: 0.7380
Epoch: 21, Loss: 4.3154, Train: 0.9643, Val: 0.7720, Test: 0.7980
Epoch: 22, Loss: 4.0876, Train: 0.9714, Val: 0.7820, Test: 0.8040
Epoch: 23, Loss: 3.8882, Train: 0.9714, Val: 0.7740, Test: 0.8170
Epoch: 24, Loss: 4.1579, Train: 0.9643, Val: 0.7660, Test: 0.8100
Epoch: 25, Loss: 4.2501, Train: 0.9714, Val: 0.7620, Test: 0.7990
Epoch: 26, Loss: 3.9844, Train: 0.9714, Val: 0.7660, Test: 0.7930
Epoch: 27, Loss: 4.0018, Train: 0.9714, Val: 0.7620, Test: 0.7930
Epoch: 28, Loss: 3.7596, Train: 0.9714, Val: 0.7580, Test: 0.7860
Epoch: 29, Loss: 4.2161, Train: 0.9786, Val: 0.7660, Test: 0.7860
Epoch: 30, Loss: 4.0086, Train: 0.9714, Val: 0.7620, Test: 0.7840
Epoch: 31, Loss: 4.0392, Train: 0.9714, Val: 0.7740, Test: 0.7790
Epoch: 32, Loss: 4.1605, Train: 0.9714, Val: 0.7720, Test: 0.7840
Epoch: 33, Loss: 3.9026, Train: 0.9714, Val: 0.7740, Test: 0.7860
Epoch: 34, Loss: 4.1221, Train: 0.9714, Val: 0.7720, Test: 0.7850
Epoch: 35, Loss: 4.0302, Train: 0.9857, Val: 0.7780, Test: 0.7820
Epoch: 36, Loss: 3.8067, Train: 0.9929, Val: 0.7740, Test: 0.7890
Epoch: 37, Loss: 3.7537, Train: 0.9857, Val: 0.7740, Test: 0.7950
Epoch: 38, Loss: 4.0369, Train: 0.9857, Val: 0.7720, Test: 0.7940
Epoch: 39, Loss: 3.9035, Train: 0.9857, Val: 0.7820, Test: 0.7950
Epoch: 40, Loss: 3.4995, Train: 0.9857, Val: 0.7840, Test: 0.7950
Epoch: 41, Loss: 4.2141, Train: 0.9857, Val: 0.7800, Test: 0.7980
Epoch: 42, Loss: 4.1392, Train: 0.9857, Val: 0.7760, Test: 0.8010
Epoch: 43, Loss: 4.2346, Train: 0.9929, Val: 0.7800, Test: 0.7990
Epoch: 44, Loss: 3.6925, Train: 0.9929, Val: 0.7780, Test: 0.7970
Epoch: 45, Loss: 3.6834, Train: 0.9929, Val: 0.7740, Test: 0.7960
Epoch: 46, Loss: 3.9619, Train: 0.9929, Val: 0.7780, Test: 0.8080
Epoch: 47, Loss: 3.8884, Train: 1.0000, Val: 0.7740, Test: 0.8130
Epoch: 48, Loss: 3.6296, Train: 0.9929, Val: 0.7800, Test: 0.8160
Epoch: 49, Loss: 3.5448, Train: 0.9929, Val: 0.7800, Test: 0.8190
Epoch: 50, Loss: 3.7813, Train: 0.9929, Val: 0.7780, Test: 0.8170
MAD:  0.4055
Best Test Accuracy: 0.8190, Val Accuracy: 0.7800, Train Accuracy: 0.9929
Training completed.
Average Test Accuracy:  0.8161999999999999 ± 0.011813551540497855
Average MAD:  0.30518999999999996 ± 0.1174797893256538

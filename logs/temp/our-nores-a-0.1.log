/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8583, Train: 0.1000, Val: 0.0620, Test: 0.0580
Epoch: 2, Loss: 4.8468, Train: 0.3357, Val: 0.2860, Test: 0.2920
Epoch: 3, Loss: 4.8351, Train: 0.4429, Val: 0.3820, Test: 0.3970
Epoch: 4, Loss: 4.7912, Train: 0.4571, Val: 0.3900, Test: 0.3990
Epoch: 5, Loss: 4.7893, Train: 0.5000, Val: 0.4180, Test: 0.4300
Epoch: 6, Loss: 4.7359, Train: 0.5143, Val: 0.4360, Test: 0.4460
Epoch: 7, Loss: 4.7508, Train: 0.5571, Val: 0.4460, Test: 0.4870
Epoch: 8, Loss: 4.6876, Train: 0.5571, Val: 0.4880, Test: 0.5190
Epoch: 9, Loss: 4.7090, Train: 0.5857, Val: 0.4980, Test: 0.5300
Epoch: 10, Loss: 4.6703, Train: 0.6357, Val: 0.5140, Test: 0.5480
Epoch: 11, Loss: 4.5276, Train: 0.6429, Val: 0.5220, Test: 0.5510
Epoch: 12, Loss: 4.6197, Train: 0.6500, Val: 0.5380, Test: 0.5610
Epoch: 13, Loss: 4.5392, Train: 0.6500, Val: 0.5220, Test: 0.5530
Epoch: 14, Loss: 4.4411, Train: 0.6500, Val: 0.5220, Test: 0.5570
Epoch: 15, Loss: 4.4032, Train: 0.6500, Val: 0.5120, Test: 0.5500
Epoch: 16, Loss: 4.2791, Train: 0.6714, Val: 0.5140, Test: 0.5550
Epoch: 17, Loss: 4.1610, Train: 0.7000, Val: 0.5280, Test: 0.5640
Epoch: 18, Loss: 4.2334, Train: 0.7357, Val: 0.5500, Test: 0.6040
Epoch: 19, Loss: 3.9975, Train: 0.7500, Val: 0.5880, Test: 0.6450
Epoch: 20, Loss: 4.1329, Train: 0.7857, Val: 0.6120, Test: 0.6710
Epoch: 21, Loss: 4.3908, Train: 0.8143, Val: 0.6360, Test: 0.6800
Epoch: 22, Loss: 4.2458, Train: 0.8857, Val: 0.6480, Test: 0.7120
Epoch: 23, Loss: 4.3396, Train: 0.9286, Val: 0.6860, Test: 0.7250
Epoch: 24, Loss: 3.9445, Train: 0.9214, Val: 0.7060, Test: 0.7420
Epoch: 25, Loss: 4.2494, Train: 0.9214, Val: 0.7420, Test: 0.7600
Epoch: 26, Loss: 4.0675, Train: 0.9286, Val: 0.7380, Test: 0.7550
Epoch: 27, Loss: 4.0670, Train: 0.9286, Val: 0.7340, Test: 0.7570
Epoch: 28, Loss: 4.0708, Train: 0.9357, Val: 0.7360, Test: 0.7470
Epoch: 29, Loss: 3.9160, Train: 0.9429, Val: 0.7200, Test: 0.7330
Epoch: 30, Loss: 3.9375, Train: 0.9500, Val: 0.7100, Test: 0.7340
Epoch: 31, Loss: 4.3337, Train: 0.9571, Val: 0.7140, Test: 0.7470
Epoch: 32, Loss: 3.9946, Train: 0.9643, Val: 0.7320, Test: 0.7600
Epoch: 33, Loss: 4.0903, Train: 0.9643, Val: 0.7400, Test: 0.7740
Epoch: 34, Loss: 3.6720, Train: 0.9786, Val: 0.7540, Test: 0.7800
Epoch: 35, Loss: 4.1987, Train: 0.9857, Val: 0.7560, Test: 0.7900
Epoch: 36, Loss: 3.9083, Train: 0.9857, Val: 0.7720, Test: 0.7970
Epoch: 37, Loss: 4.0572, Train: 0.9857, Val: 0.7860, Test: 0.8040
Epoch: 38, Loss: 3.7396, Train: 0.9857, Val: 0.7980, Test: 0.8050
Epoch: 39, Loss: 4.0094, Train: 0.9786, Val: 0.8020, Test: 0.8140
Epoch: 40, Loss: 3.8886, Train: 0.9786, Val: 0.8040, Test: 0.8150
Epoch: 41, Loss: 3.8607, Train: 0.9786, Val: 0.8040, Test: 0.8150
Epoch: 42, Loss: 4.0815, Train: 0.9786, Val: 0.8020, Test: 0.8160
Epoch: 43, Loss: 3.6289, Train: 0.9786, Val: 0.8040, Test: 0.8150
Epoch: 44, Loss: 3.8871, Train: 0.9857, Val: 0.8060, Test: 0.8160
Epoch: 45, Loss: 3.9863, Train: 0.9857, Val: 0.8040, Test: 0.8170
Epoch: 46, Loss: 3.7628, Train: 0.9857, Val: 0.8060, Test: 0.8190
Epoch: 47, Loss: 3.9021, Train: 0.9857, Val: 0.8020, Test: 0.8220
Epoch: 48, Loss: 3.4395, Train: 0.9857, Val: 0.8040, Test: 0.8230
Epoch: 49, Loss: 3.7263, Train: 0.9857, Val: 0.8040, Test: 0.8230
Epoch: 50, Loss: 4.0469, Train: 0.9857, Val: 0.8040, Test: 0.8220
MAD:  0.3387
Best Test Accuracy: 0.8230, Val Accuracy: 0.8040, Train Accuracy: 0.9857
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8522, Train: 0.1429, Val: 0.0740, Test: 0.1030
Epoch: 2, Loss: 4.8248, Train: 0.3214, Val: 0.1820, Test: 0.2070
Epoch: 3, Loss: 4.8216, Train: 0.3929, Val: 0.2340, Test: 0.2500
Epoch: 4, Loss: 4.7889, Train: 0.4071, Val: 0.2440, Test: 0.2570
Epoch: 5, Loss: 4.7625, Train: 0.4357, Val: 0.2740, Test: 0.3010
Epoch: 6, Loss: 4.7555, Train: 0.4357, Val: 0.3100, Test: 0.3280
Epoch: 7, Loss: 4.7702, Train: 0.4571, Val: 0.3120, Test: 0.3420
Epoch: 8, Loss: 4.6563, Train: 0.4643, Val: 0.3300, Test: 0.3640
Epoch: 9, Loss: 4.6646, Train: 0.4857, Val: 0.3620, Test: 0.3980
Epoch: 10, Loss: 4.6325, Train: 0.5286, Val: 0.3840, Test: 0.4310
Epoch: 11, Loss: 4.5070, Train: 0.5786, Val: 0.3960, Test: 0.4470
Epoch: 12, Loss: 4.5565, Train: 0.5857, Val: 0.4260, Test: 0.4790
Epoch: 13, Loss: 4.5215, Train: 0.6143, Val: 0.4460, Test: 0.5000
Epoch: 14, Loss: 4.2749, Train: 0.6286, Val: 0.4500, Test: 0.5150
Epoch: 15, Loss: 4.5123, Train: 0.6643, Val: 0.4840, Test: 0.5340
Epoch: 16, Loss: 4.2795, Train: 0.6714, Val: 0.5080, Test: 0.5610
Epoch: 17, Loss: 4.1567, Train: 0.6929, Val: 0.5440, Test: 0.5810
Epoch: 18, Loss: 4.3642, Train: 0.7000, Val: 0.5580, Test: 0.5990
Epoch: 19, Loss: 4.2225, Train: 0.7500, Val: 0.5840, Test: 0.6280
Epoch: 20, Loss: 4.1067, Train: 0.8000, Val: 0.6240, Test: 0.6580
Epoch: 21, Loss: 4.1810, Train: 0.8286, Val: 0.6480, Test: 0.6780
Epoch: 22, Loss: 3.9558, Train: 0.8714, Val: 0.6780, Test: 0.7090
Epoch: 23, Loss: 4.0522, Train: 0.9286, Val: 0.7240, Test: 0.7730
Epoch: 24, Loss: 4.1116, Train: 0.9714, Val: 0.7360, Test: 0.7940
Epoch: 25, Loss: 3.8566, Train: 0.9714, Val: 0.7480, Test: 0.7920
Epoch: 26, Loss: 3.8701, Train: 0.9929, Val: 0.7680, Test: 0.7940
Epoch: 27, Loss: 3.8649, Train: 0.9857, Val: 0.7800, Test: 0.7870
Epoch: 28, Loss: 4.0969, Train: 0.9857, Val: 0.7800, Test: 0.7760
Epoch: 29, Loss: 3.8366, Train: 0.9786, Val: 0.7720, Test: 0.7710
Epoch: 30, Loss: 3.6734, Train: 0.9643, Val: 0.7700, Test: 0.7670
Epoch: 31, Loss: 3.8852, Train: 0.9643, Val: 0.7740, Test: 0.7700
Epoch: 32, Loss: 4.1207, Train: 0.9643, Val: 0.7760, Test: 0.7760
Epoch: 33, Loss: 3.8161, Train: 0.9714, Val: 0.7720, Test: 0.7820
Epoch: 34, Loss: 3.9155, Train: 0.9786, Val: 0.7880, Test: 0.7900
Epoch: 35, Loss: 3.6643, Train: 0.9786, Val: 0.7880, Test: 0.7980
Epoch: 36, Loss: 3.8417, Train: 0.9786, Val: 0.7960, Test: 0.8020
Epoch: 37, Loss: 4.0058, Train: 0.9714, Val: 0.7980, Test: 0.8030
Epoch: 38, Loss: 3.8896, Train: 0.9714, Val: 0.8000, Test: 0.8060
Epoch: 39, Loss: 3.7452, Train: 0.9786, Val: 0.7940, Test: 0.7990
Epoch: 40, Loss: 4.0066, Train: 0.9857, Val: 0.7900, Test: 0.8000
Epoch: 41, Loss: 3.9370, Train: 0.9857, Val: 0.7840, Test: 0.8000
Epoch: 42, Loss: 3.4192, Train: 0.9929, Val: 0.7820, Test: 0.8090
Epoch: 43, Loss: 3.7162, Train: 0.9929, Val: 0.7820, Test: 0.8070
Epoch: 44, Loss: 4.1126, Train: 0.9929, Val: 0.7880, Test: 0.8080
Epoch: 45, Loss: 3.6076, Train: 0.9929, Val: 0.7920, Test: 0.8120
Epoch: 46, Loss: 4.1425, Train: 0.9929, Val: 0.7940, Test: 0.8160
Epoch: 47, Loss: 3.5281, Train: 0.9929, Val: 0.7940, Test: 0.8200
Epoch: 48, Loss: 3.9457, Train: 0.9929, Val: 0.8000, Test: 0.8180
Epoch: 49, Loss: 3.6210, Train: 0.9929, Val: 0.7980, Test: 0.8200
Epoch: 50, Loss: 3.4072, Train: 0.9929, Val: 0.8000, Test: 0.8200
MAD:  0.3733
Best Test Accuracy: 0.8200, Val Accuracy: 0.7940, Train Accuracy: 0.9929
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8557, Train: 0.1500, Val: 0.1180, Test: 0.1190
Epoch: 2, Loss: 4.8399, Train: 0.2786, Val: 0.2700, Test: 0.2650
Epoch: 3, Loss: 4.8300, Train: 0.3000, Val: 0.2760, Test: 0.2750
Epoch: 4, Loss: 4.7971, Train: 0.3357, Val: 0.2600, Test: 0.2800
Epoch: 5, Loss: 4.7705, Train: 0.3643, Val: 0.2600, Test: 0.2650
Epoch: 6, Loss: 4.7630, Train: 0.3857, Val: 0.2620, Test: 0.2660
Epoch: 7, Loss: 4.7573, Train: 0.4000, Val: 0.2600, Test: 0.2750
Epoch: 8, Loss: 4.6845, Train: 0.4071, Val: 0.2580, Test: 0.2690
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 9, Loss: 4.6416, Train: 0.4214, Val: 0.2600, Test: 0.2730
Epoch: 10, Loss: 4.5783, Train: 0.4357, Val: 0.2700, Test: 0.2850
Epoch: 11, Loss: 4.6825, Train: 0.4786, Val: 0.2960, Test: 0.3020
Epoch: 12, Loss: 4.5156, Train: 0.5071, Val: 0.3100, Test: 0.3260
Epoch: 13, Loss: 4.5127, Train: 0.5286, Val: 0.3320, Test: 0.3480
Epoch: 14, Loss: 4.5652, Train: 0.5571, Val: 0.3860, Test: 0.4070
Epoch: 15, Loss: 4.2693, Train: 0.5857, Val: 0.4100, Test: 0.4280
Epoch: 16, Loss: 4.3384, Train: 0.6143, Val: 0.4540, Test: 0.4840
Epoch: 17, Loss: 4.1740, Train: 0.6143, Val: 0.5020, Test: 0.5320
Epoch: 18, Loss: 4.3243, Train: 0.6214, Val: 0.5100, Test: 0.5540
Epoch: 19, Loss: 4.2549, Train: 0.6500, Val: 0.5320, Test: 0.5710
Epoch: 20, Loss: 3.9862, Train: 0.7000, Val: 0.5680, Test: 0.6020
Epoch: 21, Loss: 4.1493, Train: 0.7786, Val: 0.6160, Test: 0.6450
Epoch: 22, Loss: 4.0145, Train: 0.8000, Val: 0.6640, Test: 0.6840
Epoch: 23, Loss: 4.4399, Train: 0.8643, Val: 0.6740, Test: 0.7140
Epoch: 24, Loss: 4.1106, Train: 0.9500, Val: 0.7060, Test: 0.7330
Epoch: 25, Loss: 4.0719, Train: 0.9357, Val: 0.6940, Test: 0.7240
Epoch: 26, Loss: 4.0442, Train: 0.9357, Val: 0.6560, Test: 0.6920
Epoch: 27, Loss: 3.9910, Train: 0.9429, Val: 0.6400, Test: 0.6740
Epoch: 28, Loss: 4.3148, Train: 0.9286, Val: 0.6300, Test: 0.6470
Epoch: 29, Loss: 4.0324, Train: 0.9143, Val: 0.6240, Test: 0.6300
Epoch: 30, Loss: 3.9571, Train: 0.9214, Val: 0.6380, Test: 0.6270
Epoch: 31, Loss: 3.9885, Train: 0.9357, Val: 0.6300, Test: 0.6310
Epoch: 32, Loss: 4.0409, Train: 0.9357, Val: 0.6400, Test: 0.6570
Epoch: 33, Loss: 3.9992, Train: 0.9500, Val: 0.6740, Test: 0.6840
Epoch: 34, Loss: 3.9447, Train: 0.9643, Val: 0.6920, Test: 0.7070
Epoch: 35, Loss: 4.0752, Train: 0.9714, Val: 0.7180, Test: 0.7360
Epoch: 36, Loss: 3.9944, Train: 0.9714, Val: 0.7460, Test: 0.7580
Epoch: 37, Loss: 4.0562, Train: 0.9786, Val: 0.7580, Test: 0.7740
Epoch: 38, Loss: 3.8990, Train: 0.9714, Val: 0.7780, Test: 0.7770
Epoch: 39, Loss: 4.1319, Train: 0.9786, Val: 0.7720, Test: 0.7780
Epoch: 40, Loss: 3.7249, Train: 0.9786, Val: 0.7780, Test: 0.7850
Epoch: 41, Loss: 3.9035, Train: 0.9857, Val: 0.7840, Test: 0.7920
Epoch: 42, Loss: 3.7891, Train: 0.9857, Val: 0.7900, Test: 0.7980
Epoch: 43, Loss: 3.9086, Train: 0.9857, Val: 0.7880, Test: 0.8080
Epoch: 44, Loss: 4.1180, Train: 0.9857, Val: 0.7860, Test: 0.8140
Epoch: 45, Loss: 3.9649, Train: 0.9786, Val: 0.7900, Test: 0.8210
Epoch: 46, Loss: 3.9245, Train: 0.9929, Val: 0.7940, Test: 0.8180
Epoch: 47, Loss: 3.8045, Train: 0.9857, Val: 0.7920, Test: 0.8210
Epoch: 48, Loss: 3.9677, Train: 0.9857, Val: 0.7940, Test: 0.8200
Epoch: 49, Loss: 3.7605, Train: 0.9857, Val: 0.7900, Test: 0.8120
Epoch: 50, Loss: 3.8606, Train: 0.9857, Val: 0.7860, Test: 0.8050
MAD:  0.2811
Best Test Accuracy: 0.8210, Val Accuracy: 0.7900, Train Accuracy: 0.9786
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8517, Train: 0.1571, Val: 0.1120, Test: 0.1020
Epoch: 2, Loss: 4.8418, Train: 0.3571, Val: 0.2400, Test: 0.2510
Epoch: 3, Loss: 4.8143, Train: 0.4286, Val: 0.2840, Test: 0.2920
Epoch: 4, Loss: 4.8049, Train: 0.4357, Val: 0.3000, Test: 0.2940
Epoch: 5, Loss: 4.7769, Train: 0.4500, Val: 0.2720, Test: 0.2870
Epoch: 6, Loss: 4.6999, Train: 0.4500, Val: 0.2760, Test: 0.2800
Epoch: 7, Loss: 4.6723, Train: 0.4643, Val: 0.2820, Test: 0.2890
Epoch: 8, Loss: 4.6198, Train: 0.4714, Val: 0.2900, Test: 0.3010
Epoch: 9, Loss: 4.6087, Train: 0.4786, Val: 0.3080, Test: 0.3050
Epoch: 10, Loss: 4.6306, Train: 0.4857, Val: 0.3140, Test: 0.3110
Epoch: 11, Loss: 4.5143, Train: 0.5000, Val: 0.3180, Test: 0.3210
Epoch: 12, Loss: 4.5741, Train: 0.5143, Val: 0.3280, Test: 0.3260
Epoch: 13, Loss: 4.3633, Train: 0.5143, Val: 0.3440, Test: 0.3390
Epoch: 14, Loss: 4.5545, Train: 0.5286, Val: 0.3520, Test: 0.3590
Epoch: 15, Loss: 4.0898, Train: 0.5571, Val: 0.3760, Test: 0.3750
Epoch: 16, Loss: 4.2957, Train: 0.5786, Val: 0.3980, Test: 0.3950
Epoch: 17, Loss: 4.4098, Train: 0.6000, Val: 0.4340, Test: 0.4290
Epoch: 18, Loss: 4.2026, Train: 0.6429, Val: 0.4660, Test: 0.4600
Epoch: 19, Loss: 4.1822, Train: 0.6500, Val: 0.4660, Test: 0.4790
Epoch: 20, Loss: 4.0953, Train: 0.6643, Val: 0.4900, Test: 0.4890
Epoch: 21, Loss: 4.3542, Train: 0.7000, Val: 0.5080, Test: 0.5080
Epoch: 22, Loss: 4.2557, Train: 0.7929, Val: 0.5380, Test: 0.5340
Epoch: 23, Loss: 4.0052, Train: 0.8714, Val: 0.5980, Test: 0.5980
Epoch: 24, Loss: 4.4056, Train: 0.9071, Val: 0.6660, Test: 0.6690
Epoch: 25, Loss: 4.1987, Train: 0.9357, Val: 0.7060, Test: 0.7100
Epoch: 26, Loss: 3.6517, Train: 0.9571, Val: 0.7420, Test: 0.7370
Epoch: 27, Loss: 4.1214, Train: 0.9571, Val: 0.7680, Test: 0.7610
Epoch: 28, Loss: 4.0120, Train: 0.9571, Val: 0.7740, Test: 0.7720
Epoch: 29, Loss: 3.7448, Train: 0.9643, Val: 0.7820, Test: 0.7840
Epoch: 30, Loss: 4.0754, Train: 0.9643, Val: 0.7840, Test: 0.7820
Epoch: 31, Loss: 4.0374, Train: 0.9643, Val: 0.7800, Test: 0.7790
Epoch: 32, Loss: 4.0754, Train: 0.9643, Val: 0.7840, Test: 0.7790
Epoch: 33, Loss: 3.9267, Train: 0.9643, Val: 0.7800, Test: 0.7830
Epoch: 34, Loss: 4.0095, Train: 0.9643, Val: 0.7780, Test: 0.7900
Epoch: 35, Loss: 4.0617, Train: 0.9643, Val: 0.7800, Test: 0.7900
Epoch: 36, Loss: 3.9014, Train: 0.9714, Val: 0.7800, Test: 0.7790
Epoch: 37, Loss: 3.9756, Train: 0.9714, Val: 0.7840, Test: 0.7710
Epoch: 38, Loss: 4.1077, Train: 0.9714, Val: 0.7660, Test: 0.7680
Epoch: 39, Loss: 4.1065, Train: 0.9714, Val: 0.7640, Test: 0.7620
Epoch: 40, Loss: 3.8886, Train: 0.9786, Val: 0.7540, Test: 0.7560
Epoch: 41, Loss: 3.9196, Train: 0.9786, Val: 0.7440, Test: 0.7560
Epoch: 42, Loss: 3.9118, Train: 0.9857, Val: 0.7460, Test: 0.7560
Epoch: 43, Loss: 3.7717, Train: 0.9786, Val: 0.7500, Test: 0.7590
Epoch: 44, Loss: 3.6364, Train: 0.9786, Val: 0.7500, Test: 0.7610
Epoch: 45, Loss: 4.0030, Train: 0.9714, Val: 0.7540, Test: 0.7680
Epoch: 46, Loss: 3.9453, Train: 0.9714, Val: 0.7580, Test: 0.7680
Epoch: 47, Loss: 3.7273, Train: 0.9714, Val: 0.7640, Test: 0.7710
Epoch: 48, Loss: 3.7904, Train: 0.9714, Val: 0.7680, Test: 0.7740
Epoch: 49, Loss: 3.8422, Train: 0.9714, Val: 0.7720, Test: 0.7730
Epoch: 50, Loss: 3.6272, Train: 0.9786, Val: 0.7720, Test: 0.7760
MAD:  0.1882
Best Test Accuracy: 0.7900, Val Accuracy: 0.7780, Train Accuracy: 0.9643
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8556, Train: 0.1429, Val: 0.1240, Test: 0.1150
Epoch: 2, Loss: 4.8276, Train: 0.2571, Val: 0.1620, Test: 0.1690
Epoch: 3, Loss: 4.8132, Train: 0.2643, Val: 0.1820, Test: 0.1750
Epoch: 4, Loss: 4.7895, Train: 0.2786, Val: 0.1900, Test: 0.1900
Epoch: 5, Loss: 4.7554, Train: 0.3071, Val: 0.2040, Test: 0.2040
Epoch: 6, Loss: 4.7092, Train: 0.3429, Val: 0.2180, Test: 0.2160
Epoch: 7, Loss: 4.6384, Train: 0.3500, Val: 0.2260, Test: 0.2180
Epoch: 8, Loss: 4.6151, Train: 0.3643, Val: 0.2260, Test: 0.2220
Epoch: 9, Loss: 4.5518, Train: 0.3643, Val: 0.2260, Test: 0.2230
Epoch: 10, Loss: 4.5474, Train: 0.3786, Val: 0.2360, Test: 0.2290
Epoch: 11, Loss: 4.5722, Train: 0.3929, Val: 0.2440, Test: 0.2380
Epoch: 12, Loss: 4.3908, Train: 0.4214, Val: 0.2620, Test: 0.2480
Epoch: 13, Loss: 4.2746, Train: 0.4286, Val: 0.2820, Test: 0.2610
Epoch: 14, Loss: 4.2941, Train: 0.4857, Val: 0.3040, Test: 0.3000
Epoch: 15, Loss: 4.2848, Train: 0.5714, Val: 0.3620, Test: 0.3770
Epoch: 16, Loss: 4.4301, Train: 0.6500, Val: 0.4860, Test: 0.5000
Epoch: 17, Loss: 4.2021, Train: 0.7214, Val: 0.5940, Test: 0.6060
Epoch: 18, Loss: 4.3523, Train: 0.8357, Val: 0.6800, Test: 0.6910
Epoch: 19, Loss: 4.1368, Train: 0.9286, Val: 0.7520, Test: 0.7650
Epoch: 20, Loss: 4.3603, Train: 0.9429, Val: 0.7820, Test: 0.7950
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 21, Loss: 4.3719, Train: 0.9500, Val: 0.7840, Test: 0.8090
Epoch: 22, Loss: 4.1109, Train: 0.9500, Val: 0.7680, Test: 0.7940
Epoch: 23, Loss: 4.1052, Train: 0.9500, Val: 0.7660, Test: 0.7700
Epoch: 24, Loss: 4.0447, Train: 0.9357, Val: 0.7420, Test: 0.7530
Epoch: 25, Loss: 4.1506, Train: 0.9357, Val: 0.7280, Test: 0.7370
Epoch: 26, Loss: 4.0260, Train: 0.9429, Val: 0.7320, Test: 0.7350
Epoch: 27, Loss: 4.2726, Train: 0.9429, Val: 0.7320, Test: 0.7430
Epoch: 28, Loss: 3.8921, Train: 0.9571, Val: 0.7500, Test: 0.7590
Epoch: 29, Loss: 3.7446, Train: 0.9571, Val: 0.7620, Test: 0.7690
Epoch: 30, Loss: 3.9163, Train: 0.9643, Val: 0.7740, Test: 0.7820
Epoch: 31, Loss: 4.1560, Train: 0.9643, Val: 0.7740, Test: 0.7930
Epoch: 32, Loss: 4.0880, Train: 0.9643, Val: 0.7760, Test: 0.8070
Epoch: 33, Loss: 3.8554, Train: 0.9714, Val: 0.7800, Test: 0.8120
Epoch: 34, Loss: 4.0127, Train: 0.9786, Val: 0.7920, Test: 0.8190
Epoch: 35, Loss: 3.8398, Train: 0.9786, Val: 0.7960, Test: 0.8160
Epoch: 36, Loss: 3.9341, Train: 0.9714, Val: 0.8000, Test: 0.8180
Epoch: 37, Loss: 3.9466, Train: 0.9714, Val: 0.8040, Test: 0.8200
Epoch: 38, Loss: 3.9008, Train: 0.9714, Val: 0.8060, Test: 0.8270
Epoch: 39, Loss: 3.6948, Train: 0.9714, Val: 0.8080, Test: 0.8280
Epoch: 40, Loss: 3.5884, Train: 0.9643, Val: 0.8020, Test: 0.8250
Epoch: 41, Loss: 3.9294, Train: 0.9643, Val: 0.8020, Test: 0.8220
Epoch: 42, Loss: 3.9729, Train: 0.9786, Val: 0.7980, Test: 0.8160
Epoch: 43, Loss: 4.0372, Train: 0.9786, Val: 0.7900, Test: 0.8160
Epoch: 44, Loss: 4.0505, Train: 0.9786, Val: 0.7920, Test: 0.8120
Epoch: 45, Loss: 3.9678, Train: 0.9786, Val: 0.7800, Test: 0.8100
Epoch: 46, Loss: 3.8339, Train: 0.9857, Val: 0.7780, Test: 0.8140
Epoch: 47, Loss: 3.9768, Train: 0.9857, Val: 0.7820, Test: 0.8090
Epoch: 48, Loss: 3.8417, Train: 0.9929, Val: 0.7880, Test: 0.8060
Epoch: 49, Loss: 3.8734, Train: 0.9929, Val: 0.7900, Test: 0.7970
Epoch: 50, Loss: 3.6437, Train: 0.9929, Val: 0.7900, Test: 0.7940
MAD:  0.2121
Best Test Accuracy: 0.8280, Val Accuracy: 0.8080, Train Accuracy: 0.9714
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8524, Train: 0.2143, Val: 0.1000, Test: 0.1050
Epoch: 2, Loss: 4.8385, Train: 0.2786, Val: 0.1680, Test: 0.1690
Epoch: 3, Loss: 4.8377, Train: 0.2786, Val: 0.1960, Test: 0.1960
Epoch: 4, Loss: 4.7874, Train: 0.3071, Val: 0.2080, Test: 0.2150
Epoch: 5, Loss: 4.7721, Train: 0.3214, Val: 0.2060, Test: 0.2130
Epoch: 6, Loss: 4.6903, Train: 0.3214, Val: 0.2040, Test: 0.2190
Epoch: 7, Loss: 4.7261, Train: 0.3429, Val: 0.2040, Test: 0.2260
Epoch: 8, Loss: 4.7112, Train: 0.3643, Val: 0.2180, Test: 0.2320
Epoch: 9, Loss: 4.6212, Train: 0.3643, Val: 0.2180, Test: 0.2350
Epoch: 10, Loss: 4.6125, Train: 0.3857, Val: 0.2320, Test: 0.2430
Epoch: 11, Loss: 4.6167, Train: 0.4071, Val: 0.2460, Test: 0.2470
Epoch: 12, Loss: 4.6442, Train: 0.4143, Val: 0.2540, Test: 0.2590
Epoch: 13, Loss: 4.5158, Train: 0.4143, Val: 0.2600, Test: 0.2710
Epoch: 14, Loss: 4.5035, Train: 0.4286, Val: 0.2740, Test: 0.2850
Epoch: 15, Loss: 4.3098, Train: 0.4357, Val: 0.2800, Test: 0.2890
Epoch: 16, Loss: 4.3464, Train: 0.4357, Val: 0.2800, Test: 0.2890
Epoch: 17, Loss: 4.2417, Train: 0.4500, Val: 0.2920, Test: 0.2940
Epoch: 18, Loss: 4.1716, Train: 0.4857, Val: 0.3160, Test: 0.3270
Epoch: 19, Loss: 4.3639, Train: 0.6000, Val: 0.4100, Test: 0.4080
Epoch: 20, Loss: 4.4194, Train: 0.7857, Val: 0.5620, Test: 0.5570
Epoch: 21, Loss: 4.1023, Train: 0.8500, Val: 0.6620, Test: 0.6880
Epoch: 22, Loss: 4.2503, Train: 0.9071, Val: 0.7400, Test: 0.7710
Epoch: 23, Loss: 4.1396, Train: 0.9071, Val: 0.7720, Test: 0.7780
Epoch: 24, Loss: 4.1922, Train: 0.9143, Val: 0.7720, Test: 0.7800
Epoch: 25, Loss: 4.2496, Train: 0.8929, Val: 0.7600, Test: 0.7720
Epoch: 26, Loss: 4.1644, Train: 0.8857, Val: 0.7680, Test: 0.7780
Epoch: 27, Loss: 4.2295, Train: 0.8929, Val: 0.7740, Test: 0.7780
Epoch: 28, Loss: 4.1326, Train: 0.9071, Val: 0.7660, Test: 0.7790
Epoch: 29, Loss: 3.9206, Train: 0.9143, Val: 0.7660, Test: 0.7730
Epoch: 30, Loss: 4.1808, Train: 0.9286, Val: 0.7660, Test: 0.7720
Epoch: 31, Loss: 3.9640, Train: 0.9357, Val: 0.7760, Test: 0.7770
Epoch: 32, Loss: 4.1495, Train: 0.9429, Val: 0.7720, Test: 0.7860
Epoch: 33, Loss: 3.9454, Train: 0.9357, Val: 0.7780, Test: 0.7750
Epoch: 34, Loss: 4.0536, Train: 0.9429, Val: 0.7740, Test: 0.7720
Epoch: 35, Loss: 3.8049, Train: 0.9429, Val: 0.7780, Test: 0.7730
Epoch: 36, Loss: 4.0769, Train: 0.9500, Val: 0.7720, Test: 0.7700
Epoch: 37, Loss: 4.1823, Train: 0.9571, Val: 0.7780, Test: 0.7720
Epoch: 38, Loss: 3.9654, Train: 0.9571, Val: 0.7700, Test: 0.7770
Epoch: 39, Loss: 3.9297, Train: 0.9714, Val: 0.7740, Test: 0.7810
Epoch: 40, Loss: 3.8703, Train: 0.9714, Val: 0.7860, Test: 0.7850
Epoch: 41, Loss: 3.6910, Train: 0.9786, Val: 0.7900, Test: 0.7930
Epoch: 42, Loss: 3.9929, Train: 0.9786, Val: 0.7920, Test: 0.7910
Epoch: 43, Loss: 3.8687, Train: 0.9786, Val: 0.8000, Test: 0.7940
Epoch: 44, Loss: 3.6894, Train: 0.9857, Val: 0.8000, Test: 0.7930
Epoch: 45, Loss: 3.8522, Train: 0.9857, Val: 0.7980, Test: 0.8000
Epoch: 46, Loss: 3.9558, Train: 0.9857, Val: 0.7940, Test: 0.7980
Epoch: 47, Loss: 3.7987, Train: 0.9857, Val: 0.7940, Test: 0.7970
Epoch: 48, Loss: 3.9263, Train: 0.9857, Val: 0.8000, Test: 0.7980
Epoch: 49, Loss: 4.3489, Train: 0.9857, Val: 0.7960, Test: 0.7990
Epoch: 50, Loss: 3.7130, Train: 0.9857, Val: 0.7920, Test: 0.8020
MAD:  0.3645
Best Test Accuracy: 0.8020, Val Accuracy: 0.7920, Train Accuracy: 0.9857
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8454, Train: 0.1714, Val: 0.0900, Test: 0.0990
Epoch: 2, Loss: 4.8273, Train: 0.2214, Val: 0.1020, Test: 0.1220
Epoch: 3, Loss: 4.8130, Train: 0.3143, Val: 0.1440, Test: 0.1670
Epoch: 4, Loss: 4.7600, Train: 0.4214, Val: 0.1860, Test: 0.2140
Epoch: 5, Loss: 4.7880, Train: 0.5286, Val: 0.2520, Test: 0.3010
Epoch: 6, Loss: 4.7261, Train: 0.5786, Val: 0.3340, Test: 0.3910
Epoch: 7, Loss: 4.6767, Train: 0.6929, Val: 0.4360, Test: 0.4600
Epoch: 8, Loss: 4.6646, Train: 0.7143, Val: 0.4860, Test: 0.5220
Epoch: 9, Loss: 4.6291, Train: 0.7286, Val: 0.5280, Test: 0.5560
Epoch: 10, Loss: 4.6157, Train: 0.7357, Val: 0.5480, Test: 0.5750
Epoch: 11, Loss: 4.5855, Train: 0.7429, Val: 0.5680, Test: 0.5950
Epoch: 12, Loss: 4.4674, Train: 0.7429, Val: 0.5760, Test: 0.5910
Epoch: 13, Loss: 4.4490, Train: 0.7429, Val: 0.5880, Test: 0.5950
Epoch: 14, Loss: 4.2587, Train: 0.7571, Val: 0.5940, Test: 0.6110
Epoch: 15, Loss: 4.4460, Train: 0.7786, Val: 0.6000, Test: 0.6140
Epoch: 16, Loss: 4.5027, Train: 0.8000, Val: 0.6100, Test: 0.6240
Epoch: 17, Loss: 4.2654, Train: 0.8143, Val: 0.6020, Test: 0.6240
Epoch: 18, Loss: 4.2446, Train: 0.8071, Val: 0.5860, Test: 0.6150
Epoch: 19, Loss: 4.1913, Train: 0.8000, Val: 0.5680, Test: 0.5900
Epoch: 20, Loss: 4.1418, Train: 0.7929, Val: 0.5540, Test: 0.5680
Epoch: 21, Loss: 4.3469, Train: 0.8000, Val: 0.5560, Test: 0.5750
Epoch: 22, Loss: 4.2288, Train: 0.7929, Val: 0.5600, Test: 0.5820
Epoch: 23, Loss: 4.3076, Train: 0.8000, Val: 0.5660, Test: 0.5770
Epoch: 24, Loss: 4.1528, Train: 0.8214, Val: 0.5600, Test: 0.5820
Epoch: 25, Loss: 4.0338, Train: 0.8214, Val: 0.5680, Test: 0.5890
Epoch: 26, Loss: 4.2811, Train: 0.8286, Val: 0.5820, Test: 0.6050
Epoch: 27, Loss: 4.2609, Train: 0.8286, Val: 0.5920, Test: 0.6230
Epoch: 28, Loss: 4.2433, Train: 0.8357, Val: 0.6120, Test: 0.6390
Epoch: 29, Loss: 4.2483, Train: 0.8786, Val: 0.6380, Test: 0.6690
Epoch: 30, Loss: 4.2353, Train: 0.9357, Val: 0.6780, Test: 0.7080
Epoch: 31, Loss: 3.9160, Train: 0.9500, Val: 0.7100, Test: 0.7460
Epoch: 32, Loss: 4.2589, Train: 0.9786, Val: 0.7400, Test: 0.7780
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 33, Loss: 3.8889, Train: 0.9786, Val: 0.7680, Test: 0.7950
Epoch: 34, Loss: 4.0943, Train: 0.9857, Val: 0.7820, Test: 0.8000
Epoch: 35, Loss: 4.0030, Train: 0.9857, Val: 0.7920, Test: 0.8080
Epoch: 36, Loss: 3.9850, Train: 0.9857, Val: 0.7920, Test: 0.8150
Epoch: 37, Loss: 3.9820, Train: 0.9857, Val: 0.7940, Test: 0.8200
Epoch: 38, Loss: 3.8464, Train: 0.9857, Val: 0.8020, Test: 0.8170
Epoch: 39, Loss: 3.9284, Train: 0.9857, Val: 0.8040, Test: 0.8180
Epoch: 40, Loss: 3.9432, Train: 0.9714, Val: 0.8040, Test: 0.8180
Epoch: 41, Loss: 3.8044, Train: 0.9714, Val: 0.8020, Test: 0.8160
Epoch: 42, Loss: 3.8609, Train: 0.9714, Val: 0.8020, Test: 0.8120
Epoch: 43, Loss: 4.1144, Train: 0.9857, Val: 0.7980, Test: 0.8020
Epoch: 44, Loss: 4.0331, Train: 0.9857, Val: 0.7940, Test: 0.7990
Epoch: 45, Loss: 3.7414, Train: 0.9857, Val: 0.7880, Test: 0.7990
Epoch: 46, Loss: 3.7387, Train: 0.9857, Val: 0.7880, Test: 0.7940
Epoch: 47, Loss: 3.8998, Train: 0.9857, Val: 0.7820, Test: 0.7860
Epoch: 48, Loss: 4.0973, Train: 0.9857, Val: 0.7780, Test: 0.7880
Epoch: 49, Loss: 4.0786, Train: 0.9857, Val: 0.7820, Test: 0.7800
Epoch: 50, Loss: 3.7073, Train: 0.9857, Val: 0.7740, Test: 0.7780
MAD:  0.1829
Best Test Accuracy: 0.8200, Val Accuracy: 0.7940, Train Accuracy: 0.9857
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8496, Train: 0.1643, Val: 0.1760, Test: 0.1970
Epoch: 2, Loss: 4.8367, Train: 0.2571, Val: 0.3020, Test: 0.3090
Epoch: 3, Loss: 4.8276, Train: 0.2786, Val: 0.3380, Test: 0.3490
Epoch: 4, Loss: 4.8089, Train: 0.3000, Val: 0.3460, Test: 0.3540
Epoch: 5, Loss: 4.7558, Train: 0.3071, Val: 0.3480, Test: 0.3640
Epoch: 6, Loss: 4.7513, Train: 0.3143, Val: 0.3620, Test: 0.3810
Epoch: 7, Loss: 4.7118, Train: 0.3214, Val: 0.3680, Test: 0.3960
Epoch: 8, Loss: 4.6129, Train: 0.3500, Val: 0.3640, Test: 0.4020
Epoch: 9, Loss: 4.5541, Train: 0.3643, Val: 0.3420, Test: 0.3910
Epoch: 10, Loss: 4.6263, Train: 0.3714, Val: 0.3280, Test: 0.3830
Epoch: 11, Loss: 4.5580, Train: 0.3714, Val: 0.3180, Test: 0.3740
Epoch: 12, Loss: 4.5662, Train: 0.3714, Val: 0.3100, Test: 0.3710
Epoch: 13, Loss: 4.4187, Train: 0.3786, Val: 0.3140, Test: 0.3720
Epoch: 14, Loss: 4.4190, Train: 0.3857, Val: 0.3240, Test: 0.3760
Epoch: 15, Loss: 4.2945, Train: 0.4071, Val: 0.3400, Test: 0.3920
Epoch: 16, Loss: 4.2576, Train: 0.4500, Val: 0.3600, Test: 0.4050
Epoch: 17, Loss: 4.3279, Train: 0.5143, Val: 0.3760, Test: 0.4240
Epoch: 18, Loss: 4.4563, Train: 0.5643, Val: 0.4080, Test: 0.4640
Epoch: 19, Loss: 4.3605, Train: 0.6714, Val: 0.4460, Test: 0.5120
Epoch: 20, Loss: 4.2339, Train: 0.7357, Val: 0.5000, Test: 0.5600
Epoch: 21, Loss: 4.1346, Train: 0.8929, Val: 0.5780, Test: 0.6350
Epoch: 22, Loss: 4.0923, Train: 0.9071, Val: 0.6340, Test: 0.7170
Epoch: 23, Loss: 3.9951, Train: 0.9571, Val: 0.6960, Test: 0.7580
Epoch: 24, Loss: 4.2780, Train: 0.9714, Val: 0.7180, Test: 0.7700
Epoch: 25, Loss: 4.2957, Train: 0.9643, Val: 0.7340, Test: 0.7680
Epoch: 26, Loss: 4.0983, Train: 0.9643, Val: 0.7320, Test: 0.7610
Epoch: 27, Loss: 3.9460, Train: 0.9643, Val: 0.7360, Test: 0.7550
Epoch: 28, Loss: 4.2058, Train: 0.9714, Val: 0.7360, Test: 0.7520
Epoch: 29, Loss: 4.2169, Train: 0.9714, Val: 0.7340, Test: 0.7540
Epoch: 30, Loss: 3.9660, Train: 0.9714, Val: 0.7360, Test: 0.7680
Epoch: 31, Loss: 4.0611, Train: 0.9643, Val: 0.7420, Test: 0.7670
Epoch: 32, Loss: 3.9898, Train: 0.9714, Val: 0.7380, Test: 0.7720
Epoch: 33, Loss: 4.0203, Train: 0.9714, Val: 0.7440, Test: 0.7790
Epoch: 34, Loss: 3.9114, Train: 0.9714, Val: 0.7580, Test: 0.7980
Epoch: 35, Loss: 4.0817, Train: 0.9714, Val: 0.7640, Test: 0.7980
Epoch: 36, Loss: 3.7463, Train: 0.9714, Val: 0.7620, Test: 0.8000
Epoch: 37, Loss: 3.7959, Train: 0.9714, Val: 0.7600, Test: 0.7990
Epoch: 38, Loss: 3.9043, Train: 0.9714, Val: 0.7600, Test: 0.8050
Epoch: 39, Loss: 4.1447, Train: 0.9786, Val: 0.7620, Test: 0.8080
Epoch: 40, Loss: 3.9290, Train: 0.9786, Val: 0.7660, Test: 0.8110
Epoch: 41, Loss: 3.8046, Train: 0.9786, Val: 0.7720, Test: 0.8100
Epoch: 42, Loss: 3.9070, Train: 0.9786, Val: 0.7740, Test: 0.8070
Epoch: 43, Loss: 3.7985, Train: 0.9786, Val: 0.7760, Test: 0.8100
Epoch: 44, Loss: 3.8437, Train: 0.9714, Val: 0.7780, Test: 0.8060
Epoch: 45, Loss: 3.8560, Train: 0.9714, Val: 0.7800, Test: 0.8080
Epoch: 46, Loss: 3.8106, Train: 0.9786, Val: 0.7860, Test: 0.8060
Epoch: 47, Loss: 3.8665, Train: 0.9786, Val: 0.7860, Test: 0.8030
Epoch: 48, Loss: 4.0015, Train: 0.9786, Val: 0.7760, Test: 0.8030
Epoch: 49, Loss: 3.6118, Train: 0.9857, Val: 0.7760, Test: 0.8040
Epoch: 50, Loss: 3.7833, Train: 0.9857, Val: 0.7760, Test: 0.8030
MAD:  0.1952
Best Test Accuracy: 0.8110, Val Accuracy: 0.7660, Train Accuracy: 0.9786
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8600, Train: 0.1071, Val: 0.0600, Test: 0.0790
Epoch: 2, Loss: 4.8439, Train: 0.1857, Val: 0.1560, Test: 0.1680
Epoch: 3, Loss: 4.8302, Train: 0.1857, Val: 0.1540, Test: 0.1520
Epoch: 4, Loss: 4.8145, Train: 0.1929, Val: 0.1520, Test: 0.1430
Epoch: 5, Loss: 4.7928, Train: 0.1857, Val: 0.1440, Test: 0.1430
Epoch: 6, Loss: 4.7734, Train: 0.1786, Val: 0.1440, Test: 0.1440
Epoch: 7, Loss: 4.7380, Train: 0.1786, Val: 0.1380, Test: 0.1400
Epoch: 8, Loss: 4.7122, Train: 0.1786, Val: 0.1360, Test: 0.1400
Epoch: 9, Loss: 4.6886, Train: 0.1786, Val: 0.1340, Test: 0.1410
Epoch: 10, Loss: 4.6935, Train: 0.1786, Val: 0.1360, Test: 0.1430
Epoch: 11, Loss: 4.6612, Train: 0.1929, Val: 0.1420, Test: 0.1480
Epoch: 12, Loss: 4.6703, Train: 0.2286, Val: 0.1640, Test: 0.1560
Epoch: 13, Loss: 4.5318, Train: 0.2714, Val: 0.1780, Test: 0.1730
Epoch: 14, Loss: 4.7106, Train: 0.2786, Val: 0.2020, Test: 0.1990
Epoch: 15, Loss: 4.5029, Train: 0.3214, Val: 0.2240, Test: 0.2190
Epoch: 16, Loss: 4.6407, Train: 0.3643, Val: 0.2760, Test: 0.2760
Epoch: 17, Loss: 4.5717, Train: 0.4071, Val: 0.3280, Test: 0.3530
Epoch: 18, Loss: 4.4060, Train: 0.4286, Val: 0.3640, Test: 0.3980
Epoch: 19, Loss: 4.5004, Train: 0.4571, Val: 0.4160, Test: 0.4420
Epoch: 20, Loss: 4.4294, Train: 0.5071, Val: 0.4400, Test: 0.4880
Epoch: 21, Loss: 4.4479, Train: 0.5714, Val: 0.4980, Test: 0.5220
Epoch: 22, Loss: 4.4152, Train: 0.6857, Val: 0.5600, Test: 0.5770
Epoch: 23, Loss: 4.4100, Train: 0.7500, Val: 0.6160, Test: 0.6420
Epoch: 24, Loss: 4.2298, Train: 0.7714, Val: 0.6580, Test: 0.6820
Epoch: 25, Loss: 4.2788, Train: 0.7786, Val: 0.6660, Test: 0.6940
Epoch: 26, Loss: 4.2022, Train: 0.8143, Val: 0.6760, Test: 0.6980
Epoch: 27, Loss: 4.3727, Train: 0.8214, Val: 0.6720, Test: 0.6990
Epoch: 28, Loss: 4.0843, Train: 0.8214, Val: 0.6740, Test: 0.6980
Epoch: 29, Loss: 4.2812, Train: 0.8286, Val: 0.6700, Test: 0.7040
Epoch: 30, Loss: 4.0883, Train: 0.8214, Val: 0.6740, Test: 0.7050
Epoch: 31, Loss: 4.0879, Train: 0.8286, Val: 0.6720, Test: 0.7090
Epoch: 32, Loss: 3.8829, Train: 0.8286, Val: 0.6840, Test: 0.7120
Epoch: 33, Loss: 4.0164, Train: 0.8571, Val: 0.6900, Test: 0.7190
Epoch: 34, Loss: 3.9295, Train: 0.8929, Val: 0.6860, Test: 0.7280
Epoch: 35, Loss: 3.9624, Train: 0.9286, Val: 0.6940, Test: 0.7390
Epoch: 36, Loss: 4.0949, Train: 0.9357, Val: 0.7100, Test: 0.7520
Epoch: 37, Loss: 4.2546, Train: 0.9429, Val: 0.7240, Test: 0.7680
Epoch: 38, Loss: 4.1836, Train: 0.9500, Val: 0.7440, Test: 0.7800
Epoch: 39, Loss: 3.9973, Train: 0.9500, Val: 0.7620, Test: 0.7920
Epoch: 40, Loss: 3.9735, Train: 0.9714, Val: 0.7700, Test: 0.8050
Epoch: 41, Loss: 4.1751, Train: 0.9786, Val: 0.7740, Test: 0.7990
Epoch: 42, Loss: 3.9584, Train: 0.9786, Val: 0.7500, Test: 0.8020
Epoch: 43, Loss: 3.8493, Train: 0.9714, Val: 0.7440, Test: 0.7990
Epoch: 44, Loss: 4.0585, Train: 0.9714, Val: 0.7520, Test: 0.7970
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 45, Loss: 3.7958, Train: 0.9714, Val: 0.7540, Test: 0.7980
Epoch: 46, Loss: 3.8993, Train: 0.9714, Val: 0.7580, Test: 0.7960
Epoch: 47, Loss: 3.8451, Train: 0.9857, Val: 0.7620, Test: 0.7950
Epoch: 48, Loss: 4.0258, Train: 0.9857, Val: 0.7660, Test: 0.7990
Epoch: 49, Loss: 3.9476, Train: 0.9857, Val: 0.7640, Test: 0.7980
Epoch: 50, Loss: 3.8850, Train: 0.9857, Val: 0.7600, Test: 0.7970
MAD:  0.1937
Best Test Accuracy: 0.8050, Val Accuracy: 0.7700, Train Accuracy: 0.9714
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8535, Train: 0.1643, Val: 0.0940, Test: 0.1030
Epoch: 2, Loss: 4.8378, Train: 0.2643, Val: 0.1320, Test: 0.1550
Epoch: 3, Loss: 4.8294, Train: 0.3429, Val: 0.1940, Test: 0.2110
Epoch: 4, Loss: 4.8162, Train: 0.4286, Val: 0.2320, Test: 0.2560
Epoch: 5, Loss: 4.7736, Train: 0.4214, Val: 0.2380, Test: 0.2660
Epoch: 6, Loss: 4.7151, Train: 0.4214, Val: 0.2480, Test: 0.2680
Epoch: 7, Loss: 4.7135, Train: 0.4143, Val: 0.2480, Test: 0.2630
Epoch: 8, Loss: 4.6188, Train: 0.3929, Val: 0.2300, Test: 0.2470
Epoch: 9, Loss: 4.6055, Train: 0.3929, Val: 0.2240, Test: 0.2440
Epoch: 10, Loss: 4.5820, Train: 0.4143, Val: 0.2280, Test: 0.2450
Epoch: 11, Loss: 4.4473, Train: 0.4071, Val: 0.2200, Test: 0.2430
Epoch: 12, Loss: 4.4675, Train: 0.4000, Val: 0.2180, Test: 0.2460
Epoch: 13, Loss: 4.4993, Train: 0.4143, Val: 0.2180, Test: 0.2430
Epoch: 14, Loss: 4.2170, Train: 0.4500, Val: 0.2280, Test: 0.2500
Epoch: 15, Loss: 4.3594, Train: 0.4786, Val: 0.2540, Test: 0.2810
Epoch: 16, Loss: 4.2932, Train: 0.5500, Val: 0.2780, Test: 0.3150
Epoch: 17, Loss: 4.3986, Train: 0.6000, Val: 0.3260, Test: 0.3500
Epoch: 18, Loss: 4.3790, Train: 0.6571, Val: 0.3840, Test: 0.3950
Epoch: 19, Loss: 4.2138, Train: 0.7357, Val: 0.4560, Test: 0.4640
Epoch: 20, Loss: 4.2289, Train: 0.8571, Val: 0.5920, Test: 0.6110
Epoch: 21, Loss: 4.3490, Train: 0.9214, Val: 0.6940, Test: 0.7300
Epoch: 22, Loss: 4.1213, Train: 0.9571, Val: 0.7620, Test: 0.7980
Epoch: 23, Loss: 3.9290, Train: 0.9643, Val: 0.7800, Test: 0.8180
Epoch: 24, Loss: 4.1939, Train: 0.9643, Val: 0.7640, Test: 0.8150
Epoch: 25, Loss: 4.2780, Train: 0.9571, Val: 0.7520, Test: 0.8080
Epoch: 26, Loss: 4.0138, Train: 0.9500, Val: 0.7380, Test: 0.7930
Epoch: 27, Loss: 4.0364, Train: 0.9500, Val: 0.7440, Test: 0.7810
Epoch: 28, Loss: 3.8150, Train: 0.9500, Val: 0.7400, Test: 0.7670
Epoch: 29, Loss: 4.2633, Train: 0.9500, Val: 0.7400, Test: 0.7610
Epoch: 30, Loss: 4.0514, Train: 0.9571, Val: 0.7400, Test: 0.7640
Epoch: 31, Loss: 4.0809, Train: 0.9429, Val: 0.7500, Test: 0.7650
Epoch: 32, Loss: 4.1994, Train: 0.9571, Val: 0.7580, Test: 0.7700
Epoch: 33, Loss: 3.9470, Train: 0.9571, Val: 0.7700, Test: 0.7790
Epoch: 34, Loss: 4.1674, Train: 0.9643, Val: 0.7720, Test: 0.7770
Epoch: 35, Loss: 4.0665, Train: 0.9714, Val: 0.7700, Test: 0.7900
Epoch: 36, Loss: 3.8565, Train: 0.9714, Val: 0.7760, Test: 0.7940
Epoch: 37, Loss: 3.7957, Train: 0.9714, Val: 0.7780, Test: 0.7980
Epoch: 38, Loss: 4.0683, Train: 0.9786, Val: 0.7760, Test: 0.8000
Epoch: 39, Loss: 3.9338, Train: 0.9786, Val: 0.7840, Test: 0.7990
Epoch: 40, Loss: 3.5413, Train: 0.9857, Val: 0.7840, Test: 0.8020
Epoch: 41, Loss: 4.2396, Train: 0.9857, Val: 0.7860, Test: 0.8010
Epoch: 42, Loss: 4.1702, Train: 0.9857, Val: 0.7880, Test: 0.8010
Epoch: 43, Loss: 4.2614, Train: 0.9857, Val: 0.7860, Test: 0.8020
Epoch: 44, Loss: 3.7254, Train: 0.9857, Val: 0.7760, Test: 0.8030
Epoch: 45, Loss: 3.7071, Train: 0.9857, Val: 0.7760, Test: 0.8070
Epoch: 46, Loss: 4.0007, Train: 0.9857, Val: 0.7780, Test: 0.8150
Epoch: 47, Loss: 3.9136, Train: 0.9857, Val: 0.7820, Test: 0.8220
Epoch: 48, Loss: 3.6569, Train: 0.9857, Val: 0.7800, Test: 0.8230
Epoch: 49, Loss: 3.5696, Train: 0.9857, Val: 0.7820, Test: 0.8220
Epoch: 50, Loss: 3.8075, Train: 0.9929, Val: 0.7780, Test: 0.8240
MAD:  0.3665
Best Test Accuracy: 0.8240, Val Accuracy: 0.7780, Train Accuracy: 0.9929
Training completed.
Average Test Accuracy:  0.8144 ± 0.011429785649783602
Average MAD:  0.26962 ± 0.07924748324079446

/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9494, Train: 0.2214, Val: 0.1880, Test: 0.2080
Epoch: 2, Loss: 1.9395, Train: 0.2929, Val: 0.2020, Test: 0.2390
Epoch: 3, Loss: 1.9327, Train: 0.4214, Val: 0.2420, Test: 0.2520
Epoch: 4, Loss: 1.9207, Train: 0.5071, Val: 0.2420, Test: 0.2700
Epoch: 5, Loss: 1.9165, Train: 0.5214, Val: 0.2620, Test: 0.2780
Epoch: 6, Loss: 1.9048, Train: 0.5643, Val: 0.2800, Test: 0.2800
Epoch: 7, Loss: 1.8972, Train: 0.5929, Val: 0.2900, Test: 0.2830
Epoch: 8, Loss: 1.8737, Train: 0.5857, Val: 0.2780, Test: 0.2890
Epoch: 9, Loss: 1.8779, Train: 0.5929, Val: 0.2900, Test: 0.3070
Epoch: 10, Loss: 1.7901, Train: 0.6214, Val: 0.3140, Test: 0.3250
Epoch: 11, Loss: 1.7903, Train: 0.6286, Val: 0.3320, Test: 0.3390
Epoch: 12, Loss: 1.8138, Train: 0.6286, Val: 0.3520, Test: 0.3600
Epoch: 13, Loss: 1.8249, Train: 0.6714, Val: 0.3780, Test: 0.3860
Epoch: 14, Loss: 1.7555, Train: 0.6857, Val: 0.3960, Test: 0.4080
Epoch: 15, Loss: 1.7415, Train: 0.6929, Val: 0.4080, Test: 0.4260
Epoch: 16, Loss: 1.6445, Train: 0.7071, Val: 0.4180, Test: 0.4550
Epoch: 17, Loss: 1.6180, Train: 0.7214, Val: 0.4400, Test: 0.4790
Epoch: 18, Loss: 1.6972, Train: 0.7429, Val: 0.4760, Test: 0.5020
Epoch: 19, Loss: 1.6316, Train: 0.7643, Val: 0.5000, Test: 0.5250
Epoch: 20, Loss: 1.5913, Train: 0.8071, Val: 0.5240, Test: 0.5430
Epoch: 21, Loss: 1.5972, Train: 0.8286, Val: 0.5420, Test: 0.5590
Epoch: 22, Loss: 1.5329, Train: 0.8286, Val: 0.5620, Test: 0.5820
Epoch: 23, Loss: 1.5328, Train: 0.8500, Val: 0.5820, Test: 0.6050
Epoch: 24, Loss: 1.5872, Train: 0.8714, Val: 0.5920, Test: 0.6230
Epoch: 25, Loss: 1.4738, Train: 0.8857, Val: 0.6300, Test: 0.6330
Epoch: 26, Loss: 1.5038, Train: 0.9000, Val: 0.6380, Test: 0.6360
Epoch: 27, Loss: 1.4745, Train: 0.9071, Val: 0.6420, Test: 0.6560
Epoch: 28, Loss: 1.4531, Train: 0.9143, Val: 0.6580, Test: 0.6740
Epoch: 29, Loss: 1.4717, Train: 0.9286, Val: 0.6680, Test: 0.6840
Epoch: 30, Loss: 1.3766, Train: 0.9500, Val: 0.6760, Test: 0.6930
Epoch: 31, Loss: 1.3671, Train: 0.9571, Val: 0.6840, Test: 0.6950
Epoch: 32, Loss: 1.3550, Train: 0.9643, Val: 0.6880, Test: 0.7070
Epoch: 33, Loss: 1.3313, Train: 0.9786, Val: 0.7080, Test: 0.7220
Epoch: 34, Loss: 1.2892, Train: 0.9786, Val: 0.7240, Test: 0.7290
Epoch: 35, Loss: 1.3794, Train: 0.9857, Val: 0.7320, Test: 0.7410
Epoch: 36, Loss: 1.3134, Train: 0.9857, Val: 0.7400, Test: 0.7470
Epoch: 37, Loss: 1.2348, Train: 0.9857, Val: 0.7520, Test: 0.7560
Epoch: 38, Loss: 1.2837, Train: 0.9857, Val: 0.7480, Test: 0.7680
Epoch: 39, Loss: 1.1575, Train: 0.9786, Val: 0.7540, Test: 0.7710
Epoch: 40, Loss: 1.1826, Train: 0.9786, Val: 0.7560, Test: 0.7840
Epoch: 41, Loss: 1.1092, Train: 0.9929, Val: 0.7560, Test: 0.7890
Epoch: 42, Loss: 1.1955, Train: 0.9929, Val: 0.7580, Test: 0.7950
Epoch: 43, Loss: 1.1276, Train: 1.0000, Val: 0.7600, Test: 0.8000
Epoch: 44, Loss: 1.0436, Train: 1.0000, Val: 0.7640, Test: 0.8040
Epoch: 45, Loss: 1.1852, Train: 1.0000, Val: 0.7680, Test: 0.8050
Epoch: 46, Loss: 1.0478, Train: 1.0000, Val: 0.7680, Test: 0.8060
Epoch: 47, Loss: 1.1375, Train: 1.0000, Val: 0.7700, Test: 0.8040
Epoch: 48, Loss: 1.0965, Train: 1.0000, Val: 0.7720, Test: 0.8040
Epoch: 49, Loss: 1.1714, Train: 1.0000, Val: 0.7760, Test: 0.8080
Epoch: 50, Loss: 1.0949, Train: 1.0000, Val: 0.7840, Test: 0.8030
MAD:  0.5507
Best Test Accuracy: 0.8080, Val Accuracy: 0.7760, Train Accuracy: 1.0000
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9622, Train: 0.1786, Val: 0.1700, Test: 0.1650
Epoch: 2, Loss: 1.9312, Train: 0.2929, Val: 0.1940, Test: 0.1810
Epoch: 3, Loss: 1.9171, Train: 0.4071, Val: 0.2340, Test: 0.2200
Epoch: 4, Loss: 1.8971, Train: 0.4786, Val: 0.2740, Test: 0.2590
Epoch: 5, Loss: 1.8763, Train: 0.5643, Val: 0.3200, Test: 0.3000
Epoch: 6, Loss: 1.8927, Train: 0.6357, Val: 0.3740, Test: 0.3380
Epoch: 7, Loss: 1.8726, Train: 0.7000, Val: 0.4000, Test: 0.3700
Epoch: 8, Loss: 1.8398, Train: 0.7643, Val: 0.4300, Test: 0.4140
Epoch: 9, Loss: 1.8482, Train: 0.8071, Val: 0.4560, Test: 0.4410
Epoch: 10, Loss: 1.7764, Train: 0.8214, Val: 0.4780, Test: 0.4610
Epoch: 11, Loss: 1.8305, Train: 0.8357, Val: 0.4920, Test: 0.4770
Epoch: 12, Loss: 1.7282, Train: 0.8571, Val: 0.5020, Test: 0.4970
Epoch: 13, Loss: 1.7229, Train: 0.8643, Val: 0.5080, Test: 0.5140
Epoch: 14, Loss: 1.7089, Train: 0.8857, Val: 0.5260, Test: 0.5280
Epoch: 15, Loss: 1.6106, Train: 0.8857, Val: 0.5420, Test: 0.5410
Epoch: 16, Loss: 1.6175, Train: 0.9000, Val: 0.5540, Test: 0.5550
Epoch: 17, Loss: 1.5791, Train: 0.9071, Val: 0.5560, Test: 0.5580
Epoch: 18, Loss: 1.5866, Train: 0.9071, Val: 0.5600, Test: 0.5640
Epoch: 19, Loss: 1.5307, Train: 0.9071, Val: 0.5700, Test: 0.5740
Epoch: 20, Loss: 1.6465, Train: 0.9143, Val: 0.5740, Test: 0.5830
Epoch: 21, Loss: 1.4936, Train: 0.9143, Val: 0.5860, Test: 0.5930
Epoch: 22, Loss: 1.5599, Train: 0.9143, Val: 0.5980, Test: 0.6060
Epoch: 23, Loss: 1.4548, Train: 0.9286, Val: 0.6060, Test: 0.6140
Epoch: 24, Loss: 1.4792, Train: 0.9571, Val: 0.6140, Test: 0.6280
Epoch: 25, Loss: 1.3066, Train: 0.9571, Val: 0.6300, Test: 0.6450
Epoch: 26, Loss: 1.2948, Train: 0.9714, Val: 0.6340, Test: 0.6510
Epoch: 27, Loss: 1.3695, Train: 0.9714, Val: 0.6360, Test: 0.6570
Epoch: 28, Loss: 1.4910, Train: 0.9786, Val: 0.6560, Test: 0.6640
Epoch: 29, Loss: 1.2801, Train: 0.9857, Val: 0.6640, Test: 0.6700
Epoch: 30, Loss: 1.3207, Train: 0.9929, Val: 0.6620, Test: 0.6800
Epoch: 31, Loss: 1.2227, Train: 0.9929, Val: 0.6620, Test: 0.6880
Epoch: 32, Loss: 1.3142, Train: 0.9929, Val: 0.6720, Test: 0.7030
Epoch: 33, Loss: 1.3392, Train: 0.9929, Val: 0.6940, Test: 0.7250
Epoch: 34, Loss: 1.3426, Train: 0.9929, Val: 0.7060, Test: 0.7290
Epoch: 35, Loss: 1.2149, Train: 1.0000, Val: 0.7120, Test: 0.7330
Epoch: 36, Loss: 1.3050, Train: 1.0000, Val: 0.7220, Test: 0.7410
Epoch: 37, Loss: 1.1052, Train: 1.0000, Val: 0.7320, Test: 0.7540
Epoch: 38, Loss: 1.1770, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 39, Loss: 1.2149, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 40, Loss: 1.1788, Train: 1.0000, Val: 0.7560, Test: 0.7830
Epoch: 41, Loss: 1.1734, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 42, Loss: 1.2380, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 43, Loss: 1.0677, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 44, Loss: 1.1386, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 45, Loss: 1.0084, Train: 1.0000, Val: 0.7620, Test: 0.7920
Epoch: 46, Loss: 1.1682, Train: 1.0000, Val: 0.7640, Test: 0.7940
Epoch: 47, Loss: 1.1184, Train: 1.0000, Val: 0.7580, Test: 0.7940
Epoch: 48, Loss: 1.0668, Train: 1.0000, Val: 0.7640, Test: 0.7930
Epoch: 49, Loss: 1.0509, Train: 1.0000, Val: 0.7620, Test: 0.7950
Epoch: 50, Loss: 1.0640, Train: 1.0000, Val: 0.7600, Test: 0.7950
MAD:  0.567
Best Test Accuracy: 0.7950, Val Accuracy: 0.7620, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9637, Train: 0.2143, Val: 0.1060, Test: 0.1050
Epoch: 2, Loss: 1.9331, Train: 0.3071, Val: 0.1160, Test: 0.1320
Epoch: 3, Loss: 1.9405, Train: 0.3929, Val: 0.1380, Test: 0.1540
Epoch: 4, Loss: 1.9101, Train: 0.4714, Val: 0.1660, Test: 0.1780
Epoch: 5, Loss: 1.9060, Train: 0.6071, Val: 0.2040, Test: 0.2060
Epoch: 6, Loss: 1.8747, Train: 0.6500, Val: 0.2420, Test: 0.2430
Epoch: 7, Loss: 1.8584, Train: 0.7071, Val: 0.2800, Test: 0.2870
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 8, Loss: 1.8211, Train: 0.7786, Val: 0.3240, Test: 0.3270
Epoch: 9, Loss: 1.7970, Train: 0.8286, Val: 0.3520, Test: 0.3680
Epoch: 10, Loss: 1.8314, Train: 0.8429, Val: 0.4060, Test: 0.4130
Epoch: 11, Loss: 1.7596, Train: 0.8929, Val: 0.4700, Test: 0.4490
Epoch: 12, Loss: 1.7256, Train: 0.9286, Val: 0.5120, Test: 0.4870
Epoch: 13, Loss: 1.7044, Train: 0.9500, Val: 0.5620, Test: 0.5240
Epoch: 14, Loss: 1.6861, Train: 0.9643, Val: 0.6080, Test: 0.5650
Epoch: 15, Loss: 1.6285, Train: 0.9643, Val: 0.6300, Test: 0.6080
Epoch: 16, Loss: 1.6366, Train: 0.9714, Val: 0.6520, Test: 0.6290
Epoch: 17, Loss: 1.5328, Train: 0.9714, Val: 0.6600, Test: 0.6450
Epoch: 18, Loss: 1.5803, Train: 0.9786, Val: 0.6660, Test: 0.6590
Epoch: 19, Loss: 1.5314, Train: 0.9857, Val: 0.6700, Test: 0.6740
Epoch: 20, Loss: 1.4763, Train: 0.9929, Val: 0.6920, Test: 0.6820
Epoch: 21, Loss: 1.6445, Train: 0.9929, Val: 0.7000, Test: 0.6890
Epoch: 22, Loss: 1.4344, Train: 0.9929, Val: 0.7120, Test: 0.7000
Epoch: 23, Loss: 1.4489, Train: 0.9929, Val: 0.7260, Test: 0.7070
Epoch: 24, Loss: 1.3848, Train: 0.9929, Val: 0.7320, Test: 0.7120
Epoch: 25, Loss: 1.4350, Train: 0.9929, Val: 0.7320, Test: 0.7190
Epoch: 26, Loss: 1.3448, Train: 0.9929, Val: 0.7380, Test: 0.7300
Epoch: 27, Loss: 1.4258, Train: 0.9929, Val: 0.7440, Test: 0.7320
Epoch: 28, Loss: 1.2994, Train: 0.9929, Val: 0.7460, Test: 0.7330
Epoch: 29, Loss: 1.4019, Train: 0.9929, Val: 0.7480, Test: 0.7350
Epoch: 30, Loss: 1.2240, Train: 0.9929, Val: 0.7520, Test: 0.7400
Epoch: 31, Loss: 1.3899, Train: 0.9929, Val: 0.7560, Test: 0.7400
Epoch: 32, Loss: 1.0860, Train: 0.9929, Val: 0.7620, Test: 0.7440
Epoch: 33, Loss: 1.2659, Train: 0.9929, Val: 0.7660, Test: 0.7490
Epoch: 34, Loss: 1.2792, Train: 0.9929, Val: 0.7660, Test: 0.7540
Epoch: 35, Loss: 1.1157, Train: 0.9929, Val: 0.7640, Test: 0.7560
Epoch: 36, Loss: 1.2365, Train: 1.0000, Val: 0.7660, Test: 0.7600
Epoch: 37, Loss: 1.2161, Train: 1.0000, Val: 0.7660, Test: 0.7650
Epoch: 38, Loss: 1.1935, Train: 1.0000, Val: 0.7740, Test: 0.7720
Epoch: 39, Loss: 1.0777, Train: 1.0000, Val: 0.7740, Test: 0.7740
Epoch: 40, Loss: 1.2038, Train: 1.0000, Val: 0.7760, Test: 0.7750
Epoch: 41, Loss: 1.2159, Train: 1.0000, Val: 0.7740, Test: 0.7760
Epoch: 42, Loss: 1.0738, Train: 1.0000, Val: 0.7740, Test: 0.7780
Epoch: 43, Loss: 1.1074, Train: 1.0000, Val: 0.7760, Test: 0.7770
Epoch: 44, Loss: 1.2032, Train: 1.0000, Val: 0.7740, Test: 0.7760
Epoch: 45, Loss: 1.0811, Train: 1.0000, Val: 0.7740, Test: 0.7750
Epoch: 46, Loss: 1.0502, Train: 1.0000, Val: 0.7740, Test: 0.7770
Epoch: 47, Loss: 0.9994, Train: 1.0000, Val: 0.7720, Test: 0.7750
Epoch: 48, Loss: 1.1680, Train: 1.0000, Val: 0.7760, Test: 0.7770
Epoch: 49, Loss: 1.1700, Train: 1.0000, Val: 0.7780, Test: 0.7800
Epoch: 50, Loss: 1.1722, Train: 1.0000, Val: 0.7820, Test: 0.7850
MAD:  0.5521
Best Test Accuracy: 0.7850, Val Accuracy: 0.7820, Train Accuracy: 1.0000
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9700, Train: 0.2571, Val: 0.2040, Test: 0.1920
Epoch: 2, Loss: 1.9425, Train: 0.3714, Val: 0.2760, Test: 0.2540
Epoch: 3, Loss: 1.9135, Train: 0.5143, Val: 0.3160, Test: 0.3120
Epoch: 4, Loss: 1.9193, Train: 0.6214, Val: 0.3460, Test: 0.3670
Epoch: 5, Loss: 1.8824, Train: 0.6714, Val: 0.3980, Test: 0.4010
Epoch: 6, Loss: 1.8783, Train: 0.7286, Val: 0.4300, Test: 0.4470
Epoch: 7, Loss: 1.8370, Train: 0.7714, Val: 0.4400, Test: 0.4840
Epoch: 8, Loss: 1.7976, Train: 0.8071, Val: 0.4500, Test: 0.5060
Epoch: 9, Loss: 1.7855, Train: 0.8286, Val: 0.4720, Test: 0.5330
Epoch: 10, Loss: 1.7690, Train: 0.8714, Val: 0.4980, Test: 0.5550
Epoch: 11, Loss: 1.7209, Train: 0.8857, Val: 0.5140, Test: 0.5700
Epoch: 12, Loss: 1.7281, Train: 0.9214, Val: 0.5420, Test: 0.5940
Epoch: 13, Loss: 1.6589, Train: 0.9286, Val: 0.5520, Test: 0.6130
Epoch: 14, Loss: 1.7011, Train: 0.9429, Val: 0.5720, Test: 0.6210
Epoch: 15, Loss: 1.6800, Train: 0.9500, Val: 0.5920, Test: 0.6300
Epoch: 16, Loss: 1.6039, Train: 0.9786, Val: 0.6180, Test: 0.6530
Epoch: 17, Loss: 1.6274, Train: 0.9857, Val: 0.6420, Test: 0.6740
Epoch: 18, Loss: 1.5111, Train: 0.9857, Val: 0.6560, Test: 0.6900
Epoch: 19, Loss: 1.5538, Train: 0.9929, Val: 0.6720, Test: 0.7060
Epoch: 20, Loss: 1.5178, Train: 0.9929, Val: 0.6980, Test: 0.7150
Epoch: 21, Loss: 1.4461, Train: 0.9929, Val: 0.7140, Test: 0.7320
Epoch: 22, Loss: 1.4665, Train: 0.9857, Val: 0.7160, Test: 0.7370
Epoch: 23, Loss: 1.3719, Train: 0.9857, Val: 0.7220, Test: 0.7400
Epoch: 24, Loss: 1.4298, Train: 0.9929, Val: 0.7280, Test: 0.7450
Epoch: 25, Loss: 1.4221, Train: 1.0000, Val: 0.7320, Test: 0.7530
Epoch: 26, Loss: 1.2795, Train: 1.0000, Val: 0.7320, Test: 0.7580
Epoch: 27, Loss: 1.4104, Train: 1.0000, Val: 0.7320, Test: 0.7680
Epoch: 28, Loss: 1.3066, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 29, Loss: 1.4231, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 30, Loss: 1.3198, Train: 1.0000, Val: 0.7520, Test: 0.7810
Epoch: 31, Loss: 1.2485, Train: 1.0000, Val: 0.7520, Test: 0.7860
Epoch: 32, Loss: 1.2456, Train: 1.0000, Val: 0.7580, Test: 0.7900
Epoch: 33, Loss: 1.1089, Train: 1.0000, Val: 0.7600, Test: 0.7920
Epoch: 34, Loss: 1.1769, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 35, Loss: 1.1649, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 36, Loss: 1.1215, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 37, Loss: 1.1447, Train: 1.0000, Val: 0.7700, Test: 0.7950
Epoch: 38, Loss: 1.3356, Train: 1.0000, Val: 0.7680, Test: 0.7990
Epoch: 39, Loss: 1.2725, Train: 1.0000, Val: 0.7720, Test: 0.8010
Epoch: 40, Loss: 1.1402, Train: 1.0000, Val: 0.7720, Test: 0.8050
Epoch: 41, Loss: 1.0839, Train: 1.0000, Val: 0.7740, Test: 0.8070
Epoch: 42, Loss: 1.2453, Train: 1.0000, Val: 0.7700, Test: 0.8040
Epoch: 43, Loss: 0.9781, Train: 1.0000, Val: 0.7680, Test: 0.8030
Epoch: 44, Loss: 1.0804, Train: 1.0000, Val: 0.7680, Test: 0.7990
Epoch: 45, Loss: 1.0706, Train: 1.0000, Val: 0.7700, Test: 0.7960
Epoch: 46, Loss: 1.0593, Train: 1.0000, Val: 0.7680, Test: 0.7970
Epoch: 47, Loss: 1.1520, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 48, Loss: 1.0533, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 49, Loss: 1.2908, Train: 1.0000, Val: 0.7660, Test: 0.7980
Epoch: 50, Loss: 1.0207, Train: 1.0000, Val: 0.7680, Test: 0.7950
MAD:  0.5328
Best Test Accuracy: 0.8070, Val Accuracy: 0.7740, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9362, Train: 0.3357, Val: 0.2340, Test: 0.1970
Epoch: 2, Loss: 1.9450, Train: 0.4286, Val: 0.2800, Test: 0.2470
Epoch: 3, Loss: 1.9149, Train: 0.5286, Val: 0.3320, Test: 0.2870
Epoch: 4, Loss: 1.8878, Train: 0.6143, Val: 0.3780, Test: 0.3390
Epoch: 5, Loss: 1.8668, Train: 0.6786, Val: 0.4220, Test: 0.3890
Epoch: 6, Loss: 1.8433, Train: 0.7643, Val: 0.4620, Test: 0.4370
Epoch: 7, Loss: 1.8747, Train: 0.8071, Val: 0.5040, Test: 0.4920
Epoch: 8, Loss: 1.7947, Train: 0.8286, Val: 0.5320, Test: 0.5370
Epoch: 9, Loss: 1.7938, Train: 0.8571, Val: 0.5660, Test: 0.5860
Epoch: 10, Loss: 1.7694, Train: 0.8929, Val: 0.5980, Test: 0.6160
Epoch: 11, Loss: 1.7149, Train: 0.9286, Val: 0.6200, Test: 0.6390
Epoch: 12, Loss: 1.7165, Train: 0.9429, Val: 0.6460, Test: 0.6650
Epoch: 13, Loss: 1.7039, Train: 0.9429, Val: 0.6700, Test: 0.6780
Epoch: 14, Loss: 1.5853, Train: 0.9500, Val: 0.6920, Test: 0.6910
Epoch: 15, Loss: 1.6212, Train: 0.9571, Val: 0.7080, Test: 0.7010
Epoch: 16, Loss: 1.6549, Train: 0.9571, Val: 0.7200, Test: 0.7150
Epoch: 17, Loss: 1.6116, Train: 0.9643, Val: 0.7220, Test: 0.7250
Epoch: 18, Loss: 1.5122, Train: 0.9643, Val: 0.7360, Test: 0.7310
Epoch: 19, Loss: 1.5020, Train: 0.9643, Val: 0.7380, Test: 0.7380
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 20, Loss: 1.5473, Train: 0.9786, Val: 0.7420, Test: 0.7380
Epoch: 21, Loss: 1.4733, Train: 0.9857, Val: 0.7520, Test: 0.7460
Epoch: 22, Loss: 1.4503, Train: 0.9857, Val: 0.7640, Test: 0.7490
Epoch: 23, Loss: 1.3528, Train: 0.9857, Val: 0.7640, Test: 0.7490
Epoch: 24, Loss: 1.3257, Train: 0.9929, Val: 0.7720, Test: 0.7540
Epoch: 25, Loss: 1.3257, Train: 1.0000, Val: 0.7700, Test: 0.7540
Epoch: 26, Loss: 1.3164, Train: 1.0000, Val: 0.7720, Test: 0.7560
Epoch: 27, Loss: 1.2521, Train: 1.0000, Val: 0.7760, Test: 0.7590
Epoch: 28, Loss: 1.4061, Train: 0.9929, Val: 0.7760, Test: 0.7620
Epoch: 29, Loss: 1.2867, Train: 0.9929, Val: 0.7780, Test: 0.7670
Epoch: 30, Loss: 1.2629, Train: 0.9929, Val: 0.7700, Test: 0.7690
Epoch: 31, Loss: 1.2757, Train: 0.9929, Val: 0.7760, Test: 0.7720
Epoch: 32, Loss: 1.3241, Train: 0.9929, Val: 0.7760, Test: 0.7740
Epoch: 33, Loss: 1.2483, Train: 1.0000, Val: 0.7760, Test: 0.7790
Epoch: 34, Loss: 1.2274, Train: 1.0000, Val: 0.7780, Test: 0.7810
Epoch: 35, Loss: 1.2779, Train: 1.0000, Val: 0.7820, Test: 0.7820
Epoch: 36, Loss: 1.3014, Train: 1.0000, Val: 0.7860, Test: 0.7870
Epoch: 37, Loss: 1.1298, Train: 1.0000, Val: 0.7860, Test: 0.7870
Epoch: 38, Loss: 1.1763, Train: 1.0000, Val: 0.7840, Test: 0.7860
Epoch: 39, Loss: 1.2656, Train: 1.0000, Val: 0.7840, Test: 0.7860
Epoch: 40, Loss: 1.1359, Train: 1.0000, Val: 0.7820, Test: 0.7880
Epoch: 41, Loss: 1.1900, Train: 1.0000, Val: 0.7780, Test: 0.7900
Epoch: 42, Loss: 1.1541, Train: 1.0000, Val: 0.7780, Test: 0.7910
Epoch: 43, Loss: 1.1368, Train: 1.0000, Val: 0.7780, Test: 0.7920
Epoch: 44, Loss: 1.1631, Train: 1.0000, Val: 0.7760, Test: 0.7920
Epoch: 45, Loss: 1.1684, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 46, Loss: 1.0962, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 47, Loss: 1.1404, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 48, Loss: 1.0674, Train: 1.0000, Val: 0.7720, Test: 0.7950
Epoch: 49, Loss: 1.2835, Train: 1.0000, Val: 0.7720, Test: 0.7980
Epoch: 50, Loss: 1.1762, Train: 1.0000, Val: 0.7700, Test: 0.7980
MAD:  0.5921
Best Test Accuracy: 0.7980, Val Accuracy: 0.7740, Train Accuracy: 1.0000
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9516, Train: 0.2429, Val: 0.1500, Test: 0.1420
Epoch: 2, Loss: 1.9387, Train: 0.3786, Val: 0.1940, Test: 0.1710
Epoch: 3, Loss: 1.9431, Train: 0.5000, Val: 0.2440, Test: 0.2210
Epoch: 4, Loss: 1.9254, Train: 0.5857, Val: 0.2640, Test: 0.2510
Epoch: 5, Loss: 1.9084, Train: 0.6000, Val: 0.2780, Test: 0.2700
Epoch: 6, Loss: 1.8948, Train: 0.6214, Val: 0.3060, Test: 0.2870
Epoch: 7, Loss: 1.8969, Train: 0.6500, Val: 0.2940, Test: 0.2930
Epoch: 8, Loss: 1.8552, Train: 0.6500, Val: 0.3120, Test: 0.3070
Epoch: 9, Loss: 1.8700, Train: 0.7071, Val: 0.3360, Test: 0.3220
Epoch: 10, Loss: 1.8089, Train: 0.7357, Val: 0.3540, Test: 0.3400
Epoch: 11, Loss: 1.7801, Train: 0.7429, Val: 0.3720, Test: 0.3600
Epoch: 12, Loss: 1.7817, Train: 0.7429, Val: 0.3900, Test: 0.3730
Epoch: 13, Loss: 1.7800, Train: 0.7643, Val: 0.3980, Test: 0.3850
Epoch: 14, Loss: 1.7598, Train: 0.7857, Val: 0.4180, Test: 0.4140
Epoch: 15, Loss: 1.7174, Train: 0.8000, Val: 0.4320, Test: 0.4310
Epoch: 16, Loss: 1.7171, Train: 0.8143, Val: 0.4540, Test: 0.4440
Epoch: 17, Loss: 1.6585, Train: 0.8357, Val: 0.4780, Test: 0.4720
Epoch: 18, Loss: 1.6949, Train: 0.8429, Val: 0.4960, Test: 0.4880
Epoch: 19, Loss: 1.5939, Train: 0.8643, Val: 0.5140, Test: 0.5120
Epoch: 20, Loss: 1.5271, Train: 0.8714, Val: 0.5300, Test: 0.5260
Epoch: 21, Loss: 1.5440, Train: 0.8786, Val: 0.5460, Test: 0.5370
Epoch: 22, Loss: 1.4639, Train: 0.8857, Val: 0.5460, Test: 0.5460
Epoch: 23, Loss: 1.4765, Train: 0.8857, Val: 0.5540, Test: 0.5500
Epoch: 24, Loss: 1.4410, Train: 0.8929, Val: 0.5540, Test: 0.5550
Epoch: 25, Loss: 1.3921, Train: 0.8929, Val: 0.5540, Test: 0.5600
Epoch: 26, Loss: 1.3767, Train: 0.9000, Val: 0.5540, Test: 0.5680
Epoch: 27, Loss: 1.3833, Train: 0.9143, Val: 0.5620, Test: 0.5690
Epoch: 28, Loss: 1.3462, Train: 0.9286, Val: 0.5760, Test: 0.5780
Epoch: 29, Loss: 1.4809, Train: 0.9429, Val: 0.5880, Test: 0.5870
Epoch: 30, Loss: 1.2695, Train: 0.9500, Val: 0.6260, Test: 0.6030
Epoch: 31, Loss: 1.3327, Train: 0.9643, Val: 0.6420, Test: 0.6300
Epoch: 32, Loss: 1.3687, Train: 0.9714, Val: 0.6580, Test: 0.6420
Epoch: 33, Loss: 1.2501, Train: 0.9786, Val: 0.6640, Test: 0.6570
Epoch: 34, Loss: 1.2972, Train: 0.9929, Val: 0.6720, Test: 0.6790
Epoch: 35, Loss: 1.3147, Train: 0.9929, Val: 0.6820, Test: 0.6840
Epoch: 36, Loss: 1.1683, Train: 0.9929, Val: 0.6960, Test: 0.6910
Epoch: 37, Loss: 1.1382, Train: 0.9929, Val: 0.7080, Test: 0.7020
Epoch: 38, Loss: 1.2528, Train: 0.9929, Val: 0.7160, Test: 0.7160
Epoch: 39, Loss: 1.3298, Train: 0.9929, Val: 0.7300, Test: 0.7290
Epoch: 40, Loss: 1.1287, Train: 1.0000, Val: 0.7320, Test: 0.7350
Epoch: 41, Loss: 1.1484, Train: 1.0000, Val: 0.7440, Test: 0.7480
Epoch: 42, Loss: 1.1560, Train: 1.0000, Val: 0.7420, Test: 0.7570
Epoch: 43, Loss: 1.1053, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 44, Loss: 1.1103, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 45, Loss: 1.1701, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 46, Loss: 1.1621, Train: 1.0000, Val: 0.7680, Test: 0.7660
Epoch: 47, Loss: 1.0934, Train: 1.0000, Val: 0.7720, Test: 0.7670
Epoch: 48, Loss: 1.1967, Train: 1.0000, Val: 0.7720, Test: 0.7680
Epoch: 49, Loss: 0.9512, Train: 1.0000, Val: 0.7700, Test: 0.7670
Epoch: 50, Loss: 1.2501, Train: 1.0000, Val: 0.7740, Test: 0.7670
MAD:  0.5673
Best Test Accuracy: 0.7680, Val Accuracy: 0.7720, Train Accuracy: 1.0000
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9709, Train: 0.2143, Val: 0.1860, Test: 0.1690
Epoch: 2, Loss: 1.9440, Train: 0.3429, Val: 0.2080, Test: 0.2280
Epoch: 3, Loss: 1.9275, Train: 0.4286, Val: 0.2520, Test: 0.2730
Epoch: 4, Loss: 1.9111, Train: 0.5429, Val: 0.2860, Test: 0.3060
Epoch: 5, Loss: 1.9043, Train: 0.6286, Val: 0.3120, Test: 0.3360
Epoch: 6, Loss: 1.8941, Train: 0.6857, Val: 0.3440, Test: 0.3770
Epoch: 7, Loss: 1.8336, Train: 0.7357, Val: 0.3780, Test: 0.4150
Epoch: 8, Loss: 1.8281, Train: 0.8214, Val: 0.4200, Test: 0.4610
Epoch: 9, Loss: 1.7876, Train: 0.8643, Val: 0.4500, Test: 0.5010
Epoch: 10, Loss: 1.8153, Train: 0.8929, Val: 0.4840, Test: 0.5340
Epoch: 11, Loss: 1.7413, Train: 0.9286, Val: 0.5220, Test: 0.5520
Epoch: 12, Loss: 1.7424, Train: 0.9500, Val: 0.5440, Test: 0.5710
Epoch: 13, Loss: 1.7149, Train: 0.9643, Val: 0.5620, Test: 0.5870
Epoch: 14, Loss: 1.6881, Train: 0.9714, Val: 0.5800, Test: 0.6080
Epoch: 15, Loss: 1.6085, Train: 0.9714, Val: 0.5980, Test: 0.6180
Epoch: 16, Loss: 1.6564, Train: 0.9714, Val: 0.6220, Test: 0.6380
Epoch: 17, Loss: 1.6377, Train: 0.9786, Val: 0.6380, Test: 0.6540
Epoch: 18, Loss: 1.5325, Train: 0.9929, Val: 0.6460, Test: 0.6830
Epoch: 19, Loss: 1.5829, Train: 0.9929, Val: 0.6740, Test: 0.7050
Epoch: 20, Loss: 1.5656, Train: 0.9929, Val: 0.6920, Test: 0.7220
Epoch: 21, Loss: 1.4152, Train: 0.9929, Val: 0.7080, Test: 0.7380
Epoch: 22, Loss: 1.4536, Train: 0.9929, Val: 0.7140, Test: 0.7520
Epoch: 23, Loss: 1.3212, Train: 0.9929, Val: 0.7240, Test: 0.7590
Epoch: 24, Loss: 1.3794, Train: 0.9929, Val: 0.7320, Test: 0.7640
Epoch: 25, Loss: 1.5228, Train: 0.9929, Val: 0.7460, Test: 0.7640
Epoch: 26, Loss: 1.4128, Train: 0.9929, Val: 0.7540, Test: 0.7720
Epoch: 27, Loss: 1.3896, Train: 1.0000, Val: 0.7640, Test: 0.7790
Epoch: 28, Loss: 1.3717, Train: 1.0000, Val: 0.7640, Test: 0.7830
Epoch: 29, Loss: 1.3046, Train: 1.0000, Val: 0.7720, Test: 0.7830
Epoch: 30, Loss: 1.1963, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 31, Loss: 1.3028, Train: 1.0000, Val: 0.7840, Test: 0.7840
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 32, Loss: 1.2685, Train: 1.0000, Val: 0.7820, Test: 0.7810
Epoch: 33, Loss: 1.2718, Train: 1.0000, Val: 0.7820, Test: 0.7800
Epoch: 34, Loss: 1.2679, Train: 1.0000, Val: 0.7820, Test: 0.7830
Epoch: 35, Loss: 1.1341, Train: 1.0000, Val: 0.7820, Test: 0.7800
Epoch: 36, Loss: 1.1220, Train: 1.0000, Val: 0.7820, Test: 0.7800
Epoch: 37, Loss: 1.1714, Train: 1.0000, Val: 0.7800, Test: 0.7830
Epoch: 38, Loss: 1.1305, Train: 1.0000, Val: 0.7820, Test: 0.7890
Epoch: 39, Loss: 1.1107, Train: 1.0000, Val: 0.7820, Test: 0.7910
Epoch: 40, Loss: 1.1392, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 41, Loss: 1.1830, Train: 1.0000, Val: 0.7800, Test: 0.7980
Epoch: 42, Loss: 1.1603, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 43, Loss: 1.2494, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 44, Loss: 1.2763, Train: 1.0000, Val: 0.7780, Test: 0.8020
Epoch: 45, Loss: 1.2719, Train: 1.0000, Val: 0.7760, Test: 0.8020
Epoch: 46, Loss: 1.1559, Train: 1.0000, Val: 0.7780, Test: 0.8010
Epoch: 47, Loss: 1.1180, Train: 1.0000, Val: 0.7800, Test: 0.8020
Epoch: 48, Loss: 0.9737, Train: 1.0000, Val: 0.7800, Test: 0.8030
Epoch: 49, Loss: 1.1060, Train: 1.0000, Val: 0.7800, Test: 0.8030
Epoch: 50, Loss: 1.1284, Train: 1.0000, Val: 0.7840, Test: 0.8050
MAD:  0.5877
Best Test Accuracy: 0.8050, Val Accuracy: 0.7840, Train Accuracy: 1.0000
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9325, Train: 0.2286, Val: 0.2420, Test: 0.2590
Epoch: 2, Loss: 1.9364, Train: 0.3429, Val: 0.2920, Test: 0.3190
Epoch: 3, Loss: 1.9391, Train: 0.4286, Val: 0.3500, Test: 0.3620
Epoch: 4, Loss: 1.9282, Train: 0.4500, Val: 0.3700, Test: 0.3730
Epoch: 5, Loss: 1.9167, Train: 0.5286, Val: 0.3800, Test: 0.3940
Epoch: 6, Loss: 1.8897, Train: 0.5571, Val: 0.4060, Test: 0.4080
Epoch: 7, Loss: 1.8858, Train: 0.6071, Val: 0.4300, Test: 0.4280
Epoch: 8, Loss: 1.8771, Train: 0.6429, Val: 0.4520, Test: 0.4530
Epoch: 9, Loss: 1.8706, Train: 0.6929, Val: 0.4900, Test: 0.4890
Epoch: 10, Loss: 1.8073, Train: 0.7214, Val: 0.5160, Test: 0.5310
Epoch: 11, Loss: 1.7982, Train: 0.7714, Val: 0.5420, Test: 0.5560
Epoch: 12, Loss: 1.8083, Train: 0.8000, Val: 0.5720, Test: 0.5780
Epoch: 13, Loss: 1.7775, Train: 0.8429, Val: 0.5900, Test: 0.5980
Epoch: 14, Loss: 1.7351, Train: 0.8429, Val: 0.5920, Test: 0.6120
Epoch: 15, Loss: 1.6686, Train: 0.8643, Val: 0.5840, Test: 0.6120
Epoch: 16, Loss: 1.7499, Train: 0.8786, Val: 0.5900, Test: 0.6180
Epoch: 17, Loss: 1.5868, Train: 0.8857, Val: 0.5860, Test: 0.6260
Epoch: 18, Loss: 1.6277, Train: 0.8929, Val: 0.5880, Test: 0.6330
Epoch: 19, Loss: 1.5589, Train: 0.8929, Val: 0.6040, Test: 0.6400
Epoch: 20, Loss: 1.6135, Train: 0.8929, Val: 0.6080, Test: 0.6420
Epoch: 21, Loss: 1.4872, Train: 0.8929, Val: 0.6260, Test: 0.6560
Epoch: 22, Loss: 1.4226, Train: 0.8929, Val: 0.6280, Test: 0.6600
Epoch: 23, Loss: 1.4683, Train: 0.9000, Val: 0.6400, Test: 0.6640
Epoch: 24, Loss: 1.5338, Train: 0.9214, Val: 0.6480, Test: 0.6750
Epoch: 25, Loss: 1.4526, Train: 0.9286, Val: 0.6600, Test: 0.6840
Epoch: 26, Loss: 1.5230, Train: 0.9357, Val: 0.6700, Test: 0.6900
Epoch: 27, Loss: 1.4125, Train: 0.9429, Val: 0.6800, Test: 0.6910
Epoch: 28, Loss: 1.2555, Train: 0.9500, Val: 0.6940, Test: 0.6960
Epoch: 29, Loss: 1.3803, Train: 0.9500, Val: 0.6980, Test: 0.7040
Epoch: 30, Loss: 1.4349, Train: 0.9571, Val: 0.7040, Test: 0.7140
Epoch: 31, Loss: 1.3895, Train: 0.9643, Val: 0.7140, Test: 0.7210
Epoch: 32, Loss: 1.3331, Train: 0.9786, Val: 0.7240, Test: 0.7330
Epoch: 33, Loss: 1.2916, Train: 0.9786, Val: 0.7360, Test: 0.7430
Epoch: 34, Loss: 1.2561, Train: 0.9786, Val: 0.7420, Test: 0.7530
Epoch: 35, Loss: 1.2283, Train: 0.9786, Val: 0.7460, Test: 0.7560
Epoch: 36, Loss: 1.1365, Train: 0.9786, Val: 0.7420, Test: 0.7650
Epoch: 37, Loss: 1.2219, Train: 0.9857, Val: 0.7620, Test: 0.7690
Epoch: 38, Loss: 1.1018, Train: 0.9857, Val: 0.7620, Test: 0.7720
Epoch: 39, Loss: 1.2675, Train: 0.9857, Val: 0.7620, Test: 0.7750
Epoch: 40, Loss: 1.1553, Train: 0.9929, Val: 0.7720, Test: 0.7800
Epoch: 41, Loss: 1.2150, Train: 0.9929, Val: 0.7700, Test: 0.7860
Epoch: 42, Loss: 1.1936, Train: 0.9929, Val: 0.7640, Test: 0.7870
Epoch: 43, Loss: 1.0578, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 44, Loss: 1.1873, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 45, Loss: 1.1801, Train: 1.0000, Val: 0.7740, Test: 0.7880
Epoch: 46, Loss: 1.1907, Train: 1.0000, Val: 0.7800, Test: 0.7850
Epoch: 47, Loss: 1.0928, Train: 1.0000, Val: 0.7700, Test: 0.7870
Epoch: 48, Loss: 1.1804, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 49, Loss: 1.0961, Train: 1.0000, Val: 0.7640, Test: 0.7870
Epoch: 50, Loss: 1.2046, Train: 1.0000, Val: 0.7600, Test: 0.7860
MAD:  0.5461
Best Test Accuracy: 0.7880, Val Accuracy: 0.7740, Train Accuracy: 1.0000
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9430, Train: 0.2357, Val: 0.1800, Test: 0.2090
Epoch: 2, Loss: 1.9413, Train: 0.3143, Val: 0.2160, Test: 0.2350
Epoch: 3, Loss: 1.9182, Train: 0.4357, Val: 0.2820, Test: 0.3010
Epoch: 4, Loss: 1.9190, Train: 0.5571, Val: 0.3580, Test: 0.3720
Epoch: 5, Loss: 1.8533, Train: 0.6500, Val: 0.4460, Test: 0.4420
Epoch: 6, Loss: 1.8895, Train: 0.7000, Val: 0.4920, Test: 0.5000
Epoch: 7, Loss: 1.8505, Train: 0.8071, Val: 0.5500, Test: 0.5480
Epoch: 8, Loss: 1.8593, Train: 0.8357, Val: 0.5740, Test: 0.5840
Epoch: 9, Loss: 1.8261, Train: 0.8857, Val: 0.6000, Test: 0.6150
Epoch: 10, Loss: 1.7376, Train: 0.9000, Val: 0.6200, Test: 0.6370
Epoch: 11, Loss: 1.7549, Train: 0.9286, Val: 0.6200, Test: 0.6510
Epoch: 12, Loss: 1.7756, Train: 0.9429, Val: 0.6400, Test: 0.6610
Epoch: 13, Loss: 1.6986, Train: 0.9429, Val: 0.6540, Test: 0.6760
Epoch: 14, Loss: 1.6476, Train: 0.9500, Val: 0.6700, Test: 0.6870
Epoch: 15, Loss: 1.6557, Train: 0.9500, Val: 0.6880, Test: 0.6970
Epoch: 16, Loss: 1.6600, Train: 0.9643, Val: 0.6920, Test: 0.7020
Epoch: 17, Loss: 1.5907, Train: 0.9714, Val: 0.7000, Test: 0.7160
Epoch: 18, Loss: 1.6487, Train: 0.9714, Val: 0.7100, Test: 0.7260
Epoch: 19, Loss: 1.5766, Train: 0.9714, Val: 0.7180, Test: 0.7370
Epoch: 20, Loss: 1.4246, Train: 0.9857, Val: 0.7280, Test: 0.7560
Epoch: 21, Loss: 1.5066, Train: 0.9857, Val: 0.7320, Test: 0.7640
Epoch: 22, Loss: 1.4200, Train: 0.9857, Val: 0.7420, Test: 0.7700
Epoch: 23, Loss: 1.4286, Train: 0.9857, Val: 0.7480, Test: 0.7780
Epoch: 24, Loss: 1.4647, Train: 0.9929, Val: 0.7540, Test: 0.7840
Epoch: 25, Loss: 1.3842, Train: 0.9929, Val: 0.7540, Test: 0.7810
Epoch: 26, Loss: 1.3441, Train: 1.0000, Val: 0.7480, Test: 0.7790
Epoch: 27, Loss: 1.2639, Train: 1.0000, Val: 0.7560, Test: 0.7790
Epoch: 28, Loss: 1.2861, Train: 1.0000, Val: 0.7560, Test: 0.7780
Epoch: 29, Loss: 1.3181, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 30, Loss: 1.3125, Train: 1.0000, Val: 0.7620, Test: 0.7830
Epoch: 31, Loss: 1.3066, Train: 1.0000, Val: 0.7640, Test: 0.7890
Epoch: 32, Loss: 1.3667, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 33, Loss: 1.2270, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 34, Loss: 1.1239, Train: 1.0000, Val: 0.7660, Test: 0.7940
Epoch: 35, Loss: 1.1937, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 36, Loss: 1.1713, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 37, Loss: 1.1785, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 38, Loss: 1.0474, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 39, Loss: 1.1511, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 40, Loss: 1.0566, Train: 1.0000, Val: 0.7680, Test: 0.7970
Epoch: 41, Loss: 1.2148, Train: 1.0000, Val: 0.7700, Test: 0.8000
Epoch: 42, Loss: 1.0729, Train: 1.0000, Val: 0.7700, Test: 0.8000
Epoch: 43, Loss: 1.0776, Train: 1.0000, Val: 0.7740, Test: 0.7990
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:475: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 44, Loss: 1.2615, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 45, Loss: 0.8771, Train: 1.0000, Val: 0.7820, Test: 0.8030
Epoch: 46, Loss: 1.0761, Train: 1.0000, Val: 0.7860, Test: 0.8020
Epoch: 47, Loss: 1.1786, Train: 1.0000, Val: 0.7880, Test: 0.8040
Epoch: 48, Loss: 1.0565, Train: 1.0000, Val: 0.7860, Test: 0.8060
Epoch: 49, Loss: 1.0153, Train: 1.0000, Val: 0.7820, Test: 0.8060
Epoch: 50, Loss: 1.0530, Train: 1.0000, Val: 0.7800, Test: 0.8080
MAD:  0.5401
Best Test Accuracy: 0.8080, Val Accuracy: 0.7800, Train Accuracy: 1.0000
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(1433, 128)
      (conv2): GCNConv(1433, 128)
    )
    (1): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 1.9619, Train: 0.2286, Val: 0.1160, Test: 0.0940
Epoch: 2, Loss: 1.9655, Train: 0.3429, Val: 0.1220, Test: 0.1260
Epoch: 3, Loss: 1.9185, Train: 0.4714, Val: 0.1700, Test: 0.1690
Epoch: 4, Loss: 1.9029, Train: 0.5929, Val: 0.2180, Test: 0.2250
Epoch: 5, Loss: 1.9051, Train: 0.6786, Val: 0.2740, Test: 0.2740
Epoch: 6, Loss: 1.8822, Train: 0.7214, Val: 0.3120, Test: 0.3230
Epoch: 7, Loss: 1.8623, Train: 0.7786, Val: 0.3480, Test: 0.3430
Epoch: 8, Loss: 1.8379, Train: 0.8214, Val: 0.4000, Test: 0.3810
Epoch: 9, Loss: 1.8043, Train: 0.8571, Val: 0.4280, Test: 0.4150
Epoch: 10, Loss: 1.8197, Train: 0.8857, Val: 0.4620, Test: 0.4500
Epoch: 11, Loss: 1.7678, Train: 0.9071, Val: 0.4980, Test: 0.4780
Epoch: 12, Loss: 1.7098, Train: 0.9143, Val: 0.5340, Test: 0.5100
Epoch: 13, Loss: 1.7552, Train: 0.9286, Val: 0.5420, Test: 0.5450
Epoch: 14, Loss: 1.6642, Train: 0.9429, Val: 0.5640, Test: 0.5720
Epoch: 15, Loss: 1.6684, Train: 0.9500, Val: 0.5820, Test: 0.5870
Epoch: 16, Loss: 1.6904, Train: 0.9571, Val: 0.5980, Test: 0.6080
Epoch: 17, Loss: 1.5993, Train: 0.9786, Val: 0.6300, Test: 0.6340
Epoch: 18, Loss: 1.5606, Train: 0.9786, Val: 0.6400, Test: 0.6560
Epoch: 19, Loss: 1.5323, Train: 0.9857, Val: 0.6580, Test: 0.6740
Epoch: 20, Loss: 1.5272, Train: 0.9857, Val: 0.6700, Test: 0.6850
Epoch: 21, Loss: 1.4902, Train: 0.9857, Val: 0.6860, Test: 0.7020
Epoch: 22, Loss: 1.5367, Train: 0.9929, Val: 0.7120, Test: 0.7250
Epoch: 23, Loss: 1.3824, Train: 0.9929, Val: 0.7260, Test: 0.7340
Epoch: 24, Loss: 1.4437, Train: 0.9929, Val: 0.7340, Test: 0.7400
Epoch: 25, Loss: 1.3798, Train: 0.9929, Val: 0.7400, Test: 0.7440
Epoch: 26, Loss: 1.3544, Train: 0.9929, Val: 0.7500, Test: 0.7590
Epoch: 27, Loss: 1.4063, Train: 1.0000, Val: 0.7580, Test: 0.7610
Epoch: 28, Loss: 1.3550, Train: 1.0000, Val: 0.7560, Test: 0.7670
Epoch: 29, Loss: 1.2412, Train: 1.0000, Val: 0.7620, Test: 0.7700
Epoch: 30, Loss: 1.3099, Train: 1.0000, Val: 0.7660, Test: 0.7650
Epoch: 31, Loss: 1.4085, Train: 1.0000, Val: 0.7640, Test: 0.7660
Epoch: 32, Loss: 1.3487, Train: 1.0000, Val: 0.7600, Test: 0.7620
Epoch: 33, Loss: 1.2563, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 34, Loss: 1.2936, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 35, Loss: 1.0762, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 36, Loss: 0.9734, Train: 1.0000, Val: 0.7560, Test: 0.7560
Epoch: 37, Loss: 1.0636, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 38, Loss: 1.2245, Train: 1.0000, Val: 0.7580, Test: 0.7610
Epoch: 39, Loss: 1.0556, Train: 1.0000, Val: 0.7620, Test: 0.7650
Epoch: 40, Loss: 1.1088, Train: 1.0000, Val: 0.7640, Test: 0.7690
Epoch: 41, Loss: 1.1156, Train: 1.0000, Val: 0.7660, Test: 0.7760
Epoch: 42, Loss: 1.1011, Train: 1.0000, Val: 0.7700, Test: 0.7820
Epoch: 43, Loss: 1.0584, Train: 1.0000, Val: 0.7680, Test: 0.7860
Epoch: 44, Loss: 1.0476, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 45, Loss: 1.2216, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 46, Loss: 1.2820, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 47, Loss: 1.1810, Train: 1.0000, Val: 0.7780, Test: 0.7980
Epoch: 48, Loss: 1.0331, Train: 1.0000, Val: 0.7780, Test: 0.8020
Epoch: 49, Loss: 1.0247, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 50, Loss: 1.0957, Train: 1.0000, Val: 0.7800, Test: 0.8010
MAD:  0.5525
Best Test Accuracy: 0.8020, Val Accuracy: 0.7780, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.7964 ± 0.012240914998479496
Average MAD:  0.55884 ± 0.01849525344514098

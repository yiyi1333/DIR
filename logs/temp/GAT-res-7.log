/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.4835, Train: 0.2143, Val: 0.1040, Test: 0.1110
Epoch: 2, Loss: 2.3365, Train: 0.2714, Val: 0.1660, Test: 0.1780
Epoch: 3, Loss: 2.1631, Train: 0.3786, Val: 0.1880, Test: 0.1960
Epoch: 4, Loss: 2.0544, Train: 0.3286, Val: 0.1980, Test: 0.2080
Epoch: 5, Loss: 2.0401, Train: 0.3143, Val: 0.1900, Test: 0.2000
Epoch: 6, Loss: 2.0043, Train: 0.3143, Val: 0.1920, Test: 0.2030
Epoch: 7, Loss: 1.9470, Train: 0.3214, Val: 0.2060, Test: 0.2130
Epoch: 8, Loss: 1.9079, Train: 0.3571, Val: 0.2160, Test: 0.2390
Epoch: 9, Loss: 1.9099, Train: 0.4000, Val: 0.2560, Test: 0.2720
Epoch: 10, Loss: 1.9844, Train: 0.4429, Val: 0.3100, Test: 0.3320
Epoch: 11, Loss: 1.8702, Train: 0.5071, Val: 0.3560, Test: 0.4000
Epoch: 12, Loss: 1.9168, Train: 0.5571, Val: 0.4140, Test: 0.4730
Epoch: 13, Loss: 1.8628, Train: 0.6143, Val: 0.4660, Test: 0.5220
Epoch: 14, Loss: 1.8205, Train: 0.6643, Val: 0.4900, Test: 0.5520
Epoch: 15, Loss: 1.7704, Train: 0.6643, Val: 0.5240, Test: 0.5860
Epoch: 16, Loss: 1.7365, Train: 0.7071, Val: 0.5560, Test: 0.6010
Epoch: 17, Loss: 1.7371, Train: 0.7214, Val: 0.5840, Test: 0.6170
Epoch: 18, Loss: 1.7194, Train: 0.7357, Val: 0.6020, Test: 0.6270
Epoch: 19, Loss: 1.6685, Train: 0.7500, Val: 0.6180, Test: 0.6320
Epoch: 20, Loss: 1.6853, Train: 0.7429, Val: 0.6400, Test: 0.6370
Epoch: 21, Loss: 1.6962, Train: 0.7500, Val: 0.6360, Test: 0.6440
Epoch: 22, Loss: 1.6086, Train: 0.7357, Val: 0.6400, Test: 0.6480
Epoch: 23, Loss: 1.5385, Train: 0.7500, Val: 0.6440, Test: 0.6510
Epoch: 24, Loss: 1.6524, Train: 0.7571, Val: 0.6420, Test: 0.6540
Epoch: 25, Loss: 1.6384, Train: 0.7643, Val: 0.6340, Test: 0.6510
Epoch: 26, Loss: 1.5108, Train: 0.7643, Val: 0.6160, Test: 0.6440
Epoch: 27, Loss: 1.4787, Train: 0.7643, Val: 0.6100, Test: 0.6340
Epoch: 28, Loss: 1.4211, Train: 0.7643, Val: 0.6080, Test: 0.6390
Epoch: 29, Loss: 1.4141, Train: 0.7714, Val: 0.6140, Test: 0.6400
Epoch: 30, Loss: 1.3724, Train: 0.7786, Val: 0.6340, Test: 0.6480
Epoch: 31, Loss: 1.3037, Train: 0.7714, Val: 0.6520, Test: 0.6570
Epoch: 32, Loss: 1.3560, Train: 0.7786, Val: 0.6640, Test: 0.6620
Epoch: 33, Loss: 1.1297, Train: 0.7786, Val: 0.6920, Test: 0.6850
Epoch: 34, Loss: 1.2587, Train: 0.7929, Val: 0.6940, Test: 0.7050
Epoch: 35, Loss: 1.1109, Train: 0.8000, Val: 0.6840, Test: 0.7140
Epoch: 36, Loss: 1.1344, Train: 0.8214, Val: 0.6840, Test: 0.7050
Epoch: 37, Loss: 1.0610, Train: 0.8286, Val: 0.6840, Test: 0.6940
Epoch: 38, Loss: 0.9533, Train: 0.8143, Val: 0.6900, Test: 0.6880
Epoch: 39, Loss: 0.9721, Train: 0.8286, Val: 0.6920, Test: 0.6910
Epoch: 40, Loss: 0.9031, Train: 0.8571, Val: 0.6940, Test: 0.6850
Epoch: 41, Loss: 0.8155, Train: 0.8714, Val: 0.6980, Test: 0.6970
Epoch: 42, Loss: 0.7389, Train: 0.8857, Val: 0.7140, Test: 0.7200
Epoch: 43, Loss: 0.7823, Train: 0.9071, Val: 0.7500, Test: 0.7520
Epoch: 44, Loss: 0.9248, Train: 0.9357, Val: 0.7520, Test: 0.7690
Epoch: 45, Loss: 0.8530, Train: 0.9500, Val: 0.7580, Test: 0.7720
Epoch: 46, Loss: 0.7138, Train: 0.9500, Val: 0.7600, Test: 0.7770
Epoch: 47, Loss: 0.7715, Train: 0.9500, Val: 0.7680, Test: 0.7740
Epoch: 48, Loss: 0.6965, Train: 0.9571, Val: 0.7820, Test: 0.7770
Epoch: 49, Loss: 0.7235, Train: 0.9571, Val: 0.7800, Test: 0.7780
Epoch: 50, Loss: 0.5932, Train: 0.9571, Val: 0.7720, Test: 0.7730
MAD:  0.9085
Best Test Accuracy: 0.7780, Val Accuracy: 0.7800, Train Accuracy: 0.9571
Training completed.
Seed:  1
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.6702, Train: 0.1429, Val: 0.3140, Test: 0.3180
Epoch: 2, Loss: 2.3436, Train: 0.1429, Val: 0.2980, Test: 0.3050
Epoch: 3, Loss: 2.2317, Train: 0.1929, Val: 0.2440, Test: 0.2600
Epoch: 4, Loss: 2.1899, Train: 0.1786, Val: 0.1580, Test: 0.1760
Epoch: 5, Loss: 2.0202, Train: 0.1643, Val: 0.1320, Test: 0.1450
Epoch: 6, Loss: 1.9947, Train: 0.1571, Val: 0.1280, Test: 0.1480
Epoch: 7, Loss: 2.0288, Train: 0.1857, Val: 0.1380, Test: 0.1450
Epoch: 8, Loss: 2.0165, Train: 0.1857, Val: 0.1480, Test: 0.1490
Epoch: 9, Loss: 2.0262, Train: 0.2071, Val: 0.1540, Test: 0.1610
Epoch: 10, Loss: 1.8489, Train: 0.2357, Val: 0.1680, Test: 0.1750
Epoch: 11, Loss: 1.9683, Train: 0.2714, Val: 0.1840, Test: 0.1940
Epoch: 12, Loss: 1.8631, Train: 0.3000, Val: 0.2040, Test: 0.2260
Epoch: 13, Loss: 1.9154, Train: 0.3786, Val: 0.2380, Test: 0.2580
Epoch: 14, Loss: 1.8473, Train: 0.4571, Val: 0.2660, Test: 0.2910
Epoch: 15, Loss: 1.8397, Train: 0.5500, Val: 0.3160, Test: 0.3380
Epoch: 16, Loss: 1.8480, Train: 0.6143, Val: 0.3740, Test: 0.3980
Epoch: 17, Loss: 1.8923, Train: 0.6571, Val: 0.4040, Test: 0.4340
Epoch: 18, Loss: 1.8762, Train: 0.6714, Val: 0.4340, Test: 0.4620
Epoch: 19, Loss: 1.8983, Train: 0.7000, Val: 0.4380, Test: 0.4770
Epoch: 20, Loss: 1.7148, Train: 0.7214, Val: 0.4420, Test: 0.4950
Epoch: 21, Loss: 1.7035, Train: 0.7571, Val: 0.4600, Test: 0.5080
Epoch: 22, Loss: 1.6736, Train: 0.7643, Val: 0.4820, Test: 0.5260
Epoch: 23, Loss: 1.6802, Train: 0.7643, Val: 0.4900, Test: 0.5360
Epoch: 24, Loss: 1.7126, Train: 0.7714, Val: 0.5040, Test: 0.5460
Epoch: 25, Loss: 1.6453, Train: 0.8000, Val: 0.5320, Test: 0.5570
Epoch: 26, Loss: 1.6840, Train: 0.7929, Val: 0.5280, Test: 0.5740
Epoch: 27, Loss: 1.6018, Train: 0.8071, Val: 0.5480, Test: 0.5860
Epoch: 28, Loss: 1.5277, Train: 0.8214, Val: 0.5680, Test: 0.5930
Epoch: 29, Loss: 1.5423, Train: 0.8071, Val: 0.5640, Test: 0.5910
Epoch: 30, Loss: 1.4947, Train: 0.7929, Val: 0.5760, Test: 0.5990
Epoch: 31, Loss: 1.3289, Train: 0.8071, Val: 0.5780, Test: 0.6010
Epoch: 32, Loss: 1.4587, Train: 0.8071, Val: 0.5820, Test: 0.6080
Epoch: 33, Loss: 1.3641, Train: 0.8000, Val: 0.5860, Test: 0.6070
Epoch: 34, Loss: 1.3498, Train: 0.8143, Val: 0.5960, Test: 0.6100
Epoch: 35, Loss: 1.2524, Train: 0.8143, Val: 0.6100, Test: 0.6250
Epoch: 36, Loss: 1.2960, Train: 0.8429, Val: 0.6260, Test: 0.6430
Epoch: 37, Loss: 1.2564, Train: 0.8643, Val: 0.6420, Test: 0.6590
Epoch: 38, Loss: 1.0718, Train: 0.8929, Val: 0.6600, Test: 0.6690
Epoch: 39, Loss: 1.1318, Train: 0.9214, Val: 0.6720, Test: 0.6840
Epoch: 40, Loss: 1.1094, Train: 0.9214, Val: 0.6960, Test: 0.7040
Epoch: 41, Loss: 1.0500, Train: 0.9429, Val: 0.7060, Test: 0.7100
Epoch: 42, Loss: 1.0619, Train: 0.9429, Val: 0.7220, Test: 0.7220
Epoch: 43, Loss: 0.9523, Train: 0.9500, Val: 0.7420, Test: 0.7440
Epoch: 44, Loss: 0.8783, Train: 0.9571, Val: 0.7380, Test: 0.7560
Epoch: 45, Loss: 1.1216, Train: 0.9571, Val: 0.7460, Test: 0.7670
Epoch: 46, Loss: 0.9879, Train: 0.9643, Val: 0.7520, Test: 0.7850
Epoch: 47, Loss: 0.8069, Train: 0.9571, Val: 0.7560, Test: 0.7920
Epoch: 48, Loss: 0.8207, Train: 0.9500, Val: 0.7580, Test: 0.7930
Epoch: 49, Loss: 0.7233, Train: 0.9500, Val: 0.7560, Test: 0.7930
Epoch: 50, Loss: 0.5659, Train: 0.9500, Val: 0.7560, Test: 0.7950
MAD:  0.8466
Best Test Accuracy: 0.7950, Val Accuracy: 0.7560, Train Accuracy: 0.9500
Training completed.
Seed:  2
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.9567, Train: 0.1357, Val: 0.1540, Test: 0.1460
Epoch: 2, Loss: 2.5459, Train: 0.2857, Val: 0.2260, Test: 0.2140
Epoch: 3, Loss: 2.2979, Train: 0.2286, Val: 0.1520, Test: 0.1560
Epoch: 4, Loss: 2.2541, Train: 0.2786, Val: 0.1720, Test: 0.1910
Epoch: 5, Loss: 2.1441, Train: 0.2929, Val: 0.1780, Test: 0.1910
Epoch: 6, Loss: 2.1452, Train: 0.2643, Val: 0.1660, Test: 0.1660
Epoch: 7, Loss: 2.1864, Train: 0.2143, Val: 0.1340, Test: 0.1460
Epoch: 8, Loss: 1.8953, Train: 0.1857, Val: 0.1240, Test: 0.1380
Epoch: 9, Loss: 2.0127, Train: 0.1786, Val: 0.1260, Test: 0.1350
Epoch: 10, Loss: 1.8815, Train: 0.1714, Val: 0.1260, Test: 0.1360
Epoch: 11, Loss: 1.9612, Train: 0.1786, Val: 0.1260, Test: 0.1350
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 12, Loss: 1.8979, Train: 0.1929, Val: 0.1260, Test: 0.1350
Epoch: 13, Loss: 1.8775, Train: 0.2000, Val: 0.1260, Test: 0.1370
Epoch: 14, Loss: 1.8991, Train: 0.2000, Val: 0.1280, Test: 0.1420
Epoch: 15, Loss: 1.8927, Train: 0.2429, Val: 0.1320, Test: 0.1530
Epoch: 16, Loss: 1.8834, Train: 0.2643, Val: 0.1700, Test: 0.1780
Epoch: 17, Loss: 1.8463, Train: 0.3143, Val: 0.1960, Test: 0.2090
Epoch: 18, Loss: 1.7489, Train: 0.3500, Val: 0.2380, Test: 0.2360
Epoch: 19, Loss: 1.8272, Train: 0.3714, Val: 0.2700, Test: 0.2650
Epoch: 20, Loss: 1.8045, Train: 0.4357, Val: 0.2800, Test: 0.3000
Epoch: 21, Loss: 1.7543, Train: 0.4857, Val: 0.3160, Test: 0.3360
Epoch: 22, Loss: 1.7863, Train: 0.5786, Val: 0.3600, Test: 0.3720
Epoch: 23, Loss: 1.7637, Train: 0.6357, Val: 0.3820, Test: 0.3970
Epoch: 24, Loss: 1.7697, Train: 0.6643, Val: 0.4080, Test: 0.4300
Epoch: 25, Loss: 1.6905, Train: 0.6786, Val: 0.4260, Test: 0.4680
Epoch: 26, Loss: 1.6880, Train: 0.7071, Val: 0.4680, Test: 0.5020
Epoch: 27, Loss: 1.6682, Train: 0.7500, Val: 0.5000, Test: 0.5280
Epoch: 28, Loss: 1.6061, Train: 0.7643, Val: 0.5420, Test: 0.5460
Epoch: 29, Loss: 1.5870, Train: 0.7714, Val: 0.5560, Test: 0.5580
Epoch: 30, Loss: 1.5374, Train: 0.7643, Val: 0.5580, Test: 0.5610
Epoch: 31, Loss: 1.5659, Train: 0.7643, Val: 0.5560, Test: 0.5420
Epoch: 32, Loss: 1.4759, Train: 0.7571, Val: 0.5360, Test: 0.5440
Epoch: 33, Loss: 1.4192, Train: 0.7786, Val: 0.5260, Test: 0.5320
Epoch: 34, Loss: 1.3849, Train: 0.7714, Val: 0.5120, Test: 0.5190
Epoch: 35, Loss: 1.3496, Train: 0.7643, Val: 0.5080, Test: 0.5180
Epoch: 36, Loss: 1.3709, Train: 0.7357, Val: 0.5100, Test: 0.5170
Epoch: 37, Loss: 1.2742, Train: 0.7357, Val: 0.5080, Test: 0.5130
Epoch: 38, Loss: 1.2692, Train: 0.7429, Val: 0.5220, Test: 0.5110
Epoch: 39, Loss: 1.2915, Train: 0.7500, Val: 0.5240, Test: 0.5130
Epoch: 40, Loss: 1.1120, Train: 0.7643, Val: 0.5280, Test: 0.5270
Epoch: 41, Loss: 1.1246, Train: 0.7714, Val: 0.5360, Test: 0.5370
Epoch: 42, Loss: 1.1613, Train: 0.8071, Val: 0.5500, Test: 0.5410
Epoch: 43, Loss: 1.1494, Train: 0.8143, Val: 0.5560, Test: 0.5500
Epoch: 44, Loss: 0.9527, Train: 0.8357, Val: 0.5700, Test: 0.5590
Epoch: 45, Loss: 0.9959, Train: 0.8357, Val: 0.5860, Test: 0.5720
Epoch: 46, Loss: 0.9206, Train: 0.8357, Val: 0.5800, Test: 0.5830
Epoch: 47, Loss: 0.9897, Train: 0.8500, Val: 0.5840, Test: 0.5910
Epoch: 48, Loss: 0.8906, Train: 0.8571, Val: 0.5820, Test: 0.6020
Epoch: 49, Loss: 0.8766, Train: 0.8786, Val: 0.5900, Test: 0.6060
Epoch: 50, Loss: 0.7870, Train: 0.8929, Val: 0.6240, Test: 0.6220
MAD:  0.6387
Best Test Accuracy: 0.6220, Val Accuracy: 0.6240, Train Accuracy: 0.8929
Training completed.
Seed:  3
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.5279, Train: 0.1857, Val: 0.2360, Test: 0.2610
Epoch: 2, Loss: 2.3763, Train: 0.2143, Val: 0.2520, Test: 0.2560
Epoch: 3, Loss: 2.2044, Train: 0.2000, Val: 0.1680, Test: 0.1710
Epoch: 4, Loss: 2.2274, Train: 0.2214, Val: 0.1440, Test: 0.1380
Epoch: 5, Loss: 2.0729, Train: 0.2214, Val: 0.1240, Test: 0.1240
Epoch: 6, Loss: 1.9976, Train: 0.2214, Val: 0.1300, Test: 0.1190
Epoch: 7, Loss: 2.0725, Train: 0.2214, Val: 0.1360, Test: 0.1240
Epoch: 8, Loss: 2.0171, Train: 0.2357, Val: 0.1360, Test: 0.1340
Epoch: 9, Loss: 1.9170, Train: 0.2714, Val: 0.1440, Test: 0.1480
Epoch: 10, Loss: 1.9062, Train: 0.3000, Val: 0.1560, Test: 0.1610
Epoch: 11, Loss: 1.8765, Train: 0.3429, Val: 0.1700, Test: 0.1820
Epoch: 12, Loss: 1.9620, Train: 0.3857, Val: 0.2040, Test: 0.1990
Epoch: 13, Loss: 1.9282, Train: 0.4071, Val: 0.2300, Test: 0.2420
Epoch: 14, Loss: 1.8668, Train: 0.4714, Val: 0.2740, Test: 0.2980
Epoch: 15, Loss: 1.8337, Train: 0.5643, Val: 0.3580, Test: 0.3560
Epoch: 16, Loss: 1.7809, Train: 0.6357, Val: 0.4240, Test: 0.4250
Epoch: 17, Loss: 1.8230, Train: 0.6857, Val: 0.4760, Test: 0.4850
Epoch: 18, Loss: 1.8033, Train: 0.7071, Val: 0.5140, Test: 0.5230
Epoch: 19, Loss: 1.8497, Train: 0.7714, Val: 0.5340, Test: 0.5560
Epoch: 20, Loss: 1.7830, Train: 0.8143, Val: 0.5720, Test: 0.5790
Epoch: 21, Loss: 1.6648, Train: 0.8286, Val: 0.6140, Test: 0.6040
Epoch: 22, Loss: 1.6576, Train: 0.8643, Val: 0.6240, Test: 0.6270
Epoch: 23, Loss: 1.6671, Train: 0.8643, Val: 0.6380, Test: 0.6350
Epoch: 24, Loss: 1.6702, Train: 0.8643, Val: 0.6480, Test: 0.6430
Epoch: 25, Loss: 1.5709, Train: 0.8786, Val: 0.6560, Test: 0.6510
Epoch: 26, Loss: 1.6121, Train: 0.8929, Val: 0.6620, Test: 0.6580
Epoch: 27, Loss: 1.5792, Train: 0.9000, Val: 0.6600, Test: 0.6640
Epoch: 28, Loss: 1.5605, Train: 0.9000, Val: 0.6640, Test: 0.6610
Epoch: 29, Loss: 1.5518, Train: 0.9071, Val: 0.6540, Test: 0.6630
Epoch: 30, Loss: 1.5306, Train: 0.9000, Val: 0.6540, Test: 0.6510
Epoch: 31, Loss: 1.4399, Train: 0.9000, Val: 0.6560, Test: 0.6430
Epoch: 32, Loss: 1.4308, Train: 0.9000, Val: 0.6480, Test: 0.6460
Epoch: 33, Loss: 1.4105, Train: 0.9000, Val: 0.6560, Test: 0.6450
Epoch: 34, Loss: 1.2648, Train: 0.9000, Val: 0.6520, Test: 0.6490
Epoch: 35, Loss: 1.2781, Train: 0.9000, Val: 0.6540, Test: 0.6510
Epoch: 36, Loss: 1.2502, Train: 0.9071, Val: 0.6520, Test: 0.6470
Epoch: 37, Loss: 1.3148, Train: 0.9071, Val: 0.6480, Test: 0.6420
Epoch: 38, Loss: 1.2914, Train: 0.8857, Val: 0.6320, Test: 0.6430
Epoch: 39, Loss: 1.1581, Train: 0.9071, Val: 0.6340, Test: 0.6480
Epoch: 40, Loss: 1.1054, Train: 0.9071, Val: 0.6320, Test: 0.6490
Epoch: 41, Loss: 1.0916, Train: 0.9071, Val: 0.6320, Test: 0.6470
Epoch: 42, Loss: 1.1542, Train: 0.9000, Val: 0.6260, Test: 0.6420
Epoch: 43, Loss: 0.9084, Train: 0.9071, Val: 0.6340, Test: 0.6410
Epoch: 44, Loss: 0.8247, Train: 0.9071, Val: 0.6460, Test: 0.6550
Epoch: 45, Loss: 0.8761, Train: 0.9143, Val: 0.6620, Test: 0.6700
Epoch: 46, Loss: 0.8862, Train: 0.9357, Val: 0.6940, Test: 0.6860
Epoch: 47, Loss: 0.7517, Train: 0.9429, Val: 0.7280, Test: 0.7190
Epoch: 48, Loss: 0.7993, Train: 0.9429, Val: 0.7400, Test: 0.7270
Epoch: 49, Loss: 0.7095, Train: 0.9500, Val: 0.7560, Test: 0.7420
Epoch: 50, Loss: 0.7573, Train: 0.9500, Val: 0.7700, Test: 0.7510
MAD:  0.8334
Best Test Accuracy: 0.7510, Val Accuracy: 0.7700, Train Accuracy: 0.9500
Training completed.
Seed:  4
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.5527, Train: 0.2286, Val: 0.1120, Test: 0.1040
Epoch: 2, Loss: 2.4867, Train: 0.2429, Val: 0.1480, Test: 0.1460
Epoch: 3, Loss: 2.0691, Train: 0.2714, Val: 0.1840, Test: 0.1930
Epoch: 4, Loss: 2.2046, Train: 0.3000, Val: 0.2160, Test: 0.2220
Epoch: 5, Loss: 2.0905, Train: 0.3643, Val: 0.2540, Test: 0.2550
Epoch: 6, Loss: 1.9433, Train: 0.3929, Val: 0.2880, Test: 0.2940
Epoch: 7, Loss: 2.0268, Train: 0.4500, Val: 0.3140, Test: 0.3160
Epoch: 8, Loss: 1.9145, Train: 0.5000, Val: 0.3220, Test: 0.3240
Epoch: 9, Loss: 1.9049, Train: 0.5286, Val: 0.3120, Test: 0.3240
Epoch: 10, Loss: 1.8725, Train: 0.5500, Val: 0.3060, Test: 0.3280
Epoch: 11, Loss: 1.9779, Train: 0.5571, Val: 0.3000, Test: 0.3290
Epoch: 12, Loss: 1.7994, Train: 0.5857, Val: 0.3000, Test: 0.3290
Epoch: 13, Loss: 1.7425, Train: 0.6071, Val: 0.3100, Test: 0.3360
Epoch: 14, Loss: 1.8765, Train: 0.6214, Val: 0.3200, Test: 0.3470
Epoch: 15, Loss: 1.8085, Train: 0.6357, Val: 0.3380, Test: 0.3550
Epoch: 16, Loss: 1.7706, Train: 0.6643, Val: 0.3560, Test: 0.3610
Epoch: 17, Loss: 1.7908, Train: 0.6929, Val: 0.3600, Test: 0.3780
Epoch: 18, Loss: 1.7263, Train: 0.6929, Val: 0.3760, Test: 0.4060
Epoch: 19, Loss: 1.7043, Train: 0.7143, Val: 0.4020, Test: 0.4360
Epoch: 20, Loss: 1.7851, Train: 0.7500, Val: 0.4320, Test: 0.4740
Epoch: 21, Loss: 1.6437, Train: 0.7714, Val: 0.4760, Test: 0.5200
Epoch: 22, Loss: 1.5507, Train: 0.8000, Val: 0.4960, Test: 0.5580
Epoch: 23, Loss: 1.5503, Train: 0.8214, Val: 0.5220, Test: 0.5810
Epoch: 24, Loss: 1.5444, Train: 0.8286, Val: 0.5300, Test: 0.5930
Epoch: 25, Loss: 1.4990, Train: 0.8571, Val: 0.5600, Test: 0.6080
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 26, Loss: 1.4827, Train: 0.8571, Val: 0.5820, Test: 0.6160
Epoch: 27, Loss: 1.4865, Train: 0.8714, Val: 0.5920, Test: 0.6250
Epoch: 28, Loss: 1.3747, Train: 0.8714, Val: 0.6060, Test: 0.6330
Epoch: 29, Loss: 1.2652, Train: 0.8714, Val: 0.6260, Test: 0.6400
Epoch: 30, Loss: 1.3618, Train: 0.8714, Val: 0.6340, Test: 0.6510
Epoch: 31, Loss: 1.3510, Train: 0.8714, Val: 0.6460, Test: 0.6670
Epoch: 32, Loss: 1.1968, Train: 0.8857, Val: 0.6600, Test: 0.6780
Epoch: 33, Loss: 1.1596, Train: 0.8929, Val: 0.6660, Test: 0.6900
Epoch: 34, Loss: 1.1325, Train: 0.8929, Val: 0.6780, Test: 0.6970
Epoch: 35, Loss: 1.1740, Train: 0.8929, Val: 0.6900, Test: 0.7010
Epoch: 36, Loss: 1.0783, Train: 0.9000, Val: 0.7000, Test: 0.7120
Epoch: 37, Loss: 0.9934, Train: 0.9143, Val: 0.7080, Test: 0.7230
Epoch: 38, Loss: 1.0085, Train: 0.9214, Val: 0.7220, Test: 0.7210
Epoch: 39, Loss: 0.9496, Train: 0.9143, Val: 0.7180, Test: 0.7190
Epoch: 40, Loss: 0.8445, Train: 0.9143, Val: 0.7140, Test: 0.7230
Epoch: 41, Loss: 0.8959, Train: 0.9143, Val: 0.7160, Test: 0.7220
Epoch: 42, Loss: 0.7941, Train: 0.9214, Val: 0.7200, Test: 0.7280
Epoch: 43, Loss: 0.8922, Train: 0.9357, Val: 0.7220, Test: 0.7340
Epoch: 44, Loss: 0.8165, Train: 0.9429, Val: 0.7400, Test: 0.7490
Epoch: 45, Loss: 0.8138, Train: 0.9500, Val: 0.7520, Test: 0.7590
Epoch: 46, Loss: 0.7024, Train: 0.9500, Val: 0.7620, Test: 0.7770
Epoch: 47, Loss: 0.6819, Train: 0.9643, Val: 0.7700, Test: 0.7870
Epoch: 48, Loss: 0.5281, Train: 0.9714, Val: 0.7720, Test: 0.7870
Epoch: 49, Loss: 0.5125, Train: 0.9786, Val: 0.7680, Test: 0.7920
Epoch: 50, Loss: 0.5435, Train: 0.9786, Val: 0.7720, Test: 0.7940
MAD:  0.8453
Best Test Accuracy: 0.7940, Val Accuracy: 0.7720, Train Accuracy: 0.9786
Training completed.
Seed:  5
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 3.1084, Train: 0.1071, Val: 0.1480, Test: 0.1310
Epoch: 2, Loss: 2.4901, Train: 0.1214, Val: 0.1520, Test: 0.1490
Epoch: 3, Loss: 2.3888, Train: 0.2357, Val: 0.2880, Test: 0.2910
Epoch: 4, Loss: 2.1374, Train: 0.1500, Val: 0.2960, Test: 0.3200
Epoch: 5, Loss: 1.9710, Train: 0.1500, Val: 0.3160, Test: 0.3190
Epoch: 6, Loss: 2.0528, Train: 0.1500, Val: 0.3200, Test: 0.3200
Epoch: 7, Loss: 2.0090, Train: 0.1500, Val: 0.3220, Test: 0.3200
Epoch: 8, Loss: 2.0520, Train: 0.1500, Val: 0.3220, Test: 0.3210
Epoch: 9, Loss: 1.9793, Train: 0.1571, Val: 0.3220, Test: 0.3220
Epoch: 10, Loss: 1.8915, Train: 0.1571, Val: 0.3200, Test: 0.3220
Epoch: 11, Loss: 1.8838, Train: 0.1929, Val: 0.3220, Test: 0.3250
Epoch: 12, Loss: 2.0080, Train: 0.2000, Val: 0.3240, Test: 0.3300
Epoch: 13, Loss: 1.8842, Train: 0.2500, Val: 0.3380, Test: 0.3380
Epoch: 14, Loss: 1.9354, Train: 0.2714, Val: 0.3460, Test: 0.3540
Epoch: 15, Loss: 1.8750, Train: 0.3000, Val: 0.3580, Test: 0.3760
Epoch: 16, Loss: 1.8806, Train: 0.3643, Val: 0.3740, Test: 0.3950
Epoch: 17, Loss: 1.9182, Train: 0.4214, Val: 0.4060, Test: 0.4150
Epoch: 18, Loss: 1.9072, Train: 0.4500, Val: 0.4340, Test: 0.4560
Epoch: 19, Loss: 1.8656, Train: 0.5214, Val: 0.4560, Test: 0.4700
Epoch: 20, Loss: 1.8812, Train: 0.5571, Val: 0.4900, Test: 0.5000
Epoch: 21, Loss: 1.8555, Train: 0.6000, Val: 0.5180, Test: 0.5360
Epoch: 22, Loss: 1.7996, Train: 0.6500, Val: 0.5300, Test: 0.5590
Epoch: 23, Loss: 1.7372, Train: 0.7143, Val: 0.5520, Test: 0.5770
Epoch: 24, Loss: 1.7212, Train: 0.7286, Val: 0.5760, Test: 0.6040
Epoch: 25, Loss: 1.7567, Train: 0.7643, Val: 0.5980, Test: 0.6160
Epoch: 26, Loss: 1.6311, Train: 0.7643, Val: 0.5960, Test: 0.6170
Epoch: 27, Loss: 1.6824, Train: 0.7571, Val: 0.5960, Test: 0.6260
Epoch: 28, Loss: 1.6698, Train: 0.7714, Val: 0.6000, Test: 0.6360
Epoch: 29, Loss: 1.6753, Train: 0.7929, Val: 0.6000, Test: 0.6370
Epoch: 30, Loss: 1.7609, Train: 0.8000, Val: 0.6160, Test: 0.6460
Epoch: 31, Loss: 1.4768, Train: 0.8000, Val: 0.6300, Test: 0.6570
Epoch: 32, Loss: 1.5844, Train: 0.8143, Val: 0.6440, Test: 0.6600
Epoch: 33, Loss: 1.6267, Train: 0.8143, Val: 0.6380, Test: 0.6530
Epoch: 34, Loss: 1.6229, Train: 0.8143, Val: 0.6380, Test: 0.6490
Epoch: 35, Loss: 1.4675, Train: 0.8143, Val: 0.6320, Test: 0.6510
Epoch: 36, Loss: 1.3745, Train: 0.8214, Val: 0.6200, Test: 0.6410
Epoch: 37, Loss: 1.3778, Train: 0.8357, Val: 0.6260, Test: 0.6450
Epoch: 38, Loss: 1.3304, Train: 0.8429, Val: 0.6320, Test: 0.6450
Epoch: 39, Loss: 1.3392, Train: 0.8429, Val: 0.6500, Test: 0.6530
Epoch: 40, Loss: 1.2585, Train: 0.8429, Val: 0.6620, Test: 0.6580
Epoch: 41, Loss: 1.1855, Train: 0.8571, Val: 0.6700, Test: 0.6710
Epoch: 42, Loss: 1.2816, Train: 0.8786, Val: 0.6740, Test: 0.6940
Epoch: 43, Loss: 1.1289, Train: 0.8929, Val: 0.6800, Test: 0.7060
Epoch: 44, Loss: 1.0884, Train: 0.9071, Val: 0.7020, Test: 0.7240
Epoch: 45, Loss: 1.1939, Train: 0.9214, Val: 0.7240, Test: 0.7470
Epoch: 46, Loss: 1.0349, Train: 0.9214, Val: 0.7400, Test: 0.7650
Epoch: 47, Loss: 0.9581, Train: 0.9214, Val: 0.7500, Test: 0.7730
Epoch: 48, Loss: 0.9993, Train: 0.9214, Val: 0.7600, Test: 0.7800
Epoch: 49, Loss: 0.8700, Train: 0.9214, Val: 0.7600, Test: 0.7850
Epoch: 50, Loss: 0.7884, Train: 0.9214, Val: 0.7620, Test: 0.7800
MAD:  0.6925
Best Test Accuracy: 0.7850, Val Accuracy: 0.7600, Train Accuracy: 0.9214
Training completed.
Seed:  6
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.4494, Train: 0.1857, Val: 0.2000, Test: 0.1900
Epoch: 2, Loss: 2.2114, Train: 0.2786, Val: 0.2740, Test: 0.2510
Epoch: 3, Loss: 2.0872, Train: 0.3786, Val: 0.3440, Test: 0.3170
Epoch: 4, Loss: 2.0638, Train: 0.4214, Val: 0.3700, Test: 0.3560
Epoch: 5, Loss: 2.0165, Train: 0.4500, Val: 0.3640, Test: 0.3510
Epoch: 6, Loss: 2.0678, Train: 0.4071, Val: 0.3220, Test: 0.3220
Epoch: 7, Loss: 1.9704, Train: 0.4000, Val: 0.3120, Test: 0.3230
Epoch: 8, Loss: 2.0084, Train: 0.4071, Val: 0.3200, Test: 0.3290
Epoch: 9, Loss: 1.8205, Train: 0.4214, Val: 0.3320, Test: 0.3410
Epoch: 10, Loss: 1.8839, Train: 0.4571, Val: 0.3500, Test: 0.3470
Epoch: 11, Loss: 1.8941, Train: 0.4857, Val: 0.3680, Test: 0.3610
Epoch: 12, Loss: 1.8921, Train: 0.5214, Val: 0.3960, Test: 0.3830
Epoch: 13, Loss: 1.8570, Train: 0.5500, Val: 0.4260, Test: 0.4080
Epoch: 14, Loss: 1.8015, Train: 0.6143, Val: 0.4520, Test: 0.4400
Epoch: 15, Loss: 1.7396, Train: 0.6571, Val: 0.4760, Test: 0.4510
Epoch: 16, Loss: 1.7593, Train: 0.6929, Val: 0.4860, Test: 0.4800
Epoch: 17, Loss: 1.7683, Train: 0.6929, Val: 0.5020, Test: 0.4950
Epoch: 18, Loss: 1.7607, Train: 0.7500, Val: 0.5200, Test: 0.5110
Epoch: 19, Loss: 1.7132, Train: 0.7714, Val: 0.5300, Test: 0.5300
Epoch: 20, Loss: 1.6270, Train: 0.8214, Val: 0.5500, Test: 0.5450
Epoch: 21, Loss: 1.6795, Train: 0.8429, Val: 0.5540, Test: 0.5620
Epoch: 22, Loss: 1.6714, Train: 0.8500, Val: 0.5600, Test: 0.5650
Epoch: 23, Loss: 1.6263, Train: 0.8571, Val: 0.5640, Test: 0.5750
Epoch: 24, Loss: 1.5724, Train: 0.8500, Val: 0.5700, Test: 0.5810
Epoch: 25, Loss: 1.5574, Train: 0.8643, Val: 0.5940, Test: 0.5930
Epoch: 26, Loss: 1.5164, Train: 0.8500, Val: 0.6100, Test: 0.6040
Epoch: 27, Loss: 1.4739, Train: 0.8500, Val: 0.6320, Test: 0.6100
Epoch: 28, Loss: 1.3639, Train: 0.8857, Val: 0.6480, Test: 0.6160
Epoch: 29, Loss: 1.4950, Train: 0.8857, Val: 0.6500, Test: 0.6220
Epoch: 30, Loss: 1.3548, Train: 0.8929, Val: 0.6500, Test: 0.6240
Epoch: 31, Loss: 1.3292, Train: 0.8857, Val: 0.6600, Test: 0.6310
Epoch: 32, Loss: 1.3637, Train: 0.8643, Val: 0.6700, Test: 0.6250
Epoch: 33, Loss: 1.3289, Train: 0.8643, Val: 0.6680, Test: 0.6270
Epoch: 34, Loss: 1.1914, Train: 0.8643, Val: 0.6720, Test: 0.6320
Epoch: 35, Loss: 1.1454, Train: 0.8571, Val: 0.6720, Test: 0.6350
Epoch: 36, Loss: 1.1497, Train: 0.8571, Val: 0.6740, Test: 0.6350
Epoch: 37, Loss: 1.0223, Train: 0.8500, Val: 0.6780, Test: 0.6380
Epoch: 38, Loss: 1.0647, Train: 0.8643, Val: 0.6700, Test: 0.6360
Epoch: 39, Loss: 1.0532, Train: 0.8714, Val: 0.6640, Test: 0.6400
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 40, Loss: 0.8715, Train: 0.8786, Val: 0.6700, Test: 0.6450
Epoch: 41, Loss: 0.9159, Train: 0.8857, Val: 0.6680, Test: 0.6470
Epoch: 42, Loss: 0.8159, Train: 0.8857, Val: 0.6760, Test: 0.6560
Epoch: 43, Loss: 1.0498, Train: 0.8929, Val: 0.6880, Test: 0.6720
Epoch: 44, Loss: 0.7759, Train: 0.9143, Val: 0.7060, Test: 0.6800
Epoch: 45, Loss: 0.8446, Train: 0.9286, Val: 0.7180, Test: 0.6960
Epoch: 46, Loss: 0.7419, Train: 0.9500, Val: 0.7380, Test: 0.7130
Epoch: 47, Loss: 0.7159, Train: 0.9571, Val: 0.7560, Test: 0.7270
Epoch: 48, Loss: 0.7440, Train: 0.9643, Val: 0.7660, Test: 0.7310
Epoch: 49, Loss: 0.6026, Train: 0.9643, Val: 0.7720, Test: 0.7460
Epoch: 50, Loss: 0.7741, Train: 0.9714, Val: 0.7760, Test: 0.7570
MAD:  0.8157
Best Test Accuracy: 0.7570, Val Accuracy: 0.7760, Train Accuracy: 0.9714
Training completed.
Seed:  7
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.7819, Train: 0.1571, Val: 0.1760, Test: 0.1540
Epoch: 2, Loss: 2.3550, Train: 0.2786, Val: 0.2200, Test: 0.1890
Epoch: 3, Loss: 2.1689, Train: 0.2571, Val: 0.1720, Test: 0.1450
Epoch: 4, Loss: 2.1201, Train: 0.2286, Val: 0.1980, Test: 0.1730
Epoch: 5, Loss: 2.0110, Train: 0.2571, Val: 0.2540, Test: 0.2130
Epoch: 6, Loss: 1.8906, Train: 0.2714, Val: 0.2760, Test: 0.2550
Epoch: 7, Loss: 1.9560, Train: 0.2786, Val: 0.3080, Test: 0.2890
Epoch: 8, Loss: 2.0238, Train: 0.3000, Val: 0.3180, Test: 0.3120
Epoch: 9, Loss: 1.9787, Train: 0.3214, Val: 0.3340, Test: 0.3350
Epoch: 10, Loss: 1.8811, Train: 0.3571, Val: 0.3480, Test: 0.3560
Epoch: 11, Loss: 1.9270, Train: 0.3714, Val: 0.3600, Test: 0.3630
Epoch: 12, Loss: 1.8426, Train: 0.3857, Val: 0.3660, Test: 0.3660
Epoch: 13, Loss: 1.8201, Train: 0.4000, Val: 0.3740, Test: 0.3850
Epoch: 14, Loss: 1.7511, Train: 0.4643, Val: 0.3960, Test: 0.4130
Epoch: 15, Loss: 1.7623, Train: 0.5214, Val: 0.4320, Test: 0.4380
Epoch: 16, Loss: 1.7942, Train: 0.5500, Val: 0.4680, Test: 0.4670
Epoch: 17, Loss: 1.7422, Train: 0.6000, Val: 0.4960, Test: 0.4990
Epoch: 18, Loss: 1.7297, Train: 0.6571, Val: 0.5240, Test: 0.5420
Epoch: 19, Loss: 1.6375, Train: 0.7214, Val: 0.5780, Test: 0.5690
Epoch: 20, Loss: 1.6677, Train: 0.7357, Val: 0.6280, Test: 0.5990
Epoch: 21, Loss: 1.6454, Train: 0.7571, Val: 0.6540, Test: 0.6340
Epoch: 22, Loss: 1.5625, Train: 0.8000, Val: 0.6820, Test: 0.6590
Epoch: 23, Loss: 1.5597, Train: 0.8143, Val: 0.7060, Test: 0.6860
Epoch: 24, Loss: 1.5612, Train: 0.8429, Val: 0.7100, Test: 0.6980
Epoch: 25, Loss: 1.4982, Train: 0.8500, Val: 0.7300, Test: 0.7150
Epoch: 26, Loss: 1.6494, Train: 0.8571, Val: 0.7400, Test: 0.7250
Epoch: 27, Loss: 1.5343, Train: 0.8643, Val: 0.7420, Test: 0.7330
Epoch: 28, Loss: 1.4655, Train: 0.8714, Val: 0.7420, Test: 0.7440
Epoch: 29, Loss: 1.4830, Train: 0.8786, Val: 0.7360, Test: 0.7420
Epoch: 30, Loss: 1.3810, Train: 0.8786, Val: 0.7300, Test: 0.7400
Epoch: 31, Loss: 1.3346, Train: 0.8857, Val: 0.7300, Test: 0.7390
Epoch: 32, Loss: 1.2778, Train: 0.8643, Val: 0.7120, Test: 0.7370
Epoch: 33, Loss: 1.3005, Train: 0.8929, Val: 0.7200, Test: 0.7260
Epoch: 34, Loss: 1.2967, Train: 0.8929, Val: 0.7180, Test: 0.7210
Epoch: 35, Loss: 1.2812, Train: 0.8929, Val: 0.7040, Test: 0.7160
Epoch: 36, Loss: 1.1532, Train: 0.8929, Val: 0.6900, Test: 0.7080
Epoch: 37, Loss: 1.0972, Train: 0.8929, Val: 0.6940, Test: 0.7160
Epoch: 38, Loss: 1.1364, Train: 0.9000, Val: 0.6980, Test: 0.7210
Epoch: 39, Loss: 1.0705, Train: 0.9000, Val: 0.7080, Test: 0.7320
Epoch: 40, Loss: 0.8133, Train: 0.9071, Val: 0.7120, Test: 0.7410
Epoch: 41, Loss: 0.8809, Train: 0.8929, Val: 0.7200, Test: 0.7400
Epoch: 42, Loss: 0.9572, Train: 0.9000, Val: 0.7300, Test: 0.7460
Epoch: 43, Loss: 0.9333, Train: 0.9071, Val: 0.7280, Test: 0.7440
Epoch: 44, Loss: 0.7630, Train: 0.8929, Val: 0.7300, Test: 0.7470
Epoch: 45, Loss: 0.8715, Train: 0.9071, Val: 0.7440, Test: 0.7470
Epoch: 46, Loss: 0.7911, Train: 0.9286, Val: 0.7600, Test: 0.7560
Epoch: 47, Loss: 0.7985, Train: 0.9286, Val: 0.7660, Test: 0.7590
Epoch: 48, Loss: 0.7720, Train: 0.9500, Val: 0.7740, Test: 0.7650
Epoch: 49, Loss: 0.7383, Train: 0.9429, Val: 0.7780, Test: 0.7720
Epoch: 50, Loss: 0.7540, Train: 0.9429, Val: 0.7800, Test: 0.7810
MAD:  0.804
Best Test Accuracy: 0.7810, Val Accuracy: 0.7800, Train Accuracy: 0.9429
Training completed.
Seed:  8
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.7549, Train: 0.2214, Val: 0.1440, Test: 0.1620
Epoch: 2, Loss: 2.4562, Train: 0.2000, Val: 0.1460, Test: 0.1770
Epoch: 3, Loss: 2.2226, Train: 0.2429, Val: 0.1780, Test: 0.1950
Epoch: 4, Loss: 2.0764, Train: 0.2500, Val: 0.2120, Test: 0.2130
Epoch: 5, Loss: 1.9668, Train: 0.1929, Val: 0.1700, Test: 0.1790
Epoch: 6, Loss: 1.9587, Train: 0.1571, Val: 0.1680, Test: 0.1620
Epoch: 7, Loss: 2.0356, Train: 0.1500, Val: 0.1620, Test: 0.1550
Epoch: 8, Loss: 1.8711, Train: 0.1714, Val: 0.1620, Test: 0.1590
Epoch: 9, Loss: 1.9758, Train: 0.1929, Val: 0.1720, Test: 0.1660
Epoch: 10, Loss: 1.9301, Train: 0.2429, Val: 0.1860, Test: 0.1750
Epoch: 11, Loss: 1.9591, Train: 0.2714, Val: 0.2060, Test: 0.1910
Epoch: 12, Loss: 1.8589, Train: 0.3500, Val: 0.2240, Test: 0.2120
Epoch: 13, Loss: 1.8764, Train: 0.4000, Val: 0.2420, Test: 0.2390
Epoch: 14, Loss: 1.8328, Train: 0.4500, Val: 0.2720, Test: 0.2730
Epoch: 15, Loss: 1.9186, Train: 0.4929, Val: 0.3140, Test: 0.3050
Epoch: 16, Loss: 1.7842, Train: 0.5071, Val: 0.3260, Test: 0.3300
Epoch: 17, Loss: 1.8155, Train: 0.5429, Val: 0.3480, Test: 0.3450
Epoch: 18, Loss: 1.7376, Train: 0.5929, Val: 0.3580, Test: 0.3710
Epoch: 19, Loss: 1.6554, Train: 0.6500, Val: 0.3920, Test: 0.4050
Epoch: 20, Loss: 1.6923, Train: 0.6929, Val: 0.4180, Test: 0.4260
Epoch: 21, Loss: 1.7689, Train: 0.7214, Val: 0.4320, Test: 0.4520
Epoch: 22, Loss: 1.5871, Train: 0.7357, Val: 0.4680, Test: 0.4710
Epoch: 23, Loss: 1.6673, Train: 0.7571, Val: 0.4840, Test: 0.4920
Epoch: 24, Loss: 1.5523, Train: 0.7643, Val: 0.5100, Test: 0.5250
Epoch: 25, Loss: 1.6220, Train: 0.7714, Val: 0.5320, Test: 0.5360
Epoch: 26, Loss: 1.5551, Train: 0.7929, Val: 0.5560, Test: 0.5590
Epoch: 27, Loss: 1.6442, Train: 0.8071, Val: 0.5700, Test: 0.5720
Epoch: 28, Loss: 1.6132, Train: 0.8286, Val: 0.5840, Test: 0.5810
Epoch: 29, Loss: 1.5009, Train: 0.8571, Val: 0.5980, Test: 0.5960
Epoch: 30, Loss: 1.3850, Train: 0.8643, Val: 0.6100, Test: 0.6160
Epoch: 31, Loss: 1.4978, Train: 0.8857, Val: 0.6160, Test: 0.6270
Epoch: 32, Loss: 1.4268, Train: 0.8857, Val: 0.6220, Test: 0.6320
Epoch: 33, Loss: 1.2820, Train: 0.8714, Val: 0.6220, Test: 0.6380
Epoch: 34, Loss: 1.3301, Train: 0.8643, Val: 0.6400, Test: 0.6510
Epoch: 35, Loss: 1.3461, Train: 0.8571, Val: 0.6580, Test: 0.6630
Epoch: 36, Loss: 1.2860, Train: 0.8643, Val: 0.6660, Test: 0.6710
Epoch: 37, Loss: 1.1645, Train: 0.8786, Val: 0.6760, Test: 0.6830
Epoch: 38, Loss: 1.1170, Train: 0.8929, Val: 0.6840, Test: 0.6910
Epoch: 39, Loss: 1.0894, Train: 0.8929, Val: 0.6940, Test: 0.7010
Epoch: 40, Loss: 1.0824, Train: 0.9071, Val: 0.6980, Test: 0.7120
Epoch: 41, Loss: 0.9247, Train: 0.9143, Val: 0.7240, Test: 0.7250
Epoch: 42, Loss: 0.8938, Train: 0.9143, Val: 0.7400, Test: 0.7400
Epoch: 43, Loss: 0.8967, Train: 0.9286, Val: 0.7460, Test: 0.7550
Epoch: 44, Loss: 0.8316, Train: 0.9429, Val: 0.7560, Test: 0.7600
Epoch: 45, Loss: 0.7974, Train: 0.9500, Val: 0.7580, Test: 0.7680
Epoch: 46, Loss: 0.8612, Train: 0.9500, Val: 0.7740, Test: 0.7780
Epoch: 47, Loss: 0.7288, Train: 0.9429, Val: 0.7800, Test: 0.7780
Epoch: 48, Loss: 0.6423, Train: 0.9214, Val: 0.7740, Test: 0.7830
Epoch: 49, Loss: 0.7195, Train: 0.9214, Val: 0.7660, Test: 0.7830
Epoch: 50, Loss: 0.7050, Train: 0.9214, Val: 0.7620, Test: 0.7830
MAD:  0.8497
Best Test Accuracy: 0.7830, Val Accuracy: 0.7740, Train Accuracy: 0.9214
Training completed.
Seed:  9
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-5): 5 x GATConv(128, 128, heads=1)
    (6): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.4360, Train: 0.1714, Val: 0.1220, Test: 0.1150
Epoch: 2, Loss: 2.2688, Train: 0.2286, Val: 0.1020, Test: 0.1180
Epoch: 3, Loss: 2.3128, Train: 0.2500, Val: 0.1020, Test: 0.1250
Epoch: 4, Loss: 2.0945, Train: 0.2500, Val: 0.1100, Test: 0.1200
Epoch: 5, Loss: 2.2675, Train: 0.2500, Val: 0.1160, Test: 0.1210
Epoch: 6, Loss: 2.0287, Train: 0.2571, Val: 0.1300, Test: 0.1320
Epoch: 7, Loss: 1.9674, Train: 0.2571, Val: 0.1360, Test: 0.1350
Epoch: 8, Loss: 1.9283, Train: 0.2786, Val: 0.1500, Test: 0.1570
Epoch: 9, Loss: 1.9111, Train: 0.3214, Val: 0.1580, Test: 0.1670
Epoch: 10, Loss: 1.9605, Train: 0.3714, Val: 0.1880, Test: 0.1800
Epoch: 11, Loss: 1.8101, Train: 0.4143, Val: 0.2120, Test: 0.2130
Epoch: 12, Loss: 1.8961, Train: 0.4571, Val: 0.2480, Test: 0.2490
Epoch: 13, Loss: 1.7770, Train: 0.4786, Val: 0.2800, Test: 0.2890
Epoch: 14, Loss: 1.7723, Train: 0.5286, Val: 0.3140, Test: 0.3260
Epoch: 15, Loss: 1.8307, Train: 0.5786, Val: 0.3480, Test: 0.3730
Epoch: 16, Loss: 1.6988, Train: 0.6286, Val: 0.4060, Test: 0.4130
Epoch: 17, Loss: 1.7174, Train: 0.6786, Val: 0.4700, Test: 0.4540
Epoch: 18, Loss: 1.7100, Train: 0.7286, Val: 0.5100, Test: 0.5040
Epoch: 19, Loss: 1.6918, Train: 0.7571, Val: 0.5360, Test: 0.5530
Epoch: 20, Loss: 1.6730, Train: 0.7786, Val: 0.5680, Test: 0.5820
Epoch: 21, Loss: 1.5819, Train: 0.8071, Val: 0.5940, Test: 0.6180
Epoch: 22, Loss: 1.5323, Train: 0.8214, Val: 0.6100, Test: 0.6480
Epoch: 23, Loss: 1.4940, Train: 0.8429, Val: 0.6360, Test: 0.6710
Epoch: 24, Loss: 1.4082, Train: 0.8571, Val: 0.6560, Test: 0.6930
Epoch: 25, Loss: 1.4580, Train: 0.8571, Val: 0.6700, Test: 0.7020
Epoch: 26, Loss: 1.3460, Train: 0.8714, Val: 0.6940, Test: 0.7080
Epoch: 27, Loss: 1.3260, Train: 0.8643, Val: 0.6940, Test: 0.7040
Epoch: 28, Loss: 1.6556, Train: 0.8643, Val: 0.6860, Test: 0.6990
Epoch: 29, Loss: 1.2580, Train: 0.8643, Val: 0.6780, Test: 0.6980
Epoch: 30, Loss: 1.3158, Train: 0.8500, Val: 0.6680, Test: 0.6920
Epoch: 31, Loss: 1.2718, Train: 0.8500, Val: 0.6700, Test: 0.6950
Epoch: 32, Loss: 1.2286, Train: 0.8500, Val: 0.6740, Test: 0.6920
Epoch: 33, Loss: 1.0827, Train: 0.8714, Val: 0.6800, Test: 0.6910
Epoch: 34, Loss: 1.0735, Train: 0.8786, Val: 0.6940, Test: 0.7030
Epoch: 35, Loss: 1.0113, Train: 0.8929, Val: 0.6940, Test: 0.7030
Epoch: 36, Loss: 1.1028, Train: 0.9000, Val: 0.6880, Test: 0.6950
Epoch: 37, Loss: 1.0075, Train: 0.9000, Val: 0.6740, Test: 0.6770
Epoch: 38, Loss: 0.9349, Train: 0.9071, Val: 0.6640, Test: 0.6620
Epoch: 39, Loss: 0.8844, Train: 0.9000, Val: 0.6540, Test: 0.6420
Epoch: 40, Loss: 0.8313, Train: 0.8929, Val: 0.6340, Test: 0.6390
Epoch: 41, Loss: 0.7948, Train: 0.8857, Val: 0.6160, Test: 0.6260
Epoch: 42, Loss: 0.7589, Train: 0.8857, Val: 0.6160, Test: 0.6130
Epoch: 43, Loss: 0.8037, Train: 0.8857, Val: 0.6140, Test: 0.6150
Epoch: 44, Loss: 0.6892, Train: 0.8857, Val: 0.6200, Test: 0.6190
Epoch: 45, Loss: 0.7251, Train: 0.8929, Val: 0.6180, Test: 0.6230
Epoch: 46, Loss: 0.7107, Train: 0.8929, Val: 0.6180, Test: 0.6200
Epoch: 47, Loss: 0.7276, Train: 0.9000, Val: 0.6200, Test: 0.6240
Epoch: 48, Loss: 0.6444, Train: 0.9143, Val: 0.6380, Test: 0.6420
Epoch: 49, Loss: 0.6097, Train: 0.9143, Val: 0.6680, Test: 0.6600
Epoch: 50, Loss: 0.6542, Train: 0.9214, Val: 0.6900, Test: 0.6820
MAD:  0.9195
Best Test Accuracy: 0.7080, Val Accuracy: 0.6940, Train Accuracy: 0.8714
Training completed.
Average Test Accuracy:  0.7554000000000001 ± 0.05091797325110261
Average MAD:  0.8153900000000001 ± 0.0832644636084326

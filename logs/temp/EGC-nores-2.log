/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.2357, Val: 0.1240, Test: 0.1290
Epoch: 2, Loss: 1.9459, Train: 0.2214, Val: 0.1940, Test: 0.1620
Epoch: 3, Loss: 1.9458, Train: 0.2571, Val: 0.2240, Test: 0.2570
Epoch: 4, Loss: 1.9458, Train: 0.3214, Val: 0.2680, Test: 0.3190
Epoch: 5, Loss: 1.9457, Train: 0.3143, Val: 0.2760, Test: 0.3200
Epoch: 6, Loss: 1.9456, Train: 0.3929, Val: 0.3000, Test: 0.3390
Epoch: 7, Loss: 1.9455, Train: 0.8714, Val: 0.5640, Test: 0.5840
Epoch: 8, Loss: 1.9453, Train: 0.7643, Val: 0.4520, Test: 0.4530
Epoch: 9, Loss: 1.9450, Train: 0.7571, Val: 0.4740, Test: 0.4720
Epoch: 10, Loss: 1.9447, Train: 0.8000, Val: 0.5180, Test: 0.5300
Epoch: 11, Loss: 1.9443, Train: 0.8571, Val: 0.6020, Test: 0.6270
Epoch: 12, Loss: 1.9437, Train: 0.9143, Val: 0.6920, Test: 0.7150
Epoch: 13, Loss: 1.9429, Train: 0.9286, Val: 0.7240, Test: 0.7540
Epoch: 14, Loss: 1.9419, Train: 0.9143, Val: 0.7220, Test: 0.7580
Epoch: 15, Loss: 1.9405, Train: 0.9143, Val: 0.7120, Test: 0.7520
Epoch: 16, Loss: 1.9379, Train: 0.9143, Val: 0.6800, Test: 0.7110
Epoch: 17, Loss: 1.9339, Train: 0.8500, Val: 0.6080, Test: 0.6470
Epoch: 18, Loss: 1.9266, Train: 0.7786, Val: 0.4980, Test: 0.5260
Epoch: 19, Loss: 1.9142, Train: 0.6500, Val: 0.3420, Test: 0.3770
Epoch: 20, Loss: 1.8945, Train: 0.4786, Val: 0.2240, Test: 0.2510
Epoch: 21, Loss: 1.8511, Train: 0.3857, Val: 0.1580, Test: 0.1920
Epoch: 22, Loss: 1.8051, Train: 0.3214, Val: 0.1380, Test: 0.1720
Epoch: 23, Loss: 1.7177, Train: 0.3286, Val: 0.1440, Test: 0.1740
Epoch: 24, Loss: 1.6035, Train: 0.4857, Val: 0.2200, Test: 0.2480
Epoch: 25, Loss: 1.4277, Train: 0.6714, Val: 0.4080, Test: 0.4520
Epoch: 26, Loss: 1.2608, Train: 0.9000, Val: 0.6060, Test: 0.6560
Epoch: 27, Loss: 1.0028, Train: 0.9429, Val: 0.7740, Test: 0.7960
Epoch: 28, Loss: 0.8466, Train: 0.9714, Val: 0.7900, Test: 0.8170
Epoch: 29, Loss: 0.6911, Train: 0.9500, Val: 0.7620, Test: 0.7760
Epoch: 30, Loss: 0.4655, Train: 0.9429, Val: 0.6960, Test: 0.7150
Epoch: 31, Loss: 0.4637, Train: 0.9643, Val: 0.7640, Test: 0.7780
Epoch: 32, Loss: 0.3782, Train: 0.9786, Val: 0.7820, Test: 0.7950
Epoch: 33, Loss: 0.2509, Train: 0.9929, Val: 0.7960, Test: 0.8110
Epoch: 34, Loss: 0.2641, Train: 0.9857, Val: 0.7820, Test: 0.7900
Epoch: 35, Loss: 0.2611, Train: 0.9929, Val: 0.7820, Test: 0.7880
Epoch: 36, Loss: 0.1845, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 37, Loss: 0.1588, Train: 1.0000, Val: 0.7700, Test: 0.7990
Epoch: 38, Loss: 0.1246, Train: 1.0000, Val: 0.7760, Test: 0.7890
Epoch: 39, Loss: 0.1007, Train: 1.0000, Val: 0.7640, Test: 0.7820
Epoch: 40, Loss: 0.0731, Train: 0.9929, Val: 0.7380, Test: 0.7720
Epoch: 41, Loss: 0.0919, Train: 1.0000, Val: 0.7380, Test: 0.7650
Epoch: 42, Loss: 0.0491, Train: 1.0000, Val: 0.7240, Test: 0.7520
Epoch: 43, Loss: 0.0480, Train: 1.0000, Val: 0.7240, Test: 0.7490
Epoch: 44, Loss: 0.0398, Train: 1.0000, Val: 0.7240, Test: 0.7620
Epoch: 45, Loss: 0.0549, Train: 1.0000, Val: 0.7600, Test: 0.7810
Epoch: 46, Loss: 0.0364, Train: 1.0000, Val: 0.7600, Test: 0.7860
Epoch: 47, Loss: 0.0327, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 48, Loss: 0.0353, Train: 1.0000, Val: 0.7660, Test: 0.7810
Epoch: 49, Loss: 0.0286, Train: 1.0000, Val: 0.7500, Test: 0.7770
Epoch: 50, Loss: 0.0520, Train: 1.0000, Val: 0.7500, Test: 0.7690
MAD:  0.6545
Best Test Accuracy: 0.8170, Val Accuracy: 0.7900, Train Accuracy: 0.9714
Training completed.
Seed:  1
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.1786, Val: 0.0940, Test: 0.0870
Epoch: 2, Loss: 1.9458, Train: 0.2857, Val: 0.1320, Test: 0.1320
Epoch: 3, Loss: 1.9458, Train: 0.2929, Val: 0.2020, Test: 0.2370
Epoch: 4, Loss: 1.9457, Train: 0.4500, Val: 0.3860, Test: 0.4220
Epoch: 5, Loss: 1.9456, Train: 0.4714, Val: 0.4220, Test: 0.4490
Epoch: 6, Loss: 1.9455, Train: 0.6071, Val: 0.4520, Test: 0.5050
Epoch: 7, Loss: 1.9453, Train: 0.8286, Val: 0.5660, Test: 0.6080
Epoch: 8, Loss: 1.9451, Train: 0.7857, Val: 0.4260, Test: 0.4460
Epoch: 9, Loss: 1.9447, Train: 0.7714, Val: 0.4300, Test: 0.4430
Epoch: 10, Loss: 1.9442, Train: 0.8143, Val: 0.4960, Test: 0.5120
Epoch: 11, Loss: 1.9436, Train: 0.8143, Val: 0.5820, Test: 0.5980
Epoch: 12, Loss: 1.9430, Train: 0.8429, Val: 0.6400, Test: 0.6360
Epoch: 13, Loss: 1.9410, Train: 0.7786, Val: 0.5900, Test: 0.5890
Epoch: 14, Loss: 1.9391, Train: 0.7000, Val: 0.4720, Test: 0.4850
Epoch: 15, Loss: 1.9351, Train: 0.5786, Val: 0.3260, Test: 0.3580
Epoch: 16, Loss: 1.9265, Train: 0.4071, Val: 0.1840, Test: 0.2230
Epoch: 17, Loss: 1.9138, Train: 0.2929, Val: 0.1180, Test: 0.1490
Epoch: 18, Loss: 1.8892, Train: 0.2000, Val: 0.0920, Test: 0.1140
Epoch: 19, Loss: 1.8485, Train: 0.1929, Val: 0.0800, Test: 0.1060
Epoch: 20, Loss: 1.7841, Train: 0.1714, Val: 0.0820, Test: 0.1010
Epoch: 21, Loss: 1.7256, Train: 0.2214, Val: 0.1340, Test: 0.1540
Epoch: 22, Loss: 1.6735, Train: 0.3286, Val: 0.1900, Test: 0.2220
Epoch: 23, Loss: 1.5558, Train: 0.4429, Val: 0.1940, Test: 0.2370
Epoch: 24, Loss: 1.4767, Train: 0.6143, Val: 0.4140, Test: 0.4550
Epoch: 25, Loss: 1.3534, Train: 0.7714, Val: 0.5440, Test: 0.5760
Epoch: 26, Loss: 1.1830, Train: 0.8143, Val: 0.5520, Test: 0.5980
Epoch: 27, Loss: 1.0845, Train: 0.8500, Val: 0.6320, Test: 0.6650
Epoch: 28, Loss: 0.9310, Train: 0.9286, Val: 0.6340, Test: 0.6730
Epoch: 29, Loss: 0.7786, Train: 0.9429, Val: 0.6380, Test: 0.6820
Epoch: 30, Loss: 0.5803, Train: 0.9786, Val: 0.6780, Test: 0.7140
Epoch: 31, Loss: 0.5536, Train: 0.9643, Val: 0.7220, Test: 0.7260
Epoch: 32, Loss: 0.3753, Train: 0.9857, Val: 0.7480, Test: 0.7530
Epoch: 33, Loss: 0.3328, Train: 0.9929, Val: 0.7500, Test: 0.7880
Epoch: 34, Loss: 0.2395, Train: 0.9714, Val: 0.7440, Test: 0.7680
Epoch: 35, Loss: 0.3222, Train: 0.9714, Val: 0.7400, Test: 0.7680
Epoch: 36, Loss: 0.2965, Train: 0.9929, Val: 0.7680, Test: 0.7900
Epoch: 37, Loss: 0.1981, Train: 0.9929, Val: 0.7560, Test: 0.7830
Epoch: 38, Loss: 0.1722, Train: 0.9929, Val: 0.7200, Test: 0.7450
Epoch: 39, Loss: 0.1419, Train: 0.9857, Val: 0.6820, Test: 0.7180
Epoch: 40, Loss: 0.1647, Train: 1.0000, Val: 0.7160, Test: 0.7360
Epoch: 41, Loss: 0.1613, Train: 1.0000, Val: 0.7240, Test: 0.7460
Epoch: 42, Loss: 0.0464, Train: 1.0000, Val: 0.7420, Test: 0.7560
Epoch: 43, Loss: 0.0795, Train: 1.0000, Val: 0.7540, Test: 0.7720
Epoch: 44, Loss: 0.0434, Train: 1.0000, Val: 0.7620, Test: 0.7800
Epoch: 45, Loss: 0.0665, Train: 1.0000, Val: 0.7580, Test: 0.7760
Epoch: 46, Loss: 0.0764, Train: 1.0000, Val: 0.7620, Test: 0.7850
Epoch: 47, Loss: 0.0369, Train: 1.0000, Val: 0.7640, Test: 0.7800
Epoch: 48, Loss: 0.1474, Train: 1.0000, Val: 0.7660, Test: 0.7910
Epoch: 49, Loss: 0.0313, Train: 1.0000, Val: 0.7760, Test: 0.7970
Epoch: 50, Loss: 0.0206, Train: 1.0000, Val: 0.7700, Test: 0.7930
MAD:  0.6921
Best Test Accuracy: 0.7970, Val Accuracy: 0.7760, Train Accuracy: 1.0000
Training completed.
Seed:  2
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.2571, Val: 0.1760, Test: 0.2020
Epoch: 2, Loss: 1.9458, Train: 0.2714, Val: 0.2120, Test: 0.2230
Epoch: 3, Loss: 1.9457, Train: 0.3429, Val: 0.2400, Test: 0.2310
Epoch: 4, Loss: 1.9456, Train: 0.3643, Val: 0.2820, Test: 0.2700
Epoch: 5, Loss: 1.9454, Train: 0.4071, Val: 0.2820, Test: 0.2750
Epoch: 6, Loss: 1.9452, Train: 0.7071, Val: 0.4700, Test: 0.4980
Epoch: 7, Loss: 1.9449, Train: 0.8786, Val: 0.6980, Test: 0.7310
Epoch: 8, Loss: 1.9444, Train: 0.8714, Val: 0.6980, Test: 0.7180
Epoch: 9, Loss: 1.9438, Train: 0.8857, Val: 0.6940, Test: 0.7220
Epoch: 10, Loss: 1.9430, Train: 0.8786, Val: 0.6640, Test: 0.7060
Epoch: 11, Loss: 1.9416, Train: 0.8857, Val: 0.6580, Test: 0.6840
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 12, Loss: 1.9399, Train: 0.8857, Val: 0.6500, Test: 0.6580
Epoch: 13, Loss: 1.9371, Train: 0.8571, Val: 0.6000, Test: 0.6130
Epoch: 14, Loss: 1.9321, Train: 0.7571, Val: 0.4840, Test: 0.5120
Epoch: 15, Loss: 1.9236, Train: 0.6857, Val: 0.3260, Test: 0.3650
Epoch: 16, Loss: 1.9096, Train: 0.6000, Val: 0.2300, Test: 0.2420
Epoch: 17, Loss: 1.8854, Train: 0.3929, Val: 0.1500, Test: 0.1850
Epoch: 18, Loss: 1.8457, Train: 0.3286, Val: 0.1240, Test: 0.1430
Epoch: 19, Loss: 1.7737, Train: 0.3071, Val: 0.1100, Test: 0.1240
Epoch: 20, Loss: 1.6793, Train: 0.3000, Val: 0.1160, Test: 0.1380
Epoch: 21, Loss: 1.5740, Train: 0.4429, Val: 0.1880, Test: 0.2060
Epoch: 22, Loss: 1.4445, Train: 0.7357, Val: 0.3900, Test: 0.4220
Epoch: 23, Loss: 1.2211, Train: 0.8429, Val: 0.5280, Test: 0.5740
Epoch: 24, Loss: 1.0240, Train: 0.9214, Val: 0.7000, Test: 0.7070
Epoch: 25, Loss: 0.8343, Train: 0.9714, Val: 0.7600, Test: 0.8020
Epoch: 26, Loss: 0.6724, Train: 0.9714, Val: 0.7860, Test: 0.8100
Epoch: 27, Loss: 0.5604, Train: 0.9571, Val: 0.7200, Test: 0.7710
Epoch: 28, Loss: 0.6291, Train: 0.9500, Val: 0.7260, Test: 0.7640
Epoch: 29, Loss: 0.5726, Train: 0.9643, Val: 0.7420, Test: 0.7830
Epoch: 30, Loss: 0.4324, Train: 0.9857, Val: 0.7620, Test: 0.7840
Epoch: 31, Loss: 0.2168, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 32, Loss: 0.2862, Train: 0.9786, Val: 0.7540, Test: 0.7540
Epoch: 33, Loss: 0.2742, Train: 0.9929, Val: 0.7460, Test: 0.7690
Epoch: 34, Loss: 0.1661, Train: 1.0000, Val: 0.7520, Test: 0.7770
Epoch: 35, Loss: 0.1036, Train: 0.9929, Val: 0.7620, Test: 0.7720
Epoch: 36, Loss: 0.1742, Train: 0.9929, Val: 0.7600, Test: 0.7750
Epoch: 37, Loss: 0.0856, Train: 1.0000, Val: 0.7600, Test: 0.7770
Epoch: 38, Loss: 0.0953, Train: 1.0000, Val: 0.7680, Test: 0.7810
Epoch: 39, Loss: 0.0704, Train: 1.0000, Val: 0.7640, Test: 0.7810
Epoch: 40, Loss: 0.0638, Train: 1.0000, Val: 0.7660, Test: 0.7800
Epoch: 41, Loss: 0.0455, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 42, Loss: 0.0420, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 43, Loss: 0.0392, Train: 1.0000, Val: 0.7360, Test: 0.7390
Epoch: 44, Loss: 0.0339, Train: 1.0000, Val: 0.7200, Test: 0.7320
Epoch: 45, Loss: 0.0388, Train: 1.0000, Val: 0.7200, Test: 0.7290
Epoch: 46, Loss: 0.0295, Train: 1.0000, Val: 0.7220, Test: 0.7270
Epoch: 47, Loss: 0.0445, Train: 1.0000, Val: 0.7260, Test: 0.7300
Epoch: 48, Loss: 0.0199, Train: 1.0000, Val: 0.7340, Test: 0.7390
Epoch: 49, Loss: 0.0216, Train: 1.0000, Val: 0.7340, Test: 0.7450
Epoch: 50, Loss: 0.0219, Train: 1.0000, Val: 0.7440, Test: 0.7510
MAD:  0.5428
Best Test Accuracy: 0.8100, Val Accuracy: 0.7860, Train Accuracy: 0.9714
Training completed.
Seed:  3
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9460, Train: 0.2000, Val: 0.0940, Test: 0.1010
Epoch: 2, Loss: 1.9457, Train: 0.3786, Val: 0.1560, Test: 0.1690
Epoch: 3, Loss: 1.9455, Train: 0.6643, Val: 0.4860, Test: 0.4980
Epoch: 4, Loss: 1.9452, Train: 0.6786, Val: 0.5160, Test: 0.5350
Epoch: 5, Loss: 1.9448, Train: 0.6857, Val: 0.5340, Test: 0.5440
Epoch: 6, Loss: 1.9442, Train: 0.8500, Val: 0.6220, Test: 0.6470
Epoch: 7, Loss: 1.9436, Train: 0.8929, Val: 0.6880, Test: 0.6920
Epoch: 8, Loss: 1.9425, Train: 0.9286, Val: 0.7020, Test: 0.6970
Epoch: 9, Loss: 1.9413, Train: 0.9500, Val: 0.7020, Test: 0.7150
Epoch: 10, Loss: 1.9392, Train: 0.9500, Val: 0.7160, Test: 0.7280
Epoch: 11, Loss: 1.9362, Train: 0.9143, Val: 0.7120, Test: 0.7360
Epoch: 12, Loss: 1.9325, Train: 0.9000, Val: 0.7140, Test: 0.7320
Epoch: 13, Loss: 1.9258, Train: 0.8786, Val: 0.6800, Test: 0.6900
Epoch: 14, Loss: 1.9124, Train: 0.8286, Val: 0.6360, Test: 0.6480
Epoch: 15, Loss: 1.8938, Train: 0.8071, Val: 0.5560, Test: 0.5920
Epoch: 16, Loss: 1.8586, Train: 0.7357, Val: 0.5080, Test: 0.5420
Epoch: 17, Loss: 1.7966, Train: 0.6786, Val: 0.4680, Test: 0.4890
Epoch: 18, Loss: 1.7143, Train: 0.6071, Val: 0.4280, Test: 0.4590
Epoch: 19, Loss: 1.5713, Train: 0.6071, Val: 0.4320, Test: 0.4620
Epoch: 20, Loss: 1.4126, Train: 0.6714, Val: 0.4860, Test: 0.5200
Epoch: 21, Loss: 1.2718, Train: 0.8000, Val: 0.5900, Test: 0.6430
Epoch: 22, Loss: 1.0588, Train: 0.9571, Val: 0.7400, Test: 0.7730
Epoch: 23, Loss: 0.8451, Train: 0.9286, Val: 0.7380, Test: 0.7580
Epoch: 24, Loss: 0.7047, Train: 0.9357, Val: 0.6940, Test: 0.7050
Epoch: 25, Loss: 0.6461, Train: 0.9500, Val: 0.7040, Test: 0.7270
Epoch: 26, Loss: 0.4633, Train: 0.9786, Val: 0.7460, Test: 0.7660
Epoch: 27, Loss: 0.4104, Train: 0.9571, Val: 0.7340, Test: 0.7730
Epoch: 28, Loss: 0.3379, Train: 0.9571, Val: 0.7120, Test: 0.7670
Epoch: 29, Loss: 0.2913, Train: 0.9714, Val: 0.7520, Test: 0.7670
Epoch: 30, Loss: 0.3421, Train: 0.9786, Val: 0.7340, Test: 0.7520
Epoch: 31, Loss: 0.1753, Train: 0.9714, Val: 0.7200, Test: 0.7300
Epoch: 32, Loss: 0.2185, Train: 0.9714, Val: 0.7360, Test: 0.7320
Epoch: 33, Loss: 0.2083, Train: 0.9786, Val: 0.7560, Test: 0.7700
Epoch: 34, Loss: 0.1409, Train: 0.9857, Val: 0.7660, Test: 0.7960
Epoch: 35, Loss: 0.1098, Train: 0.9857, Val: 0.7740, Test: 0.8120
Epoch: 36, Loss: 0.0937, Train: 0.9857, Val: 0.7760, Test: 0.8120
Epoch: 37, Loss: 0.1439, Train: 1.0000, Val: 0.7860, Test: 0.8190
Epoch: 38, Loss: 0.0657, Train: 1.0000, Val: 0.7720, Test: 0.8070
Epoch: 39, Loss: 0.0601, Train: 1.0000, Val: 0.7620, Test: 0.7830
Epoch: 40, Loss: 0.0528, Train: 1.0000, Val: 0.7620, Test: 0.7690
Epoch: 41, Loss: 0.0443, Train: 1.0000, Val: 0.7440, Test: 0.7540
Epoch: 42, Loss: 0.0388, Train: 1.0000, Val: 0.7320, Test: 0.7350
Epoch: 43, Loss: 0.0336, Train: 1.0000, Val: 0.7260, Test: 0.7330
Epoch: 44, Loss: 0.0269, Train: 1.0000, Val: 0.7180, Test: 0.7270
Epoch: 45, Loss: 0.0356, Train: 1.0000, Val: 0.7240, Test: 0.7300
Epoch: 46, Loss: 0.0210, Train: 1.0000, Val: 0.7380, Test: 0.7360
Epoch: 47, Loss: 0.0534, Train: 1.0000, Val: 0.7320, Test: 0.7480
Epoch: 48, Loss: 0.0490, Train: 1.0000, Val: 0.7480, Test: 0.7550
Epoch: 49, Loss: 0.0212, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 50, Loss: 0.0240, Train: 1.0000, Val: 0.7500, Test: 0.7630
MAD:  0.7466
Best Test Accuracy: 0.8190, Val Accuracy: 0.7860, Train Accuracy: 1.0000
Training completed.
Seed:  4
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.1929, Val: 0.1560, Test: 0.1420
Epoch: 2, Loss: 1.9458, Train: 0.3143, Val: 0.2320, Test: 0.2220
Epoch: 3, Loss: 1.9457, Train: 0.3571, Val: 0.2140, Test: 0.2370
Epoch: 4, Loss: 1.9456, Train: 0.4000, Val: 0.2700, Test: 0.2490
Epoch: 5, Loss: 1.9454, Train: 0.4429, Val: 0.2800, Test: 0.2690
Epoch: 6, Loss: 1.9452, Train: 0.6857, Val: 0.4500, Test: 0.4410
Epoch: 7, Loss: 1.9447, Train: 0.7571, Val: 0.6280, Test: 0.6290
Epoch: 8, Loss: 1.9444, Train: 0.7714, Val: 0.6260, Test: 0.6460
Epoch: 9, Loss: 1.9438, Train: 0.7786, Val: 0.6420, Test: 0.6590
Epoch: 10, Loss: 1.9425, Train: 0.8071, Val: 0.6420, Test: 0.6840
Epoch: 11, Loss: 1.9411, Train: 0.8143, Val: 0.6360, Test: 0.6550
Epoch: 12, Loss: 1.9395, Train: 0.7929, Val: 0.6140, Test: 0.6040
Epoch: 13, Loss: 1.9350, Train: 0.7571, Val: 0.5640, Test: 0.5470
Epoch: 14, Loss: 1.9284, Train: 0.7000, Val: 0.4600, Test: 0.4580
Epoch: 15, Loss: 1.9175, Train: 0.6143, Val: 0.3540, Test: 0.3740
Epoch: 16, Loss: 1.8995, Train: 0.5286, Val: 0.2800, Test: 0.2920
Epoch: 17, Loss: 1.8684, Train: 0.4071, Val: 0.2060, Test: 0.2300
Epoch: 18, Loss: 1.8155, Train: 0.3286, Val: 0.1460, Test: 0.1790
Epoch: 19, Loss: 1.7588, Train: 0.2929, Val: 0.1280, Test: 0.1510
Epoch: 20, Loss: 1.6792, Train: 0.3214, Val: 0.1580, Test: 0.1830
Epoch: 21, Loss: 1.6084, Train: 0.4214, Val: 0.2500, Test: 0.2710
Epoch: 22, Loss: 1.5208, Train: 0.5286, Val: 0.3280, Test: 0.3430
Epoch: 23, Loss: 1.4201, Train: 0.6357, Val: 0.3900, Test: 0.4000
Epoch: 24, Loss: 1.3250, Train: 0.6429, Val: 0.3900, Test: 0.3950
Epoch: 25, Loss: 1.2331, Train: 0.6714, Val: 0.4140, Test: 0.4160
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 26, Loss: 1.1037, Train: 0.7143, Val: 0.4420, Test: 0.4530
Epoch: 27, Loss: 0.9510, Train: 0.8571, Val: 0.5020, Test: 0.5190
Epoch: 28, Loss: 0.8072, Train: 0.8929, Val: 0.5700, Test: 0.5950
Epoch: 29, Loss: 0.6833, Train: 0.9571, Val: 0.6860, Test: 0.7020
Epoch: 30, Loss: 0.5599, Train: 0.9643, Val: 0.7760, Test: 0.7750
Epoch: 31, Loss: 0.4800, Train: 0.9786, Val: 0.7840, Test: 0.8110
Epoch: 32, Loss: 0.4994, Train: 0.9714, Val: 0.7420, Test: 0.7430
Epoch: 33, Loss: 0.4338, Train: 0.9643, Val: 0.6920, Test: 0.7190
Epoch: 34, Loss: 0.3204, Train: 0.9786, Val: 0.6700, Test: 0.7070
Epoch: 35, Loss: 0.2609, Train: 0.9786, Val: 0.6820, Test: 0.7200
Epoch: 36, Loss: 0.2819, Train: 0.9929, Val: 0.7260, Test: 0.7470
Epoch: 37, Loss: 0.2533, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 38, Loss: 0.0964, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 39, Loss: 0.1865, Train: 1.0000, Val: 0.7580, Test: 0.7870
Epoch: 40, Loss: 0.0755, Train: 1.0000, Val: 0.7580, Test: 0.7830
Epoch: 41, Loss: 0.1264, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 42, Loss: 0.0685, Train: 1.0000, Val: 0.7780, Test: 0.8100
Epoch: 43, Loss: 0.0408, Train: 1.0000, Val: 0.7880, Test: 0.8160
Epoch: 44, Loss: 0.0368, Train: 1.0000, Val: 0.7860, Test: 0.8120
Epoch: 45, Loss: 0.0404, Train: 1.0000, Val: 0.7840, Test: 0.8040
Epoch: 46, Loss: 0.0253, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 47, Loss: 0.0248, Train: 1.0000, Val: 0.7740, Test: 0.7920
Epoch: 48, Loss: 0.0328, Train: 1.0000, Val: 0.7760, Test: 0.7900
Epoch: 49, Loss: 0.0319, Train: 1.0000, Val: 0.7800, Test: 0.7880
Epoch: 50, Loss: 0.0157, Train: 1.0000, Val: 0.7820, Test: 0.7860
MAD:  0.8041
Best Test Accuracy: 0.8160, Val Accuracy: 0.7880, Train Accuracy: 1.0000
Training completed.
Seed:  5
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.2357, Val: 0.1880, Test: 0.1870
Epoch: 2, Loss: 1.9459, Train: 0.3000, Val: 0.2140, Test: 0.2400
Epoch: 3, Loss: 1.9458, Train: 0.2786, Val: 0.2440, Test: 0.2330
Epoch: 4, Loss: 1.9457, Train: 0.3571, Val: 0.2860, Test: 0.2670
Epoch: 5, Loss: 1.9457, Train: 0.3500, Val: 0.2660, Test: 0.2630
Epoch: 6, Loss: 1.9456, Train: 0.4071, Val: 0.2520, Test: 0.2610
Epoch: 7, Loss: 1.9455, Train: 0.8071, Val: 0.5860, Test: 0.6060
Epoch: 8, Loss: 1.9453, Train: 0.7214, Val: 0.5360, Test: 0.5430
Epoch: 9, Loss: 1.9451, Train: 0.6857, Val: 0.5300, Test: 0.5320
Epoch: 10, Loss: 1.9448, Train: 0.7429, Val: 0.5620, Test: 0.5620
Epoch: 11, Loss: 1.9444, Train: 0.8214, Val: 0.6360, Test: 0.6340
Epoch: 12, Loss: 1.9437, Train: 0.8429, Val: 0.7080, Test: 0.7080
Epoch: 13, Loss: 1.9431, Train: 0.8500, Val: 0.6880, Test: 0.7130
Epoch: 14, Loss: 1.9418, Train: 0.8571, Val: 0.6700, Test: 0.6830
Epoch: 15, Loss: 1.9399, Train: 0.8500, Val: 0.6580, Test: 0.6700
Epoch: 16, Loss: 1.9372, Train: 0.8143, Val: 0.6180, Test: 0.6380
Epoch: 17, Loss: 1.9305, Train: 0.7929, Val: 0.5300, Test: 0.5740
Epoch: 18, Loss: 1.9247, Train: 0.7571, Val: 0.4360, Test: 0.4870
Epoch: 19, Loss: 1.9085, Train: 0.6429, Val: 0.3400, Test: 0.3720
Epoch: 20, Loss: 1.8774, Train: 0.5714, Val: 0.2540, Test: 0.2850
Epoch: 21, Loss: 1.8348, Train: 0.4643, Val: 0.2060, Test: 0.2470
Epoch: 22, Loss: 1.7695, Train: 0.4214, Val: 0.1960, Test: 0.2310
Epoch: 23, Loss: 1.6853, Train: 0.4500, Val: 0.1980, Test: 0.2350
Epoch: 24, Loss: 1.5682, Train: 0.6429, Val: 0.2600, Test: 0.3070
Epoch: 25, Loss: 1.4287, Train: 0.8286, Val: 0.6040, Test: 0.6080
Epoch: 26, Loss: 1.2374, Train: 0.9571, Val: 0.7180, Test: 0.7680
Epoch: 27, Loss: 0.9127, Train: 0.9429, Val: 0.7380, Test: 0.7420
Epoch: 28, Loss: 0.9908, Train: 0.9643, Val: 0.7940, Test: 0.8160
Epoch: 29, Loss: 0.6976, Train: 0.9429, Val: 0.7420, Test: 0.7780
Epoch: 30, Loss: 0.6694, Train: 0.9286, Val: 0.7320, Test: 0.7520
Epoch: 31, Loss: 0.5650, Train: 0.9357, Val: 0.7360, Test: 0.7520
Epoch: 32, Loss: 0.4143, Train: 0.9714, Val: 0.7580, Test: 0.7780
Epoch: 33, Loss: 0.3778, Train: 0.9929, Val: 0.7780, Test: 0.7900
Epoch: 34, Loss: 0.2598, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 35, Loss: 0.2649, Train: 0.9929, Val: 0.7480, Test: 0.7760
Epoch: 36, Loss: 0.2629, Train: 0.9929, Val: 0.7520, Test: 0.7830
Epoch: 37, Loss: 0.2198, Train: 0.9857, Val: 0.7600, Test: 0.7740
Epoch: 38, Loss: 0.1554, Train: 0.9857, Val: 0.7360, Test: 0.7630
Epoch: 39, Loss: 0.1328, Train: 0.9786, Val: 0.7320, Test: 0.7460
Epoch: 40, Loss: 0.1815, Train: 0.9857, Val: 0.7380, Test: 0.7630
Epoch: 41, Loss: 0.1184, Train: 0.9857, Val: 0.7440, Test: 0.7580
Epoch: 42, Loss: 0.1203, Train: 0.9929, Val: 0.7460, Test: 0.7730
Epoch: 43, Loss: 0.0946, Train: 0.9929, Val: 0.7440, Test: 0.7650
Epoch: 44, Loss: 0.1460, Train: 0.9929, Val: 0.7460, Test: 0.7540
Epoch: 45, Loss: 0.0640, Train: 0.9929, Val: 0.7400, Test: 0.7510
Epoch: 46, Loss: 0.0522, Train: 0.9929, Val: 0.7320, Test: 0.7470
Epoch: 47, Loss: 0.0562, Train: 1.0000, Val: 0.7240, Test: 0.7420
Epoch: 48, Loss: 0.0354, Train: 1.0000, Val: 0.7200, Test: 0.7380
Epoch: 49, Loss: 0.0454, Train: 1.0000, Val: 0.7220, Test: 0.7340
Epoch: 50, Loss: 0.0397, Train: 1.0000, Val: 0.7260, Test: 0.7360
MAD:  0.4941
Best Test Accuracy: 0.8160, Val Accuracy: 0.7940, Train Accuracy: 0.9643
Training completed.
Seed:  6
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.3357, Val: 0.1780, Test: 0.1860
Epoch: 2, Loss: 1.9456, Train: 0.4500, Val: 0.2120, Test: 0.2500
Epoch: 3, Loss: 1.9454, Train: 0.4571, Val: 0.2040, Test: 0.2420
Epoch: 4, Loss: 1.9450, Train: 0.5071, Val: 0.2900, Test: 0.3210
Epoch: 5, Loss: 1.9446, Train: 0.7000, Val: 0.4920, Test: 0.4880
Epoch: 6, Loss: 1.9440, Train: 0.8786, Val: 0.6600, Test: 0.6700
Epoch: 7, Loss: 1.9431, Train: 0.9429, Val: 0.7220, Test: 0.7220
Epoch: 8, Loss: 1.9419, Train: 0.9286, Val: 0.7060, Test: 0.7170
Epoch: 9, Loss: 1.9403, Train: 0.9286, Val: 0.6940, Test: 0.7140
Epoch: 10, Loss: 1.9382, Train: 0.9286, Val: 0.6980, Test: 0.7180
Epoch: 11, Loss: 1.9342, Train: 0.9357, Val: 0.6940, Test: 0.7140
Epoch: 12, Loss: 1.9290, Train: 0.9286, Val: 0.6800, Test: 0.6930
Epoch: 13, Loss: 1.9212, Train: 0.9000, Val: 0.6260, Test: 0.6420
Epoch: 14, Loss: 1.9064, Train: 0.8214, Val: 0.5420, Test: 0.5740
Epoch: 15, Loss: 1.8855, Train: 0.7286, Val: 0.4500, Test: 0.4610
Epoch: 16, Loss: 1.8471, Train: 0.6643, Val: 0.3380, Test: 0.3800
Epoch: 17, Loss: 1.7823, Train: 0.6071, Val: 0.2760, Test: 0.3170
Epoch: 18, Loss: 1.6839, Train: 0.6143, Val: 0.2720, Test: 0.3040
Epoch: 19, Loss: 1.5416, Train: 0.6357, Val: 0.3200, Test: 0.3590
Epoch: 20, Loss: 1.3559, Train: 0.7500, Val: 0.4620, Test: 0.5040
Epoch: 21, Loss: 1.1881, Train: 0.8643, Val: 0.6340, Test: 0.6580
Epoch: 22, Loss: 0.9561, Train: 0.9500, Val: 0.7420, Test: 0.7700
Epoch: 23, Loss: 0.7165, Train: 0.9929, Val: 0.7860, Test: 0.8150
Epoch: 24, Loss: 0.6328, Train: 0.9714, Val: 0.7520, Test: 0.7510
Epoch: 25, Loss: 0.5892, Train: 0.9571, Val: 0.7260, Test: 0.7400
Epoch: 26, Loss: 0.4608, Train: 0.9643, Val: 0.7300, Test: 0.7450
Epoch: 27, Loss: 0.3834, Train: 0.9786, Val: 0.7600, Test: 0.7710
Epoch: 28, Loss: 0.3667, Train: 0.9929, Val: 0.7760, Test: 0.8090
Epoch: 29, Loss: 0.2509, Train: 0.9786, Val: 0.7720, Test: 0.7940
Epoch: 30, Loss: 0.4290, Train: 1.0000, Val: 0.7920, Test: 0.8140
Epoch: 31, Loss: 0.1833, Train: 0.9857, Val: 0.7920, Test: 0.7950
Epoch: 32, Loss: 0.1111, Train: 0.9571, Val: 0.7420, Test: 0.7330
Epoch: 33, Loss: 0.2231, Train: 0.9571, Val: 0.7140, Test: 0.7070
Epoch: 34, Loss: 0.1916, Train: 0.9714, Val: 0.7260, Test: 0.7240
Epoch: 35, Loss: 0.2952, Train: 0.9929, Val: 0.7540, Test: 0.7720
Epoch: 36, Loss: 0.1047, Train: 0.9929, Val: 0.7700, Test: 0.7740
Epoch: 37, Loss: 0.0976, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 38, Loss: 0.0748, Train: 0.9929, Val: 0.7440, Test: 0.7430
Epoch: 39, Loss: 0.0685, Train: 0.9786, Val: 0.7300, Test: 0.7280
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 40, Loss: 0.1856, Train: 0.9857, Val: 0.7220, Test: 0.7240
Epoch: 41, Loss: 0.1778, Train: 0.9929, Val: 0.7380, Test: 0.7230
Epoch: 42, Loss: 0.0542, Train: 1.0000, Val: 0.7540, Test: 0.7350
Epoch: 43, Loss: 0.0515, Train: 1.0000, Val: 0.7520, Test: 0.7480
Epoch: 44, Loss: 0.0322, Train: 1.0000, Val: 0.7500, Test: 0.7520
Epoch: 45, Loss: 0.0391, Train: 1.0000, Val: 0.7520, Test: 0.7540
Epoch: 46, Loss: 0.0336, Train: 1.0000, Val: 0.7520, Test: 0.7490
Epoch: 47, Loss: 0.0293, Train: 1.0000, Val: 0.7520, Test: 0.7540
Epoch: 48, Loss: 0.0335, Train: 1.0000, Val: 0.7560, Test: 0.7560
Epoch: 49, Loss: 0.0244, Train: 1.0000, Val: 0.7560, Test: 0.7610
Epoch: 50, Loss: 0.0331, Train: 1.0000, Val: 0.7640, Test: 0.7640
MAD:  0.6822
Best Test Accuracy: 0.8150, Val Accuracy: 0.7860, Train Accuracy: 0.9929
Training completed.
Seed:  7
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.2071, Val: 0.2280, Test: 0.2570
Epoch: 2, Loss: 1.9458, Train: 0.3857, Val: 0.2160, Test: 0.2680
Epoch: 3, Loss: 1.9456, Train: 0.2929, Val: 0.1740, Test: 0.1660
Epoch: 4, Loss: 1.9455, Train: 0.3000, Val: 0.2060, Test: 0.1960
Epoch: 5, Loss: 1.9452, Train: 0.4071, Val: 0.2240, Test: 0.2170
Epoch: 6, Loss: 1.9450, Train: 0.6929, Val: 0.3840, Test: 0.4080
Epoch: 7, Loss: 1.9446, Train: 0.8786, Val: 0.6300, Test: 0.6460
Epoch: 8, Loss: 1.9440, Train: 0.9000, Val: 0.6880, Test: 0.7100
Epoch: 9, Loss: 1.9432, Train: 0.8643, Val: 0.6840, Test: 0.7040
Epoch: 10, Loss: 1.9425, Train: 0.9000, Val: 0.6820, Test: 0.7070
Epoch: 11, Loss: 1.9408, Train: 0.9143, Val: 0.6840, Test: 0.7120
Epoch: 12, Loss: 1.9385, Train: 0.9143, Val: 0.6920, Test: 0.7230
Epoch: 13, Loss: 1.9355, Train: 0.8786, Val: 0.6980, Test: 0.7160
Epoch: 14, Loss: 1.9315, Train: 0.8714, Val: 0.6880, Test: 0.6970
Epoch: 15, Loss: 1.9213, Train: 0.8500, Val: 0.6640, Test: 0.6770
Epoch: 16, Loss: 1.9086, Train: 0.8143, Val: 0.5880, Test: 0.6170
Epoch: 17, Loss: 1.8898, Train: 0.7786, Val: 0.4840, Test: 0.5440
Epoch: 18, Loss: 1.8485, Train: 0.6071, Val: 0.4000, Test: 0.4340
Epoch: 19, Loss: 1.7748, Train: 0.5143, Val: 0.3220, Test: 0.3420
Epoch: 20, Loss: 1.6963, Train: 0.5143, Val: 0.3000, Test: 0.3150
Epoch: 21, Loss: 1.5945, Train: 0.5571, Val: 0.3420, Test: 0.3670
Epoch: 22, Loss: 1.4920, Train: 0.7000, Val: 0.4280, Test: 0.4660
Epoch: 23, Loss: 1.3328, Train: 0.7357, Val: 0.4560, Test: 0.4950
Epoch: 24, Loss: 1.1281, Train: 0.8429, Val: 0.6200, Test: 0.6230
Epoch: 25, Loss: 1.0174, Train: 0.9071, Val: 0.7320, Test: 0.7310
Epoch: 26, Loss: 0.8190, Train: 0.9286, Val: 0.7640, Test: 0.7780
Epoch: 27, Loss: 0.6946, Train: 0.9643, Val: 0.7660, Test: 0.7850
Epoch: 28, Loss: 0.4660, Train: 0.9714, Val: 0.7260, Test: 0.7350
Epoch: 29, Loss: 0.5791, Train: 0.9643, Val: 0.7220, Test: 0.7420
Epoch: 30, Loss: 0.4219, Train: 0.9643, Val: 0.7140, Test: 0.7390
Epoch: 31, Loss: 0.2917, Train: 0.9643, Val: 0.7160, Test: 0.7380
Epoch: 32, Loss: 0.2891, Train: 0.9929, Val: 0.7220, Test: 0.7440
Epoch: 33, Loss: 0.3015, Train: 0.9929, Val: 0.7620, Test: 0.7860
Epoch: 34, Loss: 0.1749, Train: 0.9857, Val: 0.7540, Test: 0.7820
Epoch: 35, Loss: 0.1723, Train: 0.9929, Val: 0.7640, Test: 0.7890
Epoch: 36, Loss: 0.1474, Train: 0.9929, Val: 0.7780, Test: 0.7790
Epoch: 37, Loss: 0.1133, Train: 0.9857, Val: 0.7620, Test: 0.7710
Epoch: 38, Loss: 0.0716, Train: 0.9857, Val: 0.7500, Test: 0.7520
Epoch: 39, Loss: 0.1318, Train: 0.9857, Val: 0.7440, Test: 0.7450
Epoch: 40, Loss: 0.0653, Train: 0.9857, Val: 0.7460, Test: 0.7450
Epoch: 41, Loss: 0.0945, Train: 1.0000, Val: 0.7660, Test: 0.7680
Epoch: 42, Loss: 0.0912, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 43, Loss: 0.0427, Train: 1.0000, Val: 0.7780, Test: 0.8080
Epoch: 44, Loss: 0.0596, Train: 1.0000, Val: 0.7760, Test: 0.7950
Epoch: 45, Loss: 0.0741, Train: 1.0000, Val: 0.7780, Test: 0.7980
Epoch: 46, Loss: 0.0309, Train: 1.0000, Val: 0.7700, Test: 0.7990
Epoch: 47, Loss: 0.0393, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 48, Loss: 0.0707, Train: 1.0000, Val: 0.7740, Test: 0.8080
Epoch: 49, Loss: 0.0270, Train: 1.0000, Val: 0.7720, Test: 0.8080
Epoch: 50, Loss: 0.0349, Train: 1.0000, Val: 0.7560, Test: 0.7830
MAD:  0.6856
Best Test Accuracy: 0.8080, Val Accuracy: 0.7780, Train Accuracy: 1.0000
Training completed.
Seed:  8
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.2786, Val: 0.2340, Test: 0.2160
Epoch: 2, Loss: 1.9457, Train: 0.4357, Val: 0.2420, Test: 0.2600
Epoch: 3, Loss: 1.9454, Train: 0.5214, Val: 0.3960, Test: 0.4000
Epoch: 4, Loss: 1.9451, Train: 0.5357, Val: 0.4640, Test: 0.4740
Epoch: 5, Loss: 1.9447, Train: 0.6571, Val: 0.5400, Test: 0.5290
Epoch: 6, Loss: 1.9439, Train: 0.7500, Val: 0.5840, Test: 0.5820
Epoch: 7, Loss: 1.9432, Train: 0.7571, Val: 0.5620, Test: 0.5710
Epoch: 8, Loss: 1.9419, Train: 0.7500, Val: 0.5100, Test: 0.5370
Epoch: 9, Loss: 1.9400, Train: 0.7071, Val: 0.4480, Test: 0.4830
Epoch: 10, Loss: 1.9372, Train: 0.6286, Val: 0.3580, Test: 0.3840
Epoch: 11, Loss: 1.9330, Train: 0.5143, Val: 0.2480, Test: 0.2680
Epoch: 12, Loss: 1.9251, Train: 0.3786, Val: 0.1560, Test: 0.1860
Epoch: 13, Loss: 1.9083, Train: 0.2286, Val: 0.0980, Test: 0.1370
Epoch: 14, Loss: 1.8868, Train: 0.1786, Val: 0.0780, Test: 0.1100
Epoch: 15, Loss: 1.8499, Train: 0.1500, Val: 0.0720, Test: 0.0990
Epoch: 16, Loss: 1.7898, Train: 0.1500, Val: 0.0740, Test: 0.0950
Epoch: 17, Loss: 1.7287, Train: 0.1500, Val: 0.0740, Test: 0.0930
Epoch: 18, Loss: 1.6997, Train: 0.1500, Val: 0.0740, Test: 0.0940
Epoch: 19, Loss: 1.6166, Train: 0.3429, Val: 0.1560, Test: 0.1660
Epoch: 20, Loss: 1.5310, Train: 0.4714, Val: 0.2440, Test: 0.2770
Epoch: 21, Loss: 1.3622, Train: 0.5786, Val: 0.2780, Test: 0.3120
Epoch: 22, Loss: 1.2915, Train: 0.7000, Val: 0.3540, Test: 0.3720
Epoch: 23, Loss: 1.1640, Train: 0.8286, Val: 0.4580, Test: 0.5040
Epoch: 24, Loss: 0.9809, Train: 0.9357, Val: 0.6320, Test: 0.6780
Epoch: 25, Loss: 0.7479, Train: 0.9571, Val: 0.7300, Test: 0.7630
Epoch: 26, Loss: 0.6565, Train: 0.9571, Val: 0.7480, Test: 0.7590
Epoch: 27, Loss: 0.5002, Train: 0.9643, Val: 0.7380, Test: 0.7580
Epoch: 28, Loss: 0.4329, Train: 0.9714, Val: 0.7620, Test: 0.7670
Epoch: 29, Loss: 0.4137, Train: 0.9786, Val: 0.7600, Test: 0.7580
Epoch: 30, Loss: 0.3271, Train: 0.9929, Val: 0.7680, Test: 0.7690
Epoch: 31, Loss: 0.2625, Train: 1.0000, Val: 0.7860, Test: 0.7950
Epoch: 32, Loss: 0.1784, Train: 1.0000, Val: 0.7920, Test: 0.7980
Epoch: 33, Loss: 0.4399, Train: 1.0000, Val: 0.7900, Test: 0.8050
Epoch: 34, Loss: 0.2233, Train: 1.0000, Val: 0.7800, Test: 0.7880
Epoch: 35, Loss: 0.2555, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 36, Loss: 0.0710, Train: 0.9857, Val: 0.7660, Test: 0.7720
Epoch: 37, Loss: 0.2151, Train: 0.9929, Val: 0.7320, Test: 0.7470
Epoch: 38, Loss: 0.1129, Train: 0.9857, Val: 0.7280, Test: 0.7360
Epoch: 39, Loss: 0.0874, Train: 0.9929, Val: 0.7340, Test: 0.7370
Epoch: 40, Loss: 0.0617, Train: 0.9929, Val: 0.7400, Test: 0.7470
Epoch: 41, Loss: 0.0690, Train: 1.0000, Val: 0.7480, Test: 0.7500
Epoch: 42, Loss: 0.0443, Train: 1.0000, Val: 0.7480, Test: 0.7570
Epoch: 43, Loss: 0.0415, Train: 1.0000, Val: 0.7480, Test: 0.7550
Epoch: 44, Loss: 0.0550, Train: 1.0000, Val: 0.7420, Test: 0.7590
Epoch: 45, Loss: 0.0329, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 46, Loss: 0.0273, Train: 1.0000, Val: 0.7340, Test: 0.7480
Epoch: 47, Loss: 0.0708, Train: 1.0000, Val: 0.7480, Test: 0.7560
Epoch: 48, Loss: 0.0331, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 49, Loss: 0.0244, Train: 1.0000, Val: 0.7580, Test: 0.7570
Epoch: 50, Loss: 0.0230, Train: 1.0000, Val: 0.7580, Test: 0.7540
MAD:  0.8181
Best Test Accuracy: 0.8050, Val Accuracy: 0.7900, Train Accuracy: 1.0000
Training completed.
Seed:  9
/root/code/DIR/DIR-GNN/train/cora.py:521: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
EGCNN(
  (convs): ModuleList(
    (0): EGConv(128, 128, aggregators=['symnorm'])
    (1): EGConv(128, 7, aggregators=['symnorm'])
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9459, Train: 0.1571, Val: 0.1240, Test: 0.1290
Epoch: 2, Loss: 1.9458, Train: 0.2214, Val: 0.1260, Test: 0.1450
Epoch: 3, Loss: 1.9457, Train: 0.3429, Val: 0.2040, Test: 0.2320
Epoch: 4, Loss: 1.9456, Train: 0.4643, Val: 0.3160, Test: 0.3300
Epoch: 5, Loss: 1.9455, Train: 0.4571, Val: 0.3120, Test: 0.3260
Epoch: 6, Loss: 1.9453, Train: 0.6143, Val: 0.4060, Test: 0.4100
Epoch: 7, Loss: 1.9450, Train: 0.7857, Val: 0.5440, Test: 0.5460
Epoch: 8, Loss: 1.9447, Train: 0.7429, Val: 0.4960, Test: 0.5180
Epoch: 9, Loss: 1.9441, Train: 0.7357, Val: 0.5060, Test: 0.5260
Epoch: 10, Loss: 1.9435, Train: 0.7429, Val: 0.5520, Test: 0.5640
Epoch: 11, Loss: 1.9427, Train: 0.8357, Val: 0.6200, Test: 0.6260
Epoch: 12, Loss: 1.9416, Train: 0.8643, Val: 0.6660, Test: 0.6750
Epoch: 13, Loss: 1.9396, Train: 0.8786, Val: 0.6880, Test: 0.7000
Epoch: 14, Loss: 1.9368, Train: 0.8857, Val: 0.7060, Test: 0.7080
Epoch: 15, Loss: 1.9333, Train: 0.8714, Val: 0.6940, Test: 0.7060
Epoch: 16, Loss: 1.9270, Train: 0.8500, Val: 0.6500, Test: 0.6640
Epoch: 17, Loss: 1.9173, Train: 0.8143, Val: 0.5920, Test: 0.6170
Epoch: 18, Loss: 1.8997, Train: 0.7429, Val: 0.5340, Test: 0.5670
Epoch: 19, Loss: 1.8726, Train: 0.6929, Val: 0.5000, Test: 0.5240
Epoch: 20, Loss: 1.8328, Train: 0.6500, Val: 0.4560, Test: 0.4720
Epoch: 21, Loss: 1.7607, Train: 0.5643, Val: 0.4160, Test: 0.4420
Epoch: 22, Loss: 1.6582, Train: 0.5571, Val: 0.4080, Test: 0.4210
Epoch: 23, Loss: 1.5405, Train: 0.5786, Val: 0.4300, Test: 0.4550
Epoch: 24, Loss: 1.3881, Train: 0.6857, Val: 0.5260, Test: 0.5520
Epoch: 25, Loss: 1.2459, Train: 0.7643, Val: 0.5940, Test: 0.6380
Epoch: 26, Loss: 1.0004, Train: 0.8571, Val: 0.6860, Test: 0.7170
Epoch: 27, Loss: 0.8023, Train: 0.9000, Val: 0.7440, Test: 0.7720
Epoch: 28, Loss: 0.7246, Train: 0.9643, Val: 0.7780, Test: 0.7910
Epoch: 29, Loss: 0.5655, Train: 0.9429, Val: 0.6880, Test: 0.7150
Epoch: 30, Loss: 0.4643, Train: 0.9714, Val: 0.7060, Test: 0.7500
Epoch: 31, Loss: 0.3989, Train: 0.9857, Val: 0.7420, Test: 0.7730
Epoch: 32, Loss: 0.3760, Train: 0.9857, Val: 0.7820, Test: 0.7880
Epoch: 33, Loss: 0.2844, Train: 0.9786, Val: 0.7600, Test: 0.7860
Epoch: 34, Loss: 0.2615, Train: 0.9786, Val: 0.7560, Test: 0.7920
Epoch: 35, Loss: 0.1675, Train: 0.9786, Val: 0.7600, Test: 0.7950
Epoch: 36, Loss: 0.1953, Train: 0.9929, Val: 0.7760, Test: 0.8010
Epoch: 37, Loss: 0.2540, Train: 0.9929, Val: 0.7720, Test: 0.8130
Epoch: 38, Loss: 0.0802, Train: 0.9929, Val: 0.7720, Test: 0.7890
Epoch: 39, Loss: 0.0616, Train: 1.0000, Val: 0.7720, Test: 0.7730
Epoch: 40, Loss: 0.0818, Train: 1.0000, Val: 0.7760, Test: 0.7650
Epoch: 41, Loss: 0.0756, Train: 1.0000, Val: 0.7560, Test: 0.7660
Epoch: 42, Loss: 0.0412, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 43, Loss: 0.0438, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 44, Loss: 0.0512, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 45, Loss: 0.0450, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 46, Loss: 0.0536, Train: 1.0000, Val: 0.7600, Test: 0.7740
Epoch: 47, Loss: 0.0293, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 48, Loss: 0.0308, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 49, Loss: 0.0283, Train: 1.0000, Val: 0.7760, Test: 0.7980
Epoch: 50, Loss: 0.0340, Train: 1.0000, Val: 0.7800, Test: 0.7980
MAD:  0.6297
Best Test Accuracy: 0.8130, Val Accuracy: 0.7720, Train Accuracy: 0.9929
Training completed.
Average Test Accuracy:  0.8116 ± 0.006390618123468143
Average MAD:  0.6749799999999999 ± 0.09767373034751978

/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9550, Train: 0.3429, Val: 0.2940, Test: 0.2750
Epoch: 2, Loss: 1.8866, Train: 0.6714, Val: 0.4620, Test: 0.4630
Epoch: 3, Loss: 1.8046, Train: 0.7929, Val: 0.5880, Test: 0.5640
Epoch: 4, Loss: 1.7064, Train: 0.9000, Val: 0.6440, Test: 0.6410
Epoch: 5, Loss: 1.6379, Train: 0.9429, Val: 0.6540, Test: 0.6780
Epoch: 6, Loss: 1.5365, Train: 0.9500, Val: 0.6780, Test: 0.7080
Epoch: 7, Loss: 1.4598, Train: 0.9429, Val: 0.6820, Test: 0.7300
Epoch: 8, Loss: 1.3826, Train: 0.9571, Val: 0.7100, Test: 0.7340
Epoch: 9, Loss: 1.2987, Train: 0.9643, Val: 0.7200, Test: 0.7410
Epoch: 10, Loss: 1.1771, Train: 0.9643, Val: 0.7260, Test: 0.7560
Epoch: 11, Loss: 1.0798, Train: 0.9786, Val: 0.7400, Test: 0.7580
Epoch: 12, Loss: 0.9969, Train: 0.9786, Val: 0.7460, Test: 0.7680
Epoch: 13, Loss: 0.8876, Train: 0.9857, Val: 0.7520, Test: 0.7750
Epoch: 14, Loss: 0.7957, Train: 0.9929, Val: 0.7480, Test: 0.7750
Epoch: 15, Loss: 0.7258, Train: 0.9929, Val: 0.7540, Test: 0.7790
Epoch: 16, Loss: 0.6322, Train: 0.9929, Val: 0.7540, Test: 0.7820
Epoch: 17, Loss: 0.5861, Train: 0.9929, Val: 0.7640, Test: 0.7860
Epoch: 18, Loss: 0.4844, Train: 0.9929, Val: 0.7640, Test: 0.7930
Epoch: 19, Loss: 0.4496, Train: 0.9929, Val: 0.7620, Test: 0.7920
Epoch: 20, Loss: 0.4097, Train: 0.9929, Val: 0.7660, Test: 0.7930
Epoch: 21, Loss: 0.3533, Train: 0.9929, Val: 0.7720, Test: 0.7920
Epoch: 22, Loss: 0.2896, Train: 0.9929, Val: 0.7760, Test: 0.7920
Epoch: 23, Loss: 0.2915, Train: 0.9929, Val: 0.7840, Test: 0.7960
Epoch: 24, Loss: 0.2255, Train: 0.9929, Val: 0.7760, Test: 0.7960
Epoch: 25, Loss: 0.1720, Train: 0.9929, Val: 0.7740, Test: 0.7980
Epoch: 26, Loss: 0.1689, Train: 0.9929, Val: 0.7760, Test: 0.8010
Epoch: 27, Loss: 0.1852, Train: 0.9929, Val: 0.7720, Test: 0.8000
Epoch: 28, Loss: 0.1498, Train: 0.9929, Val: 0.7800, Test: 0.7970
Epoch: 29, Loss: 0.1012, Train: 0.9929, Val: 0.7820, Test: 0.7960
Epoch: 30, Loss: 0.1183, Train: 0.9929, Val: 0.7820, Test: 0.7940
Epoch: 31, Loss: 0.1083, Train: 0.9929, Val: 0.7820, Test: 0.7960
Epoch: 32, Loss: 0.0986, Train: 1.0000, Val: 0.7780, Test: 0.7910
Epoch: 33, Loss: 0.0915, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 34, Loss: 0.0634, Train: 1.0000, Val: 0.7860, Test: 0.7910
Epoch: 35, Loss: 0.0713, Train: 1.0000, Val: 0.7820, Test: 0.7880
Epoch: 36, Loss: 0.0799, Train: 1.0000, Val: 0.7720, Test: 0.7880
Epoch: 37, Loss: 0.0678, Train: 1.0000, Val: 0.7780, Test: 0.7870
Epoch: 38, Loss: 0.0400, Train: 1.0000, Val: 0.7760, Test: 0.7890
Epoch: 39, Loss: 0.0392, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 40, Loss: 0.0392, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 41, Loss: 0.0400, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 42, Loss: 0.0345, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 43, Loss: 0.0471, Train: 1.0000, Val: 0.7660, Test: 0.7910
Epoch: 44, Loss: 0.0295, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 45, Loss: 0.0239, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 46, Loss: 0.0375, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 47, Loss: 0.0205, Train: 1.0000, Val: 0.7740, Test: 0.7880
Epoch: 48, Loss: 0.0289, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 49, Loss: 0.0243, Train: 1.0000, Val: 0.7780, Test: 0.7870
Epoch: 50, Loss: 0.0275, Train: 1.0000, Val: 0.7780, Test: 0.7880
MAD:  0.8769
Best Test Accuracy: 0.8010, Val Accuracy: 0.7760, Train Accuracy: 0.9929
Training completed.
Seed:  1
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9528, Train: 0.3214, Val: 0.1560, Test: 0.1730
Epoch: 2, Loss: 1.8706, Train: 0.5500, Val: 0.2940, Test: 0.3100
Epoch: 3, Loss: 1.8035, Train: 0.7286, Val: 0.4040, Test: 0.4130
Epoch: 4, Loss: 1.7379, Train: 0.8357, Val: 0.4980, Test: 0.5100
Epoch: 5, Loss: 1.6373, Train: 0.9286, Val: 0.5660, Test: 0.5940
Epoch: 6, Loss: 1.5848, Train: 0.9571, Val: 0.6420, Test: 0.6590
Epoch: 7, Loss: 1.4712, Train: 0.9643, Val: 0.6820, Test: 0.7080
Epoch: 8, Loss: 1.3864, Train: 0.9643, Val: 0.7220, Test: 0.7320
Epoch: 9, Loss: 1.2965, Train: 0.9643, Val: 0.7360, Test: 0.7540
Epoch: 10, Loss: 1.2248, Train: 0.9714, Val: 0.7480, Test: 0.7630
Epoch: 11, Loss: 1.1162, Train: 0.9714, Val: 0.7540, Test: 0.7710
Epoch: 12, Loss: 1.0353, Train: 0.9714, Val: 0.7520, Test: 0.7730
Epoch: 13, Loss: 0.9117, Train: 0.9714, Val: 0.7580, Test: 0.7780
Epoch: 14, Loss: 0.8368, Train: 0.9714, Val: 0.7620, Test: 0.7790
Epoch: 15, Loss: 0.7340, Train: 0.9714, Val: 0.7700, Test: 0.7850
Epoch: 16, Loss: 0.7106, Train: 0.9714, Val: 0.7740, Test: 0.7880
Epoch: 17, Loss: 0.5815, Train: 0.9857, Val: 0.7760, Test: 0.7970
Epoch: 18, Loss: 0.5191, Train: 0.9857, Val: 0.7800, Test: 0.7990
Epoch: 19, Loss: 0.4728, Train: 0.9857, Val: 0.7900, Test: 0.8010
Epoch: 20, Loss: 0.3952, Train: 0.9857, Val: 0.7980, Test: 0.8050
Epoch: 21, Loss: 0.3535, Train: 0.9857, Val: 0.8040, Test: 0.8080
Epoch: 22, Loss: 0.3083, Train: 0.9857, Val: 0.7980, Test: 0.8090
Epoch: 23, Loss: 0.2724, Train: 0.9857, Val: 0.7960, Test: 0.8100
Epoch: 24, Loss: 0.2387, Train: 0.9857, Val: 0.7960, Test: 0.8090
Epoch: 25, Loss: 0.2113, Train: 0.9857, Val: 0.7940, Test: 0.8080
Epoch: 26, Loss: 0.1775, Train: 0.9857, Val: 0.7880, Test: 0.8080
Epoch: 27, Loss: 0.1805, Train: 0.9857, Val: 0.7900, Test: 0.8070
Epoch: 28, Loss: 0.1795, Train: 0.9929, Val: 0.7880, Test: 0.8050
Epoch: 29, Loss: 0.1460, Train: 0.9929, Val: 0.7800, Test: 0.8040
Epoch: 30, Loss: 0.1241, Train: 0.9929, Val: 0.7800, Test: 0.8040
Epoch: 31, Loss: 0.1148, Train: 0.9929, Val: 0.7780, Test: 0.8010
Epoch: 32, Loss: 0.1081, Train: 0.9929, Val: 0.7740, Test: 0.7970
Epoch: 33, Loss: 0.1113, Train: 0.9929, Val: 0.7720, Test: 0.8000
Epoch: 34, Loss: 0.0966, Train: 0.9929, Val: 0.7720, Test: 0.8040
Epoch: 35, Loss: 0.0748, Train: 0.9929, Val: 0.7740, Test: 0.8060
Epoch: 36, Loss: 0.0618, Train: 0.9929, Val: 0.7760, Test: 0.8100
Epoch: 37, Loss: 0.0736, Train: 1.0000, Val: 0.7780, Test: 0.8100
Epoch: 38, Loss: 0.0556, Train: 1.0000, Val: 0.7800, Test: 0.8140
Epoch: 39, Loss: 0.0505, Train: 1.0000, Val: 0.7840, Test: 0.8140
Epoch: 40, Loss: 0.0918, Train: 1.0000, Val: 0.7860, Test: 0.8150
Epoch: 41, Loss: 0.0400, Train: 1.0000, Val: 0.7900, Test: 0.8140
Epoch: 42, Loss: 0.0421, Train: 1.0000, Val: 0.7940, Test: 0.8100
Epoch: 43, Loss: 0.0358, Train: 1.0000, Val: 0.7940, Test: 0.8090
Epoch: 44, Loss: 0.0341, Train: 1.0000, Val: 0.7960, Test: 0.8080
Epoch: 45, Loss: 0.0405, Train: 1.0000, Val: 0.7940, Test: 0.8100
Epoch: 46, Loss: 0.0432, Train: 1.0000, Val: 0.7900, Test: 0.8100
Epoch: 47, Loss: 0.0278, Train: 1.0000, Val: 0.7900, Test: 0.8050
Epoch: 48, Loss: 0.0307, Train: 1.0000, Val: 0.7940, Test: 0.8070
Epoch: 49, Loss: 0.0273, Train: 1.0000, Val: 0.7940, Test: 0.8060
Epoch: 50, Loss: 0.0459, Train: 1.0000, Val: 0.7920, Test: 0.8070
MAD:  0.8386
Best Test Accuracy: 0.8150, Val Accuracy: 0.7860, Train Accuracy: 1.0000
Training completed.
Seed:  2
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9370, Train: 0.5214, Val: 0.4480, Test: 0.4480
Epoch: 2, Loss: 1.8650, Train: 0.8000, Val: 0.5500, Test: 0.6110
Epoch: 3, Loss: 1.7867, Train: 0.8857, Val: 0.6380, Test: 0.6850
Epoch: 4, Loss: 1.6990, Train: 0.9357, Val: 0.6860, Test: 0.7170
Epoch: 5, Loss: 1.6448, Train: 0.9643, Val: 0.7120, Test: 0.7410
Epoch: 6, Loss: 1.5318, Train: 0.9643, Val: 0.7260, Test: 0.7560
Epoch: 7, Loss: 1.4438, Train: 0.9714, Val: 0.7380, Test: 0.7700
Epoch: 8, Loss: 1.3507, Train: 0.9786, Val: 0.7540, Test: 0.7730
Epoch: 9, Loss: 1.2689, Train: 0.9786, Val: 0.7780, Test: 0.7890
Epoch: 10, Loss: 1.1553, Train: 0.9857, Val: 0.7840, Test: 0.7950
Epoch: 11, Loss: 1.0702, Train: 0.9857, Val: 0.7900, Test: 0.8010
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 12, Loss: 0.9838, Train: 0.9857, Val: 0.7940, Test: 0.8110
Epoch: 13, Loss: 0.8846, Train: 0.9857, Val: 0.7940, Test: 0.8100
Epoch: 14, Loss: 0.7879, Train: 0.9929, Val: 0.7960, Test: 0.8130
Epoch: 15, Loss: 0.7014, Train: 0.9929, Val: 0.7960, Test: 0.8120
Epoch: 16, Loss: 0.6441, Train: 0.9929, Val: 0.7960, Test: 0.8100
Epoch: 17, Loss: 0.5594, Train: 0.9929, Val: 0.7960, Test: 0.8010
Epoch: 18, Loss: 0.4988, Train: 0.9929, Val: 0.7920, Test: 0.8000
Epoch: 19, Loss: 0.4124, Train: 0.9929, Val: 0.7940, Test: 0.7970
Epoch: 20, Loss: 0.3709, Train: 0.9929, Val: 0.7920, Test: 0.8010
Epoch: 21, Loss: 0.3537, Train: 0.9929, Val: 0.7920, Test: 0.8020
Epoch: 22, Loss: 0.2944, Train: 0.9929, Val: 0.7920, Test: 0.8010
Epoch: 23, Loss: 0.2870, Train: 0.9929, Val: 0.7860, Test: 0.8030
Epoch: 24, Loss: 0.2274, Train: 0.9929, Val: 0.7880, Test: 0.8060
Epoch: 25, Loss: 0.2292, Train: 0.9929, Val: 0.7900, Test: 0.8100
Epoch: 26, Loss: 0.1797, Train: 0.9929, Val: 0.7840, Test: 0.8110
Epoch: 27, Loss: 0.1626, Train: 0.9929, Val: 0.7760, Test: 0.8080
Epoch: 28, Loss: 0.1548, Train: 0.9929, Val: 0.7760, Test: 0.8100
Epoch: 29, Loss: 0.1670, Train: 0.9929, Val: 0.7760, Test: 0.8100
Epoch: 30, Loss: 0.1514, Train: 0.9929, Val: 0.7800, Test: 0.8130
Epoch: 31, Loss: 0.1008, Train: 0.9929, Val: 0.7840, Test: 0.8120
Epoch: 32, Loss: 0.1242, Train: 0.9929, Val: 0.7820, Test: 0.8120
Epoch: 33, Loss: 0.0868, Train: 1.0000, Val: 0.7800, Test: 0.8100
Epoch: 34, Loss: 0.0757, Train: 1.0000, Val: 0.7820, Test: 0.8100
Epoch: 35, Loss: 0.0940, Train: 1.0000, Val: 0.7860, Test: 0.8070
Epoch: 36, Loss: 0.0855, Train: 1.0000, Val: 0.7840, Test: 0.8050
Epoch: 37, Loss: 0.0734, Train: 1.0000, Val: 0.7820, Test: 0.8050
Epoch: 38, Loss: 0.0547, Train: 1.0000, Val: 0.7780, Test: 0.8030
Epoch: 39, Loss: 0.0562, Train: 1.0000, Val: 0.7740, Test: 0.8020
Epoch: 40, Loss: 0.0530, Train: 1.0000, Val: 0.7720, Test: 0.8050
Epoch: 41, Loss: 0.0663, Train: 1.0000, Val: 0.7720, Test: 0.8010
Epoch: 42, Loss: 0.0703, Train: 1.0000, Val: 0.7740, Test: 0.8010
Epoch: 43, Loss: 0.0634, Train: 1.0000, Val: 0.7780, Test: 0.8020
Epoch: 44, Loss: 0.0452, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 45, Loss: 0.0776, Train: 1.0000, Val: 0.7720, Test: 0.7980
Epoch: 46, Loss: 0.0725, Train: 1.0000, Val: 0.7820, Test: 0.8020
Epoch: 47, Loss: 0.0282, Train: 1.0000, Val: 0.7820, Test: 0.8050
Epoch: 48, Loss: 0.0367, Train: 1.0000, Val: 0.7840, Test: 0.8090
Epoch: 49, Loss: 0.0424, Train: 1.0000, Val: 0.7820, Test: 0.8110
Epoch: 50, Loss: 0.0363, Train: 1.0000, Val: 0.7800, Test: 0.8100
MAD:  0.7731
Best Test Accuracy: 0.8130, Val Accuracy: 0.7960, Train Accuracy: 0.9929
Training completed.
Seed:  3
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9721, Train: 0.3500, Val: 0.2920, Test: 0.3110
Epoch: 2, Loss: 1.8957, Train: 0.7143, Val: 0.5080, Test: 0.5160
Epoch: 3, Loss: 1.8108, Train: 0.8500, Val: 0.6080, Test: 0.6170
Epoch: 4, Loss: 1.7422, Train: 0.9000, Val: 0.6440, Test: 0.6750
Epoch: 5, Loss: 1.6616, Train: 0.9214, Val: 0.6880, Test: 0.6960
Epoch: 6, Loss: 1.5675, Train: 0.9500, Val: 0.7280, Test: 0.7320
Epoch: 7, Loss: 1.4713, Train: 0.9500, Val: 0.7480, Test: 0.7560
Epoch: 8, Loss: 1.3966, Train: 0.9571, Val: 0.7440, Test: 0.7700
Epoch: 9, Loss: 1.2612, Train: 0.9714, Val: 0.7420, Test: 0.7680
Epoch: 10, Loss: 1.1870, Train: 0.9714, Val: 0.7440, Test: 0.7780
Epoch: 11, Loss: 1.1096, Train: 0.9786, Val: 0.7440, Test: 0.7760
Epoch: 12, Loss: 0.9981, Train: 0.9857, Val: 0.7500, Test: 0.7760
Epoch: 13, Loss: 0.8669, Train: 0.9857, Val: 0.7540, Test: 0.7790
Epoch: 14, Loss: 0.7707, Train: 0.9857, Val: 0.7580, Test: 0.7810
Epoch: 15, Loss: 0.7224, Train: 0.9857, Val: 0.7700, Test: 0.7920
Epoch: 16, Loss: 0.6086, Train: 0.9929, Val: 0.7760, Test: 0.7970
Epoch: 17, Loss: 0.5446, Train: 0.9929, Val: 0.7820, Test: 0.8030
Epoch: 18, Loss: 0.4740, Train: 0.9929, Val: 0.7880, Test: 0.7970
Epoch: 19, Loss: 0.4279, Train: 0.9929, Val: 0.7920, Test: 0.8040
Epoch: 20, Loss: 0.3575, Train: 0.9929, Val: 0.7860, Test: 0.8020
Epoch: 21, Loss: 0.3077, Train: 0.9929, Val: 0.7840, Test: 0.8000
Epoch: 22, Loss: 0.2613, Train: 0.9929, Val: 0.7780, Test: 0.7990
Epoch: 23, Loss: 0.2615, Train: 0.9929, Val: 0.7780, Test: 0.7970
Epoch: 24, Loss: 0.2390, Train: 0.9929, Val: 0.7720, Test: 0.8000
Epoch: 25, Loss: 0.1934, Train: 0.9929, Val: 0.7720, Test: 0.7970
Epoch: 26, Loss: 0.1676, Train: 1.0000, Val: 0.7700, Test: 0.7970
Epoch: 27, Loss: 0.1598, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 28, Loss: 0.1419, Train: 1.0000, Val: 0.7700, Test: 0.7960
Epoch: 29, Loss: 0.1416, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 30, Loss: 0.1229, Train: 1.0000, Val: 0.7660, Test: 0.7960
Epoch: 31, Loss: 0.0989, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 32, Loss: 0.0769, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 33, Loss: 0.0914, Train: 1.0000, Val: 0.7660, Test: 0.7910
Epoch: 34, Loss: 0.0893, Train: 1.0000, Val: 0.7660, Test: 0.7910
Epoch: 35, Loss: 0.0678, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 36, Loss: 0.0592, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 37, Loss: 0.0666, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 38, Loss: 0.0491, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 39, Loss: 0.0558, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 40, Loss: 0.0365, Train: 1.0000, Val: 0.7640, Test: 0.7890
Epoch: 41, Loss: 0.0431, Train: 1.0000, Val: 0.7600, Test: 0.7840
Epoch: 42, Loss: 0.0287, Train: 1.0000, Val: 0.7600, Test: 0.7840
Epoch: 43, Loss: 0.0243, Train: 1.0000, Val: 0.7560, Test: 0.7830
Epoch: 44, Loss: 0.0323, Train: 1.0000, Val: 0.7560, Test: 0.7810
Epoch: 45, Loss: 0.0351, Train: 1.0000, Val: 0.7600, Test: 0.7790
Epoch: 46, Loss: 0.0198, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 47, Loss: 0.0320, Train: 1.0000, Val: 0.7600, Test: 0.7820
Epoch: 48, Loss: 0.0255, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 49, Loss: 0.0202, Train: 1.0000, Val: 0.7520, Test: 0.7860
Epoch: 50, Loss: 0.0172, Train: 1.0000, Val: 0.7520, Test: 0.7880
MAD:  0.8907
Best Test Accuracy: 0.8040, Val Accuracy: 0.7920, Train Accuracy: 0.9929
Training completed.
Seed:  4
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9518, Train: 0.4571, Val: 0.2900, Test: 0.2600
Epoch: 2, Loss: 1.8845, Train: 0.6429, Val: 0.3660, Test: 0.3760
Epoch: 3, Loss: 1.8019, Train: 0.8214, Val: 0.4940, Test: 0.4880
Epoch: 4, Loss: 1.7166, Train: 0.8929, Val: 0.5960, Test: 0.5860
Epoch: 5, Loss: 1.6419, Train: 0.9357, Val: 0.6460, Test: 0.6440
Epoch: 6, Loss: 1.5872, Train: 0.9571, Val: 0.6880, Test: 0.6940
Epoch: 7, Loss: 1.4647, Train: 0.9643, Val: 0.7160, Test: 0.7130
Epoch: 8, Loss: 1.4029, Train: 0.9643, Val: 0.7300, Test: 0.7320
Epoch: 9, Loss: 1.3046, Train: 0.9643, Val: 0.7360, Test: 0.7440
Epoch: 10, Loss: 1.2040, Train: 0.9714, Val: 0.7440, Test: 0.7560
Epoch: 11, Loss: 1.1158, Train: 0.9786, Val: 0.7480, Test: 0.7660
Epoch: 12, Loss: 0.9729, Train: 0.9786, Val: 0.7580, Test: 0.7700
Epoch: 13, Loss: 0.9252, Train: 0.9929, Val: 0.7640, Test: 0.7720
Epoch: 14, Loss: 0.8279, Train: 0.9929, Val: 0.7680, Test: 0.7750
Epoch: 15, Loss: 0.6774, Train: 0.9929, Val: 0.7720, Test: 0.7830
Epoch: 16, Loss: 0.6359, Train: 0.9929, Val: 0.7740, Test: 0.7880
Epoch: 17, Loss: 0.5950, Train: 0.9929, Val: 0.7800, Test: 0.7950
Epoch: 18, Loss: 0.5147, Train: 0.9929, Val: 0.7920, Test: 0.8000
Epoch: 19, Loss: 0.4543, Train: 0.9929, Val: 0.7960, Test: 0.8040
Epoch: 20, Loss: 0.4024, Train: 0.9929, Val: 0.7960, Test: 0.8050
Epoch: 21, Loss: 0.3530, Train: 0.9929, Val: 0.7940, Test: 0.8060
Epoch: 22, Loss: 0.3198, Train: 0.9929, Val: 0.7960, Test: 0.8120
Epoch: 23, Loss: 0.2857, Train: 0.9929, Val: 0.7960, Test: 0.8100
Epoch: 24, Loss: 0.2408, Train: 0.9929, Val: 0.7960, Test: 0.8080
Epoch: 25, Loss: 0.2299, Train: 0.9929, Val: 0.7980, Test: 0.8080
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 26, Loss: 0.2128, Train: 0.9929, Val: 0.7920, Test: 0.8090
Epoch: 27, Loss: 0.1685, Train: 0.9929, Val: 0.7920, Test: 0.8080
Epoch: 28, Loss: 0.1877, Train: 0.9929, Val: 0.7960, Test: 0.8080
Epoch: 29, Loss: 0.1588, Train: 0.9929, Val: 0.7920, Test: 0.8070
Epoch: 30, Loss: 0.1414, Train: 0.9929, Val: 0.7900, Test: 0.8100
Epoch: 31, Loss: 0.1153, Train: 0.9929, Val: 0.7860, Test: 0.8080
Epoch: 32, Loss: 0.0855, Train: 0.9929, Val: 0.7840, Test: 0.8080
Epoch: 33, Loss: 0.1088, Train: 1.0000, Val: 0.7880, Test: 0.8110
Epoch: 34, Loss: 0.0849, Train: 1.0000, Val: 0.7820, Test: 0.8090
Epoch: 35, Loss: 0.0789, Train: 1.0000, Val: 0.7800, Test: 0.8090
Epoch: 36, Loss: 0.0762, Train: 1.0000, Val: 0.7780, Test: 0.8060
Epoch: 37, Loss: 0.0675, Train: 1.0000, Val: 0.7760, Test: 0.8060
Epoch: 38, Loss: 0.0561, Train: 1.0000, Val: 0.7760, Test: 0.8020
Epoch: 39, Loss: 0.0525, Train: 1.0000, Val: 0.7700, Test: 0.8040
Epoch: 40, Loss: 0.0427, Train: 1.0000, Val: 0.7680, Test: 0.8050
Epoch: 41, Loss: 0.0455, Train: 1.0000, Val: 0.7660, Test: 0.8060
Epoch: 42, Loss: 0.0467, Train: 1.0000, Val: 0.7660, Test: 0.8050
Epoch: 43, Loss: 0.0384, Train: 1.0000, Val: 0.7700, Test: 0.8070
Epoch: 44, Loss: 0.0402, Train: 1.0000, Val: 0.7700, Test: 0.8050
Epoch: 45, Loss: 0.0428, Train: 1.0000, Val: 0.7720, Test: 0.8040
Epoch: 46, Loss: 0.0366, Train: 1.0000, Val: 0.7760, Test: 0.8060
Epoch: 47, Loss: 0.0344, Train: 1.0000, Val: 0.7780, Test: 0.8070
Epoch: 48, Loss: 0.0492, Train: 1.0000, Val: 0.7800, Test: 0.8050
Epoch: 49, Loss: 0.0271, Train: 1.0000, Val: 0.7780, Test: 0.8020
Epoch: 50, Loss: 0.0374, Train: 1.0000, Val: 0.7780, Test: 0.8000
MAD:  0.8896
Best Test Accuracy: 0.8120, Val Accuracy: 0.7960, Train Accuracy: 0.9929
Training completed.
Seed:  5
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9522, Train: 0.3643, Val: 0.1860, Test: 0.2230
Epoch: 2, Loss: 1.9052, Train: 0.4929, Val: 0.2940, Test: 0.3050
Epoch: 3, Loss: 1.8270, Train: 0.6500, Val: 0.3580, Test: 0.3810
Epoch: 4, Loss: 1.7745, Train: 0.7786, Val: 0.4240, Test: 0.4540
Epoch: 5, Loss: 1.6843, Train: 0.8429, Val: 0.5080, Test: 0.5240
Epoch: 6, Loss: 1.6021, Train: 0.9000, Val: 0.5860, Test: 0.5850
Epoch: 7, Loss: 1.5708, Train: 0.9286, Val: 0.6120, Test: 0.6270
Epoch: 8, Loss: 1.4624, Train: 0.9500, Val: 0.6400, Test: 0.6590
Epoch: 9, Loss: 1.3937, Train: 0.9643, Val: 0.6680, Test: 0.6800
Epoch: 10, Loss: 1.2952, Train: 0.9714, Val: 0.6840, Test: 0.6970
Epoch: 11, Loss: 1.2209, Train: 0.9714, Val: 0.7080, Test: 0.7120
Epoch: 12, Loss: 1.1223, Train: 0.9714, Val: 0.7080, Test: 0.7270
Epoch: 13, Loss: 1.0329, Train: 0.9714, Val: 0.7160, Test: 0.7380
Epoch: 14, Loss: 0.9550, Train: 0.9786, Val: 0.7260, Test: 0.7400
Epoch: 15, Loss: 0.8686, Train: 0.9786, Val: 0.7400, Test: 0.7430
Epoch: 16, Loss: 0.7908, Train: 0.9786, Val: 0.7460, Test: 0.7530
Epoch: 17, Loss: 0.7414, Train: 0.9786, Val: 0.7460, Test: 0.7550
Epoch: 18, Loss: 0.6629, Train: 0.9857, Val: 0.7520, Test: 0.7580
Epoch: 19, Loss: 0.5529, Train: 0.9857, Val: 0.7600, Test: 0.7660
Epoch: 20, Loss: 0.5065, Train: 0.9786, Val: 0.7580, Test: 0.7750
Epoch: 21, Loss: 0.4580, Train: 0.9786, Val: 0.7580, Test: 0.7800
Epoch: 22, Loss: 0.4083, Train: 0.9786, Val: 0.7580, Test: 0.7810
Epoch: 23, Loss: 0.3846, Train: 0.9786, Val: 0.7580, Test: 0.7800
Epoch: 24, Loss: 0.3235, Train: 0.9857, Val: 0.7580, Test: 0.7760
Epoch: 25, Loss: 0.3167, Train: 0.9786, Val: 0.7480, Test: 0.7780
Epoch: 26, Loss: 0.2369, Train: 0.9857, Val: 0.7500, Test: 0.7740
Epoch: 27, Loss: 0.1872, Train: 0.9929, Val: 0.7460, Test: 0.7720
Epoch: 28, Loss: 0.1731, Train: 0.9929, Val: 0.7460, Test: 0.7710
Epoch: 29, Loss: 0.1501, Train: 0.9929, Val: 0.7460, Test: 0.7730
Epoch: 30, Loss: 0.1295, Train: 0.9929, Val: 0.7440, Test: 0.7710
Epoch: 31, Loss: 0.1503, Train: 0.9929, Val: 0.7420, Test: 0.7710
Epoch: 32, Loss: 0.1170, Train: 0.9929, Val: 0.7420, Test: 0.7750
Epoch: 33, Loss: 0.1080, Train: 1.0000, Val: 0.7400, Test: 0.7730
Epoch: 34, Loss: 0.0814, Train: 1.0000, Val: 0.7400, Test: 0.7730
Epoch: 35, Loss: 0.1364, Train: 1.0000, Val: 0.7360, Test: 0.7700
Epoch: 36, Loss: 0.0793, Train: 1.0000, Val: 0.7340, Test: 0.7690
Epoch: 37, Loss: 0.0741, Train: 1.0000, Val: 0.7360, Test: 0.7660
Epoch: 38, Loss: 0.0840, Train: 1.0000, Val: 0.7360, Test: 0.7640
Epoch: 39, Loss: 0.0676, Train: 1.0000, Val: 0.7340, Test: 0.7660
Epoch: 40, Loss: 0.0472, Train: 1.0000, Val: 0.7340, Test: 0.7660
Epoch: 41, Loss: 0.0654, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 42, Loss: 0.0326, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 43, Loss: 0.0377, Train: 1.0000, Val: 0.7280, Test: 0.7650
Epoch: 44, Loss: 0.0321, Train: 1.0000, Val: 0.7260, Test: 0.7650
Epoch: 45, Loss: 0.0300, Train: 1.0000, Val: 0.7260, Test: 0.7670
Epoch: 46, Loss: 0.0373, Train: 1.0000, Val: 0.7280, Test: 0.7660
Epoch: 47, Loss: 0.0442, Train: 1.0000, Val: 0.7260, Test: 0.7660
Epoch: 48, Loss: 0.0312, Train: 1.0000, Val: 0.7240, Test: 0.7680
Epoch: 49, Loss: 0.0339, Train: 1.0000, Val: 0.7260, Test: 0.7690
Epoch: 50, Loss: 0.0259, Train: 1.0000, Val: 0.7280, Test: 0.7690
MAD:  0.7986
Best Test Accuracy: 0.7810, Val Accuracy: 0.7580, Train Accuracy: 0.9786
Training completed.
Seed:  6
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9369, Train: 0.3143, Val: 0.2100, Test: 0.1970
Epoch: 2, Loss: 1.8726, Train: 0.7286, Val: 0.4260, Test: 0.4280
Epoch: 3, Loss: 1.8017, Train: 0.9143, Val: 0.5900, Test: 0.6070
Epoch: 4, Loss: 1.7122, Train: 0.9500, Val: 0.6660, Test: 0.6850
Epoch: 5, Loss: 1.6237, Train: 0.9571, Val: 0.7100, Test: 0.7240
Epoch: 6, Loss: 1.5673, Train: 0.9643, Val: 0.7360, Test: 0.7480
Epoch: 7, Loss: 1.4762, Train: 0.9643, Val: 0.7460, Test: 0.7610
Epoch: 8, Loss: 1.3712, Train: 0.9643, Val: 0.7540, Test: 0.7620
Epoch: 9, Loss: 1.2934, Train: 0.9643, Val: 0.7580, Test: 0.7650
Epoch: 10, Loss: 1.1536, Train: 0.9643, Val: 0.7660, Test: 0.7630
Epoch: 11, Loss: 1.0969, Train: 0.9714, Val: 0.7660, Test: 0.7660
Epoch: 12, Loss: 0.9741, Train: 0.9714, Val: 0.7620, Test: 0.7690
Epoch: 13, Loss: 0.8557, Train: 0.9857, Val: 0.7700, Test: 0.7740
Epoch: 14, Loss: 0.7698, Train: 0.9929, Val: 0.7720, Test: 0.7810
Epoch: 15, Loss: 0.6878, Train: 0.9929, Val: 0.7680, Test: 0.7820
Epoch: 16, Loss: 0.6058, Train: 0.9929, Val: 0.7680, Test: 0.7890
Epoch: 17, Loss: 0.5735, Train: 0.9929, Val: 0.7720, Test: 0.7900
Epoch: 18, Loss: 0.4450, Train: 0.9929, Val: 0.7740, Test: 0.7890
Epoch: 19, Loss: 0.4073, Train: 0.9929, Val: 0.7740, Test: 0.7850
Epoch: 20, Loss: 0.3692, Train: 0.9929, Val: 0.7800, Test: 0.7860
Epoch: 21, Loss: 0.2908, Train: 0.9929, Val: 0.7840, Test: 0.7900
Epoch: 22, Loss: 0.2807, Train: 0.9929, Val: 0.7840, Test: 0.7990
Epoch: 23, Loss: 0.2284, Train: 0.9929, Val: 0.7840, Test: 0.8030
Epoch: 24, Loss: 0.2062, Train: 0.9929, Val: 0.7900, Test: 0.8050
Epoch: 25, Loss: 0.1842, Train: 0.9929, Val: 0.7880, Test: 0.8010
Epoch: 26, Loss: 0.1514, Train: 0.9929, Val: 0.7820, Test: 0.8050
Epoch: 27, Loss: 0.1372, Train: 0.9929, Val: 0.7880, Test: 0.8040
Epoch: 28, Loss: 0.1114, Train: 1.0000, Val: 0.7820, Test: 0.8050
Epoch: 29, Loss: 0.0887, Train: 1.0000, Val: 0.7800, Test: 0.8040
Epoch: 30, Loss: 0.0925, Train: 1.0000, Val: 0.7820, Test: 0.7980
Epoch: 31, Loss: 0.0735, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 32, Loss: 0.0669, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 33, Loss: 0.0700, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 34, Loss: 0.0807, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 35, Loss: 0.0604, Train: 1.0000, Val: 0.7840, Test: 0.7980
Epoch: 36, Loss: 0.0483, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 37, Loss: 0.0437, Train: 1.0000, Val: 0.7740, Test: 0.8010
Epoch: 38, Loss: 0.0357, Train: 1.0000, Val: 0.7660, Test: 0.7980
Epoch: 39, Loss: 0.0557, Train: 1.0000, Val: 0.7720, Test: 0.8010
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 40, Loss: 0.0324, Train: 1.0000, Val: 0.7760, Test: 0.8010
Epoch: 41, Loss: 0.0322, Train: 1.0000, Val: 0.7760, Test: 0.7950
Epoch: 42, Loss: 0.0232, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 43, Loss: 0.0241, Train: 1.0000, Val: 0.7780, Test: 0.7990
Epoch: 44, Loss: 0.0209, Train: 1.0000, Val: 0.7760, Test: 0.7970
Epoch: 45, Loss: 0.0314, Train: 1.0000, Val: 0.7760, Test: 0.7960
Epoch: 46, Loss: 0.0203, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 47, Loss: 0.0197, Train: 1.0000, Val: 0.7700, Test: 0.7950
Epoch: 48, Loss: 0.0139, Train: 1.0000, Val: 0.7660, Test: 0.7950
Epoch: 49, Loss: 0.0181, Train: 1.0000, Val: 0.7700, Test: 0.7970
Epoch: 50, Loss: 0.0164, Train: 1.0000, Val: 0.7720, Test: 0.7950
MAD:  0.9162
Best Test Accuracy: 0.8050, Val Accuracy: 0.7900, Train Accuracy: 0.9929
Training completed.
Seed:  7
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9663, Train: 0.3429, Val: 0.3340, Test: 0.3200
Epoch: 2, Loss: 1.8791, Train: 0.6071, Val: 0.4960, Test: 0.4860
Epoch: 3, Loss: 1.7898, Train: 0.8571, Val: 0.6100, Test: 0.6290
Epoch: 4, Loss: 1.7350, Train: 0.9357, Val: 0.6860, Test: 0.7100
Epoch: 5, Loss: 1.6189, Train: 0.9429, Val: 0.7400, Test: 0.7580
Epoch: 6, Loss: 1.5754, Train: 0.9714, Val: 0.7620, Test: 0.7870
Epoch: 7, Loss: 1.4572, Train: 0.9714, Val: 0.7760, Test: 0.7920
Epoch: 8, Loss: 1.3849, Train: 0.9857, Val: 0.7820, Test: 0.7970
Epoch: 9, Loss: 1.2969, Train: 0.9857, Val: 0.7940, Test: 0.7990
Epoch: 10, Loss: 1.1564, Train: 0.9857, Val: 0.7940, Test: 0.8000
Epoch: 11, Loss: 1.0926, Train: 0.9857, Val: 0.7980, Test: 0.8020
Epoch: 12, Loss: 0.9846, Train: 0.9857, Val: 0.7960, Test: 0.8060
Epoch: 13, Loss: 0.8828, Train: 0.9857, Val: 0.7900, Test: 0.8010
Epoch: 14, Loss: 0.8012, Train: 0.9929, Val: 0.7940, Test: 0.7970
Epoch: 15, Loss: 0.7193, Train: 0.9929, Val: 0.7960, Test: 0.7930
Epoch: 16, Loss: 0.6072, Train: 0.9929, Val: 0.7940, Test: 0.7880
Epoch: 17, Loss: 0.5686, Train: 0.9929, Val: 0.7880, Test: 0.7790
Epoch: 18, Loss: 0.5214, Train: 0.9929, Val: 0.7840, Test: 0.7770
Epoch: 19, Loss: 0.4636, Train: 0.9929, Val: 0.7860, Test: 0.7730
Epoch: 20, Loss: 0.4072, Train: 0.9929, Val: 0.7840, Test: 0.7730
Epoch: 21, Loss: 0.3005, Train: 0.9929, Val: 0.7860, Test: 0.7740
Epoch: 22, Loss: 0.2794, Train: 0.9929, Val: 0.7820, Test: 0.7780
Epoch: 23, Loss: 0.2490, Train: 0.9929, Val: 0.7840, Test: 0.7770
Epoch: 24, Loss: 0.2407, Train: 0.9929, Val: 0.7820, Test: 0.7770
Epoch: 25, Loss: 0.2202, Train: 0.9929, Val: 0.7820, Test: 0.7780
Epoch: 26, Loss: 0.1781, Train: 0.9929, Val: 0.7820, Test: 0.7780
Epoch: 27, Loss: 0.1805, Train: 0.9929, Val: 0.7860, Test: 0.7770
Epoch: 28, Loss: 0.1482, Train: 0.9929, Val: 0.7900, Test: 0.7760
Epoch: 29, Loss: 0.1043, Train: 0.9929, Val: 0.7900, Test: 0.7810
Epoch: 30, Loss: 0.1349, Train: 1.0000, Val: 0.7900, Test: 0.7890
Epoch: 31, Loss: 0.1151, Train: 1.0000, Val: 0.7880, Test: 0.7830
Epoch: 32, Loss: 0.1314, Train: 1.0000, Val: 0.7820, Test: 0.7740
Epoch: 33, Loss: 0.0844, Train: 1.0000, Val: 0.7780, Test: 0.7720
Epoch: 34, Loss: 0.0939, Train: 1.0000, Val: 0.7780, Test: 0.7740
Epoch: 35, Loss: 0.0733, Train: 1.0000, Val: 0.7720, Test: 0.7720
Epoch: 36, Loss: 0.0630, Train: 1.0000, Val: 0.7720, Test: 0.7720
Epoch: 37, Loss: 0.0628, Train: 1.0000, Val: 0.7780, Test: 0.7720
Epoch: 38, Loss: 0.0602, Train: 1.0000, Val: 0.7780, Test: 0.7770
Epoch: 39, Loss: 0.0557, Train: 1.0000, Val: 0.7720, Test: 0.7730
Epoch: 40, Loss: 0.0491, Train: 1.0000, Val: 0.7760, Test: 0.7740
Epoch: 41, Loss: 0.0392, Train: 1.0000, Val: 0.7780, Test: 0.7760
Epoch: 42, Loss: 0.0371, Train: 1.0000, Val: 0.7720, Test: 0.7780
Epoch: 43, Loss: 0.0334, Train: 1.0000, Val: 0.7760, Test: 0.7770
Epoch: 44, Loss: 0.0318, Train: 1.0000, Val: 0.7720, Test: 0.7760
Epoch: 45, Loss: 0.0461, Train: 1.0000, Val: 0.7740, Test: 0.7750
Epoch: 46, Loss: 0.0267, Train: 1.0000, Val: 0.7760, Test: 0.7730
Epoch: 47, Loss: 0.0342, Train: 1.0000, Val: 0.7760, Test: 0.7750
Epoch: 48, Loss: 0.0243, Train: 1.0000, Val: 0.7760, Test: 0.7740
Epoch: 49, Loss: 0.0265, Train: 1.0000, Val: 0.7760, Test: 0.7750
Epoch: 50, Loss: 0.0225, Train: 1.0000, Val: 0.7760, Test: 0.7750
MAD:  0.9149
Best Test Accuracy: 0.8060, Val Accuracy: 0.7960, Train Accuracy: 0.9857
Training completed.
Seed:  8
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9869, Train: 0.3857, Val: 0.2540, Test: 0.2370
Epoch: 2, Loss: 1.9033, Train: 0.7143, Val: 0.4200, Test: 0.4330
Epoch: 3, Loss: 1.7974, Train: 0.8571, Val: 0.5460, Test: 0.5750
Epoch: 4, Loss: 1.7549, Train: 0.9429, Val: 0.6460, Test: 0.6580
Epoch: 5, Loss: 1.6472, Train: 0.9643, Val: 0.6760, Test: 0.7100
Epoch: 6, Loss: 1.6064, Train: 0.9929, Val: 0.7140, Test: 0.7340
Epoch: 7, Loss: 1.5074, Train: 0.9929, Val: 0.7440, Test: 0.7580
Epoch: 8, Loss: 1.3915, Train: 0.9929, Val: 0.7620, Test: 0.7740
Epoch: 9, Loss: 1.3269, Train: 0.9929, Val: 0.7720, Test: 0.7920
Epoch: 10, Loss: 1.2381, Train: 0.9929, Val: 0.7760, Test: 0.7930
Epoch: 11, Loss: 1.1245, Train: 0.9929, Val: 0.7740, Test: 0.7960
Epoch: 12, Loss: 1.0172, Train: 0.9929, Val: 0.7780, Test: 0.7950
Epoch: 13, Loss: 0.9464, Train: 0.9929, Val: 0.7780, Test: 0.7990
Epoch: 14, Loss: 0.8603, Train: 0.9929, Val: 0.7800, Test: 0.7910
Epoch: 15, Loss: 0.7554, Train: 0.9929, Val: 0.7820, Test: 0.7920
Epoch: 16, Loss: 0.6224, Train: 0.9929, Val: 0.7860, Test: 0.7920
Epoch: 17, Loss: 0.6114, Train: 0.9929, Val: 0.7860, Test: 0.7940
Epoch: 18, Loss: 0.5199, Train: 0.9929, Val: 0.7800, Test: 0.7940
Epoch: 19, Loss: 0.4420, Train: 0.9929, Val: 0.7760, Test: 0.7960
Epoch: 20, Loss: 0.3823, Train: 0.9929, Val: 0.7760, Test: 0.7950
Epoch: 21, Loss: 0.3362, Train: 0.9929, Val: 0.7760, Test: 0.7950
Epoch: 22, Loss: 0.2873, Train: 0.9929, Val: 0.7800, Test: 0.7950
Epoch: 23, Loss: 0.2589, Train: 0.9929, Val: 0.7800, Test: 0.7920
Epoch: 24, Loss: 0.2492, Train: 0.9929, Val: 0.7800, Test: 0.7950
Epoch: 25, Loss: 0.2359, Train: 0.9929, Val: 0.7780, Test: 0.7960
Epoch: 26, Loss: 0.1696, Train: 0.9929, Val: 0.7760, Test: 0.7960
Epoch: 27, Loss: 0.1569, Train: 0.9929, Val: 0.7780, Test: 0.7920
Epoch: 28, Loss: 0.1618, Train: 0.9929, Val: 0.7840, Test: 0.7950
Epoch: 29, Loss: 0.1272, Train: 0.9929, Val: 0.7860, Test: 0.7960
Epoch: 30, Loss: 0.1404, Train: 0.9929, Val: 0.7800, Test: 0.7960
Epoch: 31, Loss: 0.1050, Train: 0.9929, Val: 0.7800, Test: 0.7930
Epoch: 32, Loss: 0.1247, Train: 0.9929, Val: 0.7760, Test: 0.7900
Epoch: 33, Loss: 0.1009, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 34, Loss: 0.0654, Train: 1.0000, Val: 0.7720, Test: 0.7910
Epoch: 35, Loss: 0.0738, Train: 1.0000, Val: 0.7740, Test: 0.7920
Epoch: 36, Loss: 0.0711, Train: 1.0000, Val: 0.7780, Test: 0.7880
Epoch: 37, Loss: 0.0554, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 38, Loss: 0.0869, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 39, Loss: 0.0417, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 40, Loss: 0.0347, Train: 1.0000, Val: 0.7700, Test: 0.7840
Epoch: 41, Loss: 0.0519, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 42, Loss: 0.0527, Train: 1.0000, Val: 0.7660, Test: 0.7870
Epoch: 43, Loss: 0.0393, Train: 1.0000, Val: 0.7660, Test: 0.7880
Epoch: 44, Loss: 0.0350, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 45, Loss: 0.0282, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 46, Loss: 0.0210, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 47, Loss: 0.0339, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 48, Loss: 0.0280, Train: 1.0000, Val: 0.7700, Test: 0.7920
Epoch: 49, Loss: 0.0270, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 50, Loss: 0.0302, Train: 1.0000, Val: 0.7660, Test: 0.7900
MAD:  0.8194
Best Test Accuracy: 0.7990, Val Accuracy: 0.7780, Train Accuracy: 0.9929
Training completed.
Seed:  9
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 128, heads=1)
    (2): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9642, Train: 0.2786, Val: 0.1560, Test: 0.1530
Epoch: 2, Loss: 1.8877, Train: 0.5857, Val: 0.2960, Test: 0.2860
Epoch: 3, Loss: 1.8141, Train: 0.8214, Val: 0.4700, Test: 0.4590
Epoch: 4, Loss: 1.7445, Train: 0.9571, Val: 0.5580, Test: 0.5760
Epoch: 5, Loss: 1.6534, Train: 0.9714, Val: 0.6240, Test: 0.6370
Epoch: 6, Loss: 1.5961, Train: 0.9857, Val: 0.6520, Test: 0.6750
Epoch: 7, Loss: 1.5226, Train: 0.9857, Val: 0.6820, Test: 0.6960
Epoch: 8, Loss: 1.4203, Train: 0.9857, Val: 0.7200, Test: 0.7210
Epoch: 9, Loss: 1.3117, Train: 0.9857, Val: 0.7260, Test: 0.7340
Epoch: 10, Loss: 1.2353, Train: 0.9857, Val: 0.7260, Test: 0.7420
Epoch: 11, Loss: 1.1286, Train: 0.9929, Val: 0.7520, Test: 0.7520
Epoch: 12, Loss: 1.0422, Train: 0.9929, Val: 0.7520, Test: 0.7530
Epoch: 13, Loss: 0.9530, Train: 0.9929, Val: 0.7620, Test: 0.7550
Epoch: 14, Loss: 0.8386, Train: 0.9929, Val: 0.7640, Test: 0.7590
Epoch: 15, Loss: 0.7560, Train: 0.9929, Val: 0.7640, Test: 0.7610
Epoch: 16, Loss: 0.6661, Train: 0.9929, Val: 0.7720, Test: 0.7610
Epoch: 17, Loss: 0.6073, Train: 0.9929, Val: 0.7720, Test: 0.7710
Epoch: 18, Loss: 0.5064, Train: 0.9929, Val: 0.7700, Test: 0.7740
Epoch: 19, Loss: 0.4429, Train: 0.9857, Val: 0.7680, Test: 0.7790
Epoch: 20, Loss: 0.4208, Train: 0.9857, Val: 0.7660, Test: 0.7850
Epoch: 21, Loss: 0.3535, Train: 0.9857, Val: 0.7640, Test: 0.7790
Epoch: 22, Loss: 0.3101, Train: 0.9857, Val: 0.7700, Test: 0.7800
Epoch: 23, Loss: 0.2886, Train: 0.9857, Val: 0.7660, Test: 0.7820
Epoch: 24, Loss: 0.2636, Train: 0.9857, Val: 0.7660, Test: 0.7790
Epoch: 25, Loss: 0.2351, Train: 0.9857, Val: 0.7640, Test: 0.7840
Epoch: 26, Loss: 0.2002, Train: 0.9929, Val: 0.7680, Test: 0.7850
Epoch: 27, Loss: 0.1986, Train: 0.9929, Val: 0.7720, Test: 0.7860
Epoch: 28, Loss: 0.1496, Train: 0.9929, Val: 0.7820, Test: 0.7860
Epoch: 29, Loss: 0.1353, Train: 0.9929, Val: 0.7860, Test: 0.7860
Epoch: 30, Loss: 0.1283, Train: 0.9929, Val: 0.7860, Test: 0.7870
Epoch: 31, Loss: 0.0971, Train: 0.9929, Val: 0.7840, Test: 0.7870
Epoch: 32, Loss: 0.0973, Train: 0.9929, Val: 0.7800, Test: 0.7860
Epoch: 33, Loss: 0.0977, Train: 0.9929, Val: 0.7760, Test: 0.7850
Epoch: 34, Loss: 0.1026, Train: 0.9929, Val: 0.7620, Test: 0.7820
Epoch: 35, Loss: 0.0945, Train: 0.9929, Val: 0.7620, Test: 0.7820
Epoch: 36, Loss: 0.0685, Train: 0.9929, Val: 0.7620, Test: 0.7760
Epoch: 37, Loss: 0.0547, Train: 0.9929, Val: 0.7580, Test: 0.7710
Epoch: 38, Loss: 0.0972, Train: 0.9929, Val: 0.7500, Test: 0.7630
Epoch: 39, Loss: 0.0580, Train: 0.9929, Val: 0.7460, Test: 0.7620
Epoch: 40, Loss: 0.0448, Train: 0.9929, Val: 0.7440, Test: 0.7620
Epoch: 41, Loss: 0.0412, Train: 0.9929, Val: 0.7460, Test: 0.7630
Epoch: 42, Loss: 0.0524, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 43, Loss: 0.0349, Train: 1.0000, Val: 0.7540, Test: 0.7750
Epoch: 44, Loss: 0.0264, Train: 1.0000, Val: 0.7560, Test: 0.7790
Epoch: 45, Loss: 0.0271, Train: 1.0000, Val: 0.7560, Test: 0.7850
Epoch: 46, Loss: 0.0399, Train: 1.0000, Val: 0.7600, Test: 0.7870
Epoch: 47, Loss: 0.0236, Train: 1.0000, Val: 0.7620, Test: 0.7900
Epoch: 48, Loss: 0.0194, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 49, Loss: 0.0197, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 50, Loss: 0.0318, Train: 1.0000, Val: 0.7680, Test: 0.7960
MAD:  0.9018
Best Test Accuracy: 0.7960, Val Accuracy: 0.7680, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.8032 ± 0.009442457307290285
Average MAD:  0.86198 ± 0.048386605584603684

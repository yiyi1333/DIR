/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9521, Train: 0.5571, Val: 0.3500, Test: 0.3580
Epoch: 2, Loss: 1.8696, Train: 0.8571, Val: 0.5200, Test: 0.5670
Epoch: 3, Loss: 1.7923, Train: 0.9429, Val: 0.6420, Test: 0.6660
Epoch: 4, Loss: 1.7049, Train: 0.9643, Val: 0.7200, Test: 0.7250
Epoch: 5, Loss: 1.6378, Train: 0.9857, Val: 0.7420, Test: 0.7620
Epoch: 6, Loss: 1.5436, Train: 0.9786, Val: 0.7560, Test: 0.7730
Epoch: 7, Loss: 1.4586, Train: 0.9786, Val: 0.7620, Test: 0.7850
Epoch: 8, Loss: 1.3762, Train: 0.9857, Val: 0.7620, Test: 0.7910
Epoch: 9, Loss: 1.2756, Train: 0.9857, Val: 0.7620, Test: 0.8020
Epoch: 10, Loss: 1.1794, Train: 0.9857, Val: 0.7660, Test: 0.8080
Epoch: 11, Loss: 1.0992, Train: 0.9857, Val: 0.7680, Test: 0.8050
Epoch: 12, Loss: 1.0049, Train: 0.9857, Val: 0.7700, Test: 0.8070
Epoch: 13, Loss: 0.9179, Train: 0.9857, Val: 0.7720, Test: 0.8060
Epoch: 14, Loss: 0.8420, Train: 0.9857, Val: 0.7700, Test: 0.8050
Epoch: 15, Loss: 0.7486, Train: 0.9857, Val: 0.7740, Test: 0.8050
Epoch: 16, Loss: 0.6897, Train: 0.9857, Val: 0.7700, Test: 0.8050
Epoch: 17, Loss: 0.6155, Train: 0.9857, Val: 0.7680, Test: 0.8040
Epoch: 18, Loss: 0.5446, Train: 0.9857, Val: 0.7660, Test: 0.8050
Epoch: 19, Loss: 0.4883, Train: 0.9857, Val: 0.7680, Test: 0.8060
Epoch: 20, Loss: 0.4312, Train: 0.9857, Val: 0.7680, Test: 0.8060
Epoch: 21, Loss: 0.4051, Train: 0.9857, Val: 0.7660, Test: 0.8030
Epoch: 22, Loss: 0.3544, Train: 0.9857, Val: 0.7660, Test: 0.8010
Epoch: 23, Loss: 0.3146, Train: 0.9857, Val: 0.7660, Test: 0.8020
Epoch: 24, Loss: 0.2834, Train: 0.9929, Val: 0.7660, Test: 0.8020
Epoch: 25, Loss: 0.2465, Train: 0.9929, Val: 0.7660, Test: 0.8000
Epoch: 26, Loss: 0.2275, Train: 0.9929, Val: 0.7680, Test: 0.7990
Epoch: 27, Loss: 0.2080, Train: 0.9929, Val: 0.7680, Test: 0.7970
Epoch: 28, Loss: 0.1715, Train: 0.9929, Val: 0.7660, Test: 0.7940
Epoch: 29, Loss: 0.1550, Train: 0.9929, Val: 0.7680, Test: 0.7940
Epoch: 30, Loss: 0.1391, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 31, Loss: 0.1303, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 32, Loss: 0.1207, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 33, Loss: 0.1110, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 34, Loss: 0.0922, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 35, Loss: 0.0832, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 36, Loss: 0.0738, Train: 1.0000, Val: 0.7640, Test: 0.7910
Epoch: 37, Loss: 0.0764, Train: 1.0000, Val: 0.7640, Test: 0.7930
Epoch: 38, Loss: 0.0739, Train: 1.0000, Val: 0.7600, Test: 0.7930
Epoch: 39, Loss: 0.0617, Train: 1.0000, Val: 0.7600, Test: 0.7950
Epoch: 40, Loss: 0.0572, Train: 1.0000, Val: 0.7580, Test: 0.7930
Epoch: 41, Loss: 0.0561, Train: 1.0000, Val: 0.7580, Test: 0.7960
Epoch: 42, Loss: 0.0467, Train: 1.0000, Val: 0.7560, Test: 0.7960
Epoch: 43, Loss: 0.0460, Train: 1.0000, Val: 0.7560, Test: 0.7970
Epoch: 44, Loss: 0.0461, Train: 1.0000, Val: 0.7540, Test: 0.7950
Epoch: 45, Loss: 0.0393, Train: 1.0000, Val: 0.7540, Test: 0.7940
Epoch: 46, Loss: 0.0399, Train: 1.0000, Val: 0.7520, Test: 0.7930
Epoch: 47, Loss: 0.0369, Train: 1.0000, Val: 0.7560, Test: 0.7940
Epoch: 48, Loss: 0.0304, Train: 1.0000, Val: 0.7560, Test: 0.7940
Epoch: 49, Loss: 0.0321, Train: 1.0000, Val: 0.7560, Test: 0.7940
Epoch: 50, Loss: 0.0267, Train: 1.0000, Val: 0.7560, Test: 0.7950
MAD:  0.9496
Best Test Accuracy: 0.8080, Val Accuracy: 0.7660, Train Accuracy: 0.9857
Training completed.
Seed:  1
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9363, Train: 0.5714, Val: 0.3580, Test: 0.3410
Epoch: 2, Loss: 1.8626, Train: 0.8357, Val: 0.5320, Test: 0.5210
Epoch: 3, Loss: 1.7749, Train: 0.9429, Val: 0.6180, Test: 0.6320
Epoch: 4, Loss: 1.7061, Train: 0.9643, Val: 0.6660, Test: 0.6930
Epoch: 5, Loss: 1.6305, Train: 0.9857, Val: 0.6960, Test: 0.7290
Epoch: 6, Loss: 1.5443, Train: 0.9857, Val: 0.7200, Test: 0.7460
Epoch: 7, Loss: 1.4560, Train: 0.9857, Val: 0.7480, Test: 0.7530
Epoch: 8, Loss: 1.3921, Train: 0.9857, Val: 0.7540, Test: 0.7620
Epoch: 9, Loss: 1.2951, Train: 0.9857, Val: 0.7600, Test: 0.7680
Epoch: 10, Loss: 1.2023, Train: 0.9857, Val: 0.7660, Test: 0.7720
Epoch: 11, Loss: 1.1211, Train: 0.9929, Val: 0.7700, Test: 0.7730
Epoch: 12, Loss: 1.0171, Train: 0.9929, Val: 0.7720, Test: 0.7780
Epoch: 13, Loss: 0.9338, Train: 0.9929, Val: 0.7720, Test: 0.7830
Epoch: 14, Loss: 0.8551, Train: 0.9929, Val: 0.7740, Test: 0.7810
Epoch: 15, Loss: 0.7949, Train: 0.9929, Val: 0.7740, Test: 0.7850
Epoch: 16, Loss: 0.7194, Train: 0.9929, Val: 0.7720, Test: 0.7810
Epoch: 17, Loss: 0.6439, Train: 0.9929, Val: 0.7740, Test: 0.7810
Epoch: 18, Loss: 0.5661, Train: 0.9929, Val: 0.7780, Test: 0.7820
Epoch: 19, Loss: 0.5083, Train: 0.9929, Val: 0.7800, Test: 0.7840
Epoch: 20, Loss: 0.4541, Train: 0.9929, Val: 0.7820, Test: 0.7790
Epoch: 21, Loss: 0.4018, Train: 0.9929, Val: 0.7820, Test: 0.7760
Epoch: 22, Loss: 0.3778, Train: 0.9929, Val: 0.7840, Test: 0.7770
Epoch: 23, Loss: 0.3440, Train: 0.9929, Val: 0.7840, Test: 0.7760
Epoch: 24, Loss: 0.3033, Train: 0.9929, Val: 0.7880, Test: 0.7770
Epoch: 25, Loss: 0.2740, Train: 0.9929, Val: 0.7860, Test: 0.7760
Epoch: 26, Loss: 0.2344, Train: 0.9929, Val: 0.7860, Test: 0.7740
Epoch: 27, Loss: 0.2104, Train: 0.9929, Val: 0.7840, Test: 0.7750
Epoch: 28, Loss: 0.1953, Train: 0.9929, Val: 0.7820, Test: 0.7730
Epoch: 29, Loss: 0.1669, Train: 0.9929, Val: 0.7880, Test: 0.7730
Epoch: 30, Loss: 0.1570, Train: 0.9929, Val: 0.7860, Test: 0.7710
Epoch: 31, Loss: 0.1451, Train: 0.9929, Val: 0.7860, Test: 0.7720
Epoch: 32, Loss: 0.1315, Train: 0.9929, Val: 0.7860, Test: 0.7710
Epoch: 33, Loss: 0.1150, Train: 1.0000, Val: 0.7840, Test: 0.7710
Epoch: 34, Loss: 0.1124, Train: 1.0000, Val: 0.7820, Test: 0.7690
Epoch: 35, Loss: 0.1022, Train: 1.0000, Val: 0.7840, Test: 0.7700
Epoch: 36, Loss: 0.0800, Train: 1.0000, Val: 0.7800, Test: 0.7710
Epoch: 37, Loss: 0.0776, Train: 1.0000, Val: 0.7820, Test: 0.7730
Epoch: 38, Loss: 0.0713, Train: 1.0000, Val: 0.7760, Test: 0.7740
Epoch: 39, Loss: 0.0535, Train: 1.0000, Val: 0.7760, Test: 0.7750
Epoch: 40, Loss: 0.0686, Train: 1.0000, Val: 0.7760, Test: 0.7750
Epoch: 41, Loss: 0.0600, Train: 1.0000, Val: 0.7780, Test: 0.7740
Epoch: 42, Loss: 0.0508, Train: 1.0000, Val: 0.7780, Test: 0.7760
Epoch: 43, Loss: 0.0520, Train: 1.0000, Val: 0.7760, Test: 0.7770
Epoch: 44, Loss: 0.0480, Train: 1.0000, Val: 0.7740, Test: 0.7730
Epoch: 45, Loss: 0.0403, Train: 1.0000, Val: 0.7740, Test: 0.7710
Epoch: 46, Loss: 0.0349, Train: 1.0000, Val: 0.7740, Test: 0.7700
Epoch: 47, Loss: 0.0357, Train: 1.0000, Val: 0.7780, Test: 0.7720
Epoch: 48, Loss: 0.0326, Train: 1.0000, Val: 0.7780, Test: 0.7740
Epoch: 49, Loss: 0.0320, Train: 1.0000, Val: 0.7760, Test: 0.7740
Epoch: 50, Loss: 0.0306, Train: 1.0000, Val: 0.7780, Test: 0.7740
MAD:  0.7765
Best Test Accuracy: 0.7850, Val Accuracy: 0.7740, Train Accuracy: 0.9929
Training completed.
Seed:  2
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9583, Train: 0.4500, Val: 0.3860, Test: 0.3880
Epoch: 2, Loss: 1.8692, Train: 0.8643, Val: 0.5420, Test: 0.5580
Epoch: 3, Loss: 1.7952, Train: 0.9571, Val: 0.6280, Test: 0.6790
Epoch: 4, Loss: 1.7100, Train: 0.9714, Val: 0.6940, Test: 0.7420
Epoch: 5, Loss: 1.6356, Train: 0.9714, Val: 0.7280, Test: 0.7600
Epoch: 6, Loss: 1.5548, Train: 0.9857, Val: 0.7380, Test: 0.7840
Epoch: 7, Loss: 1.4771, Train: 0.9857, Val: 0.7520, Test: 0.7860
Epoch: 8, Loss: 1.3726, Train: 0.9857, Val: 0.7520, Test: 0.7890
Epoch: 9, Loss: 1.2989, Train: 0.9857, Val: 0.7560, Test: 0.7910
Epoch: 10, Loss: 1.2144, Train: 0.9929, Val: 0.7520, Test: 0.7940
Epoch: 11, Loss: 1.1140, Train: 0.9929, Val: 0.7600, Test: 0.7980
Epoch: 12, Loss: 1.0311, Train: 0.9929, Val: 0.7660, Test: 0.8000
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 13, Loss: 0.9408, Train: 0.9929, Val: 0.7720, Test: 0.8040
Epoch: 14, Loss: 0.8757, Train: 0.9929, Val: 0.7700, Test: 0.8070
Epoch: 15, Loss: 0.7907, Train: 0.9929, Val: 0.7700, Test: 0.8090
Epoch: 16, Loss: 0.7066, Train: 0.9929, Val: 0.7740, Test: 0.8030
Epoch: 17, Loss: 0.6301, Train: 0.9929, Val: 0.7720, Test: 0.7990
Epoch: 18, Loss: 0.6112, Train: 0.9929, Val: 0.7700, Test: 0.7960
Epoch: 19, Loss: 0.5333, Train: 0.9929, Val: 0.7620, Test: 0.7970
Epoch: 20, Loss: 0.4634, Train: 0.9929, Val: 0.7600, Test: 0.7940
Epoch: 21, Loss: 0.4189, Train: 0.9929, Val: 0.7640, Test: 0.7900
Epoch: 22, Loss: 0.3624, Train: 0.9929, Val: 0.7620, Test: 0.7870
Epoch: 23, Loss: 0.3346, Train: 0.9929, Val: 0.7560, Test: 0.7810
Epoch: 24, Loss: 0.2971, Train: 0.9929, Val: 0.7540, Test: 0.7800
Epoch: 25, Loss: 0.2776, Train: 0.9929, Val: 0.7520, Test: 0.7770
Epoch: 26, Loss: 0.2470, Train: 0.9929, Val: 0.7480, Test: 0.7770
Epoch: 27, Loss: 0.2406, Train: 0.9929, Val: 0.7460, Test: 0.7730
Epoch: 28, Loss: 0.2091, Train: 0.9929, Val: 0.7480, Test: 0.7700
Epoch: 29, Loss: 0.1774, Train: 0.9929, Val: 0.7440, Test: 0.7660
Epoch: 30, Loss: 0.1690, Train: 0.9929, Val: 0.7440, Test: 0.7680
Epoch: 31, Loss: 0.1501, Train: 0.9929, Val: 0.7440, Test: 0.7680
Epoch: 32, Loss: 0.1295, Train: 0.9929, Val: 0.7440, Test: 0.7680
Epoch: 33, Loss: 0.1210, Train: 0.9929, Val: 0.7440, Test: 0.7690
Epoch: 34, Loss: 0.1081, Train: 0.9929, Val: 0.7440, Test: 0.7710
Epoch: 35, Loss: 0.1055, Train: 0.9929, Val: 0.7420, Test: 0.7720
Epoch: 36, Loss: 0.0941, Train: 0.9929, Val: 0.7440, Test: 0.7730
Epoch: 37, Loss: 0.0835, Train: 0.9929, Val: 0.7440, Test: 0.7720
Epoch: 38, Loss: 0.0733, Train: 1.0000, Val: 0.7400, Test: 0.7720
Epoch: 39, Loss: 0.0801, Train: 1.0000, Val: 0.7380, Test: 0.7750
Epoch: 40, Loss: 0.0675, Train: 1.0000, Val: 0.7380, Test: 0.7750
Epoch: 41, Loss: 0.0705, Train: 1.0000, Val: 0.7360, Test: 0.7760
Epoch: 42, Loss: 0.0600, Train: 1.0000, Val: 0.7380, Test: 0.7750
Epoch: 43, Loss: 0.0551, Train: 1.0000, Val: 0.7380, Test: 0.7720
Epoch: 44, Loss: 0.0512, Train: 1.0000, Val: 0.7360, Test: 0.7720
Epoch: 45, Loss: 0.0448, Train: 1.0000, Val: 0.7360, Test: 0.7700
Epoch: 46, Loss: 0.0412, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 47, Loss: 0.0428, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 48, Loss: 0.0370, Train: 1.0000, Val: 0.7360, Test: 0.7670
Epoch: 49, Loss: 0.0372, Train: 1.0000, Val: 0.7360, Test: 0.7670
Epoch: 50, Loss: 0.0319, Train: 1.0000, Val: 0.7360, Test: 0.7650
MAD:  0.8544
Best Test Accuracy: 0.8090, Val Accuracy: 0.7700, Train Accuracy: 0.9929
Training completed.
Seed:  3
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9320, Train: 0.5143, Val: 0.2780, Test: 0.2910
Epoch: 2, Loss: 1.8449, Train: 0.7786, Val: 0.4340, Test: 0.4940
Epoch: 3, Loss: 1.7669, Train: 0.9500, Val: 0.5560, Test: 0.6010
Epoch: 4, Loss: 1.6953, Train: 0.9857, Val: 0.6340, Test: 0.6720
Epoch: 5, Loss: 1.6106, Train: 0.9857, Val: 0.6720, Test: 0.7070
Epoch: 6, Loss: 1.5172, Train: 0.9857, Val: 0.6880, Test: 0.7280
Epoch: 7, Loss: 1.4415, Train: 0.9929, Val: 0.7180, Test: 0.7380
Epoch: 8, Loss: 1.3493, Train: 0.9929, Val: 0.7360, Test: 0.7520
Epoch: 9, Loss: 1.2471, Train: 0.9929, Val: 0.7540, Test: 0.7670
Epoch: 10, Loss: 1.1661, Train: 0.9929, Val: 0.7540, Test: 0.7720
Epoch: 11, Loss: 1.0895, Train: 0.9929, Val: 0.7560, Test: 0.7790
Epoch: 12, Loss: 0.9965, Train: 0.9929, Val: 0.7600, Test: 0.7830
Epoch: 13, Loss: 0.9106, Train: 0.9929, Val: 0.7600, Test: 0.7820
Epoch: 14, Loss: 0.8496, Train: 0.9929, Val: 0.7580, Test: 0.7820
Epoch: 15, Loss: 0.7654, Train: 0.9857, Val: 0.7560, Test: 0.7880
Epoch: 16, Loss: 0.6976, Train: 0.9857, Val: 0.7580, Test: 0.7900
Epoch: 17, Loss: 0.6197, Train: 0.9857, Val: 0.7640, Test: 0.7930
Epoch: 18, Loss: 0.5890, Train: 0.9857, Val: 0.7660, Test: 0.7950
Epoch: 19, Loss: 0.5234, Train: 0.9857, Val: 0.7660, Test: 0.7950
Epoch: 20, Loss: 0.4628, Train: 0.9857, Val: 0.7580, Test: 0.7940
Epoch: 21, Loss: 0.4265, Train: 0.9857, Val: 0.7660, Test: 0.7920
Epoch: 22, Loss: 0.3776, Train: 0.9857, Val: 0.7660, Test: 0.7940
Epoch: 23, Loss: 0.3347, Train: 0.9857, Val: 0.7660, Test: 0.7890
Epoch: 24, Loss: 0.3089, Train: 0.9857, Val: 0.7640, Test: 0.7880
Epoch: 25, Loss: 0.2682, Train: 0.9857, Val: 0.7640, Test: 0.7900
Epoch: 26, Loss: 0.2371, Train: 0.9857, Val: 0.7660, Test: 0.7880
Epoch: 27, Loss: 0.2350, Train: 0.9857, Val: 0.7660, Test: 0.7900
Epoch: 28, Loss: 0.2076, Train: 0.9857, Val: 0.7660, Test: 0.7910
Epoch: 29, Loss: 0.1884, Train: 0.9857, Val: 0.7660, Test: 0.7970
Epoch: 30, Loss: 0.1640, Train: 0.9857, Val: 0.7680, Test: 0.7990
Epoch: 31, Loss: 0.1571, Train: 0.9929, Val: 0.7660, Test: 0.7980
Epoch: 32, Loss: 0.1348, Train: 1.0000, Val: 0.7680, Test: 0.7980
Epoch: 33, Loss: 0.1170, Train: 1.0000, Val: 0.7700, Test: 0.7980
Epoch: 34, Loss: 0.1236, Train: 1.0000, Val: 0.7700, Test: 0.7990
Epoch: 35, Loss: 0.1093, Train: 1.0000, Val: 0.7620, Test: 0.7990
Epoch: 36, Loss: 0.0876, Train: 1.0000, Val: 0.7620, Test: 0.8000
Epoch: 37, Loss: 0.0848, Train: 1.0000, Val: 0.7600, Test: 0.8010
Epoch: 38, Loss: 0.0748, Train: 1.0000, Val: 0.7600, Test: 0.7990
Epoch: 39, Loss: 0.0702, Train: 1.0000, Val: 0.7580, Test: 0.8000
Epoch: 40, Loss: 0.0686, Train: 1.0000, Val: 0.7580, Test: 0.7970
Epoch: 41, Loss: 0.0632, Train: 1.0000, Val: 0.7580, Test: 0.7970
Epoch: 42, Loss: 0.0575, Train: 1.0000, Val: 0.7520, Test: 0.7990
Epoch: 43, Loss: 0.0557, Train: 1.0000, Val: 0.7500, Test: 0.7950
Epoch: 44, Loss: 0.0469, Train: 1.0000, Val: 0.7500, Test: 0.7910
Epoch: 45, Loss: 0.0483, Train: 1.0000, Val: 0.7520, Test: 0.7900
Epoch: 46, Loss: 0.0419, Train: 1.0000, Val: 0.7520, Test: 0.7880
Epoch: 47, Loss: 0.0395, Train: 1.0000, Val: 0.7520, Test: 0.7850
Epoch: 48, Loss: 0.0388, Train: 1.0000, Val: 0.7500, Test: 0.7850
Epoch: 49, Loss: 0.0407, Train: 1.0000, Val: 0.7520, Test: 0.7840
Epoch: 50, Loss: 0.0349, Train: 1.0000, Val: 0.7500, Test: 0.7810
MAD:  0.8689
Best Test Accuracy: 0.8010, Val Accuracy: 0.7600, Train Accuracy: 1.0000
Training completed.
Seed:  4
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9293, Train: 0.3714, Val: 0.2340, Test: 0.2650
Epoch: 2, Loss: 1.8589, Train: 0.6357, Val: 0.3380, Test: 0.4280
Epoch: 3, Loss: 1.7816, Train: 0.9071, Val: 0.4680, Test: 0.5470
Epoch: 4, Loss: 1.6960, Train: 0.9500, Val: 0.5640, Test: 0.6340
Epoch: 5, Loss: 1.6042, Train: 0.9643, Val: 0.6160, Test: 0.6720
Epoch: 6, Loss: 1.5195, Train: 0.9714, Val: 0.6500, Test: 0.6990
Epoch: 7, Loss: 1.4336, Train: 0.9786, Val: 0.6740, Test: 0.7190
Epoch: 8, Loss: 1.3490, Train: 0.9857, Val: 0.6980, Test: 0.7300
Epoch: 9, Loss: 1.2619, Train: 0.9857, Val: 0.7100, Test: 0.7410
Epoch: 10, Loss: 1.1807, Train: 0.9857, Val: 0.7240, Test: 0.7530
Epoch: 11, Loss: 1.0856, Train: 0.9857, Val: 0.7300, Test: 0.7710
Epoch: 12, Loss: 0.9915, Train: 0.9857, Val: 0.7360, Test: 0.7690
Epoch: 13, Loss: 0.9244, Train: 0.9857, Val: 0.7440, Test: 0.7800
Epoch: 14, Loss: 0.8166, Train: 0.9857, Val: 0.7460, Test: 0.7810
Epoch: 15, Loss: 0.7639, Train: 0.9857, Val: 0.7620, Test: 0.7790
Epoch: 16, Loss: 0.6992, Train: 0.9857, Val: 0.7580, Test: 0.7760
Epoch: 17, Loss: 0.6250, Train: 0.9857, Val: 0.7560, Test: 0.7760
Epoch: 18, Loss: 0.5750, Train: 0.9857, Val: 0.7600, Test: 0.7800
Epoch: 19, Loss: 0.5166, Train: 0.9857, Val: 0.7660, Test: 0.7820
Epoch: 20, Loss: 0.4592, Train: 0.9857, Val: 0.7580, Test: 0.7800
Epoch: 21, Loss: 0.4294, Train: 0.9929, Val: 0.7620, Test: 0.7800
Epoch: 22, Loss: 0.3778, Train: 0.9929, Val: 0.7640, Test: 0.7770
Epoch: 23, Loss: 0.3255, Train: 0.9929, Val: 0.7640, Test: 0.7770
Epoch: 24, Loss: 0.3131, Train: 0.9929, Val: 0.7660, Test: 0.7760
Epoch: 25, Loss: 0.2592, Train: 0.9929, Val: 0.7640, Test: 0.7730
Epoch: 26, Loss: 0.2485, Train: 0.9929, Val: 0.7660, Test: 0.7730
Epoch: 27, Loss: 0.2270, Train: 0.9929, Val: 0.7660, Test: 0.7690
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 28, Loss: 0.1925, Train: 0.9929, Val: 0.7660, Test: 0.7650
Epoch: 29, Loss: 0.1707, Train: 0.9929, Val: 0.7660, Test: 0.7630
Epoch: 30, Loss: 0.1660, Train: 1.0000, Val: 0.7680, Test: 0.7640
Epoch: 31, Loss: 0.1457, Train: 1.0000, Val: 0.7680, Test: 0.7640
Epoch: 32, Loss: 0.1350, Train: 1.0000, Val: 0.7680, Test: 0.7650
Epoch: 33, Loss: 0.1181, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 34, Loss: 0.1096, Train: 1.0000, Val: 0.7600, Test: 0.7620
Epoch: 35, Loss: 0.1011, Train: 1.0000, Val: 0.7620, Test: 0.7630
Epoch: 36, Loss: 0.0886, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 37, Loss: 0.0774, Train: 1.0000, Val: 0.7600, Test: 0.7640
Epoch: 38, Loss: 0.0803, Train: 1.0000, Val: 0.7580, Test: 0.7640
Epoch: 39, Loss: 0.0643, Train: 1.0000, Val: 0.7580, Test: 0.7650
Epoch: 40, Loss: 0.0627, Train: 1.0000, Val: 0.7580, Test: 0.7670
Epoch: 41, Loss: 0.0510, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 42, Loss: 0.0544, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 43, Loss: 0.0452, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 44, Loss: 0.0480, Train: 1.0000, Val: 0.7580, Test: 0.7700
Epoch: 45, Loss: 0.0409, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 46, Loss: 0.0484, Train: 1.0000, Val: 0.7580, Test: 0.7680
Epoch: 47, Loss: 0.0420, Train: 1.0000, Val: 0.7580, Test: 0.7680
Epoch: 48, Loss: 0.0366, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 49, Loss: 0.0359, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 50, Loss: 0.0372, Train: 1.0000, Val: 0.7580, Test: 0.7690
MAD:  0.8376
Best Test Accuracy: 0.7820, Val Accuracy: 0.7660, Train Accuracy: 0.9857
Training completed.
Seed:  5
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9609, Train: 0.5143, Val: 0.3300, Test: 0.3570
Epoch: 2, Loss: 1.8632, Train: 0.8500, Val: 0.4940, Test: 0.5170
Epoch: 3, Loss: 1.7741, Train: 0.9714, Val: 0.6000, Test: 0.6230
Epoch: 4, Loss: 1.6938, Train: 0.9786, Val: 0.6480, Test: 0.6750
Epoch: 5, Loss: 1.6178, Train: 0.9857, Val: 0.6980, Test: 0.7100
Epoch: 6, Loss: 1.5331, Train: 0.9857, Val: 0.7140, Test: 0.7320
Epoch: 7, Loss: 1.4337, Train: 0.9929, Val: 0.7380, Test: 0.7460
Epoch: 8, Loss: 1.3375, Train: 0.9929, Val: 0.7500, Test: 0.7570
Epoch: 9, Loss: 1.2589, Train: 0.9929, Val: 0.7600, Test: 0.7620
Epoch: 10, Loss: 1.1715, Train: 0.9929, Val: 0.7660, Test: 0.7660
Epoch: 11, Loss: 1.0879, Train: 0.9929, Val: 0.7660, Test: 0.7660
Epoch: 12, Loss: 0.9848, Train: 0.9929, Val: 0.7600, Test: 0.7660
Epoch: 13, Loss: 0.9070, Train: 0.9929, Val: 0.7600, Test: 0.7680
Epoch: 14, Loss: 0.8030, Train: 0.9929, Val: 0.7620, Test: 0.7700
Epoch: 15, Loss: 0.7370, Train: 0.9929, Val: 0.7620, Test: 0.7700
Epoch: 16, Loss: 0.6762, Train: 0.9929, Val: 0.7620, Test: 0.7730
Epoch: 17, Loss: 0.5977, Train: 0.9929, Val: 0.7640, Test: 0.7740
Epoch: 18, Loss: 0.5323, Train: 0.9929, Val: 0.7660, Test: 0.7720
Epoch: 19, Loss: 0.4755, Train: 0.9929, Val: 0.7620, Test: 0.7700
Epoch: 20, Loss: 0.4364, Train: 0.9929, Val: 0.7600, Test: 0.7740
Epoch: 21, Loss: 0.3932, Train: 0.9929, Val: 0.7620, Test: 0.7770
Epoch: 22, Loss: 0.3468, Train: 0.9929, Val: 0.7620, Test: 0.7770
Epoch: 23, Loss: 0.2991, Train: 0.9929, Val: 0.7620, Test: 0.7790
Epoch: 24, Loss: 0.2759, Train: 1.0000, Val: 0.7640, Test: 0.7760
Epoch: 25, Loss: 0.2530, Train: 1.0000, Val: 0.7660, Test: 0.7750
Epoch: 26, Loss: 0.2236, Train: 1.0000, Val: 0.7660, Test: 0.7750
Epoch: 27, Loss: 0.1858, Train: 1.0000, Val: 0.7680, Test: 0.7740
Epoch: 28, Loss: 0.1659, Train: 1.0000, Val: 0.7660, Test: 0.7740
Epoch: 29, Loss: 0.1574, Train: 1.0000, Val: 0.7640, Test: 0.7740
Epoch: 30, Loss: 0.1363, Train: 1.0000, Val: 0.7620, Test: 0.7710
Epoch: 31, Loss: 0.1305, Train: 1.0000, Val: 0.7620, Test: 0.7700
Epoch: 32, Loss: 0.0993, Train: 1.0000, Val: 0.7600, Test: 0.7680
Epoch: 33, Loss: 0.1056, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 34, Loss: 0.0873, Train: 1.0000, Val: 0.7560, Test: 0.7660
Epoch: 35, Loss: 0.0827, Train: 1.0000, Val: 0.7560, Test: 0.7660
Epoch: 36, Loss: 0.0820, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 37, Loss: 0.0738, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 38, Loss: 0.0631, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 39, Loss: 0.0583, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 40, Loss: 0.0549, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 41, Loss: 0.0473, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 42, Loss: 0.0426, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 43, Loss: 0.0478, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 44, Loss: 0.0418, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 45, Loss: 0.0450, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 46, Loss: 0.0383, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 47, Loss: 0.0330, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 48, Loss: 0.0309, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 49, Loss: 0.0284, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 50, Loss: 0.0249, Train: 1.0000, Val: 0.7480, Test: 0.7640
MAD:  0.9518
Best Test Accuracy: 0.7790, Val Accuracy: 0.7620, Train Accuracy: 0.9929
Training completed.
Seed:  6
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9279, Train: 0.6143, Val: 0.3860, Test: 0.4200
Epoch: 2, Loss: 1.8395, Train: 0.8786, Val: 0.5500, Test: 0.5790
Epoch: 3, Loss: 1.7893, Train: 0.9429, Val: 0.6400, Test: 0.6750
Epoch: 4, Loss: 1.7117, Train: 0.9786, Val: 0.6940, Test: 0.7040
Epoch: 5, Loss: 1.6215, Train: 0.9857, Val: 0.7200, Test: 0.7280
Epoch: 6, Loss: 1.5500, Train: 0.9857, Val: 0.7300, Test: 0.7470
Epoch: 7, Loss: 1.4432, Train: 0.9857, Val: 0.7520, Test: 0.7570
Epoch: 8, Loss: 1.3728, Train: 0.9857, Val: 0.7580, Test: 0.7650
Epoch: 9, Loss: 1.2871, Train: 0.9857, Val: 0.7620, Test: 0.7660
Epoch: 10, Loss: 1.1994, Train: 0.9857, Val: 0.7680, Test: 0.7700
Epoch: 11, Loss: 1.1102, Train: 0.9857, Val: 0.7720, Test: 0.7750
Epoch: 12, Loss: 1.0218, Train: 0.9857, Val: 0.7720, Test: 0.7800
Epoch: 13, Loss: 0.9658, Train: 0.9857, Val: 0.7740, Test: 0.7820
Epoch: 14, Loss: 0.8877, Train: 0.9857, Val: 0.7740, Test: 0.7880
Epoch: 15, Loss: 0.8066, Train: 0.9857, Val: 0.7800, Test: 0.7920
Epoch: 16, Loss: 0.7364, Train: 0.9857, Val: 0.7780, Test: 0.7930
Epoch: 17, Loss: 0.6561, Train: 0.9857, Val: 0.7840, Test: 0.7900
Epoch: 18, Loss: 0.5957, Train: 0.9857, Val: 0.7860, Test: 0.7880
Epoch: 19, Loss: 0.5357, Train: 0.9857, Val: 0.7820, Test: 0.7870
Epoch: 20, Loss: 0.4972, Train: 0.9857, Val: 0.7800, Test: 0.7880
Epoch: 21, Loss: 0.4525, Train: 0.9929, Val: 0.7840, Test: 0.7890
Epoch: 22, Loss: 0.4059, Train: 0.9929, Val: 0.7860, Test: 0.7880
Epoch: 23, Loss: 0.3583, Train: 0.9929, Val: 0.7860, Test: 0.7890
Epoch: 24, Loss: 0.3414, Train: 0.9929, Val: 0.7840, Test: 0.7910
Epoch: 25, Loss: 0.2963, Train: 0.9929, Val: 0.7840, Test: 0.7910
Epoch: 26, Loss: 0.2719, Train: 0.9929, Val: 0.7820, Test: 0.7910
Epoch: 27, Loss: 0.2404, Train: 0.9929, Val: 0.7780, Test: 0.7920
Epoch: 28, Loss: 0.2224, Train: 0.9929, Val: 0.7760, Test: 0.7920
Epoch: 29, Loss: 0.2106, Train: 0.9929, Val: 0.7760, Test: 0.7900
Epoch: 30, Loss: 0.1824, Train: 1.0000, Val: 0.7740, Test: 0.7890
Epoch: 31, Loss: 0.1767, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 32, Loss: 0.1416, Train: 1.0000, Val: 0.7700, Test: 0.7890
Epoch: 33, Loss: 0.1440, Train: 1.0000, Val: 0.7700, Test: 0.7870
Epoch: 34, Loss: 0.1217, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 35, Loss: 0.1161, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 36, Loss: 0.1013, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 37, Loss: 0.0963, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 38, Loss: 0.0780, Train: 1.0000, Val: 0.7640, Test: 0.7910
Epoch: 39, Loss: 0.0754, Train: 1.0000, Val: 0.7640, Test: 0.7940
Epoch: 40, Loss: 0.0668, Train: 1.0000, Val: 0.7600, Test: 0.7940
Epoch: 41, Loss: 0.0652, Train: 1.0000, Val: 0.7580, Test: 0.7950
Epoch: 42, Loss: 0.0635, Train: 1.0000, Val: 0.7600, Test: 0.7960
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 43, Loss: 0.0616, Train: 1.0000, Val: 0.7580, Test: 0.7940
Epoch: 44, Loss: 0.0545, Train: 1.0000, Val: 0.7560, Test: 0.7910
Epoch: 45, Loss: 0.0475, Train: 1.0000, Val: 0.7500, Test: 0.7890
Epoch: 46, Loss: 0.0481, Train: 1.0000, Val: 0.7560, Test: 0.7910
Epoch: 47, Loss: 0.0453, Train: 1.0000, Val: 0.7540, Test: 0.7910
Epoch: 48, Loss: 0.0424, Train: 1.0000, Val: 0.7500, Test: 0.7900
Epoch: 49, Loss: 0.0439, Train: 1.0000, Val: 0.7480, Test: 0.7900
Epoch: 50, Loss: 0.0405, Train: 1.0000, Val: 0.7480, Test: 0.7900
MAD:  0.894
Best Test Accuracy: 0.7960, Val Accuracy: 0.7600, Train Accuracy: 1.0000
Training completed.
Seed:  7
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9524, Train: 0.4714, Val: 0.3380, Test: 0.3270
Epoch: 2, Loss: 1.8639, Train: 0.7571, Val: 0.4780, Test: 0.4690
Epoch: 3, Loss: 1.7676, Train: 0.8857, Val: 0.5660, Test: 0.5750
Epoch: 4, Loss: 1.6825, Train: 0.9500, Val: 0.6140, Test: 0.6430
Epoch: 5, Loss: 1.6025, Train: 0.9786, Val: 0.6620, Test: 0.6870
Epoch: 6, Loss: 1.5032, Train: 0.9857, Val: 0.6960, Test: 0.7110
Epoch: 7, Loss: 1.4121, Train: 0.9857, Val: 0.7160, Test: 0.7260
Epoch: 8, Loss: 1.3280, Train: 0.9857, Val: 0.7220, Test: 0.7360
Epoch: 9, Loss: 1.2170, Train: 0.9857, Val: 0.7240, Test: 0.7500
Epoch: 10, Loss: 1.1418, Train: 0.9857, Val: 0.7300, Test: 0.7570
Epoch: 11, Loss: 1.0286, Train: 0.9929, Val: 0.7400, Test: 0.7610
Epoch: 12, Loss: 0.9522, Train: 0.9929, Val: 0.7440, Test: 0.7620
Epoch: 13, Loss: 0.8662, Train: 0.9929, Val: 0.7360, Test: 0.7600
Epoch: 14, Loss: 0.7822, Train: 0.9929, Val: 0.7500, Test: 0.7660
Epoch: 15, Loss: 0.6990, Train: 0.9929, Val: 0.7560, Test: 0.7690
Epoch: 16, Loss: 0.6243, Train: 0.9929, Val: 0.7600, Test: 0.7710
Epoch: 17, Loss: 0.5831, Train: 0.9929, Val: 0.7620, Test: 0.7720
Epoch: 18, Loss: 0.5141, Train: 0.9929, Val: 0.7660, Test: 0.7750
Epoch: 19, Loss: 0.4491, Train: 0.9929, Val: 0.7700, Test: 0.7760
Epoch: 20, Loss: 0.4035, Train: 0.9929, Val: 0.7680, Test: 0.7770
Epoch: 21, Loss: 0.3517, Train: 0.9929, Val: 0.7660, Test: 0.7800
Epoch: 22, Loss: 0.3188, Train: 0.9929, Val: 0.7640, Test: 0.7800
Epoch: 23, Loss: 0.2870, Train: 0.9929, Val: 0.7620, Test: 0.7800
Epoch: 24, Loss: 0.2454, Train: 0.9929, Val: 0.7640, Test: 0.7800
Epoch: 25, Loss: 0.2304, Train: 0.9929, Val: 0.7640, Test: 0.7770
Epoch: 26, Loss: 0.1984, Train: 0.9929, Val: 0.7640, Test: 0.7770
Epoch: 27, Loss: 0.1964, Train: 0.9929, Val: 0.7640, Test: 0.7760
Epoch: 28, Loss: 0.1735, Train: 0.9929, Val: 0.7600, Test: 0.7750
Epoch: 29, Loss: 0.1348, Train: 0.9929, Val: 0.7600, Test: 0.7740
Epoch: 30, Loss: 0.1403, Train: 0.9929, Val: 0.7580, Test: 0.7730
Epoch: 31, Loss: 0.1355, Train: 0.9929, Val: 0.7580, Test: 0.7720
Epoch: 32, Loss: 0.1013, Train: 1.0000, Val: 0.7600, Test: 0.7710
Epoch: 33, Loss: 0.0958, Train: 1.0000, Val: 0.7580, Test: 0.7700
Epoch: 34, Loss: 0.0901, Train: 1.0000, Val: 0.7580, Test: 0.7700
Epoch: 35, Loss: 0.0863, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 36, Loss: 0.0784, Train: 1.0000, Val: 0.7600, Test: 0.7720
Epoch: 37, Loss: 0.0690, Train: 1.0000, Val: 0.7600, Test: 0.7720
Epoch: 38, Loss: 0.0597, Train: 1.0000, Val: 0.7620, Test: 0.7740
Epoch: 39, Loss: 0.0526, Train: 1.0000, Val: 0.7620, Test: 0.7780
Epoch: 40, Loss: 0.0532, Train: 1.0000, Val: 0.7660, Test: 0.7800
Epoch: 41, Loss: 0.0481, Train: 1.0000, Val: 0.7660, Test: 0.7800
Epoch: 42, Loss: 0.0471, Train: 1.0000, Val: 0.7620, Test: 0.7810
Epoch: 43, Loss: 0.0412, Train: 1.0000, Val: 0.7620, Test: 0.7800
Epoch: 44, Loss: 0.0351, Train: 1.0000, Val: 0.7660, Test: 0.7790
Epoch: 45, Loss: 0.0353, Train: 1.0000, Val: 0.7660, Test: 0.7780
Epoch: 46, Loss: 0.0371, Train: 1.0000, Val: 0.7640, Test: 0.7770
Epoch: 47, Loss: 0.0336, Train: 1.0000, Val: 0.7620, Test: 0.7770
Epoch: 48, Loss: 0.0302, Train: 1.0000, Val: 0.7620, Test: 0.7760
Epoch: 49, Loss: 0.0317, Train: 1.0000, Val: 0.7620, Test: 0.7760
Epoch: 50, Loss: 0.0274, Train: 1.0000, Val: 0.7600, Test: 0.7760
MAD:  0.9411
Best Test Accuracy: 0.7810, Val Accuracy: 0.7620, Train Accuracy: 1.0000
Training completed.
Seed:  8
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9585, Train: 0.3214, Val: 0.2120, Test: 0.2020
Epoch: 2, Loss: 1.8850, Train: 0.7071, Val: 0.3660, Test: 0.3600
Epoch: 3, Loss: 1.8092, Train: 0.8429, Val: 0.4800, Test: 0.4810
Epoch: 4, Loss: 1.7330, Train: 0.9357, Val: 0.5500, Test: 0.5500
Epoch: 5, Loss: 1.6655, Train: 0.9500, Val: 0.5900, Test: 0.5950
Epoch: 6, Loss: 1.5889, Train: 0.9571, Val: 0.6320, Test: 0.6310
Epoch: 7, Loss: 1.5016, Train: 0.9714, Val: 0.6580, Test: 0.6620
Epoch: 8, Loss: 1.4343, Train: 0.9786, Val: 0.6720, Test: 0.6750
Epoch: 9, Loss: 1.3559, Train: 0.9929, Val: 0.6800, Test: 0.6950
Epoch: 10, Loss: 1.2486, Train: 0.9929, Val: 0.6880, Test: 0.7080
Epoch: 11, Loss: 1.1808, Train: 0.9929, Val: 0.7020, Test: 0.7120
Epoch: 12, Loss: 1.0715, Train: 0.9929, Val: 0.7100, Test: 0.7260
Epoch: 13, Loss: 0.9944, Train: 0.9929, Val: 0.7160, Test: 0.7360
Epoch: 14, Loss: 0.8986, Train: 0.9929, Val: 0.7240, Test: 0.7450
Epoch: 15, Loss: 0.8379, Train: 0.9929, Val: 0.7280, Test: 0.7630
Epoch: 16, Loss: 0.7639, Train: 0.9929, Val: 0.7400, Test: 0.7620
Epoch: 17, Loss: 0.7019, Train: 0.9929, Val: 0.7500, Test: 0.7690
Epoch: 18, Loss: 0.6300, Train: 0.9929, Val: 0.7540, Test: 0.7730
Epoch: 19, Loss: 0.5598, Train: 0.9929, Val: 0.7620, Test: 0.7730
Epoch: 20, Loss: 0.5152, Train: 0.9929, Val: 0.7740, Test: 0.7770
Epoch: 21, Loss: 0.4734, Train: 0.9929, Val: 0.7740, Test: 0.7780
Epoch: 22, Loss: 0.4173, Train: 0.9929, Val: 0.7760, Test: 0.7820
Epoch: 23, Loss: 0.3980, Train: 0.9929, Val: 0.7800, Test: 0.7840
Epoch: 24, Loss: 0.3291, Train: 0.9929, Val: 0.7780, Test: 0.7840
Epoch: 25, Loss: 0.2964, Train: 0.9929, Val: 0.7800, Test: 0.7860
Epoch: 26, Loss: 0.2702, Train: 0.9929, Val: 0.7820, Test: 0.7860
Epoch: 27, Loss: 0.2531, Train: 0.9929, Val: 0.7800, Test: 0.7880
Epoch: 28, Loss: 0.2273, Train: 0.9929, Val: 0.7780, Test: 0.7880
Epoch: 29, Loss: 0.1859, Train: 0.9929, Val: 0.7780, Test: 0.7890
Epoch: 30, Loss: 0.1744, Train: 0.9929, Val: 0.7720, Test: 0.7880
Epoch: 31, Loss: 0.1516, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 32, Loss: 0.1412, Train: 1.0000, Val: 0.7660, Test: 0.7880
Epoch: 33, Loss: 0.1336, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 34, Loss: 0.1151, Train: 1.0000, Val: 0.7620, Test: 0.7870
Epoch: 35, Loss: 0.0975, Train: 1.0000, Val: 0.7640, Test: 0.7870
Epoch: 36, Loss: 0.0930, Train: 1.0000, Val: 0.7620, Test: 0.7870
Epoch: 37, Loss: 0.0856, Train: 1.0000, Val: 0.7620, Test: 0.7850
Epoch: 38, Loss: 0.0788, Train: 1.0000, Val: 0.7620, Test: 0.7860
Epoch: 39, Loss: 0.0705, Train: 1.0000, Val: 0.7600, Test: 0.7870
Epoch: 40, Loss: 0.0650, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 41, Loss: 0.0587, Train: 1.0000, Val: 0.7560, Test: 0.7880
Epoch: 42, Loss: 0.0675, Train: 1.0000, Val: 0.7560, Test: 0.7860
Epoch: 43, Loss: 0.0603, Train: 1.0000, Val: 0.7540, Test: 0.7820
Epoch: 44, Loss: 0.0511, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 45, Loss: 0.0477, Train: 1.0000, Val: 0.7520, Test: 0.7810
Epoch: 46, Loss: 0.0441, Train: 1.0000, Val: 0.7540, Test: 0.7820
Epoch: 47, Loss: 0.0426, Train: 1.0000, Val: 0.7540, Test: 0.7760
Epoch: 48, Loss: 0.0441, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 49, Loss: 0.0395, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 50, Loss: 0.0358, Train: 1.0000, Val: 0.7480, Test: 0.7690
MAD:  0.9052
Best Test Accuracy: 0.7890, Val Accuracy: 0.7780, Train Accuracy: 0.9929
Training completed.
Seed:  9
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9486, Train: 0.4857, Val: 0.2580, Test: 0.2870
Epoch: 2, Loss: 1.8690, Train: 0.8571, Val: 0.4200, Test: 0.4300
Epoch: 3, Loss: 1.7850, Train: 0.9286, Val: 0.5400, Test: 0.5580
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 4, Loss: 1.7010, Train: 0.9643, Val: 0.6380, Test: 0.6200
Epoch: 5, Loss: 1.6224, Train: 0.9714, Val: 0.6700, Test: 0.6630
Epoch: 6, Loss: 1.5431, Train: 0.9714, Val: 0.6940, Test: 0.6930
Epoch: 7, Loss: 1.4604, Train: 0.9786, Val: 0.7080, Test: 0.7180
Epoch: 8, Loss: 1.3692, Train: 0.9786, Val: 0.7160, Test: 0.7320
Epoch: 9, Loss: 1.2647, Train: 0.9857, Val: 0.7260, Test: 0.7450
Epoch: 10, Loss: 1.1844, Train: 0.9857, Val: 0.7360, Test: 0.7500
Epoch: 11, Loss: 1.0967, Train: 0.9857, Val: 0.7440, Test: 0.7550
Epoch: 12, Loss: 0.9953, Train: 0.9929, Val: 0.7480, Test: 0.7570
Epoch: 13, Loss: 0.8911, Train: 0.9929, Val: 0.7500, Test: 0.7590
Epoch: 14, Loss: 0.8381, Train: 0.9929, Val: 0.7540, Test: 0.7600
Epoch: 15, Loss: 0.7550, Train: 0.9929, Val: 0.7560, Test: 0.7680
Epoch: 16, Loss: 0.6693, Train: 0.9929, Val: 0.7580, Test: 0.7700
Epoch: 17, Loss: 0.5985, Train: 0.9929, Val: 0.7640, Test: 0.7730
Epoch: 18, Loss: 0.5336, Train: 0.9929, Val: 0.7660, Test: 0.7740
Epoch: 19, Loss: 0.5019, Train: 0.9929, Val: 0.7660, Test: 0.7740
Epoch: 20, Loss: 0.4304, Train: 0.9929, Val: 0.7640, Test: 0.7750
Epoch: 21, Loss: 0.4033, Train: 0.9929, Val: 0.7660, Test: 0.7790
Epoch: 22, Loss: 0.3394, Train: 0.9929, Val: 0.7680, Test: 0.7790
Epoch: 23, Loss: 0.3234, Train: 0.9929, Val: 0.7680, Test: 0.7800
Epoch: 24, Loss: 0.2862, Train: 0.9929, Val: 0.7800, Test: 0.7790
Epoch: 25, Loss: 0.2481, Train: 0.9929, Val: 0.7840, Test: 0.7820
Epoch: 26, Loss: 0.2283, Train: 0.9929, Val: 0.7880, Test: 0.7850
Epoch: 27, Loss: 0.2002, Train: 1.0000, Val: 0.7780, Test: 0.7860
Epoch: 28, Loss: 0.1821, Train: 1.0000, Val: 0.7780, Test: 0.7850
Epoch: 29, Loss: 0.1668, Train: 1.0000, Val: 0.7800, Test: 0.7860
Epoch: 30, Loss: 0.1542, Train: 1.0000, Val: 0.7800, Test: 0.7850
Epoch: 31, Loss: 0.1360, Train: 1.0000, Val: 0.7800, Test: 0.7850
Epoch: 32, Loss: 0.1205, Train: 1.0000, Val: 0.7820, Test: 0.7820
Epoch: 33, Loss: 0.1108, Train: 1.0000, Val: 0.7820, Test: 0.7840
Epoch: 34, Loss: 0.1098, Train: 1.0000, Val: 0.7820, Test: 0.7860
Epoch: 35, Loss: 0.0928, Train: 1.0000, Val: 0.7800, Test: 0.7840
Epoch: 36, Loss: 0.0875, Train: 1.0000, Val: 0.7780, Test: 0.7850
Epoch: 37, Loss: 0.0817, Train: 1.0000, Val: 0.7800, Test: 0.7860
Epoch: 38, Loss: 0.0695, Train: 1.0000, Val: 0.7760, Test: 0.7820
Epoch: 39, Loss: 0.0685, Train: 1.0000, Val: 0.7760, Test: 0.7810
Epoch: 40, Loss: 0.0608, Train: 1.0000, Val: 0.7740, Test: 0.7840
Epoch: 41, Loss: 0.0534, Train: 1.0000, Val: 0.7720, Test: 0.7830
Epoch: 42, Loss: 0.0544, Train: 1.0000, Val: 0.7740, Test: 0.7830
Epoch: 43, Loss: 0.0408, Train: 1.0000, Val: 0.7740, Test: 0.7830
Epoch: 44, Loss: 0.0507, Train: 1.0000, Val: 0.7740, Test: 0.7820
Epoch: 45, Loss: 0.0416, Train: 1.0000, Val: 0.7740, Test: 0.7810
Epoch: 46, Loss: 0.0382, Train: 1.0000, Val: 0.7740, Test: 0.7810
Epoch: 47, Loss: 0.0380, Train: 1.0000, Val: 0.7760, Test: 0.7810
Epoch: 48, Loss: 0.0374, Train: 1.0000, Val: 0.7760, Test: 0.7810
Epoch: 49, Loss: 0.0366, Train: 1.0000, Val: 0.7740, Test: 0.7820
Epoch: 50, Loss: 0.0423, Train: 1.0000, Val: 0.7740, Test: 0.7820
MAD:  0.7784
Best Test Accuracy: 0.7860, Val Accuracy: 0.7780, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.7916000000000001 ± 0.010603772913449258
Average MAD:  0.87575 ± 0.061639407038030475

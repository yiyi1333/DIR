/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8601, Train: 0.0714, Val: 0.0460, Test: 0.0590
Epoch: 2, Loss: 4.8479, Train: 0.3143, Val: 0.2820, Test: 0.3010
Epoch: 3, Loss: 4.8334, Train: 0.5071, Val: 0.4120, Test: 0.4230
Epoch: 4, Loss: 4.7872, Train: 0.5571, Val: 0.4620, Test: 0.4890
Epoch: 5, Loss: 4.7843, Train: 0.5571, Val: 0.4700, Test: 0.5080
Epoch: 6, Loss: 4.7224, Train: 0.5571, Val: 0.4800, Test: 0.5110
Epoch: 7, Loss: 4.7333, Train: 0.5571, Val: 0.4880, Test: 0.5220
Epoch: 8, Loss: 4.6627, Train: 0.5500, Val: 0.4980, Test: 0.5310
Epoch: 9, Loss: 4.6841, Train: 0.6000, Val: 0.5020, Test: 0.5410
Epoch: 10, Loss: 4.6565, Train: 0.6143, Val: 0.5240, Test: 0.5520
Epoch: 11, Loss: 4.4938, Train: 0.6214, Val: 0.5320, Test: 0.5630
Epoch: 12, Loss: 4.5921, Train: 0.6357, Val: 0.5380, Test: 0.5730
Epoch: 13, Loss: 4.5141, Train: 0.6500, Val: 0.5340, Test: 0.5700
Epoch: 14, Loss: 4.4055, Train: 0.6643, Val: 0.5440, Test: 0.5680
Epoch: 15, Loss: 4.3711, Train: 0.6857, Val: 0.5400, Test: 0.5770
Epoch: 16, Loss: 4.2344, Train: 0.7000, Val: 0.5520, Test: 0.5870
Epoch: 17, Loss: 4.1176, Train: 0.7286, Val: 0.5580, Test: 0.5970
Epoch: 18, Loss: 4.2113, Train: 0.7429, Val: 0.5740, Test: 0.6370
Epoch: 19, Loss: 3.9566, Train: 0.7786, Val: 0.5940, Test: 0.6580
Epoch: 20, Loss: 4.1099, Train: 0.7929, Val: 0.6180, Test: 0.6750
Epoch: 21, Loss: 4.3873, Train: 0.8071, Val: 0.6340, Test: 0.6860
Epoch: 22, Loss: 4.2243, Train: 0.8429, Val: 0.6440, Test: 0.6950
Epoch: 23, Loss: 4.3091, Train: 0.9143, Val: 0.6680, Test: 0.7100
Epoch: 24, Loss: 3.8966, Train: 0.9357, Val: 0.6900, Test: 0.7290
Epoch: 25, Loss: 4.2107, Train: 0.9571, Val: 0.7200, Test: 0.7570
Epoch: 26, Loss: 4.0306, Train: 0.9571, Val: 0.7360, Test: 0.7620
Epoch: 27, Loss: 4.0260, Train: 0.9500, Val: 0.7380, Test: 0.7660
Epoch: 28, Loss: 4.0227, Train: 0.9571, Val: 0.7300, Test: 0.7520
Epoch: 29, Loss: 3.8748, Train: 0.9571, Val: 0.7160, Test: 0.7430
Epoch: 30, Loss: 3.9066, Train: 0.9571, Val: 0.7080, Test: 0.7470
Epoch: 31, Loss: 4.3108, Train: 0.9571, Val: 0.7240, Test: 0.7570
Epoch: 32, Loss: 3.9668, Train: 0.9571, Val: 0.7280, Test: 0.7640
Epoch: 33, Loss: 4.0561, Train: 0.9643, Val: 0.7380, Test: 0.7750
Epoch: 34, Loss: 3.6292, Train: 0.9786, Val: 0.7480, Test: 0.7840
Epoch: 35, Loss: 4.1710, Train: 0.9786, Val: 0.7580, Test: 0.7910
Epoch: 36, Loss: 3.8683, Train: 0.9857, Val: 0.7660, Test: 0.8000
Epoch: 37, Loss: 4.0334, Train: 0.9786, Val: 0.7820, Test: 0.8080
Epoch: 38, Loss: 3.6967, Train: 0.9786, Val: 0.7940, Test: 0.8120
Epoch: 39, Loss: 3.9781, Train: 0.9786, Val: 0.7980, Test: 0.8120
Epoch: 40, Loss: 3.8620, Train: 0.9786, Val: 0.7960, Test: 0.8140
Epoch: 41, Loss: 3.8278, Train: 0.9786, Val: 0.8020, Test: 0.8150
Epoch: 42, Loss: 4.0523, Train: 0.9786, Val: 0.8000, Test: 0.8170
Epoch: 43, Loss: 3.5991, Train: 0.9786, Val: 0.8020, Test: 0.8170
Epoch: 44, Loss: 3.8553, Train: 0.9786, Val: 0.8040, Test: 0.8150
Epoch: 45, Loss: 3.9563, Train: 0.9786, Val: 0.8040, Test: 0.8160
Epoch: 46, Loss: 3.7342, Train: 0.9857, Val: 0.8060, Test: 0.8130
Epoch: 47, Loss: 3.8784, Train: 0.9929, Val: 0.8060, Test: 0.8180
Epoch: 48, Loss: 3.4071, Train: 0.9929, Val: 0.8060, Test: 0.8180
Epoch: 49, Loss: 3.6999, Train: 0.9929, Val: 0.8020, Test: 0.8170
Epoch: 50, Loss: 4.0303, Train: 0.9929, Val: 0.8000, Test: 0.8180
MAD:  0.3955
Best Test Accuracy: 0.8180, Val Accuracy: 0.8060, Train Accuracy: 0.9929
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8517, Train: 0.1714, Val: 0.1060, Test: 0.1310
Epoch: 2, Loss: 4.8208, Train: 0.2714, Val: 0.1760, Test: 0.1860
Epoch: 3, Loss: 4.8193, Train: 0.3071, Val: 0.1920, Test: 0.1940
Epoch: 4, Loss: 4.7801, Train: 0.3571, Val: 0.2000, Test: 0.2100
Epoch: 5, Loss: 4.7515, Train: 0.3857, Val: 0.2140, Test: 0.2370
Epoch: 6, Loss: 4.7393, Train: 0.4000, Val: 0.2320, Test: 0.2520
Epoch: 7, Loss: 4.7502, Train: 0.4071, Val: 0.2300, Test: 0.2580
Epoch: 8, Loss: 4.6339, Train: 0.4143, Val: 0.2420, Test: 0.2760
Epoch: 9, Loss: 4.6358, Train: 0.4214, Val: 0.2860, Test: 0.2990
Epoch: 10, Loss: 4.6016, Train: 0.4643, Val: 0.3080, Test: 0.3320
Epoch: 11, Loss: 4.4635, Train: 0.5143, Val: 0.3180, Test: 0.3510
Epoch: 12, Loss: 4.5217, Train: 0.5786, Val: 0.3340, Test: 0.3860
Epoch: 13, Loss: 4.4845, Train: 0.5929, Val: 0.3720, Test: 0.4230
Epoch: 14, Loss: 4.2220, Train: 0.6429, Val: 0.4080, Test: 0.4610
Epoch: 15, Loss: 4.5069, Train: 0.7000, Val: 0.4780, Test: 0.5320
Epoch: 16, Loss: 4.2442, Train: 0.7286, Val: 0.5400, Test: 0.5860
Epoch: 17, Loss: 4.1183, Train: 0.7429, Val: 0.5960, Test: 0.6350
Epoch: 18, Loss: 4.3392, Train: 0.8071, Val: 0.6380, Test: 0.6660
Epoch: 19, Loss: 4.1862, Train: 0.8214, Val: 0.6520, Test: 0.6850
Epoch: 20, Loss: 4.0559, Train: 0.8286, Val: 0.6700, Test: 0.7010
Epoch: 21, Loss: 4.1388, Train: 0.8643, Val: 0.6780, Test: 0.7220
Epoch: 22, Loss: 3.9244, Train: 0.9357, Val: 0.7160, Test: 0.7650
Epoch: 23, Loss: 4.0182, Train: 0.9786, Val: 0.7220, Test: 0.7660
Epoch: 24, Loss: 4.0842, Train: 0.9786, Val: 0.7440, Test: 0.7740
Epoch: 25, Loss: 3.8185, Train: 0.9857, Val: 0.7660, Test: 0.7820
Epoch: 26, Loss: 3.8359, Train: 0.9929, Val: 0.7740, Test: 0.7940
Epoch: 27, Loss: 3.8085, Train: 0.9929, Val: 0.7800, Test: 0.7900
Epoch: 28, Loss: 4.0652, Train: 0.9857, Val: 0.7760, Test: 0.7820
Epoch: 29, Loss: 3.7960, Train: 0.9714, Val: 0.7700, Test: 0.7770
Epoch: 30, Loss: 3.6291, Train: 0.9714, Val: 0.7760, Test: 0.7780
Epoch: 31, Loss: 3.8444, Train: 0.9643, Val: 0.7760, Test: 0.7820
Epoch: 32, Loss: 4.0910, Train: 0.9714, Val: 0.7760, Test: 0.7860
Epoch: 33, Loss: 3.7825, Train: 0.9786, Val: 0.7800, Test: 0.7930
Epoch: 34, Loss: 3.8699, Train: 0.9857, Val: 0.7920, Test: 0.7930
Epoch: 35, Loss: 3.6249, Train: 0.9857, Val: 0.7940, Test: 0.7990
Epoch: 36, Loss: 3.8042, Train: 0.9786, Val: 0.7980, Test: 0.8080
Epoch: 37, Loss: 3.9719, Train: 0.9786, Val: 0.7980, Test: 0.8080
Epoch: 38, Loss: 3.8688, Train: 0.9786, Val: 0.8000, Test: 0.8080
Epoch: 39, Loss: 3.7113, Train: 0.9857, Val: 0.7940, Test: 0.8050
Epoch: 40, Loss: 3.9779, Train: 0.9857, Val: 0.7880, Test: 0.8060
Epoch: 41, Loss: 3.9117, Train: 0.9929, Val: 0.7820, Test: 0.8040
Epoch: 42, Loss: 3.3797, Train: 0.9929, Val: 0.7880, Test: 0.8120
Epoch: 43, Loss: 3.6927, Train: 0.9929, Val: 0.7820, Test: 0.8100
Epoch: 44, Loss: 4.0940, Train: 0.9929, Val: 0.7860, Test: 0.8130
Epoch: 45, Loss: 3.5789, Train: 0.9929, Val: 0.7920, Test: 0.8160
Epoch: 46, Loss: 4.1184, Train: 0.9929, Val: 0.7940, Test: 0.8180
Epoch: 47, Loss: 3.5112, Train: 1.0000, Val: 0.7960, Test: 0.8200
Epoch: 48, Loss: 3.9205, Train: 1.0000, Val: 0.7960, Test: 0.8210
Epoch: 49, Loss: 3.5966, Train: 1.0000, Val: 0.8000, Test: 0.8230
Epoch: 50, Loss: 3.3863, Train: 1.0000, Val: 0.8000, Test: 0.8230
MAD:  0.462
Best Test Accuracy: 0.8230, Val Accuracy: 0.8000, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8560, Train: 0.1643, Val: 0.1140, Test: 0.1140
Epoch: 2, Loss: 4.8379, Train: 0.2786, Val: 0.2580, Test: 0.2570
Epoch: 3, Loss: 4.8246, Train: 0.3214, Val: 0.2860, Test: 0.2870
Epoch: 4, Loss: 4.7848, Train: 0.3429, Val: 0.2840, Test: 0.2970
Epoch: 5, Loss: 4.7518, Train: 0.3714, Val: 0.2760, Test: 0.2810
Epoch: 6, Loss: 4.7398, Train: 0.3786, Val: 0.2640, Test: 0.2810
Epoch: 7, Loss: 4.7378, Train: 0.4000, Val: 0.2680, Test: 0.2760
Epoch: 8, Loss: 4.6504, Train: 0.4071, Val: 0.2620, Test: 0.2740
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 9, Loss: 4.5989, Train: 0.4000, Val: 0.2620, Test: 0.2730
Epoch: 10, Loss: 4.5262, Train: 0.4143, Val: 0.2640, Test: 0.2710
Epoch: 11, Loss: 4.6643, Train: 0.4357, Val: 0.2820, Test: 0.2870
Epoch: 12, Loss: 4.4713, Train: 0.4643, Val: 0.2860, Test: 0.2950
Epoch: 13, Loss: 4.4749, Train: 0.4714, Val: 0.2900, Test: 0.3070
Epoch: 14, Loss: 4.5469, Train: 0.5286, Val: 0.3260, Test: 0.3480
Epoch: 15, Loss: 4.2141, Train: 0.5571, Val: 0.3740, Test: 0.3820
Epoch: 16, Loss: 4.3081, Train: 0.6071, Val: 0.4300, Test: 0.4500
Epoch: 17, Loss: 4.1297, Train: 0.6286, Val: 0.4920, Test: 0.5150
Epoch: 18, Loss: 4.3112, Train: 0.6643, Val: 0.5400, Test: 0.5660
Epoch: 19, Loss: 4.2234, Train: 0.7000, Val: 0.5680, Test: 0.5970
Epoch: 20, Loss: 3.9500, Train: 0.7571, Val: 0.6080, Test: 0.6440
Epoch: 21, Loss: 4.1116, Train: 0.8214, Val: 0.6460, Test: 0.6960
Epoch: 22, Loss: 3.9670, Train: 0.8929, Val: 0.6780, Test: 0.7280
Epoch: 23, Loss: 4.3794, Train: 0.9357, Val: 0.7080, Test: 0.7550
Epoch: 24, Loss: 4.0670, Train: 0.9429, Val: 0.7260, Test: 0.7510
Epoch: 25, Loss: 4.0172, Train: 0.9429, Val: 0.6920, Test: 0.7250
Epoch: 26, Loss: 4.0060, Train: 0.9429, Val: 0.6780, Test: 0.7110
Epoch: 27, Loss: 3.9459, Train: 0.9429, Val: 0.6800, Test: 0.7060
Epoch: 28, Loss: 4.2757, Train: 0.9500, Val: 0.6780, Test: 0.6960
Epoch: 29, Loss: 3.9970, Train: 0.9714, Val: 0.6860, Test: 0.6900
Epoch: 30, Loss: 3.9055, Train: 0.9786, Val: 0.6860, Test: 0.6950
Epoch: 31, Loss: 3.9427, Train: 0.9714, Val: 0.6900, Test: 0.7080
Epoch: 32, Loss: 4.0029, Train: 0.9786, Val: 0.6960, Test: 0.7280
Epoch: 33, Loss: 3.9589, Train: 0.9786, Val: 0.7280, Test: 0.7560
Epoch: 34, Loss: 3.9009, Train: 0.9786, Val: 0.7420, Test: 0.7720
Epoch: 35, Loss: 4.0311, Train: 0.9786, Val: 0.7560, Test: 0.7800
Epoch: 36, Loss: 3.9565, Train: 0.9786, Val: 0.7680, Test: 0.7800
Epoch: 37, Loss: 4.0129, Train: 0.9857, Val: 0.7820, Test: 0.7810
Epoch: 38, Loss: 3.8700, Train: 0.9857, Val: 0.7800, Test: 0.7850
Epoch: 39, Loss: 4.0886, Train: 0.9857, Val: 0.7760, Test: 0.7880
Epoch: 40, Loss: 3.6821, Train: 0.9857, Val: 0.7820, Test: 0.7860
Epoch: 41, Loss: 3.8704, Train: 0.9857, Val: 0.7920, Test: 0.8000
Epoch: 42, Loss: 3.7525, Train: 0.9857, Val: 0.7940, Test: 0.8000
Epoch: 43, Loss: 3.8664, Train: 0.9857, Val: 0.7880, Test: 0.8100
Epoch: 44, Loss: 4.0961, Train: 0.9857, Val: 0.7860, Test: 0.8120
Epoch: 45, Loss: 3.9263, Train: 0.9929, Val: 0.7940, Test: 0.8150
Epoch: 46, Loss: 3.8991, Train: 0.9929, Val: 0.7940, Test: 0.8160
Epoch: 47, Loss: 3.7661, Train: 0.9929, Val: 0.7880, Test: 0.8170
Epoch: 48, Loss: 3.9342, Train: 0.9857, Val: 0.7900, Test: 0.8150
Epoch: 49, Loss: 3.7360, Train: 0.9857, Val: 0.7900, Test: 0.8120
Epoch: 50, Loss: 3.8349, Train: 0.9857, Val: 0.7800, Test: 0.8080
MAD:  0.3782
Best Test Accuracy: 0.8170, Val Accuracy: 0.7880, Train Accuracy: 0.9929
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8504, Train: 0.1500, Val: 0.1220, Test: 0.1100
Epoch: 2, Loss: 4.8394, Train: 0.3643, Val: 0.2480, Test: 0.2410
Epoch: 3, Loss: 4.8093, Train: 0.4071, Val: 0.2740, Test: 0.2670
Epoch: 4, Loss: 4.7947, Train: 0.4286, Val: 0.2720, Test: 0.2750
Epoch: 5, Loss: 4.7606, Train: 0.4500, Val: 0.2720, Test: 0.2740
Epoch: 6, Loss: 4.6678, Train: 0.4643, Val: 0.2820, Test: 0.2770
Epoch: 7, Loss: 4.6307, Train: 0.4786, Val: 0.3000, Test: 0.2920
Epoch: 8, Loss: 4.5669, Train: 0.4929, Val: 0.3100, Test: 0.3120
Epoch: 9, Loss: 4.5612, Train: 0.4929, Val: 0.3200, Test: 0.3170
Epoch: 10, Loss: 4.5972, Train: 0.5000, Val: 0.3280, Test: 0.3260
Epoch: 11, Loss: 4.4506, Train: 0.5071, Val: 0.3440, Test: 0.3360
Epoch: 12, Loss: 4.5210, Train: 0.5357, Val: 0.3520, Test: 0.3490
Epoch: 13, Loss: 4.2902, Train: 0.5500, Val: 0.3500, Test: 0.3600
Epoch: 14, Loss: 4.5367, Train: 0.5643, Val: 0.3860, Test: 0.3890
Epoch: 15, Loss: 4.0086, Train: 0.6000, Val: 0.3980, Test: 0.4110
Epoch: 16, Loss: 4.2414, Train: 0.6214, Val: 0.4280, Test: 0.4400
Epoch: 17, Loss: 4.3777, Train: 0.6786, Val: 0.4620, Test: 0.4710
Epoch: 18, Loss: 4.1588, Train: 0.7143, Val: 0.4860, Test: 0.5040
Epoch: 19, Loss: 4.1315, Train: 0.7786, Val: 0.5180, Test: 0.5160
Epoch: 20, Loss: 4.0455, Train: 0.8214, Val: 0.5520, Test: 0.5480
Epoch: 21, Loss: 4.3197, Train: 0.8429, Val: 0.5780, Test: 0.5750
Epoch: 22, Loss: 4.1957, Train: 0.8929, Val: 0.6140, Test: 0.6130
Epoch: 23, Loss: 3.9497, Train: 0.9286, Val: 0.6840, Test: 0.6790
Epoch: 24, Loss: 4.3660, Train: 0.9357, Val: 0.7120, Test: 0.7230
Epoch: 25, Loss: 4.1536, Train: 0.9429, Val: 0.7380, Test: 0.7440
Epoch: 26, Loss: 3.5916, Train: 0.9571, Val: 0.7560, Test: 0.7560
Epoch: 27, Loss: 4.0661, Train: 0.9571, Val: 0.7680, Test: 0.7730
Epoch: 28, Loss: 3.9625, Train: 0.9643, Val: 0.7700, Test: 0.7790
Epoch: 29, Loss: 3.6848, Train: 0.9714, Val: 0.7800, Test: 0.7850
Epoch: 30, Loss: 4.0254, Train: 0.9714, Val: 0.7800, Test: 0.7850
Epoch: 31, Loss: 3.9743, Train: 0.9714, Val: 0.7740, Test: 0.7820
Epoch: 32, Loss: 4.0338, Train: 0.9714, Val: 0.7700, Test: 0.7720
Epoch: 33, Loss: 3.8856, Train: 0.9714, Val: 0.7700, Test: 0.7710
Epoch: 34, Loss: 3.9705, Train: 0.9714, Val: 0.7700, Test: 0.7740
Epoch: 35, Loss: 4.0083, Train: 0.9714, Val: 0.7700, Test: 0.7780
Epoch: 36, Loss: 3.8513, Train: 0.9786, Val: 0.7700, Test: 0.7770
Epoch: 37, Loss: 3.9356, Train: 0.9857, Val: 0.7660, Test: 0.7730
Epoch: 38, Loss: 4.0725, Train: 0.9857, Val: 0.7580, Test: 0.7690
Epoch: 39, Loss: 4.0679, Train: 0.9857, Val: 0.7520, Test: 0.7680
Epoch: 40, Loss: 3.8407, Train: 0.9929, Val: 0.7440, Test: 0.7610
Epoch: 41, Loss: 3.8802, Train: 0.9929, Val: 0.7420, Test: 0.7610
Epoch: 42, Loss: 3.8735, Train: 0.9929, Val: 0.7420, Test: 0.7610
Epoch: 43, Loss: 3.7404, Train: 0.9929, Val: 0.7500, Test: 0.7630
Epoch: 44, Loss: 3.5945, Train: 0.9929, Val: 0.7500, Test: 0.7680
Epoch: 45, Loss: 3.9800, Train: 0.9857, Val: 0.7540, Test: 0.7700
Epoch: 46, Loss: 3.9259, Train: 0.9857, Val: 0.7700, Test: 0.7790
Epoch: 47, Loss: 3.6996, Train: 0.9929, Val: 0.7700, Test: 0.7790
Epoch: 48, Loss: 3.7643, Train: 0.9929, Val: 0.7680, Test: 0.7820
Epoch: 49, Loss: 3.8152, Train: 0.9929, Val: 0.7720, Test: 0.7810
Epoch: 50, Loss: 3.5996, Train: 1.0000, Val: 0.7760, Test: 0.7850
MAD:  0.1577
Best Test Accuracy: 0.7850, Val Accuracy: 0.7800, Train Accuracy: 0.9714
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8564, Train: 0.1357, Val: 0.1300, Test: 0.1220
Epoch: 2, Loss: 4.8232, Train: 0.2643, Val: 0.1680, Test: 0.1630
Epoch: 3, Loss: 4.8074, Train: 0.2786, Val: 0.1800, Test: 0.1710
Epoch: 4, Loss: 4.7764, Train: 0.2929, Val: 0.1840, Test: 0.1910
Epoch: 5, Loss: 4.7352, Train: 0.3357, Val: 0.1960, Test: 0.2110
Epoch: 6, Loss: 4.6799, Train: 0.3714, Val: 0.2160, Test: 0.2240
Epoch: 7, Loss: 4.5939, Train: 0.4000, Val: 0.2240, Test: 0.2290
Epoch: 8, Loss: 4.5709, Train: 0.4000, Val: 0.2340, Test: 0.2330
Epoch: 9, Loss: 4.4974, Train: 0.4000, Val: 0.2400, Test: 0.2340
Epoch: 10, Loss: 4.4997, Train: 0.4429, Val: 0.2660, Test: 0.2530
Epoch: 11, Loss: 4.5409, Train: 0.4786, Val: 0.2860, Test: 0.2850
Epoch: 12, Loss: 4.3321, Train: 0.5500, Val: 0.3380, Test: 0.3430
Epoch: 13, Loss: 4.2146, Train: 0.5714, Val: 0.3860, Test: 0.3880
Epoch: 14, Loss: 4.2427, Train: 0.6357, Val: 0.4500, Test: 0.4590
Epoch: 15, Loss: 4.2425, Train: 0.6643, Val: 0.5200, Test: 0.5320
Epoch: 16, Loss: 4.4124, Train: 0.7071, Val: 0.6220, Test: 0.6100
Epoch: 17, Loss: 4.1757, Train: 0.7929, Val: 0.6860, Test: 0.6930
Epoch: 18, Loss: 4.3237, Train: 0.9071, Val: 0.7420, Test: 0.7590
Epoch: 19, Loss: 4.1133, Train: 0.9357, Val: 0.7720, Test: 0.7860
Epoch: 20, Loss: 4.3373, Train: 0.9643, Val: 0.7740, Test: 0.7980
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 21, Loss: 4.3421, Train: 0.9571, Val: 0.7780, Test: 0.8020
Epoch: 22, Loss: 4.0784, Train: 0.9571, Val: 0.7620, Test: 0.7940
Epoch: 23, Loss: 4.0576, Train: 0.9571, Val: 0.7600, Test: 0.7770
Epoch: 24, Loss: 4.0201, Train: 0.9571, Val: 0.7600, Test: 0.7610
Epoch: 25, Loss: 4.1025, Train: 0.9571, Val: 0.7620, Test: 0.7540
Epoch: 26, Loss: 3.9797, Train: 0.9571, Val: 0.7560, Test: 0.7510
Epoch: 27, Loss: 4.2394, Train: 0.9643, Val: 0.7620, Test: 0.7540
Epoch: 28, Loss: 3.8543, Train: 0.9643, Val: 0.7640, Test: 0.7700
Epoch: 29, Loss: 3.6993, Train: 0.9571, Val: 0.7640, Test: 0.7820
Epoch: 30, Loss: 3.8964, Train: 0.9571, Val: 0.7720, Test: 0.7950
Epoch: 31, Loss: 4.1293, Train: 0.9571, Val: 0.7800, Test: 0.8030
Epoch: 32, Loss: 4.0565, Train: 0.9714, Val: 0.7880, Test: 0.8100
Epoch: 33, Loss: 3.8126, Train: 0.9786, Val: 0.7940, Test: 0.8110
Epoch: 34, Loss: 3.9802, Train: 0.9786, Val: 0.8060, Test: 0.8190
Epoch: 35, Loss: 3.8147, Train: 0.9786, Val: 0.8020, Test: 0.8250
Epoch: 36, Loss: 3.8988, Train: 0.9643, Val: 0.8100, Test: 0.8250
Epoch: 37, Loss: 3.9127, Train: 0.9643, Val: 0.8120, Test: 0.8240
Epoch: 38, Loss: 3.8775, Train: 0.9714, Val: 0.8100, Test: 0.8270
Epoch: 39, Loss: 3.6617, Train: 0.9714, Val: 0.8060, Test: 0.8260
Epoch: 40, Loss: 3.5623, Train: 0.9786, Val: 0.8060, Test: 0.8310
Epoch: 41, Loss: 3.9066, Train: 0.9857, Val: 0.8020, Test: 0.8270
Epoch: 42, Loss: 3.9636, Train: 0.9857, Val: 0.7960, Test: 0.8210
Epoch: 43, Loss: 4.0228, Train: 0.9857, Val: 0.7880, Test: 0.8190
Epoch: 44, Loss: 4.0234, Train: 0.9857, Val: 0.7840, Test: 0.8150
Epoch: 45, Loss: 3.9480, Train: 0.9857, Val: 0.7800, Test: 0.8090
Epoch: 46, Loss: 3.8095, Train: 0.9857, Val: 0.7840, Test: 0.8060
Epoch: 47, Loss: 3.9532, Train: 0.9857, Val: 0.7860, Test: 0.8050
Epoch: 48, Loss: 3.8217, Train: 0.9929, Val: 0.7860, Test: 0.8050
Epoch: 49, Loss: 3.8581, Train: 0.9929, Val: 0.7900, Test: 0.7990
Epoch: 50, Loss: 3.6236, Train: 0.9929, Val: 0.7880, Test: 0.7940
MAD:  0.2656
Best Test Accuracy: 0.8310, Val Accuracy: 0.8060, Train Accuracy: 0.9786
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8537, Train: 0.2143, Val: 0.0960, Test: 0.0950
Epoch: 2, Loss: 4.8360, Train: 0.2857, Val: 0.1600, Test: 0.1640
Epoch: 3, Loss: 4.8301, Train: 0.3143, Val: 0.2140, Test: 0.2020
Epoch: 4, Loss: 4.7778, Train: 0.3357, Val: 0.2260, Test: 0.2170
Epoch: 5, Loss: 4.7467, Train: 0.3500, Val: 0.2380, Test: 0.2260
Epoch: 6, Loss: 4.6618, Train: 0.3714, Val: 0.2380, Test: 0.2390
Epoch: 7, Loss: 4.7017, Train: 0.3929, Val: 0.2460, Test: 0.2470
Epoch: 8, Loss: 4.6809, Train: 0.4214, Val: 0.2620, Test: 0.2550
Epoch: 9, Loss: 4.5903, Train: 0.4286, Val: 0.2600, Test: 0.2530
Epoch: 10, Loss: 4.5706, Train: 0.4286, Val: 0.2560, Test: 0.2580
Epoch: 11, Loss: 4.5959, Train: 0.4429, Val: 0.2640, Test: 0.2670
Epoch: 12, Loss: 4.6194, Train: 0.4429, Val: 0.2640, Test: 0.2730
Epoch: 13, Loss: 4.4720, Train: 0.4500, Val: 0.2680, Test: 0.2780
Epoch: 14, Loss: 4.4708, Train: 0.4643, Val: 0.2900, Test: 0.2890
Epoch: 15, Loss: 4.2599, Train: 0.4786, Val: 0.3060, Test: 0.2990
Epoch: 16, Loss: 4.3225, Train: 0.5000, Val: 0.3280, Test: 0.3220
Epoch: 17, Loss: 4.1911, Train: 0.5500, Val: 0.3760, Test: 0.3650
Epoch: 18, Loss: 4.0975, Train: 0.6786, Val: 0.4820, Test: 0.4570
Epoch: 19, Loss: 4.3270, Train: 0.7786, Val: 0.6020, Test: 0.5810
Epoch: 20, Loss: 4.3746, Train: 0.8714, Val: 0.7260, Test: 0.7090
Epoch: 21, Loss: 4.0467, Train: 0.9143, Val: 0.7720, Test: 0.7850
Epoch: 22, Loss: 4.2077, Train: 0.9143, Val: 0.7900, Test: 0.8010
Epoch: 23, Loss: 4.1119, Train: 0.9214, Val: 0.7820, Test: 0.7980
Epoch: 24, Loss: 4.1593, Train: 0.9071, Val: 0.7700, Test: 0.7840
Epoch: 25, Loss: 4.2193, Train: 0.9000, Val: 0.7840, Test: 0.7820
Epoch: 26, Loss: 4.1220, Train: 0.9071, Val: 0.7900, Test: 0.7820
Epoch: 27, Loss: 4.1858, Train: 0.9143, Val: 0.7740, Test: 0.7800
Epoch: 28, Loss: 4.0773, Train: 0.9357, Val: 0.7780, Test: 0.7760
Epoch: 29, Loss: 3.8764, Train: 0.9286, Val: 0.7740, Test: 0.7820
Epoch: 30, Loss: 4.1315, Train: 0.9357, Val: 0.7700, Test: 0.7810
Epoch: 31, Loss: 3.9148, Train: 0.9429, Val: 0.7700, Test: 0.7790
Epoch: 32, Loss: 4.1116, Train: 0.9429, Val: 0.7720, Test: 0.7840
Epoch: 33, Loss: 3.9039, Train: 0.9429, Val: 0.7760, Test: 0.7780
Epoch: 34, Loss: 4.0036, Train: 0.9429, Val: 0.7860, Test: 0.7810
Epoch: 35, Loss: 3.7653, Train: 0.9500, Val: 0.7820, Test: 0.7820
Epoch: 36, Loss: 4.0388, Train: 0.9500, Val: 0.7720, Test: 0.7760
Epoch: 37, Loss: 4.1475, Train: 0.9571, Val: 0.7760, Test: 0.7740
Epoch: 38, Loss: 3.9263, Train: 0.9643, Val: 0.7740, Test: 0.7780
Epoch: 39, Loss: 3.8972, Train: 0.9714, Val: 0.7800, Test: 0.7830
Epoch: 40, Loss: 3.8203, Train: 0.9786, Val: 0.7800, Test: 0.7910
Epoch: 41, Loss: 3.6589, Train: 0.9857, Val: 0.7820, Test: 0.7920
Epoch: 42, Loss: 3.9664, Train: 0.9857, Val: 0.7900, Test: 0.7900
Epoch: 43, Loss: 3.8392, Train: 0.9929, Val: 0.7880, Test: 0.7930
Epoch: 44, Loss: 3.6531, Train: 0.9929, Val: 0.7960, Test: 0.7950
Epoch: 45, Loss: 3.8198, Train: 0.9857, Val: 0.7900, Test: 0.7980
Epoch: 46, Loss: 3.9336, Train: 0.9857, Val: 0.7900, Test: 0.7940
Epoch: 47, Loss: 3.7688, Train: 0.9857, Val: 0.7900, Test: 0.7960
Epoch: 48, Loss: 3.8958, Train: 0.9857, Val: 0.7880, Test: 0.7980
Epoch: 49, Loss: 4.3296, Train: 0.9857, Val: 0.7840, Test: 0.7960
Epoch: 50, Loss: 3.6934, Train: 0.9857, Val: 0.7840, Test: 0.7940
MAD:  0.0586
Best Test Accuracy: 0.8010, Val Accuracy: 0.7900, Train Accuracy: 0.9143
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8448, Train: 0.1714, Val: 0.0860, Test: 0.0940
Epoch: 2, Loss: 4.8244, Train: 0.2357, Val: 0.1080, Test: 0.1200
Epoch: 3, Loss: 4.8094, Train: 0.3143, Val: 0.1560, Test: 0.1650
Epoch: 4, Loss: 4.7435, Train: 0.4357, Val: 0.2040, Test: 0.2240
Epoch: 5, Loss: 4.7768, Train: 0.5429, Val: 0.2760, Test: 0.3110
Epoch: 6, Loss: 4.7075, Train: 0.6286, Val: 0.3660, Test: 0.4080
Epoch: 7, Loss: 4.6487, Train: 0.7071, Val: 0.4600, Test: 0.4790
Epoch: 8, Loss: 4.6417, Train: 0.7643, Val: 0.5260, Test: 0.5420
Epoch: 9, Loss: 4.5823, Train: 0.7857, Val: 0.5500, Test: 0.5680
Epoch: 10, Loss: 4.5784, Train: 0.8071, Val: 0.5680, Test: 0.5970
Epoch: 11, Loss: 4.5529, Train: 0.8071, Val: 0.5940, Test: 0.6200
Epoch: 12, Loss: 4.4149, Train: 0.7857, Val: 0.5960, Test: 0.6250
Epoch: 13, Loss: 4.3855, Train: 0.7929, Val: 0.5960, Test: 0.6270
Epoch: 14, Loss: 4.1833, Train: 0.8071, Val: 0.5820, Test: 0.6220
Epoch: 15, Loss: 4.3913, Train: 0.8143, Val: 0.5720, Test: 0.6120
Epoch: 16, Loss: 4.4136, Train: 0.8143, Val: 0.5640, Test: 0.6040
Epoch: 17, Loss: 4.2051, Train: 0.8286, Val: 0.5680, Test: 0.5880
Epoch: 18, Loss: 4.1426, Train: 0.8143, Val: 0.5560, Test: 0.5700
Epoch: 19, Loss: 4.0287, Train: 0.8214, Val: 0.5240, Test: 0.5520
Epoch: 20, Loss: 3.9700, Train: 0.8643, Val: 0.5440, Test: 0.5650
Epoch: 21, Loss: 4.2674, Train: 0.9000, Val: 0.6120, Test: 0.6190
Epoch: 22, Loss: 4.1064, Train: 0.9143, Val: 0.6760, Test: 0.6520
Epoch: 23, Loss: 4.2533, Train: 0.9357, Val: 0.6980, Test: 0.6690
Epoch: 24, Loss: 4.0078, Train: 0.9357, Val: 0.6900, Test: 0.6880
Epoch: 25, Loss: 3.9262, Train: 0.9357, Val: 0.6900, Test: 0.6930
Epoch: 26, Loss: 4.2126, Train: 0.9357, Val: 0.6720, Test: 0.6890
Epoch: 27, Loss: 4.1874, Train: 0.9357, Val: 0.6820, Test: 0.6930
Epoch: 28, Loss: 4.1629, Train: 0.9429, Val: 0.7040, Test: 0.7130
Epoch: 29, Loss: 4.1552, Train: 0.9500, Val: 0.7240, Test: 0.7380
Epoch: 30, Loss: 4.1700, Train: 0.9643, Val: 0.7460, Test: 0.7600
Epoch: 31, Loss: 3.8586, Train: 0.9643, Val: 0.7740, Test: 0.7800
Epoch: 32, Loss: 4.1685, Train: 0.9643, Val: 0.7780, Test: 0.7930
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 33, Loss: 3.7921, Train: 0.9643, Val: 0.7900, Test: 0.8010
Epoch: 34, Loss: 4.0159, Train: 0.9714, Val: 0.7960, Test: 0.8100
Epoch: 35, Loss: 3.9445, Train: 0.9714, Val: 0.8040, Test: 0.8060
Epoch: 36, Loss: 3.9080, Train: 0.9786, Val: 0.8020, Test: 0.8070
Epoch: 37, Loss: 3.9235, Train: 0.9786, Val: 0.8060, Test: 0.8140
Epoch: 38, Loss: 3.7898, Train: 0.9857, Val: 0.8120, Test: 0.8220
Epoch: 39, Loss: 3.8454, Train: 0.9857, Val: 0.8100, Test: 0.8190
Epoch: 40, Loss: 3.8868, Train: 0.9786, Val: 0.8100, Test: 0.8170
Epoch: 41, Loss: 3.7504, Train: 0.9786, Val: 0.8080, Test: 0.8120
Epoch: 42, Loss: 3.8124, Train: 0.9857, Val: 0.8080, Test: 0.8120
Epoch: 43, Loss: 4.0846, Train: 0.9857, Val: 0.8080, Test: 0.8080
Epoch: 44, Loss: 3.9962, Train: 0.9857, Val: 0.8080, Test: 0.8090
Epoch: 45, Loss: 3.6883, Train: 0.9857, Val: 0.8000, Test: 0.8010
Epoch: 46, Loss: 3.6863, Train: 0.9857, Val: 0.7960, Test: 0.7980
Epoch: 47, Loss: 3.8659, Train: 0.9857, Val: 0.7880, Test: 0.7990
Epoch: 48, Loss: 4.0488, Train: 0.9857, Val: 0.7840, Test: 0.7920
Epoch: 49, Loss: 4.0563, Train: 0.9857, Val: 0.7820, Test: 0.7920
Epoch: 50, Loss: 3.6585, Train: 0.9857, Val: 0.7800, Test: 0.7850
MAD:  0.2935
Best Test Accuracy: 0.8220, Val Accuracy: 0.8120, Train Accuracy: 0.9857
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8496, Train: 0.1429, Val: 0.1760, Test: 0.1960
Epoch: 2, Loss: 4.8345, Train: 0.2429, Val: 0.2980, Test: 0.2980
Epoch: 3, Loss: 4.8231, Train: 0.2857, Val: 0.3300, Test: 0.3430
Epoch: 4, Loss: 4.8021, Train: 0.2929, Val: 0.3420, Test: 0.3560
Epoch: 5, Loss: 4.7416, Train: 0.3071, Val: 0.3440, Test: 0.3620
Epoch: 6, Loss: 4.7363, Train: 0.3286, Val: 0.3580, Test: 0.3810
Epoch: 7, Loss: 4.6992, Train: 0.3714, Val: 0.3760, Test: 0.3970
Epoch: 8, Loss: 4.5829, Train: 0.3929, Val: 0.3720, Test: 0.4110
Epoch: 9, Loss: 4.5142, Train: 0.3929, Val: 0.3640, Test: 0.4170
Epoch: 10, Loss: 4.6124, Train: 0.4143, Val: 0.3520, Test: 0.4100
Epoch: 11, Loss: 4.5211, Train: 0.4357, Val: 0.3440, Test: 0.4030
Epoch: 12, Loss: 4.5428, Train: 0.4500, Val: 0.3400, Test: 0.4060
Epoch: 13, Loss: 4.3853, Train: 0.4786, Val: 0.3520, Test: 0.4120
Epoch: 14, Loss: 4.3878, Train: 0.4857, Val: 0.3600, Test: 0.4160
Epoch: 15, Loss: 4.2608, Train: 0.5500, Val: 0.3920, Test: 0.4350
Epoch: 16, Loss: 4.2321, Train: 0.5786, Val: 0.4160, Test: 0.4580
Epoch: 17, Loss: 4.2985, Train: 0.6429, Val: 0.4360, Test: 0.4970
Epoch: 18, Loss: 4.4381, Train: 0.6786, Val: 0.4800, Test: 0.5350
Epoch: 19, Loss: 4.3322, Train: 0.7500, Val: 0.5320, Test: 0.5890
Epoch: 20, Loss: 4.2077, Train: 0.8357, Val: 0.5840, Test: 0.6530
Epoch: 21, Loss: 4.0914, Train: 0.9143, Val: 0.6240, Test: 0.6990
Epoch: 22, Loss: 4.0411, Train: 0.9429, Val: 0.6680, Test: 0.7310
Epoch: 23, Loss: 3.9385, Train: 0.9643, Val: 0.7000, Test: 0.7600
Epoch: 24, Loss: 4.2239, Train: 0.9643, Val: 0.7240, Test: 0.7590
Epoch: 25, Loss: 4.2530, Train: 0.9714, Val: 0.7480, Test: 0.7670
Epoch: 26, Loss: 4.0563, Train: 0.9714, Val: 0.7560, Test: 0.7730
Epoch: 27, Loss: 3.9023, Train: 0.9786, Val: 0.7500, Test: 0.7700
Epoch: 28, Loss: 4.1721, Train: 0.9714, Val: 0.7480, Test: 0.7690
Epoch: 29, Loss: 4.1811, Train: 0.9786, Val: 0.7520, Test: 0.7680
Epoch: 30, Loss: 3.9103, Train: 0.9714, Val: 0.7420, Test: 0.7780
Epoch: 31, Loss: 4.0158, Train: 0.9786, Val: 0.7420, Test: 0.7810
Epoch: 32, Loss: 3.9408, Train: 0.9714, Val: 0.7560, Test: 0.7810
Epoch: 33, Loss: 3.9767, Train: 0.9714, Val: 0.7580, Test: 0.7880
Epoch: 34, Loss: 3.8580, Train: 0.9714, Val: 0.7560, Test: 0.7940
Epoch: 35, Loss: 4.0423, Train: 0.9714, Val: 0.7600, Test: 0.7960
Epoch: 36, Loss: 3.6909, Train: 0.9714, Val: 0.7640, Test: 0.8020
Epoch: 37, Loss: 3.7415, Train: 0.9714, Val: 0.7600, Test: 0.8030
Epoch: 38, Loss: 3.8663, Train: 0.9714, Val: 0.7600, Test: 0.8060
Epoch: 39, Loss: 4.1142, Train: 0.9786, Val: 0.7620, Test: 0.8070
Epoch: 40, Loss: 3.8897, Train: 0.9786, Val: 0.7680, Test: 0.8080
Epoch: 41, Loss: 3.7611, Train: 0.9786, Val: 0.7700, Test: 0.8090
Epoch: 42, Loss: 3.8693, Train: 0.9786, Val: 0.7780, Test: 0.8070
Epoch: 43, Loss: 3.7643, Train: 0.9786, Val: 0.7820, Test: 0.8060
Epoch: 44, Loss: 3.8162, Train: 0.9786, Val: 0.7780, Test: 0.8020
Epoch: 45, Loss: 3.8225, Train: 0.9786, Val: 0.7820, Test: 0.7990
Epoch: 46, Loss: 3.7735, Train: 0.9786, Val: 0.7820, Test: 0.7980
Epoch: 47, Loss: 3.8324, Train: 0.9857, Val: 0.7860, Test: 0.7970
Epoch: 48, Loss: 3.9814, Train: 0.9857, Val: 0.7820, Test: 0.7980
Epoch: 49, Loss: 3.5746, Train: 0.9857, Val: 0.7800, Test: 0.8030
Epoch: 50, Loss: 3.7454, Train: 0.9857, Val: 0.7740, Test: 0.8070
MAD:  0.2697
Best Test Accuracy: 0.8090, Val Accuracy: 0.7700, Train Accuracy: 0.9786
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8623, Train: 0.1000, Val: 0.0460, Test: 0.0560
Epoch: 2, Loss: 4.8438, Train: 0.2357, Val: 0.1980, Test: 0.2110
Epoch: 3, Loss: 4.8282, Train: 0.2000, Val: 0.1880, Test: 0.1940
Epoch: 4, Loss: 4.8042, Train: 0.2000, Val: 0.1820, Test: 0.1750
Epoch: 5, Loss: 4.7802, Train: 0.2000, Val: 0.1720, Test: 0.1740
Epoch: 6, Loss: 4.7546, Train: 0.2071, Val: 0.1640, Test: 0.1730
Epoch: 7, Loss: 4.7239, Train: 0.2000, Val: 0.1540, Test: 0.1590
Epoch: 8, Loss: 4.6702, Train: 0.2000, Val: 0.1500, Test: 0.1590
Epoch: 9, Loss: 4.6428, Train: 0.2071, Val: 0.1500, Test: 0.1610
Epoch: 10, Loss: 4.6378, Train: 0.2286, Val: 0.1620, Test: 0.1730
Epoch: 11, Loss: 4.5902, Train: 0.2643, Val: 0.1940, Test: 0.1970
Epoch: 12, Loss: 4.6239, Train: 0.3000, Val: 0.2300, Test: 0.2330
Epoch: 13, Loss: 4.4614, Train: 0.3500, Val: 0.2740, Test: 0.2860
Epoch: 14, Loss: 4.6369, Train: 0.3643, Val: 0.3160, Test: 0.3450
Epoch: 15, Loss: 4.4532, Train: 0.3786, Val: 0.3620, Test: 0.3920
Epoch: 16, Loss: 4.5910, Train: 0.4500, Val: 0.4200, Test: 0.4450
Epoch: 17, Loss: 4.4798, Train: 0.4786, Val: 0.4500, Test: 0.4960
Epoch: 18, Loss: 4.3490, Train: 0.5214, Val: 0.4760, Test: 0.5110
Epoch: 19, Loss: 4.3808, Train: 0.6071, Val: 0.5080, Test: 0.5460
Epoch: 20, Loss: 4.2857, Train: 0.7214, Val: 0.5740, Test: 0.5960
Epoch: 21, Loss: 4.3666, Train: 0.8000, Val: 0.6360, Test: 0.6550
Epoch: 22, Loss: 4.3352, Train: 0.8500, Val: 0.7060, Test: 0.7220
Epoch: 23, Loss: 4.2267, Train: 0.9143, Val: 0.7600, Test: 0.7750
Epoch: 24, Loss: 4.0244, Train: 0.9143, Val: 0.7580, Test: 0.7950
Epoch: 25, Loss: 4.2222, Train: 0.9429, Val: 0.7360, Test: 0.7760
Epoch: 26, Loss: 3.9961, Train: 0.9357, Val: 0.7080, Test: 0.7320
Epoch: 27, Loss: 4.3185, Train: 0.9429, Val: 0.7040, Test: 0.7250
Epoch: 28, Loss: 4.0314, Train: 0.9500, Val: 0.7100, Test: 0.7430
Epoch: 29, Loss: 4.1203, Train: 0.9429, Val: 0.7220, Test: 0.7510
Epoch: 30, Loss: 4.0751, Train: 0.9500, Val: 0.7380, Test: 0.7760
Epoch: 31, Loss: 3.9825, Train: 0.9571, Val: 0.7360, Test: 0.7890
Epoch: 32, Loss: 3.7937, Train: 0.9643, Val: 0.7560, Test: 0.8060
Epoch: 33, Loss: 3.8901, Train: 0.9643, Val: 0.7620, Test: 0.8070
Epoch: 34, Loss: 3.8916, Train: 0.9643, Val: 0.7620, Test: 0.8050
Epoch: 35, Loss: 3.8720, Train: 0.9714, Val: 0.7620, Test: 0.7990
Epoch: 36, Loss: 4.0152, Train: 0.9714, Val: 0.7640, Test: 0.7990
Epoch: 37, Loss: 4.1431, Train: 0.9857, Val: 0.7700, Test: 0.8010
Epoch: 38, Loss: 4.0654, Train: 0.9857, Val: 0.7720, Test: 0.8030
Epoch: 39, Loss: 3.9025, Train: 0.9857, Val: 0.7820, Test: 0.8070
Epoch: 40, Loss: 3.8899, Train: 0.9857, Val: 0.7820, Test: 0.8160
Epoch: 41, Loss: 4.1136, Train: 0.9857, Val: 0.7880, Test: 0.8160
Epoch: 42, Loss: 3.8928, Train: 0.9857, Val: 0.7920, Test: 0.8140
Epoch: 43, Loss: 3.8165, Train: 0.9786, Val: 0.7900, Test: 0.8160
Epoch: 44, Loss: 4.0139, Train: 0.9857, Val: 0.7960, Test: 0.8230
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 45, Loss: 3.6982, Train: 0.9786, Val: 0.8000, Test: 0.8240
Epoch: 46, Loss: 3.8422, Train: 0.9786, Val: 0.7960, Test: 0.8240
Epoch: 47, Loss: 3.7868, Train: 0.9786, Val: 0.7920, Test: 0.8230
Epoch: 48, Loss: 3.9932, Train: 0.9857, Val: 0.7800, Test: 0.8180
Epoch: 49, Loss: 3.9126, Train: 0.9857, Val: 0.7780, Test: 0.8080
Epoch: 50, Loss: 3.8490, Train: 0.9857, Val: 0.7760, Test: 0.8070
MAD:  0.4214
Best Test Accuracy: 0.8240, Val Accuracy: 0.8000, Train Accuracy: 0.9786
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8543, Train: 0.1286, Val: 0.0740, Test: 0.0790
Epoch: 2, Loss: 4.8354, Train: 0.2214, Val: 0.0940, Test: 0.1130
Epoch: 3, Loss: 4.8299, Train: 0.2929, Val: 0.1520, Test: 0.1670
Epoch: 4, Loss: 4.8127, Train: 0.4000, Val: 0.2360, Test: 0.2590
Epoch: 5, Loss: 4.7644, Train: 0.4643, Val: 0.2880, Test: 0.3030
Epoch: 6, Loss: 4.6991, Train: 0.4786, Val: 0.3060, Test: 0.3280
Epoch: 7, Loss: 4.6975, Train: 0.4786, Val: 0.3100, Test: 0.3310
Epoch: 8, Loss: 4.5898, Train: 0.4857, Val: 0.3060, Test: 0.3280
Epoch: 9, Loss: 4.5672, Train: 0.4857, Val: 0.3060, Test: 0.3230
Epoch: 10, Loss: 4.5472, Train: 0.4857, Val: 0.3060, Test: 0.3260
Epoch: 11, Loss: 4.4033, Train: 0.4929, Val: 0.3040, Test: 0.3280
Epoch: 12, Loss: 4.4307, Train: 0.4929, Val: 0.3060, Test: 0.3260
Epoch: 13, Loss: 4.4689, Train: 0.5071, Val: 0.3020, Test: 0.3280
Epoch: 14, Loss: 4.1641, Train: 0.5214, Val: 0.3120, Test: 0.3340
Epoch: 15, Loss: 4.3097, Train: 0.5571, Val: 0.3340, Test: 0.3480
Epoch: 16, Loss: 4.2619, Train: 0.6500, Val: 0.3700, Test: 0.3690
Epoch: 17, Loss: 4.3610, Train: 0.7286, Val: 0.4280, Test: 0.4290
Epoch: 18, Loss: 4.3449, Train: 0.8214, Val: 0.5520, Test: 0.5690
Epoch: 19, Loss: 4.1619, Train: 0.8929, Val: 0.6840, Test: 0.6900
Epoch: 20, Loss: 4.1948, Train: 0.9429, Val: 0.7480, Test: 0.7520
Epoch: 21, Loss: 4.3126, Train: 0.9643, Val: 0.7800, Test: 0.7990
Epoch: 22, Loss: 4.0819, Train: 0.9714, Val: 0.7740, Test: 0.8100
Epoch: 23, Loss: 3.8851, Train: 0.9714, Val: 0.7680, Test: 0.8180
Epoch: 24, Loss: 4.1569, Train: 0.9643, Val: 0.7660, Test: 0.8110
Epoch: 25, Loss: 4.2465, Train: 0.9714, Val: 0.7720, Test: 0.7990
Epoch: 26, Loss: 3.9819, Train: 0.9714, Val: 0.7680, Test: 0.7950
Epoch: 27, Loss: 3.9981, Train: 0.9714, Val: 0.7640, Test: 0.7910
Epoch: 28, Loss: 3.7551, Train: 0.9714, Val: 0.7580, Test: 0.7870
Epoch: 29, Loss: 4.2121, Train: 0.9714, Val: 0.7680, Test: 0.7860
Epoch: 30, Loss: 4.0060, Train: 0.9714, Val: 0.7620, Test: 0.7820
Epoch: 31, Loss: 4.0348, Train: 0.9714, Val: 0.7740, Test: 0.7810
Epoch: 32, Loss: 4.1578, Train: 0.9714, Val: 0.7720, Test: 0.7830
Epoch: 33, Loss: 3.8978, Train: 0.9714, Val: 0.7740, Test: 0.7860
Epoch: 34, Loss: 4.1186, Train: 0.9786, Val: 0.7740, Test: 0.7840
Epoch: 35, Loss: 4.0281, Train: 0.9786, Val: 0.7760, Test: 0.7850
Epoch: 36, Loss: 3.8020, Train: 0.9857, Val: 0.7780, Test: 0.7910
Epoch: 37, Loss: 3.7506, Train: 0.9857, Val: 0.7760, Test: 0.7950
Epoch: 38, Loss: 4.0344, Train: 0.9857, Val: 0.7800, Test: 0.7940
Epoch: 39, Loss: 3.9006, Train: 0.9786, Val: 0.7840, Test: 0.7950
Epoch: 40, Loss: 3.4961, Train: 0.9857, Val: 0.7820, Test: 0.7950
Epoch: 41, Loss: 4.2120, Train: 0.9857, Val: 0.7760, Test: 0.7990
Epoch: 42, Loss: 4.1359, Train: 0.9857, Val: 0.7780, Test: 0.8010
Epoch: 43, Loss: 4.2324, Train: 0.9929, Val: 0.7780, Test: 0.7950
Epoch: 44, Loss: 3.6898, Train: 0.9929, Val: 0.7760, Test: 0.7970
Epoch: 45, Loss: 3.6812, Train: 0.9929, Val: 0.7780, Test: 0.7980
Epoch: 46, Loss: 3.9579, Train: 0.9929, Val: 0.7800, Test: 0.8060
Epoch: 47, Loss: 3.8857, Train: 1.0000, Val: 0.7780, Test: 0.8150
Epoch: 48, Loss: 3.6256, Train: 0.9929, Val: 0.7800, Test: 0.8150
Epoch: 49, Loss: 3.5419, Train: 0.9929, Val: 0.7760, Test: 0.8170
Epoch: 50, Loss: 3.7794, Train: 0.9929, Val: 0.7720, Test: 0.8170
MAD:  0.0467
Best Test Accuracy: 0.8180, Val Accuracy: 0.7680, Train Accuracy: 0.9714
Training completed.
Average Test Accuracy:  0.8148 ± 0.012663332894621352
Average MAD:  0.27489 ± 0.13954933500378996

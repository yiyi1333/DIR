/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8530, Train: 0.1857, Val: 0.1360, Test: 0.1460
Epoch: 2, Loss: 4.8243, Train: 0.2000, Val: 0.1600, Test: 0.1630
Epoch: 3, Loss: 4.8104, Train: 0.1929, Val: 0.1520, Test: 0.1600
Epoch: 4, Loss: 4.7953, Train: 0.2000, Val: 0.1540, Test: 0.1570
Epoch: 5, Loss: 4.7441, Train: 0.2071, Val: 0.1660, Test: 0.1670
Epoch: 6, Loss: 4.7247, Train: 0.2286, Val: 0.1820, Test: 0.1830
Epoch: 7, Loss: 4.6602, Train: 0.2286, Val: 0.1900, Test: 0.1910
Epoch: 8, Loss: 4.6235, Train: 0.2357, Val: 0.1860, Test: 0.1890
Epoch: 9, Loss: 4.5659, Train: 0.2357, Val: 0.1840, Test: 0.1880
Epoch: 10, Loss: 4.4735, Train: 0.2357, Val: 0.1840, Test: 0.1900
Epoch: 11, Loss: 4.4004, Train: 0.2429, Val: 0.1880, Test: 0.1930
Epoch: 12, Loss: 4.3604, Train: 0.2429, Val: 0.1840, Test: 0.1880
Epoch: 13, Loss: 4.2086, Train: 0.2429, Val: 0.1860, Test: 0.1890
Epoch: 14, Loss: 4.1499, Train: 0.2429, Val: 0.1940, Test: 0.1960
Epoch: 15, Loss: 4.2021, Train: 0.2571, Val: 0.1960, Test: 0.2000
Epoch: 16, Loss: 3.9535, Train: 0.2857, Val: 0.2100, Test: 0.2130
Epoch: 17, Loss: 3.7772, Train: 0.3214, Val: 0.2380, Test: 0.2340
Epoch: 18, Loss: 3.7564, Train: 0.4429, Val: 0.3020, Test: 0.3240
Epoch: 19, Loss: 3.7421, Train: 0.6357, Val: 0.4900, Test: 0.5330
Epoch: 20, Loss: 3.7967, Train: 0.7429, Val: 0.6400, Test: 0.6670
Epoch: 21, Loss: 3.7954, Train: 0.7929, Val: 0.6660, Test: 0.6810
Epoch: 22, Loss: 3.8167, Train: 0.7500, Val: 0.5940, Test: 0.5780
Epoch: 23, Loss: 3.5472, Train: 0.7214, Val: 0.5260, Test: 0.5030
Epoch: 24, Loss: 3.6563, Train: 0.7143, Val: 0.5280, Test: 0.5120
Epoch: 25, Loss: 3.7209, Train: 0.7286, Val: 0.5800, Test: 0.5470
Epoch: 26, Loss: 3.6360, Train: 0.7643, Val: 0.6080, Test: 0.6190
Epoch: 27, Loss: 3.4954, Train: 0.7786, Val: 0.6660, Test: 0.6630
Epoch: 28, Loss: 3.7423, Train: 0.7857, Val: 0.6880, Test: 0.6930
Epoch: 29, Loss: 3.5330, Train: 0.7857, Val: 0.6720, Test: 0.7000
Epoch: 30, Loss: 3.2021, Train: 0.8071, Val: 0.6660, Test: 0.7000
Epoch: 31, Loss: 3.3685, Train: 0.8429, Val: 0.6860, Test: 0.7230
Epoch: 32, Loss: 3.4970, Train: 0.8786, Val: 0.7040, Test: 0.7380
Epoch: 33, Loss: 3.4426, Train: 0.8786, Val: 0.7120, Test: 0.7390
Epoch: 34, Loss: 3.4384, Train: 0.8786, Val: 0.7220, Test: 0.7490
Epoch: 35, Loss: 3.3417, Train: 0.8857, Val: 0.7280, Test: 0.7580
Epoch: 36, Loss: 3.6066, Train: 0.9000, Val: 0.7440, Test: 0.7710
Epoch: 37, Loss: 3.2746, Train: 0.9143, Val: 0.7620, Test: 0.7860
Epoch: 38, Loss: 3.1988, Train: 0.9214, Val: 0.7660, Test: 0.7920
Epoch: 39, Loss: 3.3028, Train: 0.9214, Val: 0.7740, Test: 0.7940
Epoch: 40, Loss: 3.5073, Train: 0.9214, Val: 0.7740, Test: 0.7980
Epoch: 41, Loss: 2.8189, Train: 0.9429, Val: 0.7720, Test: 0.8040
Epoch: 42, Loss: 2.9939, Train: 0.9500, Val: 0.7800, Test: 0.8140
Epoch: 43, Loss: 2.7956, Train: 0.9500, Val: 0.7780, Test: 0.8060
Epoch: 44, Loss: 3.1711, Train: 0.9571, Val: 0.7820, Test: 0.7950
Epoch: 45, Loss: 2.9635, Train: 0.9500, Val: 0.7820, Test: 0.7940
Epoch: 46, Loss: 3.1893, Train: 0.9429, Val: 0.7800, Test: 0.7890
Epoch: 47, Loss: 2.8851, Train: 0.9500, Val: 0.7720, Test: 0.7900
Epoch: 48, Loss: 3.0072, Train: 0.9500, Val: 0.7700, Test: 0.7920
Epoch: 49, Loss: 2.8851, Train: 0.9571, Val: 0.7700, Test: 0.7920
Epoch: 50, Loss: 3.0176, Train: 0.9571, Val: 0.7780, Test: 0.7960
MAD:  0.1073
Best Test Accuracy: 0.8140, Val Accuracy: 0.7800, Train Accuracy: 0.9500
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8541, Train: 0.0286, Val: 0.0380, Test: 0.0320
Epoch: 2, Loss: 4.8396, Train: 0.3429, Val: 0.2480, Test: 0.2420
Epoch: 3, Loss: 4.8282, Train: 0.3643, Val: 0.2660, Test: 0.2670
Epoch: 4, Loss: 4.8047, Train: 0.3786, Val: 0.2700, Test: 0.2760
Epoch: 5, Loss: 4.7726, Train: 0.4214, Val: 0.2800, Test: 0.2820
Epoch: 6, Loss: 4.7269, Train: 0.4071, Val: 0.2780, Test: 0.2720
Epoch: 7, Loss: 4.6933, Train: 0.3857, Val: 0.2740, Test: 0.2670
Epoch: 8, Loss: 4.6241, Train: 0.3857, Val: 0.2600, Test: 0.2540
Epoch: 9, Loss: 4.5631, Train: 0.3286, Val: 0.2360, Test: 0.2380
Epoch: 10, Loss: 4.4829, Train: 0.3143, Val: 0.2340, Test: 0.2350
Epoch: 11, Loss: 4.3508, Train: 0.3071, Val: 0.2320, Test: 0.2330
Epoch: 12, Loss: 4.3864, Train: 0.3000, Val: 0.2220, Test: 0.2180
Epoch: 13, Loss: 4.1672, Train: 0.2929, Val: 0.2200, Test: 0.2120
Epoch: 14, Loss: 4.0412, Train: 0.2929, Val: 0.2160, Test: 0.2070
Epoch: 15, Loss: 4.0317, Train: 0.2929, Val: 0.2160, Test: 0.2090
Epoch: 16, Loss: 3.8051, Train: 0.2929, Val: 0.2180, Test: 0.2140
Epoch: 17, Loss: 4.0088, Train: 0.2929, Val: 0.2260, Test: 0.2220
Epoch: 18, Loss: 3.8069, Train: 0.3714, Val: 0.2580, Test: 0.2690
Epoch: 19, Loss: 3.9391, Train: 0.4643, Val: 0.3160, Test: 0.3230
Epoch: 20, Loss: 3.6635, Train: 0.5143, Val: 0.3560, Test: 0.3510
Epoch: 21, Loss: 3.6044, Train: 0.7214, Val: 0.4460, Test: 0.4510
Epoch: 22, Loss: 4.0813, Train: 0.9000, Val: 0.6860, Test: 0.6880
Epoch: 23, Loss: 3.5998, Train: 0.8643, Val: 0.6800, Test: 0.6960
Epoch: 24, Loss: 3.6723, Train: 0.7857, Val: 0.5940, Test: 0.6050
Epoch: 25, Loss: 3.7403, Train: 0.7429, Val: 0.5460, Test: 0.5610
Epoch: 26, Loss: 3.9964, Train: 0.7143, Val: 0.5160, Test: 0.5330
Epoch: 27, Loss: 3.5410, Train: 0.7286, Val: 0.5200, Test: 0.5340
Epoch: 28, Loss: 3.5260, Train: 0.7500, Val: 0.5280, Test: 0.5470
Epoch: 29, Loss: 3.8353, Train: 0.7929, Val: 0.5820, Test: 0.5910
Epoch: 30, Loss: 3.4406, Train: 0.8714, Val: 0.6440, Test: 0.6530
Epoch: 31, Loss: 3.3483, Train: 0.9214, Val: 0.6860, Test: 0.7070
Epoch: 32, Loss: 3.5474, Train: 0.9429, Val: 0.7140, Test: 0.7300
Epoch: 33, Loss: 3.3070, Train: 0.9286, Val: 0.7400, Test: 0.7430
Epoch: 34, Loss: 3.2126, Train: 0.9286, Val: 0.7480, Test: 0.7460
Epoch: 35, Loss: 3.4044, Train: 0.9357, Val: 0.7480, Test: 0.7540
Epoch: 36, Loss: 3.0843, Train: 0.9286, Val: 0.7540, Test: 0.7520
Epoch: 37, Loss: 3.2995, Train: 0.9357, Val: 0.7540, Test: 0.7470
Epoch: 38, Loss: 3.1946, Train: 0.9429, Val: 0.7580, Test: 0.7460
Epoch: 39, Loss: 3.1477, Train: 0.9429, Val: 0.7540, Test: 0.7470
Epoch: 40, Loss: 2.9151, Train: 0.9429, Val: 0.7600, Test: 0.7520
Epoch: 41, Loss: 3.0705, Train: 0.9571, Val: 0.7740, Test: 0.7580
Epoch: 42, Loss: 3.2002, Train: 0.9571, Val: 0.7800, Test: 0.7690
Epoch: 43, Loss: 3.1021, Train: 0.9571, Val: 0.7840, Test: 0.7760
Epoch: 44, Loss: 3.2793, Train: 0.9571, Val: 0.7880, Test: 0.7840
Epoch: 45, Loss: 3.0555, Train: 0.9643, Val: 0.7900, Test: 0.7910
Epoch: 46, Loss: 3.3433, Train: 0.9643, Val: 0.7860, Test: 0.7930
Epoch: 47, Loss: 2.8198, Train: 0.9714, Val: 0.7860, Test: 0.7930
Epoch: 48, Loss: 3.0800, Train: 0.9714, Val: 0.7880, Test: 0.7950
Epoch: 49, Loss: 2.8903, Train: 0.9643, Val: 0.7880, Test: 0.7930
Epoch: 50, Loss: 2.9867, Train: 0.9643, Val: 0.7840, Test: 0.7890
MAD:  0.1917
Best Test Accuracy: 0.7950, Val Accuracy: 0.7880, Train Accuracy: 0.9714
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8558, Train: 0.1857, Val: 0.1560, Test: 0.1470
Epoch: 2, Loss: 4.8393, Train: 0.2500, Val: 0.1700, Test: 0.1670
Epoch: 3, Loss: 4.8175, Train: 0.2857, Val: 0.1820, Test: 0.1900
Epoch: 4, Loss: 4.7947, Train: 0.2857, Val: 0.1860, Test: 0.1980
Epoch: 5, Loss: 4.7905, Train: 0.3000, Val: 0.1960, Test: 0.2120
Epoch: 6, Loss: 4.7623, Train: 0.3286, Val: 0.2400, Test: 0.2400
Epoch: 7, Loss: 4.6529, Train: 0.3357, Val: 0.2520, Test: 0.2540
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 8, Loss: 4.6295, Train: 0.3357, Val: 0.2660, Test: 0.2670
Epoch: 9, Loss: 4.6373, Train: 0.3429, Val: 0.2960, Test: 0.2820
Epoch: 10, Loss: 4.5140, Train: 0.3643, Val: 0.3060, Test: 0.2910
Epoch: 11, Loss: 4.4255, Train: 0.3929, Val: 0.3380, Test: 0.3110
Epoch: 12, Loss: 4.3980, Train: 0.4071, Val: 0.3540, Test: 0.3300
Epoch: 13, Loss: 4.2476, Train: 0.4071, Val: 0.3660, Test: 0.3510
Epoch: 14, Loss: 4.1242, Train: 0.3929, Val: 0.3500, Test: 0.3490
Epoch: 15, Loss: 4.1473, Train: 0.3929, Val: 0.3680, Test: 0.3660
Epoch: 16, Loss: 3.9596, Train: 0.4000, Val: 0.3740, Test: 0.3760
Epoch: 17, Loss: 4.0796, Train: 0.4071, Val: 0.3660, Test: 0.3720
Epoch: 18, Loss: 3.8589, Train: 0.5000, Val: 0.4080, Test: 0.4160
Epoch: 19, Loss: 3.6358, Train: 0.5786, Val: 0.4780, Test: 0.5040
Epoch: 20, Loss: 3.8229, Train: 0.6571, Val: 0.5400, Test: 0.5590
Epoch: 21, Loss: 3.6059, Train: 0.7643, Val: 0.5740, Test: 0.6040
Epoch: 22, Loss: 3.7783, Train: 0.7429, Val: 0.5620, Test: 0.5820
Epoch: 23, Loss: 3.8991, Train: 0.7143, Val: 0.4860, Test: 0.5390
Epoch: 24, Loss: 3.5695, Train: 0.7714, Val: 0.4820, Test: 0.5210
Epoch: 25, Loss: 3.3994, Train: 0.8071, Val: 0.5320, Test: 0.5690
Epoch: 26, Loss: 3.8435, Train: 0.8357, Val: 0.5920, Test: 0.6290
Epoch: 27, Loss: 3.6652, Train: 0.8500, Val: 0.6320, Test: 0.6560
Epoch: 28, Loss: 3.6094, Train: 0.8929, Val: 0.7000, Test: 0.6880
Epoch: 29, Loss: 3.4863, Train: 0.8929, Val: 0.7040, Test: 0.7040
Epoch: 30, Loss: 3.5370, Train: 0.8714, Val: 0.7160, Test: 0.7100
Epoch: 31, Loss: 3.5855, Train: 0.8786, Val: 0.7320, Test: 0.7140
Epoch: 32, Loss: 3.2356, Train: 0.8857, Val: 0.7540, Test: 0.7400
Epoch: 33, Loss: 3.3015, Train: 0.9071, Val: 0.7640, Test: 0.7580
Epoch: 34, Loss: 3.6013, Train: 0.9071, Val: 0.7640, Test: 0.7700
Epoch: 35, Loss: 3.5699, Train: 0.9286, Val: 0.7700, Test: 0.7840
Epoch: 36, Loss: 3.4239, Train: 0.9429, Val: 0.7840, Test: 0.7980
Epoch: 37, Loss: 3.1704, Train: 0.9429, Val: 0.7920, Test: 0.7980
Epoch: 38, Loss: 3.1693, Train: 0.9643, Val: 0.7980, Test: 0.8020
Epoch: 39, Loss: 3.2069, Train: 0.9643, Val: 0.8000, Test: 0.8040
Epoch: 40, Loss: 3.1065, Train: 0.9571, Val: 0.7980, Test: 0.8080
Epoch: 41, Loss: 3.1714, Train: 0.9500, Val: 0.7940, Test: 0.8040
Epoch: 42, Loss: 3.2198, Train: 0.9571, Val: 0.7900, Test: 0.7970
Epoch: 43, Loss: 3.2453, Train: 0.9643, Val: 0.7860, Test: 0.8030
Epoch: 44, Loss: 2.7395, Train: 0.9571, Val: 0.7880, Test: 0.7980
Epoch: 45, Loss: 3.1662, Train: 0.9643, Val: 0.7840, Test: 0.7950
Epoch: 46, Loss: 2.9189, Train: 0.9714, Val: 0.7780, Test: 0.7940
Epoch: 47, Loss: 3.3887, Train: 0.9643, Val: 0.7780, Test: 0.7890
Epoch: 48, Loss: 2.9249, Train: 0.9643, Val: 0.7800, Test: 0.7870
Epoch: 49, Loss: 3.0989, Train: 0.9643, Val: 0.7820, Test: 0.7880
Epoch: 50, Loss: 2.8513, Train: 0.9714, Val: 0.7880, Test: 0.7920
MAD:  0.1386
Best Test Accuracy: 0.8080, Val Accuracy: 0.7980, Train Accuracy: 0.9571
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8524, Train: 0.2143, Val: 0.1500, Test: 0.1460
Epoch: 2, Loss: 4.8399, Train: 0.3786, Val: 0.2660, Test: 0.2490
Epoch: 3, Loss: 4.8268, Train: 0.4500, Val: 0.3120, Test: 0.3090
Epoch: 4, Loss: 4.8151, Train: 0.4714, Val: 0.3320, Test: 0.3280
Epoch: 5, Loss: 4.7657, Train: 0.4857, Val: 0.3360, Test: 0.3350
Epoch: 6, Loss: 4.7647, Train: 0.5071, Val: 0.3600, Test: 0.3540
Epoch: 7, Loss: 4.7347, Train: 0.5143, Val: 0.3760, Test: 0.3630
Epoch: 8, Loss: 4.7111, Train: 0.5286, Val: 0.3800, Test: 0.3820
Epoch: 9, Loss: 4.6496, Train: 0.5429, Val: 0.4040, Test: 0.4020
Epoch: 10, Loss: 4.5570, Train: 0.5643, Val: 0.4240, Test: 0.4060
Epoch: 11, Loss: 4.5664, Train: 0.5571, Val: 0.4260, Test: 0.4130
Epoch: 12, Loss: 4.2765, Train: 0.5643, Val: 0.4220, Test: 0.4170
Epoch: 13, Loss: 4.3525, Train: 0.5714, Val: 0.4180, Test: 0.4150
Epoch: 14, Loss: 4.3422, Train: 0.5714, Val: 0.4180, Test: 0.4030
Epoch: 15, Loss: 4.2017, Train: 0.5714, Val: 0.4220, Test: 0.4110
Epoch: 16, Loss: 3.8495, Train: 0.5786, Val: 0.4340, Test: 0.4240
Epoch: 17, Loss: 4.0370, Train: 0.5929, Val: 0.4440, Test: 0.4360
Epoch: 18, Loss: 3.9637, Train: 0.6071, Val: 0.4520, Test: 0.4560
Epoch: 19, Loss: 4.0180, Train: 0.6286, Val: 0.4680, Test: 0.4750
Epoch: 20, Loss: 3.9581, Train: 0.6429, Val: 0.4840, Test: 0.4850
Epoch: 21, Loss: 3.7660, Train: 0.6714, Val: 0.4700, Test: 0.4750
Epoch: 22, Loss: 4.0108, Train: 0.6357, Val: 0.4340, Test: 0.4310
Epoch: 23, Loss: 3.7217, Train: 0.6214, Val: 0.4140, Test: 0.4220
Epoch: 24, Loss: 3.9133, Train: 0.6571, Val: 0.4120, Test: 0.4190
Epoch: 25, Loss: 3.9324, Train: 0.7357, Val: 0.4600, Test: 0.4650
Epoch: 26, Loss: 3.6320, Train: 0.7929, Val: 0.5300, Test: 0.5140
Epoch: 27, Loss: 3.7688, Train: 0.8071, Val: 0.5480, Test: 0.5380
Epoch: 28, Loss: 3.8717, Train: 0.8071, Val: 0.5540, Test: 0.5450
Epoch: 29, Loss: 3.6155, Train: 0.8071, Val: 0.5560, Test: 0.5550
Epoch: 30, Loss: 3.5706, Train: 0.8143, Val: 0.5600, Test: 0.5700
Epoch: 31, Loss: 3.7326, Train: 0.8143, Val: 0.5640, Test: 0.5720
Epoch: 32, Loss: 3.4354, Train: 0.8143, Val: 0.5600, Test: 0.5740
Epoch: 33, Loss: 3.3016, Train: 0.8143, Val: 0.5560, Test: 0.5730
Epoch: 34, Loss: 3.5665, Train: 0.8000, Val: 0.5620, Test: 0.5630
Epoch: 35, Loss: 3.5157, Train: 0.8000, Val: 0.5620, Test: 0.5630
Epoch: 36, Loss: 3.4232, Train: 0.8000, Val: 0.5640, Test: 0.5670
Epoch: 37, Loss: 3.3383, Train: 0.8071, Val: 0.5660, Test: 0.5720
Epoch: 38, Loss: 3.2376, Train: 0.8143, Val: 0.5660, Test: 0.5730
Epoch: 39, Loss: 3.2553, Train: 0.8143, Val: 0.5760, Test: 0.5800
Epoch: 40, Loss: 3.2552, Train: 0.8143, Val: 0.5800, Test: 0.5820
Epoch: 41, Loss: 2.9205, Train: 0.8214, Val: 0.5780, Test: 0.5840
Epoch: 42, Loss: 3.4802, Train: 0.8286, Val: 0.5840, Test: 0.5870
Epoch: 43, Loss: 3.0476, Train: 0.8429, Val: 0.5860, Test: 0.5910
Epoch: 44, Loss: 3.1953, Train: 0.8500, Val: 0.5840, Test: 0.5940
Epoch: 45, Loss: 3.2986, Train: 0.8571, Val: 0.5920, Test: 0.6050
Epoch: 46, Loss: 2.9780, Train: 0.8786, Val: 0.6080, Test: 0.6170
Epoch: 47, Loss: 3.0818, Train: 0.9000, Val: 0.6440, Test: 0.6420
Epoch: 48, Loss: 2.9287, Train: 0.8929, Val: 0.6800, Test: 0.6770
Epoch: 49, Loss: 2.7980, Train: 0.9071, Val: 0.7260, Test: 0.7120
Epoch: 50, Loss: 2.9169, Train: 0.9286, Val: 0.7480, Test: 0.7390
MAD:  0.2378
Best Test Accuracy: 0.7390, Val Accuracy: 0.7480, Train Accuracy: 0.9286
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8548, Train: 0.1429, Val: 0.0900, Test: 0.0890
Epoch: 2, Loss: 4.8461, Train: 0.3286, Val: 0.2120, Test: 0.2360
Epoch: 3, Loss: 4.8278, Train: 0.3786, Val: 0.2820, Test: 0.2890
Epoch: 4, Loss: 4.8055, Train: 0.4143, Val: 0.3240, Test: 0.3090
Epoch: 5, Loss: 4.7940, Train: 0.4214, Val: 0.3380, Test: 0.3270
Epoch: 6, Loss: 4.7699, Train: 0.4214, Val: 0.3520, Test: 0.3360
Epoch: 7, Loss: 4.7310, Train: 0.4214, Val: 0.3560, Test: 0.3400
Epoch: 8, Loss: 4.7281, Train: 0.4214, Val: 0.3520, Test: 0.3390
Epoch: 9, Loss: 4.6286, Train: 0.4143, Val: 0.3480, Test: 0.3360
Epoch: 10, Loss: 4.5916, Train: 0.4143, Val: 0.3500, Test: 0.3360
Epoch: 11, Loss: 4.6392, Train: 0.4214, Val: 0.3460, Test: 0.3330
Epoch: 12, Loss: 4.5675, Train: 0.4214, Val: 0.3460, Test: 0.3310
Epoch: 13, Loss: 4.4094, Train: 0.4214, Val: 0.3460, Test: 0.3320
Epoch: 14, Loss: 4.3923, Train: 0.4214, Val: 0.3460, Test: 0.3340
Epoch: 15, Loss: 4.3013, Train: 0.4214, Val: 0.3480, Test: 0.3360
Epoch: 16, Loss: 4.3728, Train: 0.4286, Val: 0.3600, Test: 0.3440
Epoch: 17, Loss: 4.3120, Train: 0.4286, Val: 0.3540, Test: 0.3430
Epoch: 18, Loss: 4.1472, Train: 0.4286, Val: 0.3520, Test: 0.3400
Epoch: 19, Loss: 4.2510, Train: 0.4286, Val: 0.3480, Test: 0.3390
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 20, Loss: 4.1172, Train: 0.4214, Val: 0.3420, Test: 0.3320
Epoch: 21, Loss: 4.2141, Train: 0.4500, Val: 0.3560, Test: 0.3430
Epoch: 22, Loss: 4.0695, Train: 0.5571, Val: 0.4020, Test: 0.4000
Epoch: 23, Loss: 3.9628, Train: 0.5714, Val: 0.4080, Test: 0.4100
Epoch: 24, Loss: 4.1010, Train: 0.5714, Val: 0.3960, Test: 0.4070
Epoch: 25, Loss: 4.0020, Train: 0.5714, Val: 0.3880, Test: 0.3990
Epoch: 26, Loss: 4.1947, Train: 0.5643, Val: 0.3780, Test: 0.3950
Epoch: 27, Loss: 4.1122, Train: 0.5643, Val: 0.3780, Test: 0.3940
Epoch: 28, Loss: 3.8020, Train: 0.5643, Val: 0.3920, Test: 0.3970
Epoch: 29, Loss: 3.8874, Train: 0.5643, Val: 0.4000, Test: 0.4020
Epoch: 30, Loss: 3.9853, Train: 0.5714, Val: 0.4100, Test: 0.4070
Epoch: 31, Loss: 3.9781, Train: 0.5714, Val: 0.4140, Test: 0.4130
Epoch: 32, Loss: 3.8431, Train: 0.5643, Val: 0.4220, Test: 0.4190
Epoch: 33, Loss: 3.8787, Train: 0.5643, Val: 0.4240, Test: 0.4230
Epoch: 34, Loss: 3.9736, Train: 0.5643, Val: 0.4240, Test: 0.4250
Epoch: 35, Loss: 4.0155, Train: 0.5643, Val: 0.4240, Test: 0.4270
Epoch: 36, Loss: 3.9706, Train: 0.5643, Val: 0.4220, Test: 0.4300
Epoch: 37, Loss: 3.6580, Train: 0.5643, Val: 0.4240, Test: 0.4320
Epoch: 38, Loss: 3.5504, Train: 0.5643, Val: 0.4200, Test: 0.4290
Epoch: 39, Loss: 3.8427, Train: 0.5643, Val: 0.4140, Test: 0.4300
Epoch: 40, Loss: 3.8501, Train: 0.5643, Val: 0.4140, Test: 0.4250
Epoch: 41, Loss: 3.5936, Train: 0.5643, Val: 0.4140, Test: 0.4240
Epoch: 42, Loss: 3.2058, Train: 0.5643, Val: 0.4140, Test: 0.4250
Epoch: 43, Loss: 3.8753, Train: 0.5643, Val: 0.4160, Test: 0.4250
Epoch: 44, Loss: 3.3306, Train: 0.5714, Val: 0.4180, Test: 0.4260
Epoch: 45, Loss: 3.4670, Train: 0.5714, Val: 0.4180, Test: 0.4270
Epoch: 46, Loss: 3.5540, Train: 0.5714, Val: 0.4180, Test: 0.4280
Epoch: 47, Loss: 3.6944, Train: 0.5786, Val: 0.4240, Test: 0.4280
Epoch: 48, Loss: 3.4996, Train: 0.5857, Val: 0.4280, Test: 0.4370
Epoch: 49, Loss: 3.3647, Train: 0.6071, Val: 0.4380, Test: 0.4430
Epoch: 50, Loss: 3.4306, Train: 0.6214, Val: 0.4620, Test: 0.4590
MAD:  0.2223
Best Test Accuracy: 0.4590, Val Accuracy: 0.4620, Train Accuracy: 0.6214
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8548, Train: 0.2214, Val: 0.2620, Test: 0.2620
Epoch: 2, Loss: 4.8276, Train: 0.3071, Val: 0.3020, Test: 0.3290
Epoch: 3, Loss: 4.8140, Train: 0.3429, Val: 0.2800, Test: 0.3100
Epoch: 4, Loss: 4.7964, Train: 0.3357, Val: 0.2740, Test: 0.2930
Epoch: 5, Loss: 4.7532, Train: 0.3286, Val: 0.2360, Test: 0.2890
Epoch: 6, Loss: 4.7547, Train: 0.3286, Val: 0.2520, Test: 0.2890
Epoch: 7, Loss: 4.6693, Train: 0.3214, Val: 0.2500, Test: 0.2790
Epoch: 8, Loss: 4.6239, Train: 0.3357, Val: 0.2560, Test: 0.2880
Epoch: 9, Loss: 4.5932, Train: 0.3429, Val: 0.2620, Test: 0.3020
Epoch: 10, Loss: 4.5181, Train: 0.3571, Val: 0.2760, Test: 0.3310
Epoch: 11, Loss: 4.4592, Train: 0.3571, Val: 0.2840, Test: 0.3370
Epoch: 12, Loss: 4.3546, Train: 0.3786, Val: 0.2900, Test: 0.3370
Epoch: 13, Loss: 4.1932, Train: 0.3714, Val: 0.2960, Test: 0.3400
Epoch: 14, Loss: 4.1442, Train: 0.3643, Val: 0.3000, Test: 0.3470
Epoch: 15, Loss: 4.1843, Train: 0.3714, Val: 0.3100, Test: 0.3590
Epoch: 16, Loss: 3.9411, Train: 0.3857, Val: 0.3200, Test: 0.3690
Epoch: 17, Loss: 3.7995, Train: 0.4143, Val: 0.3280, Test: 0.3780
Epoch: 18, Loss: 4.1564, Train: 0.4429, Val: 0.3200, Test: 0.3680
Epoch: 19, Loss: 4.1825, Train: 0.5643, Val: 0.3640, Test: 0.4240
Epoch: 20, Loss: 3.8499, Train: 0.7214, Val: 0.5580, Test: 0.5920
Epoch: 21, Loss: 3.7921, Train: 0.7929, Val: 0.6440, Test: 0.6740
Epoch: 22, Loss: 3.8964, Train: 0.7643, Val: 0.6100, Test: 0.6250
Epoch: 23, Loss: 3.8472, Train: 0.7143, Val: 0.5660, Test: 0.5670
Epoch: 24, Loss: 3.9662, Train: 0.6500, Val: 0.5440, Test: 0.5320
Epoch: 25, Loss: 3.6252, Train: 0.7143, Val: 0.5860, Test: 0.5860
Epoch: 26, Loss: 3.9692, Train: 0.7571, Val: 0.6300, Test: 0.6340
Epoch: 27, Loss: 3.7654, Train: 0.7929, Val: 0.6480, Test: 0.6820
Epoch: 28, Loss: 3.7364, Train: 0.7929, Val: 0.6880, Test: 0.7250
Epoch: 29, Loss: 3.7476, Train: 0.8000, Val: 0.7000, Test: 0.7540
Epoch: 30, Loss: 3.3804, Train: 0.7929, Val: 0.7140, Test: 0.7580
Epoch: 31, Loss: 3.5933, Train: 0.7929, Val: 0.7100, Test: 0.7500
Epoch: 32, Loss: 3.2931, Train: 0.7786, Val: 0.7040, Test: 0.7400
Epoch: 33, Loss: 3.4278, Train: 0.7643, Val: 0.6980, Test: 0.7290
Epoch: 34, Loss: 3.5681, Train: 0.7714, Val: 0.7000, Test: 0.7250
Epoch: 35, Loss: 3.5541, Train: 0.7929, Val: 0.7020, Test: 0.7390
Epoch: 36, Loss: 3.4879, Train: 0.8000, Val: 0.7100, Test: 0.7450
Epoch: 37, Loss: 3.4655, Train: 0.8071, Val: 0.7080, Test: 0.7500
Epoch: 38, Loss: 3.1925, Train: 0.8143, Val: 0.7040, Test: 0.7490
Epoch: 39, Loss: 3.6733, Train: 0.8143, Val: 0.7020, Test: 0.7480
Epoch: 40, Loss: 3.2882, Train: 0.8286, Val: 0.6980, Test: 0.7500
Epoch: 41, Loss: 3.3672, Train: 0.8286, Val: 0.6960, Test: 0.7460
Epoch: 42, Loss: 3.1642, Train: 0.8286, Val: 0.6960, Test: 0.7440
Epoch: 43, Loss: 3.2571, Train: 0.8286, Val: 0.6940, Test: 0.7410
Epoch: 44, Loss: 3.3446, Train: 0.8357, Val: 0.6960, Test: 0.7410
Epoch: 45, Loss: 3.0271, Train: 0.8357, Val: 0.7020, Test: 0.7480
Epoch: 46, Loss: 2.9750, Train: 0.8286, Val: 0.7080, Test: 0.7460
Epoch: 47, Loss: 2.8575, Train: 0.8357, Val: 0.7160, Test: 0.7520
Epoch: 48, Loss: 3.2058, Train: 0.8429, Val: 0.7260, Test: 0.7700
Epoch: 49, Loss: 3.3114, Train: 0.8857, Val: 0.7540, Test: 0.7880
Epoch: 50, Loss: 3.1412, Train: 0.9357, Val: 0.7660, Test: 0.8090
MAD:  0.1827
Best Test Accuracy: 0.8090, Val Accuracy: 0.7660, Train Accuracy: 0.9357
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8556, Train: 0.1929, Val: 0.1280, Test: 0.1140
Epoch: 2, Loss: 4.8416, Train: 0.2929, Val: 0.1840, Test: 0.1890
Epoch: 3, Loss: 4.8258, Train: 0.3286, Val: 0.2240, Test: 0.2210
Epoch: 4, Loss: 4.8160, Train: 0.3357, Val: 0.2280, Test: 0.2260
Epoch: 5, Loss: 4.7884, Train: 0.3143, Val: 0.2180, Test: 0.2270
Epoch: 6, Loss: 4.7484, Train: 0.3143, Val: 0.2060, Test: 0.2180
Epoch: 7, Loss: 4.7619, Train: 0.3071, Val: 0.2020, Test: 0.2110
Epoch: 8, Loss: 4.7046, Train: 0.2857, Val: 0.2000, Test: 0.2030
Epoch: 9, Loss: 4.6714, Train: 0.2857, Val: 0.1960, Test: 0.1990
Epoch: 10, Loss: 4.5706, Train: 0.2643, Val: 0.1820, Test: 0.1810
Epoch: 11, Loss: 4.5211, Train: 0.2357, Val: 0.1660, Test: 0.1710
Epoch: 12, Loss: 4.5617, Train: 0.2357, Val: 0.1660, Test: 0.1670
Epoch: 13, Loss: 4.3791, Train: 0.2286, Val: 0.1640, Test: 0.1670
Epoch: 14, Loss: 4.3480, Train: 0.2286, Val: 0.1600, Test: 0.1610
Epoch: 15, Loss: 4.1059, Train: 0.2286, Val: 0.1600, Test: 0.1610
Epoch: 16, Loss: 4.2359, Train: 0.2357, Val: 0.1680, Test: 0.1730
Epoch: 17, Loss: 4.0088, Train: 0.2786, Val: 0.1880, Test: 0.1910
Epoch: 18, Loss: 4.1859, Train: 0.3071, Val: 0.2060, Test: 0.2140
Epoch: 19, Loss: 4.0285, Train: 0.3643, Val: 0.2200, Test: 0.2320
Epoch: 20, Loss: 4.0521, Train: 0.4071, Val: 0.2560, Test: 0.2600
Epoch: 21, Loss: 3.8009, Train: 0.4571, Val: 0.2700, Test: 0.2820
Epoch: 22, Loss: 3.8463, Train: 0.5000, Val: 0.2880, Test: 0.3050
Epoch: 23, Loss: 3.7897, Train: 0.6143, Val: 0.3380, Test: 0.3750
Epoch: 24, Loss: 3.9695, Train: 0.6857, Val: 0.4880, Test: 0.5110
Epoch: 25, Loss: 3.6523, Train: 0.7786, Val: 0.5880, Test: 0.6150
Epoch: 26, Loss: 3.7942, Train: 0.7929, Val: 0.6240, Test: 0.6780
Epoch: 27, Loss: 3.7603, Train: 0.7929, Val: 0.6400, Test: 0.6790
Epoch: 28, Loss: 3.6403, Train: 0.7929, Val: 0.6460, Test: 0.6630
Epoch: 29, Loss: 3.6499, Train: 0.7929, Val: 0.6220, Test: 0.6490
Epoch: 30, Loss: 3.6338, Train: 0.8071, Val: 0.6260, Test: 0.6600
Epoch: 31, Loss: 3.6782, Train: 0.8571, Val: 0.6700, Test: 0.6820
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 32, Loss: 3.3519, Train: 0.9000, Val: 0.7020, Test: 0.7200
Epoch: 33, Loss: 3.5266, Train: 0.9071, Val: 0.7360, Test: 0.7500
Epoch: 34, Loss: 3.1420, Train: 0.9143, Val: 0.7640, Test: 0.7660
Epoch: 35, Loss: 3.4276, Train: 0.9357, Val: 0.7660, Test: 0.7720
Epoch: 36, Loss: 3.2257, Train: 0.9500, Val: 0.7720, Test: 0.7730
Epoch: 37, Loss: 3.7107, Train: 0.9500, Val: 0.7740, Test: 0.7680
Epoch: 38, Loss: 3.3530, Train: 0.9571, Val: 0.7700, Test: 0.7710
Epoch: 39, Loss: 3.6623, Train: 0.9357, Val: 0.7600, Test: 0.7660
Epoch: 40, Loss: 3.3336, Train: 0.9214, Val: 0.7580, Test: 0.7710
Epoch: 41, Loss: 2.8765, Train: 0.9286, Val: 0.7500, Test: 0.7700
Epoch: 42, Loss: 3.1671, Train: 0.9143, Val: 0.7520, Test: 0.7690
Epoch: 43, Loss: 2.7299, Train: 0.9214, Val: 0.7580, Test: 0.7650
Epoch: 44, Loss: 3.1063, Train: 0.9286, Val: 0.7620, Test: 0.7560
Epoch: 45, Loss: 3.0463, Train: 0.9214, Val: 0.7540, Test: 0.7590
Epoch: 46, Loss: 3.3740, Train: 0.9286, Val: 0.7480, Test: 0.7600
Epoch: 47, Loss: 2.8253, Train: 0.9286, Val: 0.7460, Test: 0.7600
Epoch: 48, Loss: 2.6252, Train: 0.9429, Val: 0.7580, Test: 0.7660
Epoch: 49, Loss: 3.1305, Train: 0.9429, Val: 0.7660, Test: 0.7680
Epoch: 50, Loss: 3.0140, Train: 0.9500, Val: 0.7680, Test: 0.7680
MAD:  0.0448
Best Test Accuracy: 0.7730, Val Accuracy: 0.7720, Train Accuracy: 0.9500
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8477, Train: 0.1500, Val: 0.3160, Test: 0.3120
Epoch: 2, Loss: 4.8340, Train: 0.1929, Val: 0.3240, Test: 0.3260
Epoch: 3, Loss: 4.8082, Train: 0.2214, Val: 0.3320, Test: 0.3330
Epoch: 4, Loss: 4.7776, Train: 0.2286, Val: 0.3320, Test: 0.3300
Epoch: 5, Loss: 4.7374, Train: 0.2286, Val: 0.3300, Test: 0.3280
Epoch: 6, Loss: 4.7018, Train: 0.2143, Val: 0.3280, Test: 0.3280
Epoch: 7, Loss: 4.6504, Train: 0.1857, Val: 0.3200, Test: 0.3270
Epoch: 8, Loss: 4.6305, Train: 0.1857, Val: 0.3180, Test: 0.3230
Epoch: 9, Loss: 4.5836, Train: 0.1857, Val: 0.3160, Test: 0.3230
Epoch: 10, Loss: 4.5270, Train: 0.1857, Val: 0.3160, Test: 0.3230
Epoch: 11, Loss: 4.3709, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 12, Loss: 4.4044, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 13, Loss: 4.3019, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 14, Loss: 4.2318, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 15, Loss: 4.2228, Train: 0.1786, Val: 0.3160, Test: 0.3220
Epoch: 16, Loss: 4.0400, Train: 0.1857, Val: 0.3160, Test: 0.3240
Epoch: 17, Loss: 4.0387, Train: 0.2000, Val: 0.3200, Test: 0.3270
Epoch: 18, Loss: 3.8534, Train: 0.2571, Val: 0.3300, Test: 0.3370
Epoch: 19, Loss: 3.9237, Train: 0.4214, Val: 0.4000, Test: 0.4130
Epoch: 20, Loss: 4.2506, Train: 0.6929, Val: 0.5040, Test: 0.5690
Epoch: 21, Loss: 3.8759, Train: 0.7429, Val: 0.5280, Test: 0.5640
Epoch: 22, Loss: 3.6432, Train: 0.7071, Val: 0.4460, Test: 0.4830
Epoch: 23, Loss: 3.9840, Train: 0.6786, Val: 0.4080, Test: 0.4460
Epoch: 24, Loss: 3.5290, Train: 0.6643, Val: 0.4000, Test: 0.4360
Epoch: 25, Loss: 3.6125, Train: 0.6714, Val: 0.4040, Test: 0.4290
Epoch: 26, Loss: 3.7460, Train: 0.6714, Val: 0.4060, Test: 0.4350
Epoch: 27, Loss: 3.5416, Train: 0.7000, Val: 0.4160, Test: 0.4450
Epoch: 28, Loss: 3.4259, Train: 0.7143, Val: 0.4280, Test: 0.4660
Epoch: 29, Loss: 3.5809, Train: 0.7286, Val: 0.4620, Test: 0.4970
Epoch: 30, Loss: 3.7092, Train: 0.7571, Val: 0.5040, Test: 0.5490
Epoch: 31, Loss: 3.6110, Train: 0.7714, Val: 0.5540, Test: 0.5820
Epoch: 32, Loss: 3.4228, Train: 0.7857, Val: 0.5720, Test: 0.5930
Epoch: 33, Loss: 3.5559, Train: 0.7857, Val: 0.5860, Test: 0.6080
Epoch: 34, Loss: 3.3147, Train: 0.8071, Val: 0.6100, Test: 0.6390
Epoch: 35, Loss: 3.3606, Train: 0.8571, Val: 0.6820, Test: 0.6910
Epoch: 36, Loss: 3.4753, Train: 0.9000, Val: 0.7180, Test: 0.7330
Epoch: 37, Loss: 3.3411, Train: 0.9286, Val: 0.7400, Test: 0.7620
Epoch: 38, Loss: 3.3642, Train: 0.9357, Val: 0.7480, Test: 0.7830
Epoch: 39, Loss: 3.3152, Train: 0.9357, Val: 0.7560, Test: 0.7880
Epoch: 40, Loss: 3.3591, Train: 0.9429, Val: 0.7540, Test: 0.7980
Epoch: 41, Loss: 3.3958, Train: 0.9429, Val: 0.7460, Test: 0.7950
Epoch: 42, Loss: 3.1594, Train: 0.9429, Val: 0.7440, Test: 0.7980
Epoch: 43, Loss: 3.4710, Train: 0.9429, Val: 0.7380, Test: 0.7990
Epoch: 44, Loss: 3.0898, Train: 0.9500, Val: 0.7420, Test: 0.7980
Epoch: 45, Loss: 3.1780, Train: 0.9571, Val: 0.7480, Test: 0.7950
Epoch: 46, Loss: 3.1239, Train: 0.9643, Val: 0.7460, Test: 0.7950
Epoch: 47, Loss: 2.9854, Train: 0.9643, Val: 0.7480, Test: 0.7930
Epoch: 48, Loss: 2.6174, Train: 0.9643, Val: 0.7500, Test: 0.7910
Epoch: 49, Loss: 2.7730, Train: 0.9643, Val: 0.7660, Test: 0.7960
Epoch: 50, Loss: 2.8985, Train: 0.9500, Val: 0.7700, Test: 0.8000
MAD:  0.2141
Best Test Accuracy: 0.8000, Val Accuracy: 0.7700, Train Accuracy: 0.9500
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8510, Train: 0.2214, Val: 0.1920, Test: 0.2360
Epoch: 2, Loss: 4.8310, Train: 0.4000, Val: 0.4120, Test: 0.4520
Epoch: 3, Loss: 4.8003, Train: 0.4857, Val: 0.4860, Test: 0.5150
Epoch: 4, Loss: 4.7752, Train: 0.5286, Val: 0.5220, Test: 0.5540
Epoch: 5, Loss: 4.7352, Train: 0.5857, Val: 0.5720, Test: 0.5780
Epoch: 6, Loss: 4.6802, Train: 0.6143, Val: 0.5700, Test: 0.5850
Epoch: 7, Loss: 4.6509, Train: 0.6143, Val: 0.5620, Test: 0.5680
Epoch: 8, Loss: 4.5626, Train: 0.6000, Val: 0.5320, Test: 0.5460
Epoch: 9, Loss: 4.4068, Train: 0.5786, Val: 0.5120, Test: 0.5200
Epoch: 10, Loss: 4.4254, Train: 0.5500, Val: 0.4880, Test: 0.4830
Epoch: 11, Loss: 4.4487, Train: 0.5143, Val: 0.4520, Test: 0.4450
Epoch: 12, Loss: 4.2351, Train: 0.4929, Val: 0.3940, Test: 0.3930
Epoch: 13, Loss: 4.1987, Train: 0.4214, Val: 0.3300, Test: 0.3140
Epoch: 14, Loss: 3.9526, Train: 0.3857, Val: 0.2840, Test: 0.2810
Epoch: 15, Loss: 3.9093, Train: 0.3500, Val: 0.2420, Test: 0.2560
Epoch: 16, Loss: 4.0029, Train: 0.3786, Val: 0.2440, Test: 0.2600
Epoch: 17, Loss: 3.9773, Train: 0.4286, Val: 0.2820, Test: 0.3030
Epoch: 18, Loss: 3.9792, Train: 0.6143, Val: 0.3900, Test: 0.4240
Epoch: 19, Loss: 3.4887, Train: 0.7500, Val: 0.5200, Test: 0.5520
Epoch: 20, Loss: 3.5715, Train: 0.7857, Val: 0.5540, Test: 0.5920
Epoch: 21, Loss: 3.7674, Train: 0.8286, Val: 0.5760, Test: 0.6060
Epoch: 22, Loss: 3.6904, Train: 0.8143, Val: 0.5780, Test: 0.6130
Epoch: 23, Loss: 3.6881, Train: 0.8286, Val: 0.5940, Test: 0.6050
Epoch: 24, Loss: 3.8962, Train: 0.8357, Val: 0.5980, Test: 0.6120
Epoch: 25, Loss: 3.4440, Train: 0.8357, Val: 0.6060, Test: 0.6280
Epoch: 26, Loss: 3.6159, Train: 0.8357, Val: 0.6280, Test: 0.6420
Epoch: 27, Loss: 3.4921, Train: 0.8571, Val: 0.6420, Test: 0.6510
Epoch: 28, Loss: 3.5783, Train: 0.8571, Val: 0.6480, Test: 0.6690
Epoch: 29, Loss: 3.3314, Train: 0.8643, Val: 0.6680, Test: 0.6920
Epoch: 30, Loss: 3.4223, Train: 0.9000, Val: 0.6860, Test: 0.7340
Epoch: 31, Loss: 3.3508, Train: 0.9286, Val: 0.7120, Test: 0.7590
Epoch: 32, Loss: 3.2435, Train: 0.9500, Val: 0.7240, Test: 0.7750
Epoch: 33, Loss: 3.3375, Train: 0.9500, Val: 0.7400, Test: 0.7860
Epoch: 34, Loss: 3.3762, Train: 0.9500, Val: 0.7560, Test: 0.7900
Epoch: 35, Loss: 3.5580, Train: 0.9429, Val: 0.7600, Test: 0.7940
Epoch: 36, Loss: 3.1325, Train: 0.9429, Val: 0.7620, Test: 0.7970
Epoch: 37, Loss: 2.9429, Train: 0.9429, Val: 0.7600, Test: 0.7990
Epoch: 38, Loss: 3.2987, Train: 0.9286, Val: 0.7640, Test: 0.7950
Epoch: 39, Loss: 3.2470, Train: 0.9286, Val: 0.7620, Test: 0.8010
Epoch: 40, Loss: 2.9812, Train: 0.9429, Val: 0.7540, Test: 0.7930
Epoch: 41, Loss: 3.0415, Train: 0.9429, Val: 0.7560, Test: 0.7860
Epoch: 42, Loss: 3.0758, Train: 0.9429, Val: 0.7480, Test: 0.7820
Epoch: 43, Loss: 3.3229, Train: 0.9429, Val: 0.7520, Test: 0.7850
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 44, Loss: 3.2414, Train: 0.9500, Val: 0.7480, Test: 0.7840
Epoch: 45, Loss: 2.8139, Train: 0.9500, Val: 0.7500, Test: 0.7830
Epoch: 46, Loss: 2.8480, Train: 0.9643, Val: 0.7500, Test: 0.7750
Epoch: 47, Loss: 2.8568, Train: 0.9643, Val: 0.7500, Test: 0.7810
Epoch: 48, Loss: 3.2623, Train: 0.9643, Val: 0.7520, Test: 0.7860
Epoch: 49, Loss: 3.1623, Train: 0.9643, Val: 0.7600, Test: 0.7920
Epoch: 50, Loss: 2.9983, Train: 0.9643, Val: 0.7620, Test: 0.8040
MAD:  0.2618
Best Test Accuracy: 0.8040, Val Accuracy: 0.7620, Train Accuracy: 0.9643
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
    (2): GCNConv(128, 128)
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8566, Train: 0.0714, Val: 0.0140, Test: 0.0240
Epoch: 2, Loss: 4.8462, Train: 0.2214, Val: 0.0640, Test: 0.0960
Epoch: 3, Loss: 4.8389, Train: 0.3143, Val: 0.1420, Test: 0.1740
Epoch: 4, Loss: 4.8290, Train: 0.4714, Val: 0.2500, Test: 0.2850
Epoch: 5, Loss: 4.8179, Train: 0.5000, Val: 0.3200, Test: 0.3220
Epoch: 6, Loss: 4.7838, Train: 0.5286, Val: 0.3400, Test: 0.3380
Epoch: 7, Loss: 4.7468, Train: 0.5357, Val: 0.3380, Test: 0.3270
Epoch: 8, Loss: 4.7580, Train: 0.5214, Val: 0.3240, Test: 0.3240
Epoch: 9, Loss: 4.7055, Train: 0.5143, Val: 0.3120, Test: 0.3110
Epoch: 10, Loss: 4.7347, Train: 0.5357, Val: 0.3120, Test: 0.3100
Epoch: 11, Loss: 4.6252, Train: 0.5357, Val: 0.3240, Test: 0.3140
Epoch: 12, Loss: 4.6024, Train: 0.5500, Val: 0.3300, Test: 0.3200
Epoch: 13, Loss: 4.5582, Train: 0.5643, Val: 0.3440, Test: 0.3370
Epoch: 14, Loss: 4.4663, Train: 0.5714, Val: 0.3520, Test: 0.3500
Epoch: 15, Loss: 4.2910, Train: 0.5857, Val: 0.3580, Test: 0.3580
Epoch: 16, Loss: 4.2815, Train: 0.6214, Val: 0.3840, Test: 0.3850
Epoch: 17, Loss: 4.2515, Train: 0.6357, Val: 0.4000, Test: 0.4070
Epoch: 18, Loss: 4.0134, Train: 0.6357, Val: 0.4120, Test: 0.4180
Epoch: 19, Loss: 3.8160, Train: 0.6357, Val: 0.4240, Test: 0.4270
Epoch: 20, Loss: 3.9855, Train: 0.6429, Val: 0.4280, Test: 0.4290
Epoch: 21, Loss: 3.8453, Train: 0.6429, Val: 0.4280, Test: 0.4290
Epoch: 22, Loss: 3.8745, Train: 0.6500, Val: 0.4300, Test: 0.4310
Epoch: 23, Loss: 3.7850, Train: 0.6571, Val: 0.4320, Test: 0.4370
Epoch: 24, Loss: 3.6283, Train: 0.6714, Val: 0.4320, Test: 0.4430
Epoch: 25, Loss: 3.7151, Train: 0.6786, Val: 0.4300, Test: 0.4530
Epoch: 26, Loss: 3.9343, Train: 0.6857, Val: 0.4380, Test: 0.4610
Epoch: 27, Loss: 3.6927, Train: 0.8286, Val: 0.5900, Test: 0.6140
Epoch: 28, Loss: 3.4936, Train: 0.9143, Val: 0.7340, Test: 0.7550
Epoch: 29, Loss: 3.4497, Train: 0.9214, Val: 0.7860, Test: 0.8180
Epoch: 30, Loss: 3.2852, Train: 0.8857, Val: 0.7640, Test: 0.8050
Epoch: 31, Loss: 3.5945, Train: 0.8786, Val: 0.7480, Test: 0.7870
Epoch: 32, Loss: 3.1206, Train: 0.8500, Val: 0.7540, Test: 0.7560
Epoch: 33, Loss: 3.6280, Train: 0.8643, Val: 0.7420, Test: 0.7560
Epoch: 34, Loss: 3.4263, Train: 0.8929, Val: 0.7340, Test: 0.7570
Epoch: 35, Loss: 3.3200, Train: 0.9071, Val: 0.7380, Test: 0.7660
Epoch: 36, Loss: 3.0680, Train: 0.9071, Val: 0.7520, Test: 0.7800
Epoch: 37, Loss: 3.4122, Train: 0.9286, Val: 0.7680, Test: 0.7910
Epoch: 38, Loss: 2.9529, Train: 0.9500, Val: 0.7760, Test: 0.7970
Epoch: 39, Loss: 3.0785, Train: 0.9357, Val: 0.7840, Test: 0.7980
Epoch: 40, Loss: 3.1212, Train: 0.9286, Val: 0.7760, Test: 0.7960
Epoch: 41, Loss: 3.0567, Train: 0.9357, Val: 0.7680, Test: 0.7940
Epoch: 42, Loss: 3.3916, Train: 0.9286, Val: 0.7680, Test: 0.7860
Epoch: 43, Loss: 3.2817, Train: 0.9357, Val: 0.7500, Test: 0.7760
Epoch: 44, Loss: 3.3962, Train: 0.9286, Val: 0.7480, Test: 0.7680
Epoch: 45, Loss: 3.0289, Train: 0.9429, Val: 0.7460, Test: 0.7700
Epoch: 46, Loss: 3.1452, Train: 0.9429, Val: 0.7420, Test: 0.7710
Epoch: 47, Loss: 2.7576, Train: 0.9500, Val: 0.7420, Test: 0.7730
Epoch: 48, Loss: 3.1262, Train: 0.9500, Val: 0.7440, Test: 0.7760
Epoch: 49, Loss: 2.9427, Train: 0.9571, Val: 0.7580, Test: 0.7700
Epoch: 50, Loss: 2.8797, Train: 0.9643, Val: 0.7620, Test: 0.7740
MAD:  0.0414
Best Test Accuracy: 0.8180, Val Accuracy: 0.7860, Train Accuracy: 0.9214
Training completed.
Average Test Accuracy:  0.7619 ± 0.10340256283090861
Average MAD:  0.16425000000000003 ± 0.07425360933988327

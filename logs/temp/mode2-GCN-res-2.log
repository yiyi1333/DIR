Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8423, Train: 0.1714, Val: 0.0220, Test: 0.0350
Epoch: 2, Loss: 4.8149, Train: 0.3071, Val: 0.1000, Test: 0.1250
Epoch: 3, Loss: 4.7363, Train: 0.4214, Val: 0.1780, Test: 0.1980
Epoch: 4, Loss: 4.7038, Train: 0.5214, Val: 0.2600, Test: 0.2820
Epoch: 5, Loss: 4.6066, Train: 0.6000, Val: 0.3100, Test: 0.3220
Epoch: 6, Loss: 4.5506, Train: 0.6500, Val: 0.3500, Test: 0.3620
Epoch: 7, Loss: 4.5798, Train: 0.6857, Val: 0.3680, Test: 0.3890
Epoch: 8, Loss: 4.2508, Train: 0.7143, Val: 0.3760, Test: 0.4140
Epoch: 9, Loss: 4.4176, Train: 0.7357, Val: 0.4000, Test: 0.4380
Epoch: 10, Loss: 4.3766, Train: 0.7643, Val: 0.4300, Test: 0.4630
Epoch: 11, Loss: 4.1485, Train: 0.7929, Val: 0.4520, Test: 0.4860
Epoch: 12, Loss: 4.0411, Train: 0.8071, Val: 0.4820, Test: 0.5160
Epoch: 13, Loss: 4.1237, Train: 0.8429, Val: 0.5020, Test: 0.5440
Epoch: 14, Loss: 4.0336, Train: 0.8643, Val: 0.5360, Test: 0.5700
Epoch: 15, Loss: 4.1005, Train: 0.8857, Val: 0.5560, Test: 0.6050
Epoch: 16, Loss: 3.9248, Train: 0.9214, Val: 0.5900, Test: 0.6350
Epoch: 17, Loss: 3.7520, Train: 0.9500, Val: 0.6140, Test: 0.6640
Epoch: 18, Loss: 4.0646, Train: 0.9786, Val: 0.6400, Test: 0.6850
Epoch: 19, Loss: 4.1889, Train: 0.9786, Val: 0.6680, Test: 0.7030
Epoch: 20, Loss: 4.1237, Train: 0.9857, Val: 0.6800, Test: 0.7250
Epoch: 21, Loss: 3.5688, Train: 0.9857, Val: 0.6980, Test: 0.7330
Epoch: 22, Loss: 3.6813, Train: 0.9857, Val: 0.7160, Test: 0.7420
Epoch: 23, Loss: 3.8789, Train: 0.9929, Val: 0.7320, Test: 0.7510
Epoch: 24, Loss: 4.0058, Train: 0.9929, Val: 0.7340, Test: 0.7500
Epoch: 25, Loss: 4.1194, Train: 0.9929, Val: 0.7320, Test: 0.7560
Epoch: 26, Loss: 3.8728, Train: 1.0000, Val: 0.7340, Test: 0.7600
Epoch: 27, Loss: 3.7685, Train: 1.0000, Val: 0.7320, Test: 0.7550
Epoch: 28, Loss: 3.7568, Train: 1.0000, Val: 0.7240, Test: 0.7510
Epoch: 29, Loss: 3.5204, Train: 1.0000, Val: 0.7300, Test: 0.7530
Epoch: 30, Loss: 3.7980, Train: 1.0000, Val: 0.7300, Test: 0.7510
Epoch: 31, Loss: 3.7170, Train: 1.0000, Val: 0.7260, Test: 0.7540
Epoch: 32, Loss: 3.4981, Train: 1.0000, Val: 0.7280, Test: 0.7500
Epoch: 33, Loss: 3.8294, Train: 1.0000, Val: 0.7300, Test: 0.7490
Epoch: 34, Loss: 3.5238, Train: 1.0000, Val: 0.7260, Test: 0.7490
Epoch: 35, Loss: 3.5633, Train: 1.0000, Val: 0.7260, Test: 0.7480
Epoch: 36, Loss: 3.6315, Train: 1.0000, Val: 0.7260, Test: 0.7540
Epoch: 37, Loss: 3.6049, Train: 1.0000, Val: 0.7240, Test: 0.7560
Epoch: 38, Loss: 3.3876, Train: 1.0000, Val: 0.7260, Test: 0.7600
Epoch: 39, Loss: 3.6406, Train: 1.0000, Val: 0.7260, Test: 0.7610
Epoch: 40, Loss: 3.5338, Train: 1.0000, Val: 0.7300, Test: 0.7640
Epoch: 41, Loss: 3.5593, Train: 1.0000, Val: 0.7340, Test: 0.7660
Epoch: 42, Loss: 3.9082, Train: 1.0000, Val: 0.7360, Test: 0.7680
Epoch: 43, Loss: 3.5542, Train: 1.0000, Val: 0.7360, Test: 0.7690
Epoch: 44, Loss: 3.7350, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 45, Loss: 3.6222, Train: 1.0000, Val: 0.7440, Test: 0.7730
Epoch: 46, Loss: 3.8907, Train: 1.0000, Val: 0.7460, Test: 0.7760
Epoch: 47, Loss: 3.6622, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 48, Loss: 3.4786, Train: 1.0000, Val: 0.7560, Test: 0.7730
Epoch: 49, Loss: 3.6731, Train: 1.0000, Val: 0.7600, Test: 0.7740
Epoch: 50, Loss: 3.7822, Train: 1.0000, Val: 0.7580, Test: 0.7720
Epoch: 51, Loss: 3.7105, Train: 1.0000, Val: 0.7620, Test: 0.7730
Epoch: 52, Loss: 3.6834, Train: 1.0000, Val: 0.7600, Test: 0.7770
Epoch: 53, Loss: 3.5088, Train: 1.0000, Val: 0.7600, Test: 0.7770
Epoch: 54, Loss: 3.5922, Train: 1.0000, Val: 0.7560, Test: 0.7760
Epoch: 55, Loss: 3.5267, Train: 1.0000, Val: 0.7560, Test: 0.7760
Epoch: 56, Loss: 3.6327, Train: 1.0000, Val: 0.7580, Test: 0.7770
Epoch: 57, Loss: 3.7354, Train: 1.0000, Val: 0.7580, Test: 0.7770
Epoch: 58, Loss: 3.4984, Train: 1.0000, Val: 0.7580, Test: 0.7780
Epoch: 59, Loss: 3.7741, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 60, Loss: 3.5679, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 61, Loss: 3.6689, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 62, Loss: 4.0090, Train: 1.0000, Val: 0.7500, Test: 0.7750
Epoch: 63, Loss: 3.5610, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 64, Loss: 3.6993, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 65, Loss: 3.8362, Train: 1.0000, Val: 0.7480, Test: 0.7760
Epoch: 66, Loss: 3.8010, Train: 1.0000, Val: 0.7480, Test: 0.7750
Epoch: 67, Loss: 3.6596, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 68, Loss: 3.2192, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 69, Loss: 3.5916, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 70, Loss: 3.8980, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 71, Loss: 3.7268, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 72, Loss: 3.5643, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 73, Loss: 3.5293, Train: 1.0000, Val: 0.7400, Test: 0.7700
Epoch: 74, Loss: 3.8262, Train: 1.0000, Val: 0.7380, Test: 0.7710
Epoch: 75, Loss: 3.4842, Train: 1.0000, Val: 0.7380, Test: 0.7700
Epoch: 76, Loss: 3.4839, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 77, Loss: 3.6277, Train: 1.0000, Val: 0.7380, Test: 0.7690
Epoch: 78, Loss: 3.8237, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 79, Loss: 3.3075, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 80, Loss: 3.6575, Train: 1.0000, Val: 0.7380, Test: 0.7680
Epoch: 81, Loss: 3.4844, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 82, Loss: 3.6249, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 83, Loss: 3.3807, Train: 1.0000, Val: 0.7420, Test: 0.7680
Epoch: 84, Loss: 3.8981, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 85, Loss: 3.5164, Train: 1.0000, Val: 0.7400, Test: 0.7650
Epoch: 86, Loss: 3.4130, Train: 1.0000, Val: 0.7380, Test: 0.7630
Epoch: 87, Loss: 3.6862, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 88, Loss: 3.6868, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 89, Loss: 3.6205, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 90, Loss: 3.8253, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 91, Loss: 3.4788, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 92, Loss: 3.6187, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 93, Loss: 3.7182, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 94, Loss: 3.7203, Train: 1.0000, Val: 0.7420, Test: 0.7700
Epoch: 95, Loss: 3.5809, Train: 1.0000, Val: 0.7420, Test: 0.7720
Epoch: 96, Loss: 3.2380, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 97, Loss: 3.4783, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 98, Loss: 3.5814, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 99, Loss: 3.6837, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 100, Loss: 3.9259, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 101, Loss: 3.5109, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 102, Loss: 3.5823, Train: 1.0000, Val: 0.7460, Test: 0.7730
Epoch: 103, Loss: 3.3388, Train: 1.0000, Val: 0.7460, Test: 0.7720
Epoch: 104, Loss: 3.8908, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 105, Loss: 3.4424, Train: 1.0000, Val: 0.7480, Test: 0.7720
Epoch: 106, Loss: 3.4414, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 107, Loss: 3.6822, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 108, Loss: 3.6151, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 109, Loss: 3.8224, Train: 1.0000, Val: 0.7520, Test: 0.7730
Epoch: 110, Loss: 3.8541, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 111, Loss: 3.5093, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 112, Loss: 3.7167, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 113, Loss: 3.6489, Train: 1.0000, Val: 0.7560, Test: 0.7750
Epoch: 114, Loss: 3.7520, Train: 1.0000, Val: 0.7560, Test: 0.7750
Epoch: 115, Loss: 3.5794, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 116, Loss: 3.8566, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 117, Loss: 3.7166, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 118, Loss: 3.4748, Train: 1.0000, Val: 0.7540, Test: 0.7740
Epoch: 119, Loss: 4.0270, Train: 1.0000, Val: 0.7540, Test: 0.7740
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 120, Loss: 3.7511, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 121, Loss: 3.7530, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 122, Loss: 3.3735, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 123, Loss: 3.5771, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 124, Loss: 3.8876, Train: 1.0000, Val: 0.7500, Test: 0.7720
Epoch: 125, Loss: 3.7860, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 126, Loss: 3.4072, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 127, Loss: 3.9239, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 128, Loss: 3.6489, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 129, Loss: 3.5781, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 130, Loss: 3.3712, Train: 1.0000, Val: 0.7500, Test: 0.7700
Epoch: 131, Loss: 3.7184, Train: 1.0000, Val: 0.7500, Test: 0.7700
Epoch: 132, Loss: 3.4048, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 133, Loss: 3.4396, Train: 1.0000, Val: 0.7500, Test: 0.7710
Epoch: 134, Loss: 3.7507, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 135, Loss: 3.7842, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 136, Loss: 3.8548, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 137, Loss: 3.5089, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 138, Loss: 3.6123, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 139, Loss: 3.3007, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 140, Loss: 3.7155, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 141, Loss: 3.7840, Train: 1.0000, Val: 0.7480, Test: 0.7660
Epoch: 142, Loss: 3.8200, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 143, Loss: 3.8183, Train: 1.0000, Val: 0.7480, Test: 0.7660
Epoch: 144, Loss: 3.8193, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 145, Loss: 3.6117, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 146, Loss: 3.5782, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 147, Loss: 3.7495, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 148, Loss: 3.8882, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 149, Loss: 3.4734, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 150, Loss: 3.7836, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 151, Loss: 3.3007, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 152, Loss: 3.6808, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 153, Loss: 3.8171, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 154, Loss: 3.4388, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 155, Loss: 3.5767, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 156, Loss: 3.8192, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 157, Loss: 3.7145, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 158, Loss: 3.5424, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 159, Loss: 3.4750, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 160, Loss: 3.8885, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 161, Loss: 3.5079, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 162, Loss: 3.9217, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 163, Loss: 3.4730, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 164, Loss: 3.6796, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 165, Loss: 3.4721, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 166, Loss: 3.8529, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 167, Loss: 3.7144, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 168, Loss: 3.8183, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 169, Loss: 3.8182, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 170, Loss: 3.5416, Train: 1.0000, Val: 0.7480, Test: 0.7670
Epoch: 171, Loss: 3.4735, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 172, Loss: 3.5764, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 173, Loss: 3.5435, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 174, Loss: 3.7485, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 175, Loss: 3.4371, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 176, Loss: 3.7141, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 177, Loss: 3.7136, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 178, Loss: 3.7143, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 179, Loss: 3.4742, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 180, Loss: 3.7493, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 181, Loss: 3.9212, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 182, Loss: 3.6114, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 183, Loss: 3.5404, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 184, Loss: 3.6800, Train: 1.0000, Val: 0.7440, Test: 0.7710
Epoch: 185, Loss: 3.4722, Train: 1.0000, Val: 0.7400, Test: 0.7710
Epoch: 186, Loss: 3.9900, Train: 1.0000, Val: 0.7400, Test: 0.7690
Epoch: 187, Loss: 3.3350, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 188, Loss: 3.3693, Train: 1.0000, Val: 0.7400, Test: 0.7670
Epoch: 189, Loss: 3.4731, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 190, Loss: 3.5084, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 191, Loss: 3.5764, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 192, Loss: 3.7833, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 193, Loss: 3.5066, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 194, Loss: 3.6452, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 195, Loss: 3.4034, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 196, Loss: 3.2993, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 197, Loss: 3.6111, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 198, Loss: 3.2987, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 199, Loss: 3.8865, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 200, Loss: 3.6796, Train: 1.0000, Val: 0.7460, Test: 0.7700
MAD:  0.5207
Best Test Accuracy: 0.7790, Val Accuracy: 0.7540, Train Accuracy: 1.0000
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8746, Train: 0.0643, Val: 0.0320, Test: 0.0290
Epoch: 2, Loss: 4.8106, Train: 0.2071, Val: 0.0880, Test: 0.0950
Epoch: 3, Loss: 4.7887, Train: 0.3929, Val: 0.1500, Test: 0.1600
Epoch: 4, Loss: 4.7065, Train: 0.4643, Val: 0.1960, Test: 0.2100
Epoch: 5, Loss: 4.6940, Train: 0.4929, Val: 0.2420, Test: 0.2590
Epoch: 6, Loss: 4.6526, Train: 0.5214, Val: 0.2780, Test: 0.2890
Epoch: 7, Loss: 4.5435, Train: 0.5429, Val: 0.2980, Test: 0.3030
Epoch: 8, Loss: 4.4842, Train: 0.5500, Val: 0.3060, Test: 0.3160
Epoch: 9, Loss: 4.2928, Train: 0.5714, Val: 0.3260, Test: 0.3420
Epoch: 10, Loss: 4.3574, Train: 0.6000, Val: 0.3480, Test: 0.3670
Epoch: 11, Loss: 4.3446, Train: 0.6357, Val: 0.3680, Test: 0.3880
Epoch: 12, Loss: 4.3320, Train: 0.6929, Val: 0.3800, Test: 0.4240
Epoch: 13, Loss: 4.1264, Train: 0.7357, Val: 0.4040, Test: 0.4500
Epoch: 14, Loss: 4.3965, Train: 0.7786, Val: 0.4360, Test: 0.4750
Epoch: 15, Loss: 3.9648, Train: 0.8286, Val: 0.4640, Test: 0.5080
Epoch: 16, Loss: 4.0650, Train: 0.8571, Val: 0.5040, Test: 0.5510
Epoch: 17, Loss: 4.1497, Train: 0.9000, Val: 0.5440, Test: 0.5900
Epoch: 18, Loss: 4.1270, Train: 0.9286, Val: 0.5860, Test: 0.6330
Epoch: 19, Loss: 4.2687, Train: 0.9429, Val: 0.6300, Test: 0.6580
Epoch: 20, Loss: 3.9804, Train: 0.9571, Val: 0.6600, Test: 0.6790
Epoch: 21, Loss: 3.9424, Train: 0.9571, Val: 0.6700, Test: 0.6960
Epoch: 22, Loss: 3.9552, Train: 0.9643, Val: 0.6760, Test: 0.7090
Epoch: 23, Loss: 3.7454, Train: 0.9786, Val: 0.6920, Test: 0.7080
Epoch: 24, Loss: 4.0104, Train: 0.9786, Val: 0.6960, Test: 0.7120
Epoch: 25, Loss: 3.7066, Train: 0.9786, Val: 0.7080, Test: 0.7160
Epoch: 26, Loss: 3.9675, Train: 0.9929, Val: 0.7040, Test: 0.7200
Epoch: 27, Loss: 3.9133, Train: 1.0000, Val: 0.7060, Test: 0.7320
Epoch: 28, Loss: 3.6022, Train: 1.0000, Val: 0.7100, Test: 0.7350
Epoch: 29, Loss: 3.8076, Train: 1.0000, Val: 0.7120, Test: 0.7410
Epoch: 30, Loss: 4.1648, Train: 1.0000, Val: 0.7080, Test: 0.7410
Epoch: 31, Loss: 3.6609, Train: 1.0000, Val: 0.7100, Test: 0.7450
Epoch: 32, Loss: 3.7493, Train: 1.0000, Val: 0.7120, Test: 0.7480
Epoch: 33, Loss: 3.6600, Train: 1.0000, Val: 0.7140, Test: 0.7470
Epoch: 34, Loss: 3.5260, Train: 1.0000, Val: 0.7160, Test: 0.7530
Epoch: 35, Loss: 3.5624, Train: 1.0000, Val: 0.7160, Test: 0.7520
Epoch: 36, Loss: 3.9457, Train: 1.0000, Val: 0.7140, Test: 0.7530
Epoch: 37, Loss: 3.4092, Train: 1.0000, Val: 0.7220, Test: 0.7560
Epoch: 38, Loss: 3.7243, Train: 1.0000, Val: 0.7260, Test: 0.7570
Epoch: 39, Loss: 3.7584, Train: 1.0000, Val: 0.7180, Test: 0.7580
Epoch: 40, Loss: 3.5821, Train: 1.0000, Val: 0.7160, Test: 0.7600
Epoch: 41, Loss: 3.7697, Train: 1.0000, Val: 0.7180, Test: 0.7630
Epoch: 42, Loss: 3.6422, Train: 1.0000, Val: 0.7260, Test: 0.7630
Epoch: 43, Loss: 3.6280, Train: 1.0000, Val: 0.7240, Test: 0.7630
Epoch: 44, Loss: 3.4843, Train: 1.0000, Val: 0.7260, Test: 0.7670
Epoch: 45, Loss: 3.5405, Train: 1.0000, Val: 0.7240, Test: 0.7640
Epoch: 46, Loss: 3.6163, Train: 1.0000, Val: 0.7240, Test: 0.7630
Epoch: 47, Loss: 4.0211, Train: 1.0000, Val: 0.7240, Test: 0.7640
Epoch: 48, Loss: 3.6895, Train: 1.0000, Val: 0.7280, Test: 0.7650
Epoch: 49, Loss: 3.7115, Train: 1.0000, Val: 0.7320, Test: 0.7670
Epoch: 50, Loss: 3.4385, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 51, Loss: 3.7094, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 52, Loss: 3.4665, Train: 1.0000, Val: 0.7360, Test: 0.7650
Epoch: 53, Loss: 3.4791, Train: 1.0000, Val: 0.7380, Test: 0.7660
Epoch: 54, Loss: 3.3380, Train: 1.0000, Val: 0.7380, Test: 0.7670
Epoch: 55, Loss: 3.5400, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 56, Loss: 3.7312, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 57, Loss: 3.6305, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 58, Loss: 3.4568, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 59, Loss: 3.6001, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 60, Loss: 3.3328, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 61, Loss: 3.8398, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 62, Loss: 3.5903, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 63, Loss: 3.6986, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 64, Loss: 3.8342, Train: 1.0000, Val: 0.7340, Test: 0.7590
Epoch: 65, Loss: 3.8449, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 66, Loss: 3.4917, Train: 1.0000, Val: 0.7360, Test: 0.7570
Epoch: 67, Loss: 3.5561, Train: 1.0000, Val: 0.7320, Test: 0.7560
Epoch: 68, Loss: 3.6359, Train: 1.0000, Val: 0.7320, Test: 0.7540
Epoch: 69, Loss: 3.5915, Train: 1.0000, Val: 0.7320, Test: 0.7560
Epoch: 70, Loss: 3.2818, Train: 1.0000, Val: 0.7320, Test: 0.7560
Epoch: 71, Loss: 3.7979, Train: 1.0000, Val: 0.7340, Test: 0.7540
Epoch: 72, Loss: 3.5954, Train: 1.0000, Val: 0.7320, Test: 0.7530
Epoch: 73, Loss: 3.6905, Train: 1.0000, Val: 0.7320, Test: 0.7530
Epoch: 74, Loss: 3.8267, Train: 1.0000, Val: 0.7320, Test: 0.7540
Epoch: 75, Loss: 3.7927, Train: 1.0000, Val: 0.7320, Test: 0.7550
Epoch: 76, Loss: 3.6897, Train: 1.0000, Val: 0.7320, Test: 0.7550
Epoch: 77, Loss: 3.4118, Train: 1.0000, Val: 0.7320, Test: 0.7560
Epoch: 78, Loss: 3.5225, Train: 1.0000, Val: 0.7320, Test: 0.7550
Epoch: 79, Loss: 3.4142, Train: 1.0000, Val: 0.7340, Test: 0.7550
Epoch: 80, Loss: 3.8259, Train: 1.0000, Val: 0.7340, Test: 0.7570
Epoch: 81, Loss: 3.9932, Train: 1.0000, Val: 0.7340, Test: 0.7580
Epoch: 82, Loss: 3.7552, Train: 1.0000, Val: 0.7320, Test: 0.7580
Epoch: 83, Loss: 3.5801, Train: 1.0000, Val: 0.7280, Test: 0.7580
Epoch: 84, Loss: 3.1362, Train: 1.0000, Val: 0.7280, Test: 0.7590
Epoch: 85, Loss: 3.5833, Train: 1.0000, Val: 0.7280, Test: 0.7590
Epoch: 86, Loss: 3.5181, Train: 1.0000, Val: 0.7280, Test: 0.7580
Epoch: 87, Loss: 3.6528, Train: 1.0000, Val: 0.7280, Test: 0.7590
Epoch: 88, Loss: 3.9614, Train: 1.0000, Val: 0.7280, Test: 0.7590
Epoch: 89, Loss: 3.5202, Train: 1.0000, Val: 0.7280, Test: 0.7580
Epoch: 90, Loss: 3.3800, Train: 1.0000, Val: 0.7280, Test: 0.7580
Epoch: 91, Loss: 3.8261, Train: 1.0000, Val: 0.7280, Test: 0.7550
Epoch: 92, Loss: 3.9961, Train: 1.0000, Val: 0.7280, Test: 0.7550
Epoch: 93, Loss: 3.5819, Train: 1.0000, Val: 0.7300, Test: 0.7550
Epoch: 94, Loss: 3.3751, Train: 1.0000, Val: 0.7280, Test: 0.7540
Epoch: 95, Loss: 3.5504, Train: 1.0000, Val: 0.7280, Test: 0.7540
Epoch: 96, Loss: 3.7888, Train: 1.0000, Val: 0.7280, Test: 0.7560
Epoch: 97, Loss: 3.6152, Train: 1.0000, Val: 0.7300, Test: 0.7570
Epoch: 98, Loss: 3.4439, Train: 1.0000, Val: 0.7300, Test: 0.7560
Epoch: 99, Loss: 3.5449, Train: 1.0000, Val: 0.7320, Test: 0.7580
Epoch: 100, Loss: 3.2699, Train: 1.0000, Val: 0.7340, Test: 0.7580
Epoch: 101, Loss: 3.8207, Train: 1.0000, Val: 0.7320, Test: 0.7600
Epoch: 102, Loss: 3.3390, Train: 1.0000, Val: 0.7340, Test: 0.7600
Epoch: 103, Loss: 3.5137, Train: 1.0000, Val: 0.7360, Test: 0.7600
Epoch: 104, Loss: 3.6158, Train: 1.0000, Val: 0.7360, Test: 0.7600
Epoch: 105, Loss: 3.8538, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 106, Loss: 3.3759, Train: 1.0000, Val: 0.7360, Test: 0.7600
Epoch: 107, Loss: 3.5450, Train: 1.0000, Val: 0.7360, Test: 0.7600
Epoch: 108, Loss: 3.5457, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 109, Loss: 3.5784, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 110, Loss: 3.8549, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 111, Loss: 3.6822, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 112, Loss: 3.4786, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 113, Loss: 3.5776, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 114, Loss: 3.5446, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 115, Loss: 3.8190, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 116, Loss: 3.8555, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 117, Loss: 3.4787, Train: 1.0000, Val: 0.7380, Test: 0.7610
Epoch: 118, Loss: 3.6121, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 119, Loss: 3.7168, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 120, Loss: 3.4432, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 121, Loss: 3.8921, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 122, Loss: 3.8873, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 123, Loss: 3.5430, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 124, Loss: 3.5799, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 125, Loss: 3.3731, Train: 1.0000, Val: 0.7400, Test: 0.7630
Epoch: 126, Loss: 3.5778, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 127, Loss: 3.5092, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 128, Loss: 3.5804, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 129, Loss: 3.7156, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 130, Loss: 3.9219, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 131, Loss: 3.8532, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 132, Loss: 3.7500, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 133, Loss: 3.7493, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 134, Loss: 3.8197, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 135, Loss: 4.0612, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 136, Loss: 3.7852, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 137, Loss: 3.6125, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 138, Loss: 3.8534, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 139, Loss: 3.8214, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 140, Loss: 3.7842, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 141, Loss: 3.8867, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 142, Loss: 3.5784, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 143, Loss: 3.7843, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 144, Loss: 3.7155, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 145, Loss: 3.8182, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 146, Loss: 3.5079, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 147, Loss: 3.8189, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 148, Loss: 3.3336, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 149, Loss: 3.5416, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 150, Loss: 3.7141, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 151, Loss: 3.3361, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 152, Loss: 4.0946, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 153, Loss: 3.6811, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 154, Loss: 3.8529, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 155, Loss: 3.4722, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 156, Loss: 3.9214, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 157, Loss: 3.5762, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 158, Loss: 3.3001, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 159, Loss: 3.6830, Train: 1.0000, Val: 0.7460, Test: 0.7620
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 160, Loss: 3.3355, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 161, Loss: 3.8523, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 162, Loss: 3.8872, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 163, Loss: 2.9207, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 164, Loss: 3.4721, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 165, Loss: 3.6796, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 166, Loss: 3.6119, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 167, Loss: 3.5766, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 168, Loss: 3.4726, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 169, Loss: 3.9563, Train: 1.0000, Val: 0.7420, Test: 0.7590
Epoch: 170, Loss: 3.6127, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 171, Loss: 3.8876, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 172, Loss: 3.5082, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 173, Loss: 3.7154, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 174, Loss: 3.8192, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 175, Loss: 3.7485, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 176, Loss: 3.5758, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 177, Loss: 3.8172, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 178, Loss: 3.6801, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 179, Loss: 3.6458, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 180, Loss: 3.4717, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 181, Loss: 3.6097, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 182, Loss: 3.4027, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 183, Loss: 3.8179, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 184, Loss: 3.4734, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 185, Loss: 3.8867, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 186, Loss: 3.6456, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 187, Loss: 3.4715, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 188, Loss: 4.0934, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 189, Loss: 3.6103, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 190, Loss: 3.9559, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 191, Loss: 3.5749, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 192, Loss: 3.6104, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 193, Loss: 3.7831, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 194, Loss: 3.8536, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 195, Loss: 3.5068, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 196, Loss: 3.6792, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 197, Loss: 3.6452, Train: 1.0000, Val: 0.7460, Test: 0.7610
Epoch: 198, Loss: 3.5430, Train: 1.0000, Val: 0.7460, Test: 0.7610
Epoch: 199, Loss: 3.7138, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 200, Loss: 3.8531, Train: 1.0000, Val: 0.7460, Test: 0.7590
MAD:  0.51
Best Test Accuracy: 0.7690, Val Accuracy: 0.7460, Train Accuracy: 1.0000
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8715, Train: 0.0643, Val: 0.0100, Test: 0.0180
Epoch: 2, Loss: 4.8287, Train: 0.1714, Val: 0.0760, Test: 0.0810
Epoch: 3, Loss: 4.7562, Train: 0.3714, Val: 0.1680, Test: 0.1760
Epoch: 4, Loss: 4.7588, Train: 0.4357, Val: 0.2240, Test: 0.2460
Epoch: 5, Loss: 4.6743, Train: 0.5214, Val: 0.2720, Test: 0.2990
Epoch: 6, Loss: 4.6133, Train: 0.5643, Val: 0.2900, Test: 0.3100
Epoch: 7, Loss: 4.5640, Train: 0.6071, Val: 0.3040, Test: 0.3280
Epoch: 8, Loss: 4.3620, Train: 0.6571, Val: 0.3320, Test: 0.3520
Epoch: 9, Loss: 4.3397, Train: 0.7071, Val: 0.3420, Test: 0.3690
Epoch: 10, Loss: 4.1965, Train: 0.7214, Val: 0.3460, Test: 0.3910
Epoch: 11, Loss: 4.2886, Train: 0.7429, Val: 0.3680, Test: 0.4010
Epoch: 12, Loss: 4.1230, Train: 0.7571, Val: 0.3780, Test: 0.4190
Epoch: 13, Loss: 4.3624, Train: 0.7929, Val: 0.4000, Test: 0.4490
Epoch: 14, Loss: 4.2724, Train: 0.8286, Val: 0.4360, Test: 0.4770
Epoch: 15, Loss: 4.1338, Train: 0.8643, Val: 0.4860, Test: 0.5130
Epoch: 16, Loss: 4.0662, Train: 0.8929, Val: 0.5500, Test: 0.5670
Epoch: 17, Loss: 4.1415, Train: 0.9357, Val: 0.5800, Test: 0.6090
Epoch: 18, Loss: 3.7853, Train: 0.9429, Val: 0.6340, Test: 0.6480
Epoch: 19, Loss: 4.0029, Train: 0.9786, Val: 0.6600, Test: 0.6890
Epoch: 20, Loss: 3.7615, Train: 0.9857, Val: 0.6880, Test: 0.7130
Epoch: 21, Loss: 4.1092, Train: 0.9857, Val: 0.6940, Test: 0.7220
Epoch: 22, Loss: 4.2907, Train: 0.9929, Val: 0.7140, Test: 0.7320
Epoch: 23, Loss: 3.7144, Train: 0.9929, Val: 0.7100, Test: 0.7320
Epoch: 24, Loss: 3.7265, Train: 0.9929, Val: 0.7100, Test: 0.7280
Epoch: 25, Loss: 3.9084, Train: 1.0000, Val: 0.7100, Test: 0.7290
Epoch: 26, Loss: 3.9054, Train: 1.0000, Val: 0.7020, Test: 0.7220
Epoch: 27, Loss: 3.8044, Train: 1.0000, Val: 0.7060, Test: 0.7200
Epoch: 28, Loss: 4.0975, Train: 1.0000, Val: 0.7000, Test: 0.7220
Epoch: 29, Loss: 4.0072, Train: 1.0000, Val: 0.6980, Test: 0.7350
Epoch: 30, Loss: 3.6573, Train: 1.0000, Val: 0.7040, Test: 0.7410
Epoch: 31, Loss: 3.7612, Train: 1.0000, Val: 0.7060, Test: 0.7410
Epoch: 32, Loss: 3.6165, Train: 1.0000, Val: 0.7120, Test: 0.7400
Epoch: 33, Loss: 3.5685, Train: 1.0000, Val: 0.7240, Test: 0.7420
Epoch: 34, Loss: 3.5756, Train: 1.0000, Val: 0.7280, Test: 0.7440
Epoch: 35, Loss: 3.6307, Train: 1.0000, Val: 0.7320, Test: 0.7480
Epoch: 36, Loss: 3.8439, Train: 1.0000, Val: 0.7380, Test: 0.7480
Epoch: 37, Loss: 3.6932, Train: 1.0000, Val: 0.7440, Test: 0.7520
Epoch: 38, Loss: 3.7756, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 39, Loss: 3.7316, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 40, Loss: 3.3744, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 41, Loss: 3.4570, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 42, Loss: 3.6162, Train: 1.0000, Val: 0.7520, Test: 0.7650
Epoch: 43, Loss: 3.7401, Train: 1.0000, Val: 0.7580, Test: 0.7640
Epoch: 44, Loss: 3.4958, Train: 1.0000, Val: 0.7580, Test: 0.7640
Epoch: 45, Loss: 3.6267, Train: 1.0000, Val: 0.7580, Test: 0.7650
Epoch: 46, Loss: 3.8864, Train: 1.0000, Val: 0.7600, Test: 0.7670
Epoch: 47, Loss: 3.8637, Train: 1.0000, Val: 0.7600, Test: 0.7670
Epoch: 48, Loss: 3.6745, Train: 1.0000, Val: 0.7620, Test: 0.7680
Epoch: 49, Loss: 3.6756, Train: 1.0000, Val: 0.7640, Test: 0.7690
Epoch: 50, Loss: 3.5733, Train: 1.0000, Val: 0.7640, Test: 0.7690
Epoch: 51, Loss: 3.8469, Train: 1.0000, Val: 0.7620, Test: 0.7710
Epoch: 52, Loss: 3.5391, Train: 1.0000, Val: 0.7640, Test: 0.7720
Epoch: 53, Loss: 3.7781, Train: 1.0000, Val: 0.7660, Test: 0.7700
Epoch: 54, Loss: 3.3960, Train: 1.0000, Val: 0.7680, Test: 0.7690
Epoch: 55, Loss: 3.6396, Train: 1.0000, Val: 0.7680, Test: 0.7670
Epoch: 56, Loss: 3.8742, Train: 1.0000, Val: 0.7700, Test: 0.7670
Epoch: 57, Loss: 3.9374, Train: 1.0000, Val: 0.7700, Test: 0.7670
Epoch: 58, Loss: 3.6322, Train: 1.0000, Val: 0.7700, Test: 0.7700
Epoch: 59, Loss: 3.9056, Train: 1.0000, Val: 0.7720, Test: 0.7720
Epoch: 60, Loss: 3.5906, Train: 1.0000, Val: 0.7680, Test: 0.7700
Epoch: 61, Loss: 3.6633, Train: 1.0000, Val: 0.7680, Test: 0.7680
Epoch: 62, Loss: 3.6358, Train: 1.0000, Val: 0.7680, Test: 0.7690
Epoch: 63, Loss: 3.3889, Train: 1.0000, Val: 0.7640, Test: 0.7690
Epoch: 64, Loss: 3.7276, Train: 1.0000, Val: 0.7620, Test: 0.7700
Epoch: 65, Loss: 3.9667, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 66, Loss: 3.6551, Train: 1.0000, Val: 0.7580, Test: 0.7670
Epoch: 67, Loss: 3.6264, Train: 1.0000, Val: 0.7580, Test: 0.7680
Epoch: 68, Loss: 3.5937, Train: 1.0000, Val: 0.7600, Test: 0.7680
Epoch: 69, Loss: 3.3096, Train: 1.0000, Val: 0.7600, Test: 0.7670
Epoch: 70, Loss: 3.7270, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 71, Loss: 3.4539, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 72, Loss: 3.7248, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 73, Loss: 3.4869, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 74, Loss: 3.7641, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 75, Loss: 3.7950, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 76, Loss: 3.6872, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 77, Loss: 3.6576, Train: 1.0000, Val: 0.7600, Test: 0.7620
Epoch: 78, Loss: 3.8305, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 79, Loss: 3.5508, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 80, Loss: 3.4468, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 81, Loss: 3.8584, Train: 1.0000, Val: 0.7580, Test: 0.7620
Epoch: 82, Loss: 3.6221, Train: 1.0000, Val: 0.7580, Test: 0.7610
Epoch: 83, Loss: 3.8939, Train: 1.0000, Val: 0.7580, Test: 0.7610
Epoch: 84, Loss: 3.5174, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 85, Loss: 3.9609, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 86, Loss: 3.6579, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 87, Loss: 3.6206, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 88, Loss: 3.9262, Train: 1.0000, Val: 0.7580, Test: 0.7600
Epoch: 89, Loss: 3.4426, Train: 1.0000, Val: 0.7600, Test: 0.7600
Epoch: 90, Loss: 3.7560, Train: 1.0000, Val: 0.7600, Test: 0.7610
Epoch: 91, Loss: 3.6138, Train: 1.0000, Val: 0.7600, Test: 0.7610
Epoch: 92, Loss: 3.7509, Train: 1.0000, Val: 0.7600, Test: 0.7620
Epoch: 93, Loss: 3.6845, Train: 1.0000, Val: 0.7600, Test: 0.7620
Epoch: 94, Loss: 3.5824, Train: 1.0000, Val: 0.7580, Test: 0.7620
Epoch: 95, Loss: 3.5825, Train: 1.0000, Val: 0.7600, Test: 0.7640
Epoch: 96, Loss: 3.7888, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 97, Loss: 3.6495, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 98, Loss: 3.5794, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 99, Loss: 3.5452, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 100, Loss: 3.6865, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 101, Loss: 3.4800, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 102, Loss: 3.9931, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 103, Loss: 3.8555, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 104, Loss: 3.5115, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 105, Loss: 3.6143, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 106, Loss: 3.6173, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 107, Loss: 3.7540, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 108, Loss: 3.7550, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 109, Loss: 3.8897, Train: 1.0000, Val: 0.7580, Test: 0.7670
Epoch: 110, Loss: 3.3733, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 111, Loss: 3.2686, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 112, Loss: 3.9602, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 113, Loss: 3.7502, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 114, Loss: 3.8222, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 115, Loss: 3.5794, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 116, Loss: 3.4406, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 117, Loss: 3.7513, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 118, Loss: 3.8205, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 119, Loss: 3.7183, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 120, Loss: 3.6819, Train: 1.0000, Val: 0.7600, Test: 0.7710
Epoch: 121, Loss: 3.4074, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 122, Loss: 3.4081, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 123, Loss: 3.6804, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 124, Loss: 3.6494, Train: 1.0000, Val: 0.7580, Test: 0.7720
Epoch: 125, Loss: 3.6133, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 126, Loss: 3.5765, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 127, Loss: 3.5421, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 128, Loss: 3.3727, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 129, Loss: 3.8540, Train: 1.0000, Val: 0.7580, Test: 0.7680
Epoch: 130, Loss: 3.9582, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 131, Loss: 3.6483, Train: 1.0000, Val: 0.7560, Test: 0.7670
Epoch: 132, Loss: 3.5433, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 133, Loss: 3.7164, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 134, Loss: 3.7508, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 135, Loss: 3.6457, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 136, Loss: 3.6465, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 137, Loss: 3.8542, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 138, Loss: 3.8194, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 139, Loss: 3.7861, Train: 1.0000, Val: 0.7580, Test: 0.7650
Epoch: 140, Loss: 3.7169, Train: 1.0000, Val: 0.7560, Test: 0.7650
Epoch: 141, Loss: 3.5764, Train: 1.0000, Val: 0.7560, Test: 0.7650
Epoch: 142, Loss: 3.8536, Train: 1.0000, Val: 0.7560, Test: 0.7640
Epoch: 143, Loss: 3.6461, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 144, Loss: 3.8551, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 145, Loss: 3.7853, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 146, Loss: 3.3026, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 147, Loss: 3.7837, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 148, Loss: 3.3705, Train: 1.0000, Val: 0.7600, Test: 0.7640
Epoch: 149, Loss: 3.5432, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 150, Loss: 3.5429, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 151, Loss: 4.0259, Train: 1.0000, Val: 0.7600, Test: 0.7670
Epoch: 152, Loss: 3.5085, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 153, Loss: 3.4381, Train: 1.0000, Val: 0.7600, Test: 0.7640
Epoch: 154, Loss: 3.1639, Train: 1.0000, Val: 0.7600, Test: 0.7640
Epoch: 155, Loss: 3.1289, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 156, Loss: 3.5759, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 157, Loss: 3.4044, Train: 1.0000, Val: 0.7580, Test: 0.7660
Epoch: 158, Loss: 3.6112, Train: 1.0000, Val: 0.7580, Test: 0.7670
Epoch: 159, Loss: 3.5760, Train: 1.0000, Val: 0.7560, Test: 0.7670
Epoch: 160, Loss: 3.5773, Train: 1.0000, Val: 0.7560, Test: 0.7670
Epoch: 161, Loss: 3.7150, Train: 1.0000, Val: 0.7580, Test: 0.7680
Epoch: 162, Loss: 4.0258, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 163, Loss: 3.5419, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 164, Loss: 3.6451, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 165, Loss: 3.7492, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 166, Loss: 3.5430, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 167, Loss: 3.6804, Train: 1.0000, Val: 0.7580, Test: 0.7700
Epoch: 168, Loss: 3.7485, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 169, Loss: 3.8520, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 170, Loss: 3.7490, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 171, Loss: 3.5070, Train: 1.0000, Val: 0.7580, Test: 0.7710
Epoch: 172, Loss: 3.4726, Train: 1.0000, Val: 0.7580, Test: 0.7720
Epoch: 173, Loss: 3.8523, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 174, Loss: 3.6453, Train: 1.0000, Val: 0.7580, Test: 0.7690
Epoch: 175, Loss: 3.7142, Train: 1.0000, Val: 0.7560, Test: 0.7690
Epoch: 176, Loss: 3.6458, Train: 1.0000, Val: 0.7560, Test: 0.7700
Epoch: 177, Loss: 3.7488, Train: 1.0000, Val: 0.7560, Test: 0.7700
Epoch: 178, Loss: 3.5771, Train: 1.0000, Val: 0.7560, Test: 0.7700
Epoch: 179, Loss: 3.7140, Train: 1.0000, Val: 0.7560, Test: 0.7700
Epoch: 180, Loss: 3.5067, Train: 1.0000, Val: 0.7560, Test: 0.7710
Epoch: 181, Loss: 3.6111, Train: 1.0000, Val: 0.7560, Test: 0.7710
Epoch: 182, Loss: 3.5767, Train: 1.0000, Val: 0.7600, Test: 0.7710
Epoch: 183, Loss: 3.7838, Train: 1.0000, Val: 0.7600, Test: 0.7700
Epoch: 184, Loss: 3.4719, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 185, Loss: 3.4383, Train: 1.0000, Val: 0.7600, Test: 0.7680
Epoch: 186, Loss: 3.8871, Train: 1.0000, Val: 0.7600, Test: 0.7670
Epoch: 187, Loss: 3.5403, Train: 1.0000, Val: 0.7600, Test: 0.7680
Epoch: 188, Loss: 3.5763, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 189, Loss: 3.3341, Train: 1.0000, Val: 0.7600, Test: 0.7710
Epoch: 190, Loss: 3.7475, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 191, Loss: 3.7836, Train: 1.0000, Val: 0.7620, Test: 0.7690
Epoch: 192, Loss: 3.4040, Train: 1.0000, Val: 0.7620, Test: 0.7690
Epoch: 193, Loss: 3.6460, Train: 1.0000, Val: 0.7620, Test: 0.7690
Epoch: 194, Loss: 3.7823, Train: 1.0000, Val: 0.7620, Test: 0.7690
Epoch: 195, Loss: 3.4375, Train: 1.0000, Val: 0.7600, Test: 0.7680
Epoch: 196, Loss: 4.0255, Train: 1.0000, Val: 0.7600, Test: 0.7690
Epoch: 197, Loss: 3.4722, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 198, Loss: 3.7144, Train: 1.0000, Val: 0.7560, Test: 0.7660
Epoch: 199, Loss: 3.6094, Train: 1.0000, Val: 0.7560, Test: 0.7670
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 200, Loss: 3.5755, Train: 1.0000, Val: 0.7580, Test: 0.7670
MAD:  0.4978
Best Test Accuracy: 0.7720, Val Accuracy: 0.7640, Train Accuracy: 1.0000
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8602, Train: 0.0929, Val: 0.0440, Test: 0.0430
Epoch: 2, Loss: 4.8347, Train: 0.2143, Val: 0.1140, Test: 0.1030
Epoch: 3, Loss: 4.7962, Train: 0.3429, Val: 0.1700, Test: 0.1680
Epoch: 4, Loss: 4.7122, Train: 0.4214, Val: 0.2020, Test: 0.2160
Epoch: 5, Loss: 4.7411, Train: 0.4643, Val: 0.2280, Test: 0.2540
Epoch: 6, Loss: 4.7235, Train: 0.4929, Val: 0.2460, Test: 0.2830
Epoch: 7, Loss: 4.6360, Train: 0.5143, Val: 0.2720, Test: 0.3220
Epoch: 8, Loss: 4.6175, Train: 0.5429, Val: 0.2940, Test: 0.3460
Epoch: 9, Loss: 4.3952, Train: 0.5786, Val: 0.3200, Test: 0.3730
Epoch: 10, Loss: 4.4700, Train: 0.6214, Val: 0.3520, Test: 0.4110
Epoch: 11, Loss: 4.4659, Train: 0.6643, Val: 0.3900, Test: 0.4450
Epoch: 12, Loss: 4.1526, Train: 0.7286, Val: 0.4300, Test: 0.4770
Epoch: 13, Loss: 4.2721, Train: 0.7786, Val: 0.4680, Test: 0.5200
Epoch: 14, Loss: 4.1747, Train: 0.8143, Val: 0.5200, Test: 0.5490
Epoch: 15, Loss: 4.1928, Train: 0.8643, Val: 0.5720, Test: 0.5920
Epoch: 16, Loss: 3.8701, Train: 0.9000, Val: 0.6060, Test: 0.6200
Epoch: 17, Loss: 4.0344, Train: 0.9214, Val: 0.6340, Test: 0.6520
Epoch: 18, Loss: 4.0081, Train: 0.9643, Val: 0.6400, Test: 0.6810
Epoch: 19, Loss: 4.1302, Train: 0.9714, Val: 0.6600, Test: 0.7070
Epoch: 20, Loss: 4.0820, Train: 0.9714, Val: 0.6820, Test: 0.7290
Epoch: 21, Loss: 3.6961, Train: 0.9929, Val: 0.7040, Test: 0.7360
Epoch: 22, Loss: 3.8987, Train: 0.9929, Val: 0.7180, Test: 0.7340
Epoch: 23, Loss: 3.7523, Train: 0.9857, Val: 0.7200, Test: 0.7320
Epoch: 24, Loss: 4.0033, Train: 0.9929, Val: 0.7200, Test: 0.7290
Epoch: 25, Loss: 4.1326, Train: 0.9929, Val: 0.7080, Test: 0.7300
Epoch: 26, Loss: 3.6474, Train: 0.9929, Val: 0.7060, Test: 0.7250
Epoch: 27, Loss: 3.9776, Train: 0.9929, Val: 0.7080, Test: 0.7260
Epoch: 28, Loss: 4.1438, Train: 0.9929, Val: 0.7060, Test: 0.7250
Epoch: 29, Loss: 3.7422, Train: 0.9929, Val: 0.7040, Test: 0.7240
Epoch: 30, Loss: 3.2820, Train: 0.9929, Val: 0.7040, Test: 0.7240
Epoch: 31, Loss: 3.6472, Train: 0.9929, Val: 0.7060, Test: 0.7210
Epoch: 32, Loss: 3.8437, Train: 0.9929, Val: 0.7060, Test: 0.7210
Epoch: 33, Loss: 3.6721, Train: 0.9929, Val: 0.7080, Test: 0.7240
Epoch: 34, Loss: 3.9116, Train: 0.9929, Val: 0.7120, Test: 0.7250
Epoch: 35, Loss: 3.6880, Train: 0.9929, Val: 0.7100, Test: 0.7290
Epoch: 36, Loss: 3.6952, Train: 0.9929, Val: 0.7100, Test: 0.7330
Epoch: 37, Loss: 3.4425, Train: 0.9929, Val: 0.7160, Test: 0.7380
Epoch: 38, Loss: 3.6921, Train: 0.9929, Val: 0.7200, Test: 0.7390
Epoch: 39, Loss: 3.5808, Train: 0.9929, Val: 0.7240, Test: 0.7430
Epoch: 40, Loss: 3.5489, Train: 1.0000, Val: 0.7280, Test: 0.7450
Epoch: 41, Loss: 3.3337, Train: 1.0000, Val: 0.7300, Test: 0.7500
Epoch: 42, Loss: 3.9140, Train: 1.0000, Val: 0.7340, Test: 0.7490
Epoch: 43, Loss: 3.9577, Train: 1.0000, Val: 0.7360, Test: 0.7520
Epoch: 44, Loss: 3.7634, Train: 1.0000, Val: 0.7380, Test: 0.7520
Epoch: 45, Loss: 3.5853, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 46, Loss: 3.5477, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 47, Loss: 3.8191, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 48, Loss: 4.0396, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 49, Loss: 4.1027, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 50, Loss: 3.8480, Train: 1.0000, Val: 0.7420, Test: 0.7670
Epoch: 51, Loss: 3.7090, Train: 1.0000, Val: 0.7420, Test: 0.7660
Epoch: 52, Loss: 3.1960, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 53, Loss: 3.5644, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 54, Loss: 3.7716, Train: 1.0000, Val: 0.7440, Test: 0.7670
Epoch: 55, Loss: 3.6702, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 56, Loss: 3.6019, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 57, Loss: 3.4262, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 58, Loss: 3.3476, Train: 1.0000, Val: 0.7480, Test: 0.7660
Epoch: 59, Loss: 3.5262, Train: 1.0000, Val: 0.7520, Test: 0.7660
Epoch: 60, Loss: 3.7653, Train: 1.0000, Val: 0.7520, Test: 0.7640
Epoch: 61, Loss: 3.5608, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 62, Loss: 3.6646, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 63, Loss: 3.9029, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 64, Loss: 3.8346, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 65, Loss: 3.4860, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 66, Loss: 3.6597, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 67, Loss: 3.9658, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 68, Loss: 3.7582, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 69, Loss: 3.4172, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 70, Loss: 3.7606, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 71, Loss: 3.8970, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 72, Loss: 3.6575, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 73, Loss: 3.4833, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 74, Loss: 3.7572, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 75, Loss: 3.4840, Train: 1.0000, Val: 0.7540, Test: 0.7550
Epoch: 76, Loss: 3.8617, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 77, Loss: 3.4522, Train: 1.0000, Val: 0.7540, Test: 0.7560
Epoch: 78, Loss: 3.8616, Train: 1.0000, Val: 0.7520, Test: 0.7560
Epoch: 79, Loss: 4.0346, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 80, Loss: 3.6543, Train: 1.0000, Val: 0.7520, Test: 0.7560
Epoch: 81, Loss: 3.5835, Train: 1.0000, Val: 0.7520, Test: 0.7530
Epoch: 82, Loss: 3.7197, Train: 1.0000, Val: 0.7520, Test: 0.7540
Epoch: 83, Loss: 3.3805, Train: 1.0000, Val: 0.7520, Test: 0.7540
Epoch: 84, Loss: 3.6892, Train: 1.0000, Val: 0.7500, Test: 0.7540
Epoch: 85, Loss: 3.9280, Train: 1.0000, Val: 0.7500, Test: 0.7550
Epoch: 86, Loss: 3.5806, Train: 1.0000, Val: 0.7540, Test: 0.7530
Epoch: 87, Loss: 3.5127, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 88, Loss: 3.3771, Train: 1.0000, Val: 0.7540, Test: 0.7550
Epoch: 89, Loss: 3.8598, Train: 1.0000, Val: 0.7540, Test: 0.7550
Epoch: 90, Loss: 3.8573, Train: 1.0000, Val: 0.7540, Test: 0.7550
Epoch: 91, Loss: 3.6846, Train: 1.0000, Val: 0.7520, Test: 0.7550
Epoch: 92, Loss: 3.8224, Train: 1.0000, Val: 0.7500, Test: 0.7550
Epoch: 93, Loss: 3.5462, Train: 1.0000, Val: 0.7500, Test: 0.7550
Epoch: 94, Loss: 3.5827, Train: 1.0000, Val: 0.7500, Test: 0.7540
Epoch: 95, Loss: 3.6495, Train: 1.0000, Val: 0.7520, Test: 0.7520
Epoch: 96, Loss: 3.6516, Train: 1.0000, Val: 0.7520, Test: 0.7510
Epoch: 97, Loss: 3.5498, Train: 1.0000, Val: 0.7500, Test: 0.7510
Epoch: 98, Loss: 3.6843, Train: 1.0000, Val: 0.7500, Test: 0.7510
Epoch: 99, Loss: 3.7200, Train: 1.0000, Val: 0.7460, Test: 0.7510
Epoch: 100, Loss: 3.4429, Train: 1.0000, Val: 0.7460, Test: 0.7510
Epoch: 101, Loss: 3.6491, Train: 1.0000, Val: 0.7460, Test: 0.7510
Epoch: 102, Loss: 3.6503, Train: 1.0000, Val: 0.7460, Test: 0.7510
Epoch: 103, Loss: 3.4776, Train: 1.0000, Val: 0.7440, Test: 0.7520
Epoch: 104, Loss: 3.4748, Train: 1.0000, Val: 0.7440, Test: 0.7530
Epoch: 105, Loss: 3.7877, Train: 1.0000, Val: 0.7440, Test: 0.7550
Epoch: 106, Loss: 3.7167, Train: 1.0000, Val: 0.7460, Test: 0.7550
Epoch: 107, Loss: 3.8198, Train: 1.0000, Val: 0.7460, Test: 0.7550
Epoch: 108, Loss: 3.6473, Train: 1.0000, Val: 0.7460, Test: 0.7550
Epoch: 109, Loss: 3.7859, Train: 1.0000, Val: 0.7460, Test: 0.7550
Epoch: 110, Loss: 3.7200, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 111, Loss: 3.6117, Train: 1.0000, Val: 0.7420, Test: 0.7540
Epoch: 112, Loss: 3.6149, Train: 1.0000, Val: 0.7400, Test: 0.7540
Epoch: 113, Loss: 3.6827, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 114, Loss: 3.3372, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 115, Loss: 3.5096, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 116, Loss: 3.5792, Train: 1.0000, Val: 0.7440, Test: 0.7570
Epoch: 117, Loss: 3.5803, Train: 1.0000, Val: 0.7440, Test: 0.7570
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 118, Loss: 3.8561, Train: 1.0000, Val: 0.7440, Test: 0.7570
Epoch: 119, Loss: 3.9562, Train: 1.0000, Val: 0.7440, Test: 0.7570
Epoch: 120, Loss: 3.6120, Train: 1.0000, Val: 0.7440, Test: 0.7570
Epoch: 121, Loss: 3.6807, Train: 1.0000, Val: 0.7440, Test: 0.7570
Epoch: 122, Loss: 3.5100, Train: 1.0000, Val: 0.7440, Test: 0.7570
Epoch: 123, Loss: 3.7860, Train: 1.0000, Val: 0.7460, Test: 0.7570
Epoch: 124, Loss: 3.6846, Train: 1.0000, Val: 0.7460, Test: 0.7570
Epoch: 125, Loss: 3.8555, Train: 1.0000, Val: 0.7460, Test: 0.7570
Epoch: 126, Loss: 3.8549, Train: 1.0000, Val: 0.7460, Test: 0.7570
Epoch: 127, Loss: 3.3360, Train: 1.0000, Val: 0.7480, Test: 0.7580
Epoch: 128, Loss: 3.5108, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 129, Loss: 3.5807, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 130, Loss: 3.4053, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 131, Loss: 3.8552, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 132, Loss: 3.7165, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 133, Loss: 3.5794, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 134, Loss: 3.7167, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 135, Loss: 3.7155, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 136, Loss: 3.8540, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 137, Loss: 3.7158, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 138, Loss: 3.8899, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 139, Loss: 3.8196, Train: 1.0000, Val: 0.7560, Test: 0.7590
Epoch: 140, Loss: 3.4051, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 141, Loss: 3.5086, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 142, Loss: 3.6447, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 143, Loss: 3.9225, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 144, Loss: 3.6808, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 145, Loss: 3.6811, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 146, Loss: 3.3697, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 147, Loss: 3.5068, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 148, Loss: 4.1288, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 149, Loss: 3.4036, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 150, Loss: 3.5083, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 151, Loss: 3.7492, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 152, Loss: 3.5777, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 153, Loss: 3.6809, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 154, Loss: 3.6110, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 155, Loss: 3.8870, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 156, Loss: 3.4748, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 157, Loss: 3.4743, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 158, Loss: 3.7505, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 159, Loss: 3.7498, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 160, Loss: 3.3362, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 161, Loss: 3.4738, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 162, Loss: 3.4033, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 163, Loss: 3.7487, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 164, Loss: 3.7154, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 165, Loss: 3.7494, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 166, Loss: 3.6451, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 167, Loss: 3.8531, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 168, Loss: 3.4729, Train: 1.0000, Val: 0.7520, Test: 0.7560
Epoch: 169, Loss: 3.8195, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 170, Loss: 3.6803, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 171, Loss: 3.5415, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 172, Loss: 3.6816, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 173, Loss: 3.7143, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 174, Loss: 3.4728, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 175, Loss: 3.5760, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 176, Loss: 3.6811, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 177, Loss: 3.3006, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 178, Loss: 3.7138, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 179, Loss: 3.6803, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 180, Loss: 3.4725, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 181, Loss: 3.7479, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 182, Loss: 3.6790, Train: 1.0000, Val: 0.7480, Test: 0.7580
Epoch: 183, Loss: 3.3693, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 184, Loss: 3.6109, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 185, Loss: 3.7825, Train: 1.0000, Val: 0.7480, Test: 0.7570
Epoch: 186, Loss: 3.7487, Train: 1.0000, Val: 0.7460, Test: 0.7550
Epoch: 187, Loss: 3.8179, Train: 1.0000, Val: 0.7480, Test: 0.7550
Epoch: 188, Loss: 3.7152, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 189, Loss: 3.6126, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 190, Loss: 3.6114, Train: 1.0000, Val: 0.7480, Test: 0.7520
Epoch: 191, Loss: 3.8515, Train: 1.0000, Val: 0.7480, Test: 0.7520
Epoch: 192, Loss: 3.9219, Train: 1.0000, Val: 0.7480, Test: 0.7520
Epoch: 193, Loss: 3.8175, Train: 1.0000, Val: 0.7480, Test: 0.7530
Epoch: 194, Loss: 3.6106, Train: 1.0000, Val: 0.7480, Test: 0.7540
Epoch: 195, Loss: 3.6787, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 196, Loss: 3.7136, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 197, Loss: 3.5752, Train: 1.0000, Val: 0.7480, Test: 0.7530
Epoch: 198, Loss: 3.7482, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 199, Loss: 3.6452, Train: 1.0000, Val: 0.7440, Test: 0.7530
Epoch: 200, Loss: 3.4372, Train: 1.0000, Val: 0.7440, Test: 0.7530
MAD:  0.5193
Best Test Accuracy: 0.7680, Val Accuracy: 0.7460, Train Accuracy: 1.0000
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8346, Train: 0.1071, Val: 0.0380, Test: 0.0550
Epoch: 2, Loss: 4.7786, Train: 0.2643, Val: 0.1020, Test: 0.1150
Epoch: 3, Loss: 4.7857, Train: 0.3357, Val: 0.1320, Test: 0.1510
Epoch: 4, Loss: 4.6762, Train: 0.4500, Val: 0.1660, Test: 0.1850
Epoch: 5, Loss: 4.6208, Train: 0.5143, Val: 0.2040, Test: 0.2180
Epoch: 6, Loss: 4.5674, Train: 0.5857, Val: 0.2400, Test: 0.2580
Epoch: 7, Loss: 4.6115, Train: 0.6143, Val: 0.2880, Test: 0.2850
Epoch: 8, Loss: 4.4832, Train: 0.6357, Val: 0.3180, Test: 0.3220
Epoch: 9, Loss: 4.2604, Train: 0.6714, Val: 0.3440, Test: 0.3550
Epoch: 10, Loss: 4.2543, Train: 0.7143, Val: 0.3820, Test: 0.3880
Epoch: 11, Loss: 4.2269, Train: 0.7786, Val: 0.4200, Test: 0.4240
Epoch: 12, Loss: 4.1018, Train: 0.8286, Val: 0.4460, Test: 0.4580
Epoch: 13, Loss: 4.3021, Train: 0.8571, Val: 0.4840, Test: 0.4880
Epoch: 14, Loss: 3.8865, Train: 0.8714, Val: 0.5140, Test: 0.5150
Epoch: 15, Loss: 4.1352, Train: 0.8929, Val: 0.5360, Test: 0.5330
Epoch: 16, Loss: 3.8622, Train: 0.9071, Val: 0.5580, Test: 0.5530
Epoch: 17, Loss: 4.1159, Train: 0.9429, Val: 0.5760, Test: 0.5740
Epoch: 18, Loss: 3.8682, Train: 0.9714, Val: 0.6060, Test: 0.5970
Epoch: 19, Loss: 4.1723, Train: 0.9857, Val: 0.6500, Test: 0.6340
Epoch: 20, Loss: 3.9311, Train: 0.9857, Val: 0.6840, Test: 0.6670
Epoch: 21, Loss: 4.0578, Train: 0.9929, Val: 0.7080, Test: 0.6870
Epoch: 22, Loss: 3.9984, Train: 0.9857, Val: 0.7120, Test: 0.7030
Epoch: 23, Loss: 3.7690, Train: 0.9857, Val: 0.7240, Test: 0.7240
Epoch: 24, Loss: 3.7198, Train: 0.9929, Val: 0.7380, Test: 0.7320
Epoch: 25, Loss: 3.9912, Train: 0.9929, Val: 0.7380, Test: 0.7430
Epoch: 26, Loss: 3.6556, Train: 0.9929, Val: 0.7440, Test: 0.7490
Epoch: 27, Loss: 3.9630, Train: 0.9929, Val: 0.7460, Test: 0.7520
Epoch: 28, Loss: 3.6147, Train: 0.9929, Val: 0.7460, Test: 0.7540
Epoch: 29, Loss: 3.4443, Train: 0.9929, Val: 0.7480, Test: 0.7590
Epoch: 30, Loss: 3.6737, Train: 0.9929, Val: 0.7500, Test: 0.7620
Epoch: 31, Loss: 3.7365, Train: 0.9929, Val: 0.7540, Test: 0.7620
Epoch: 32, Loss: 3.9326, Train: 0.9929, Val: 0.7540, Test: 0.7620
Epoch: 33, Loss: 3.6336, Train: 0.9929, Val: 0.7560, Test: 0.7620
Epoch: 34, Loss: 3.6258, Train: 0.9929, Val: 0.7560, Test: 0.7650
Epoch: 35, Loss: 3.7657, Train: 0.9929, Val: 0.7440, Test: 0.7630
Epoch: 36, Loss: 3.8038, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 37, Loss: 3.9217, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 38, Loss: 3.6706, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 39, Loss: 3.7796, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 40, Loss: 3.9267, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 41, Loss: 3.6912, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 42, Loss: 3.9324, Train: 1.0000, Val: 0.7460, Test: 0.7610
Epoch: 43, Loss: 3.6313, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 44, Loss: 3.7137, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 45, Loss: 3.7957, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 46, Loss: 3.6586, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 47, Loss: 3.5512, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 48, Loss: 3.6424, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 49, Loss: 3.5713, Train: 1.0000, Val: 0.7560, Test: 0.7570
Epoch: 50, Loss: 3.7091, Train: 1.0000, Val: 0.7560, Test: 0.7520
Epoch: 51, Loss: 3.5078, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 52, Loss: 3.6392, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 53, Loss: 3.6532, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 54, Loss: 3.9817, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 55, Loss: 3.5732, Train: 1.0000, Val: 0.7560, Test: 0.7550
Epoch: 56, Loss: 3.4271, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 57, Loss: 3.5455, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 58, Loss: 3.3238, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 59, Loss: 3.7729, Train: 1.0000, Val: 0.7540, Test: 0.7550
Epoch: 60, Loss: 3.4965, Train: 1.0000, Val: 0.7540, Test: 0.7570
Epoch: 61, Loss: 3.5581, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 62, Loss: 3.8338, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 63, Loss: 3.5919, Train: 1.0000, Val: 0.7560, Test: 0.7660
Epoch: 64, Loss: 3.7965, Train: 1.0000, Val: 0.7620, Test: 0.7630
Epoch: 65, Loss: 3.8314, Train: 1.0000, Val: 0.7600, Test: 0.7660
Epoch: 66, Loss: 3.4935, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 67, Loss: 3.4509, Train: 1.0000, Val: 0.7560, Test: 0.7640
Epoch: 68, Loss: 3.7590, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 69, Loss: 3.8004, Train: 1.0000, Val: 0.7600, Test: 0.7630
Epoch: 70, Loss: 3.5552, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 71, Loss: 3.9367, Train: 1.0000, Val: 0.7620, Test: 0.7620
Epoch: 72, Loss: 3.6896, Train: 1.0000, Val: 0.7620, Test: 0.7630
Epoch: 73, Loss: 3.6897, Train: 1.0000, Val: 0.7620, Test: 0.7640
Epoch: 74, Loss: 3.6882, Train: 1.0000, Val: 0.7620, Test: 0.7650
Epoch: 75, Loss: 3.6543, Train: 1.0000, Val: 0.7640, Test: 0.7640
Epoch: 76, Loss: 3.7213, Train: 1.0000, Val: 0.7640, Test: 0.7650
Epoch: 77, Loss: 3.5184, Train: 1.0000, Val: 0.7640, Test: 0.7650
Epoch: 78, Loss: 3.3830, Train: 1.0000, Val: 0.7660, Test: 0.7650
Epoch: 79, Loss: 3.8613, Train: 1.0000, Val: 0.7640, Test: 0.7650
Epoch: 80, Loss: 3.3424, Train: 1.0000, Val: 0.7600, Test: 0.7650
Epoch: 81, Loss: 3.6523, Train: 1.0000, Val: 0.7560, Test: 0.7650
Epoch: 82, Loss: 3.7225, Train: 1.0000, Val: 0.7540, Test: 0.7640
Epoch: 83, Loss: 3.7920, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 84, Loss: 3.7920, Train: 1.0000, Val: 0.7500, Test: 0.7660
Epoch: 85, Loss: 3.6523, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 86, Loss: 3.8577, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 87, Loss: 3.9294, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 88, Loss: 3.8266, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 89, Loss: 3.6889, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 90, Loss: 3.8229, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 91, Loss: 3.6865, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 92, Loss: 3.6193, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 93, Loss: 3.6524, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 94, Loss: 3.7890, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 95, Loss: 3.8566, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 96, Loss: 3.6860, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 97, Loss: 3.8935, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 98, Loss: 3.7183, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 99, Loss: 3.3418, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 100, Loss: 3.5110, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 101, Loss: 3.6835, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 102, Loss: 3.7537, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 103, Loss: 3.7536, Train: 1.0000, Val: 0.7500, Test: 0.7580
Epoch: 104, Loss: 3.6824, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 105, Loss: 3.3037, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 106, Loss: 3.6833, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 107, Loss: 3.8572, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 108, Loss: 3.5807, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 109, Loss: 3.8888, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 110, Loss: 3.3715, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 111, Loss: 3.1639, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 112, Loss: 3.8205, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 113, Loss: 3.4408, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 114, Loss: 3.6477, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 115, Loss: 3.5798, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 116, Loss: 3.8559, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 117, Loss: 3.6477, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 118, Loss: 3.4067, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 119, Loss: 3.9244, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 120, Loss: 3.6478, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 121, Loss: 3.2672, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 122, Loss: 3.7170, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 123, Loss: 3.5784, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 124, Loss: 3.3716, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 125, Loss: 3.7849, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 126, Loss: 3.5443, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 127, Loss: 3.3017, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 128, Loss: 3.9238, Train: 1.0000, Val: 0.7540, Test: 0.7630
Epoch: 129, Loss: 3.6473, Train: 1.0000, Val: 0.7540, Test: 0.7640
Epoch: 130, Loss: 3.4390, Train: 1.0000, Val: 0.7580, Test: 0.7620
Epoch: 131, Loss: 3.7497, Train: 1.0000, Val: 0.7580, Test: 0.7630
Epoch: 132, Loss: 3.8536, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 133, Loss: 3.7511, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 134, Loss: 3.5781, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 135, Loss: 3.8199, Train: 1.0000, Val: 0.7560, Test: 0.7620
Epoch: 136, Loss: 3.5770, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 137, Loss: 3.9233, Train: 1.0000, Val: 0.7560, Test: 0.7620
Epoch: 138, Loss: 3.4768, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 139, Loss: 3.5439, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 140, Loss: 3.5424, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 141, Loss: 3.8169, Train: 1.0000, Val: 0.7520, Test: 0.7640
Epoch: 142, Loss: 3.5757, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 143, Loss: 3.4380, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 144, Loss: 3.7502, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 145, Loss: 3.5765, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 146, Loss: 3.8519, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 147, Loss: 3.6471, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 148, Loss: 3.4745, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 149, Loss: 3.5421, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 150, Loss: 3.7165, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 151, Loss: 3.6117, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 152, Loss: 2.9891, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 153, Loss: 3.3004, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 154, Loss: 3.9569, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 155, Loss: 3.1974, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 156, Loss: 3.6118, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 157, Loss: 3.4738, Train: 1.0000, Val: 0.7520, Test: 0.7610
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 158, Loss: 3.8181, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 159, Loss: 3.4738, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 160, Loss: 3.5423, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 161, Loss: 3.8533, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 162, Loss: 3.7145, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 163, Loss: 3.6453, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 164, Loss: 3.6109, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 165, Loss: 3.3690, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 166, Loss: 3.6808, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 167, Loss: 3.4380, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 168, Loss: 3.5418, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 169, Loss: 3.5064, Train: 1.0000, Val: 0.7540, Test: 0.7610
Epoch: 170, Loss: 3.6462, Train: 1.0000, Val: 0.7560, Test: 0.7620
Epoch: 171, Loss: 3.5410, Train: 1.0000, Val: 0.7560, Test: 0.7630
Epoch: 172, Loss: 3.4371, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 173, Loss: 3.8179, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 174, Loss: 3.5064, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 175, Loss: 3.7836, Train: 1.0000, Val: 0.7520, Test: 0.7640
Epoch: 176, Loss: 3.6102, Train: 1.0000, Val: 0.7520, Test: 0.7630
Epoch: 177, Loss: 3.5421, Train: 1.0000, Val: 0.7540, Test: 0.7630
Epoch: 178, Loss: 3.5425, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 179, Loss: 3.8169, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 180, Loss: 3.6449, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 181, Loss: 3.5406, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 182, Loss: 3.5767, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 183, Loss: 3.6438, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 184, Loss: 3.7841, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 185, Loss: 3.3695, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 186, Loss: 3.4714, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 187, Loss: 3.2310, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 188, Loss: 3.8181, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 189, Loss: 3.8524, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 190, Loss: 3.6457, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 191, Loss: 3.7476, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 192, Loss: 3.7136, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 193, Loss: 3.4722, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 194, Loss: 3.6446, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 195, Loss: 3.5412, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 196, Loss: 4.0602, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 197, Loss: 3.7819, Train: 1.0000, Val: 0.7500, Test: 0.7650
Epoch: 198, Loss: 3.6449, Train: 1.0000, Val: 0.7540, Test: 0.7650
Epoch: 199, Loss: 3.5074, Train: 1.0000, Val: 0.7540, Test: 0.7640
Epoch: 200, Loss: 3.4375, Train: 1.0000, Val: 0.7500, Test: 0.7640
MAD:  0.5398
Best Test Accuracy: 0.7660, Val Accuracy: 0.7560, Train Accuracy: 1.0000
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8799, Train: 0.0429, Val: 0.0120, Test: 0.0080
Epoch: 2, Loss: 4.8586, Train: 0.0929, Val: 0.0380, Test: 0.0380
Epoch: 3, Loss: 4.8422, Train: 0.2357, Val: 0.1040, Test: 0.1030
Epoch: 4, Loss: 4.7704, Train: 0.3500, Val: 0.1660, Test: 0.1760
Epoch: 5, Loss: 4.6926, Train: 0.4429, Val: 0.2260, Test: 0.2220
Epoch: 6, Loss: 4.7135, Train: 0.5286, Val: 0.2620, Test: 0.2660
Epoch: 7, Loss: 4.5455, Train: 0.5714, Val: 0.2920, Test: 0.3060
Epoch: 8, Loss: 4.4958, Train: 0.6143, Val: 0.3220, Test: 0.3350
Epoch: 9, Loss: 4.5161, Train: 0.6643, Val: 0.3500, Test: 0.3680
Epoch: 10, Loss: 4.3065, Train: 0.7071, Val: 0.3780, Test: 0.3800
Epoch: 11, Loss: 4.2998, Train: 0.7143, Val: 0.3960, Test: 0.4010
Epoch: 12, Loss: 4.1156, Train: 0.7429, Val: 0.4200, Test: 0.4150
Epoch: 13, Loss: 4.0929, Train: 0.7571, Val: 0.4300, Test: 0.4360
Epoch: 14, Loss: 4.2642, Train: 0.7786, Val: 0.4380, Test: 0.4590
Epoch: 15, Loss: 4.0640, Train: 0.8000, Val: 0.4500, Test: 0.4740
Epoch: 16, Loss: 4.3206, Train: 0.8143, Val: 0.4800, Test: 0.4980
Epoch: 17, Loss: 4.3096, Train: 0.8429, Val: 0.5060, Test: 0.5110
Epoch: 18, Loss: 4.0275, Train: 0.8714, Val: 0.5320, Test: 0.5260
Epoch: 19, Loss: 4.1844, Train: 0.8857, Val: 0.5400, Test: 0.5410
Epoch: 20, Loss: 3.9078, Train: 0.9000, Val: 0.5620, Test: 0.5700
Epoch: 21, Loss: 4.1496, Train: 0.9143, Val: 0.5940, Test: 0.6010
Epoch: 22, Loss: 4.1708, Train: 0.9429, Val: 0.6180, Test: 0.6280
Epoch: 23, Loss: 3.7562, Train: 0.9500, Val: 0.6440, Test: 0.6460
Epoch: 24, Loss: 4.2574, Train: 0.9571, Val: 0.6740, Test: 0.6710
Epoch: 25, Loss: 3.7531, Train: 0.9786, Val: 0.7000, Test: 0.6910
Epoch: 26, Loss: 3.8410, Train: 1.0000, Val: 0.7100, Test: 0.7130
Epoch: 27, Loss: 3.4406, Train: 1.0000, Val: 0.7300, Test: 0.7430
Epoch: 28, Loss: 3.9610, Train: 1.0000, Val: 0.7380, Test: 0.7470
Epoch: 29, Loss: 4.0721, Train: 1.0000, Val: 0.7360, Test: 0.7510
Epoch: 30, Loss: 3.6637, Train: 1.0000, Val: 0.7380, Test: 0.7550
Epoch: 31, Loss: 3.3987, Train: 1.0000, Val: 0.7420, Test: 0.7530
Epoch: 32, Loss: 3.8408, Train: 1.0000, Val: 0.7460, Test: 0.7560
Epoch: 33, Loss: 3.9624, Train: 1.0000, Val: 0.7480, Test: 0.7590
Epoch: 34, Loss: 3.6483, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 35, Loss: 3.8831, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 36, Loss: 3.4467, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 37, Loss: 3.4843, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 38, Loss: 3.8231, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 39, Loss: 3.6480, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 40, Loss: 3.9198, Train: 1.0000, Val: 0.7440, Test: 0.7650
Epoch: 41, Loss: 3.8567, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 42, Loss: 3.6011, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 43, Loss: 3.4913, Train: 1.0000, Val: 0.7480, Test: 0.7730
Epoch: 44, Loss: 3.7573, Train: 1.0000, Val: 0.7480, Test: 0.7710
Epoch: 45, Loss: 3.9738, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 46, Loss: 3.7097, Train: 1.0000, Val: 0.7360, Test: 0.7610
Epoch: 47, Loss: 3.6174, Train: 1.0000, Val: 0.7320, Test: 0.7550
Epoch: 48, Loss: 3.8803, Train: 1.0000, Val: 0.7300, Test: 0.7580
Epoch: 49, Loss: 3.8637, Train: 1.0000, Val: 0.7260, Test: 0.7520
Epoch: 50, Loss: 3.8211, Train: 1.0000, Val: 0.7240, Test: 0.7480
Epoch: 51, Loss: 3.7815, Train: 1.0000, Val: 0.7220, Test: 0.7490
Epoch: 52, Loss: 3.7334, Train: 1.0000, Val: 0.7220, Test: 0.7520
Epoch: 53, Loss: 3.4500, Train: 1.0000, Val: 0.7220, Test: 0.7530
Epoch: 54, Loss: 3.8795, Train: 1.0000, Val: 0.7220, Test: 0.7520
Epoch: 55, Loss: 3.9186, Train: 1.0000, Val: 0.7200, Test: 0.7520
Epoch: 56, Loss: 3.7319, Train: 1.0000, Val: 0.7240, Test: 0.7540
Epoch: 57, Loss: 3.4953, Train: 1.0000, Val: 0.7240, Test: 0.7530
Epoch: 58, Loss: 3.5622, Train: 1.0000, Val: 0.7220, Test: 0.7540
Epoch: 59, Loss: 3.7326, Train: 1.0000, Val: 0.7200, Test: 0.7530
Epoch: 60, Loss: 3.8433, Train: 1.0000, Val: 0.7220, Test: 0.7540
Epoch: 61, Loss: 3.6979, Train: 1.0000, Val: 0.7220, Test: 0.7520
Epoch: 62, Loss: 3.6283, Train: 1.0000, Val: 0.7220, Test: 0.7530
Epoch: 63, Loss: 3.5585, Train: 1.0000, Val: 0.7260, Test: 0.7530
Epoch: 64, Loss: 3.8334, Train: 1.0000, Val: 0.7280, Test: 0.7550
Epoch: 65, Loss: 4.0425, Train: 1.0000, Val: 0.7320, Test: 0.7540
Epoch: 66, Loss: 3.6927, Train: 1.0000, Val: 0.7300, Test: 0.7560
Epoch: 67, Loss: 4.0472, Train: 1.0000, Val: 0.7300, Test: 0.7550
Epoch: 68, Loss: 3.7993, Train: 1.0000, Val: 0.7320, Test: 0.7570
Epoch: 69, Loss: 3.6928, Train: 1.0000, Val: 0.7320, Test: 0.7590
Epoch: 70, Loss: 3.4866, Train: 1.0000, Val: 0.7340, Test: 0.7590
Epoch: 71, Loss: 3.8017, Train: 1.0000, Val: 0.7380, Test: 0.7600
Epoch: 72, Loss: 3.8293, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 73, Loss: 3.8303, Train: 1.0000, Val: 0.7380, Test: 0.7620
Epoch: 74, Loss: 3.9012, Train: 1.0000, Val: 0.7360, Test: 0.7610
Epoch: 75, Loss: 3.6242, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 76, Loss: 3.7266, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 77, Loss: 3.9666, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 78, Loss: 3.6899, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 79, Loss: 3.3084, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 80, Loss: 3.5857, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 81, Loss: 4.0691, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 82, Loss: 3.4817, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 83, Loss: 3.8250, Train: 1.0000, Val: 0.7460, Test: 0.7610
Epoch: 84, Loss: 3.7573, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 85, Loss: 3.7252, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 86, Loss: 3.6520, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 87, Loss: 3.5167, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 88, Loss: 3.4132, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 89, Loss: 3.7246, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 90, Loss: 3.6871, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 91, Loss: 3.6866, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 92, Loss: 3.7901, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 93, Loss: 3.6173, Train: 1.0000, Val: 0.7420, Test: 0.7620
Epoch: 94, Loss: 3.6203, Train: 1.0000, Val: 0.7400, Test: 0.7600
Epoch: 95, Loss: 3.7554, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 96, Loss: 3.7557, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 97, Loss: 3.7209, Train: 1.0000, Val: 0.7380, Test: 0.7570
Epoch: 98, Loss: 4.2003, Train: 1.0000, Val: 0.7360, Test: 0.7560
Epoch: 99, Loss: 3.7864, Train: 1.0000, Val: 0.7340, Test: 0.7540
Epoch: 100, Loss: 3.5452, Train: 1.0000, Val: 0.7340, Test: 0.7530
Epoch: 101, Loss: 3.4074, Train: 1.0000, Val: 0.7340, Test: 0.7530
Epoch: 102, Loss: 3.7184, Train: 1.0000, Val: 0.7340, Test: 0.7530
Epoch: 103, Loss: 3.8230, Train: 1.0000, Val: 0.7340, Test: 0.7530
Epoch: 104, Loss: 3.5129, Train: 1.0000, Val: 0.7340, Test: 0.7530
Epoch: 105, Loss: 3.5137, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 106, Loss: 3.8572, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 107, Loss: 3.8575, Train: 1.0000, Val: 0.7400, Test: 0.7530
Epoch: 108, Loss: 3.8589, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 109, Loss: 3.6827, Train: 1.0000, Val: 0.7420, Test: 0.7570
Epoch: 110, Loss: 3.5788, Train: 1.0000, Val: 0.7420, Test: 0.7560
Epoch: 111, Loss: 3.6508, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 112, Loss: 3.5488, Train: 1.0000, Val: 0.7400, Test: 0.7580
Epoch: 113, Loss: 3.4419, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 114, Loss: 3.1997, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 115, Loss: 3.5783, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 116, Loss: 3.8554, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 117, Loss: 3.5445, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 118, Loss: 3.4429, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 119, Loss: 3.5110, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 120, Loss: 3.6840, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 121, Loss: 3.7168, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 122, Loss: 3.3723, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 123, Loss: 3.8183, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 124, Loss: 3.6494, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 125, Loss: 3.7853, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 126, Loss: 3.5083, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 127, Loss: 3.7173, Train: 1.0000, Val: 0.7420, Test: 0.7570
Epoch: 128, Loss: 3.7163, Train: 1.0000, Val: 0.7420, Test: 0.7580
Epoch: 129, Loss: 3.7166, Train: 1.0000, Val: 0.7420, Test: 0.7580
Epoch: 130, Loss: 3.5803, Train: 1.0000, Val: 0.7420, Test: 0.7580
Epoch: 131, Loss: 3.7520, Train: 1.0000, Val: 0.7420, Test: 0.7580
Epoch: 132, Loss: 3.5097, Train: 1.0000, Val: 0.7420, Test: 0.7590
Epoch: 133, Loss: 3.6809, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 134, Loss: 3.7181, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 135, Loss: 3.6832, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 136, Loss: 3.8185, Train: 1.0000, Val: 0.7440, Test: 0.7620
Epoch: 137, Loss: 3.6807, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 138, Loss: 3.7519, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 139, Loss: 3.8888, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 140, Loss: 3.7151, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 141, Loss: 3.6475, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 142, Loss: 3.5106, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 143, Loss: 3.3700, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 144, Loss: 3.4399, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 145, Loss: 3.4743, Train: 1.0000, Val: 0.7440, Test: 0.7640
Epoch: 146, Loss: 3.7146, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 147, Loss: 3.6811, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 148, Loss: 3.8542, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 149, Loss: 3.4735, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 150, Loss: 3.5432, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 151, Loss: 3.8536, Train: 1.0000, Val: 0.7480, Test: 0.7640
Epoch: 152, Loss: 3.3710, Train: 1.0000, Val: 0.7480, Test: 0.7640
Epoch: 153, Loss: 3.5091, Train: 1.0000, Val: 0.7480, Test: 0.7640
Epoch: 154, Loss: 3.5421, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 155, Loss: 3.7164, Train: 1.0000, Val: 0.7460, Test: 0.7650
Epoch: 156, Loss: 3.5757, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 157, Loss: 3.4040, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 158, Loss: 3.6441, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 159, Loss: 3.5422, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 160, Loss: 3.6799, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 161, Loss: 3.3356, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 162, Loss: 4.0596, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 163, Loss: 3.5426, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 164, Loss: 4.0605, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 165, Loss: 3.7140, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 166, Loss: 3.6136, Train: 1.0000, Val: 0.7440, Test: 0.7630
Epoch: 167, Loss: 3.6110, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 168, Loss: 3.6448, Train: 1.0000, Val: 0.7460, Test: 0.7630
Epoch: 169, Loss: 3.6120, Train: 1.0000, Val: 0.7460, Test: 0.7660
Epoch: 170, Loss: 3.7153, Train: 1.0000, Val: 0.7480, Test: 0.7640
Epoch: 171, Loss: 3.5775, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 172, Loss: 3.4721, Train: 1.0000, Val: 0.7460, Test: 0.7640
Epoch: 173, Loss: 3.6465, Train: 1.0000, Val: 0.7460, Test: 0.7620
Epoch: 174, Loss: 3.6113, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 175, Loss: 3.4729, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 176, Loss: 3.3690, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 177, Loss: 3.3336, Train: 1.0000, Val: 0.7480, Test: 0.7640
Epoch: 178, Loss: 3.5778, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 179, Loss: 3.7143, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 180, Loss: 3.6107, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 181, Loss: 3.5761, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 182, Loss: 3.5416, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 183, Loss: 3.8529, Train: 1.0000, Val: 0.7500, Test: 0.7630
Epoch: 184, Loss: 3.6109, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 185, Loss: 3.8186, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 186, Loss: 3.6454, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 187, Loss: 3.7140, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 188, Loss: 3.6443, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 189, Loss: 3.6789, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 190, Loss: 3.6803, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 191, Loss: 3.7828, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 192, Loss: 3.5066, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 193, Loss: 3.5067, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 194, Loss: 3.8171, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 195, Loss: 3.8179, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 196, Loss: 3.6105, Train: 1.0000, Val: 0.7480, Test: 0.7630
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 197, Loss: 3.6796, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 198, Loss: 3.8165, Train: 1.0000, Val: 0.7500, Test: 0.7640
Epoch: 199, Loss: 3.5757, Train: 1.0000, Val: 0.7520, Test: 0.7640
Epoch: 200, Loss: 3.6102, Train: 1.0000, Val: 0.7500, Test: 0.7640
MAD:  0.3817
Best Test Accuracy: 0.7730, Val Accuracy: 0.7480, Train Accuracy: 1.0000
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8493, Train: 0.1500, Val: 0.0760, Test: 0.1040
Epoch: 2, Loss: 4.7835, Train: 0.3071, Val: 0.1420, Test: 0.1840
Epoch: 3, Loss: 4.7569, Train: 0.4500, Val: 0.2220, Test: 0.2650
Epoch: 4, Loss: 4.6504, Train: 0.5286, Val: 0.2960, Test: 0.3240
Epoch: 5, Loss: 4.5879, Train: 0.5786, Val: 0.3420, Test: 0.3620
Epoch: 6, Loss: 4.5304, Train: 0.6357, Val: 0.3640, Test: 0.3960
Epoch: 7, Loss: 4.4994, Train: 0.7000, Val: 0.3980, Test: 0.4130
Epoch: 8, Loss: 4.3554, Train: 0.7214, Val: 0.4180, Test: 0.4420
Epoch: 9, Loss: 4.4339, Train: 0.7500, Val: 0.4480, Test: 0.4640
Epoch: 10, Loss: 4.4335, Train: 0.7714, Val: 0.4820, Test: 0.4930
Epoch: 11, Loss: 4.2167, Train: 0.7857, Val: 0.5120, Test: 0.5250
Epoch: 12, Loss: 4.2060, Train: 0.7929, Val: 0.5260, Test: 0.5430
Epoch: 13, Loss: 4.1685, Train: 0.8000, Val: 0.5480, Test: 0.5700
Epoch: 14, Loss: 4.0754, Train: 0.8357, Val: 0.5740, Test: 0.6100
Epoch: 15, Loss: 4.1789, Train: 0.8786, Val: 0.6000, Test: 0.6200
Epoch: 16, Loss: 3.9851, Train: 0.9000, Val: 0.6400, Test: 0.6410
Epoch: 17, Loss: 3.8888, Train: 0.9286, Val: 0.6580, Test: 0.6720
Epoch: 18, Loss: 3.8937, Train: 0.9500, Val: 0.6680, Test: 0.6910
Epoch: 19, Loss: 3.6069, Train: 0.9500, Val: 0.6800, Test: 0.7130
Epoch: 20, Loss: 4.0600, Train: 0.9786, Val: 0.7060, Test: 0.7280
Epoch: 21, Loss: 3.9075, Train: 0.9857, Val: 0.7200, Test: 0.7410
Epoch: 22, Loss: 4.0435, Train: 0.9929, Val: 0.7200, Test: 0.7450
Epoch: 23, Loss: 3.7319, Train: 0.9929, Val: 0.7280, Test: 0.7450
Epoch: 24, Loss: 3.8578, Train: 0.9929, Val: 0.7340, Test: 0.7420
Epoch: 25, Loss: 3.7178, Train: 1.0000, Val: 0.7400, Test: 0.7420
Epoch: 26, Loss: 3.7140, Train: 1.0000, Val: 0.7400, Test: 0.7370
Epoch: 27, Loss: 3.5575, Train: 1.0000, Val: 0.7360, Test: 0.7390
Epoch: 28, Loss: 3.5685, Train: 1.0000, Val: 0.7260, Test: 0.7340
Epoch: 29, Loss: 3.8767, Train: 1.0000, Val: 0.7300, Test: 0.7330
Epoch: 30, Loss: 3.9896, Train: 1.0000, Val: 0.7320, Test: 0.7290
Epoch: 31, Loss: 3.6207, Train: 1.0000, Val: 0.7340, Test: 0.7260
Epoch: 32, Loss: 3.9607, Train: 1.0000, Val: 0.7260, Test: 0.7280
Epoch: 33, Loss: 4.0686, Train: 1.0000, Val: 0.7280, Test: 0.7280
Epoch: 34, Loss: 3.6896, Train: 1.0000, Val: 0.7340, Test: 0.7340
Epoch: 35, Loss: 3.7728, Train: 1.0000, Val: 0.7380, Test: 0.7350
Epoch: 36, Loss: 3.6794, Train: 1.0000, Val: 0.7380, Test: 0.7380
Epoch: 37, Loss: 3.7053, Train: 1.0000, Val: 0.7380, Test: 0.7420
Epoch: 38, Loss: 3.5149, Train: 1.0000, Val: 0.7380, Test: 0.7400
Epoch: 39, Loss: 3.7108, Train: 1.0000, Val: 0.7400, Test: 0.7400
Epoch: 40, Loss: 3.4713, Train: 1.0000, Val: 0.7400, Test: 0.7390
Epoch: 41, Loss: 3.8686, Train: 1.0000, Val: 0.7420, Test: 0.7450
Epoch: 42, Loss: 3.8359, Train: 1.0000, Val: 0.7420, Test: 0.7440
Epoch: 43, Loss: 3.2313, Train: 1.0000, Val: 0.7440, Test: 0.7420
Epoch: 44, Loss: 3.7311, Train: 1.0000, Val: 0.7460, Test: 0.7440
Epoch: 45, Loss: 3.6640, Train: 1.0000, Val: 0.7460, Test: 0.7430
Epoch: 46, Loss: 3.9001, Train: 1.0000, Val: 0.7420, Test: 0.7480
Epoch: 47, Loss: 3.3005, Train: 1.0000, Val: 0.7500, Test: 0.7500
Epoch: 48, Loss: 3.6239, Train: 1.0000, Val: 0.7500, Test: 0.7490
Epoch: 49, Loss: 4.0462, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 50, Loss: 3.5390, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 51, Loss: 3.6632, Train: 1.0000, Val: 0.7580, Test: 0.7530
Epoch: 52, Loss: 3.9401, Train: 1.0000, Val: 0.7560, Test: 0.7520
Epoch: 53, Loss: 3.4964, Train: 1.0000, Val: 0.7540, Test: 0.7500
Epoch: 54, Loss: 3.9011, Train: 1.0000, Val: 0.7600, Test: 0.7510
Epoch: 55, Loss: 3.7138, Train: 1.0000, Val: 0.7600, Test: 0.7520
Epoch: 56, Loss: 3.8736, Train: 1.0000, Val: 0.7580, Test: 0.7530
Epoch: 57, Loss: 4.0090, Train: 1.0000, Val: 0.7580, Test: 0.7550
Epoch: 58, Loss: 3.8538, Train: 1.0000, Val: 0.7580, Test: 0.7540
Epoch: 59, Loss: 3.5557, Train: 1.0000, Val: 0.7580, Test: 0.7540
Epoch: 60, Loss: 3.9035, Train: 1.0000, Val: 0.7580, Test: 0.7560
Epoch: 61, Loss: 3.9067, Train: 1.0000, Val: 0.7620, Test: 0.7570
Epoch: 62, Loss: 3.5260, Train: 1.0000, Val: 0.7600, Test: 0.7570
Epoch: 63, Loss: 3.5590, Train: 1.0000, Val: 0.7640, Test: 0.7550
Epoch: 64, Loss: 3.9142, Train: 1.0000, Val: 0.7640, Test: 0.7570
Epoch: 65, Loss: 3.9353, Train: 1.0000, Val: 0.7680, Test: 0.7580
Epoch: 66, Loss: 3.4249, Train: 1.0000, Val: 0.7680, Test: 0.7580
Epoch: 67, Loss: 3.4873, Train: 1.0000, Val: 0.7680, Test: 0.7560
Epoch: 68, Loss: 3.7358, Train: 1.0000, Val: 0.7680, Test: 0.7550
Epoch: 69, Loss: 3.4166, Train: 1.0000, Val: 0.7660, Test: 0.7530
Epoch: 70, Loss: 3.6917, Train: 1.0000, Val: 0.7680, Test: 0.7540
Epoch: 71, Loss: 3.7576, Train: 1.0000, Val: 0.7680, Test: 0.7540
Epoch: 72, Loss: 3.6607, Train: 1.0000, Val: 0.7680, Test: 0.7550
Epoch: 73, Loss: 3.5876, Train: 1.0000, Val: 0.7660, Test: 0.7560
Epoch: 74, Loss: 3.7541, Train: 1.0000, Val: 0.7640, Test: 0.7540
Epoch: 75, Loss: 3.8986, Train: 1.0000, Val: 0.7620, Test: 0.7540
Epoch: 76, Loss: 3.5598, Train: 1.0000, Val: 0.7620, Test: 0.7550
Epoch: 77, Loss: 3.4134, Train: 1.0000, Val: 0.7600, Test: 0.7550
Epoch: 78, Loss: 3.5882, Train: 1.0000, Val: 0.7600, Test: 0.7570
Epoch: 79, Loss: 3.5837, Train: 1.0000, Val: 0.7580, Test: 0.7570
Epoch: 80, Loss: 3.6884, Train: 1.0000, Val: 0.7580, Test: 0.7560
Epoch: 81, Loss: 3.6544, Train: 1.0000, Val: 0.7580, Test: 0.7550
Epoch: 82, Loss: 3.5557, Train: 1.0000, Val: 0.7560, Test: 0.7550
Epoch: 83, Loss: 3.8213, Train: 1.0000, Val: 0.7580, Test: 0.7540
Epoch: 84, Loss: 3.6526, Train: 1.0000, Val: 0.7580, Test: 0.7550
Epoch: 85, Loss: 3.6490, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 86, Loss: 3.9262, Train: 1.0000, Val: 0.7560, Test: 0.7550
Epoch: 87, Loss: 3.6164, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 88, Loss: 3.8564, Train: 1.0000, Val: 0.7560, Test: 0.7530
Epoch: 89, Loss: 3.6859, Train: 1.0000, Val: 0.7560, Test: 0.7520
Epoch: 90, Loss: 3.5466, Train: 1.0000, Val: 0.7560, Test: 0.7510
Epoch: 91, Loss: 3.8219, Train: 1.0000, Val: 0.7540, Test: 0.7510
Epoch: 92, Loss: 3.5464, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 93, Loss: 3.8577, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 94, Loss: 3.7549, Train: 1.0000, Val: 0.7540, Test: 0.7510
Epoch: 95, Loss: 3.8243, Train: 1.0000, Val: 0.7540, Test: 0.7500
Epoch: 96, Loss: 3.8933, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 97, Loss: 3.6848, Train: 1.0000, Val: 0.7560, Test: 0.7510
Epoch: 98, Loss: 3.9927, Train: 1.0000, Val: 0.7580, Test: 0.7510
Epoch: 99, Loss: 3.4781, Train: 1.0000, Val: 0.7580, Test: 0.7510
Epoch: 100, Loss: 3.5454, Train: 1.0000, Val: 0.7580, Test: 0.7520
Epoch: 101, Loss: 3.7859, Train: 1.0000, Val: 0.7580, Test: 0.7530
Epoch: 102, Loss: 4.0279, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 103, Loss: 3.5834, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 104, Loss: 3.7180, Train: 1.0000, Val: 0.7540, Test: 0.7540
Epoch: 105, Loss: 3.6145, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 106, Loss: 3.5467, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 107, Loss: 3.7513, Train: 1.0000, Val: 0.7560, Test: 0.7530
Epoch: 108, Loss: 3.5094, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 109, Loss: 3.5815, Train: 1.0000, Val: 0.7540, Test: 0.7500
Epoch: 110, Loss: 3.6824, Train: 1.0000, Val: 0.7540, Test: 0.7510
Epoch: 111, Loss: 4.0960, Train: 1.0000, Val: 0.7540, Test: 0.7510
Epoch: 112, Loss: 3.5795, Train: 1.0000, Val: 0.7540, Test: 0.7530
Epoch: 113, Loss: 3.6825, Train: 1.0000, Val: 0.7540, Test: 0.7530
Epoch: 114, Loss: 3.5093, Train: 1.0000, Val: 0.7560, Test: 0.7540
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 115, Loss: 3.6481, Train: 1.0000, Val: 0.7560, Test: 0.7530
Epoch: 116, Loss: 3.6497, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 117, Loss: 3.3032, Train: 1.0000, Val: 0.7560, Test: 0.7550
Epoch: 118, Loss: 3.4408, Train: 1.0000, Val: 0.7580, Test: 0.7550
Epoch: 119, Loss: 3.6151, Train: 1.0000, Val: 0.7580, Test: 0.7540
Epoch: 120, Loss: 3.2359, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 121, Loss: 3.6819, Train: 1.0000, Val: 0.7540, Test: 0.7550
Epoch: 122, Loss: 3.7507, Train: 1.0000, Val: 0.7560, Test: 0.7590
Epoch: 123, Loss: 3.7866, Train: 1.0000, Val: 0.7560, Test: 0.7590
Epoch: 124, Loss: 3.4058, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 125, Loss: 3.4405, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 126, Loss: 3.6476, Train: 1.0000, Val: 0.7520, Test: 0.7590
Epoch: 127, Loss: 3.6464, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 128, Loss: 3.6142, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 129, Loss: 3.8890, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 130, Loss: 3.5102, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 131, Loss: 3.3354, Train: 1.0000, Val: 0.7560, Test: 0.7570
Epoch: 132, Loss: 3.4732, Train: 1.0000, Val: 0.7560, Test: 0.7570
Epoch: 133, Loss: 3.4392, Train: 1.0000, Val: 0.7560, Test: 0.7560
Epoch: 134, Loss: 3.4765, Train: 1.0000, Val: 0.7560, Test: 0.7570
Epoch: 135, Loss: 3.6143, Train: 1.0000, Val: 0.7560, Test: 0.7570
Epoch: 136, Loss: 3.7842, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 137, Loss: 3.8194, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 138, Loss: 3.4755, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 139, Loss: 3.7147, Train: 1.0000, Val: 0.7560, Test: 0.7580
Epoch: 140, Loss: 3.6470, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 141, Loss: 3.7493, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 142, Loss: 3.5447, Train: 1.0000, Val: 0.7540, Test: 0.7590
Epoch: 143, Loss: 3.4060, Train: 1.0000, Val: 0.7540, Test: 0.7600
Epoch: 144, Loss: 3.3008, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 145, Loss: 3.4044, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 146, Loss: 3.6809, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 147, Loss: 3.5768, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 148, Loss: 3.3377, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 149, Loss: 3.7507, Train: 1.0000, Val: 0.7540, Test: 0.7580
Epoch: 150, Loss: 3.3699, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 151, Loss: 3.7152, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 152, Loss: 3.4047, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 153, Loss: 3.4042, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 154, Loss: 3.7503, Train: 1.0000, Val: 0.7520, Test: 0.7580
Epoch: 155, Loss: 3.8873, Train: 1.0000, Val: 0.7520, Test: 0.7570
Epoch: 156, Loss: 3.7490, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 157, Loss: 3.6450, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 158, Loss: 3.7496, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 159, Loss: 3.6808, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 160, Loss: 3.7500, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 161, Loss: 3.3351, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 162, Loss: 3.7141, Train: 1.0000, Val: 0.7500, Test: 0.7570
Epoch: 163, Loss: 3.7136, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 164, Loss: 3.5073, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 165, Loss: 3.8180, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 166, Loss: 3.6114, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 167, Loss: 3.6800, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 168, Loss: 3.7840, Train: 1.0000, Val: 0.7500, Test: 0.7590
Epoch: 169, Loss: 3.6102, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 170, Loss: 3.3671, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 171, Loss: 3.5764, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 172, Loss: 3.5763, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 173, Loss: 3.9207, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 174, Loss: 3.7143, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 175, Loss: 3.6454, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 176, Loss: 3.5065, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 177, Loss: 3.9572, Train: 1.0000, Val: 0.7480, Test: 0.7610
Epoch: 178, Loss: 3.8177, Train: 1.0000, Val: 0.7480, Test: 0.7600
Epoch: 179, Loss: 3.5748, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 180, Loss: 3.8521, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 181, Loss: 3.9210, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 182, Loss: 3.8516, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 183, Loss: 3.5419, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 184, Loss: 3.2991, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 185, Loss: 3.6102, Train: 1.0000, Val: 0.7520, Test: 0.7610
Epoch: 186, Loss: 3.8867, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 187, Loss: 3.4378, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 188, Loss: 3.7842, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 189, Loss: 3.4033, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 190, Loss: 3.6442, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 191, Loss: 3.7141, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 192, Loss: 3.7134, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 193, Loss: 3.6113, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 194, Loss: 3.4371, Train: 1.0000, Val: 0.7520, Test: 0.7600
Epoch: 195, Loss: 3.5063, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 196, Loss: 3.9209, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 197, Loss: 3.6789, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 198, Loss: 3.5076, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 199, Loss: 3.7137, Train: 1.0000, Val: 0.7500, Test: 0.7600
Epoch: 200, Loss: 3.7149, Train: 1.0000, Val: 0.7500, Test: 0.7600
MAD:  0.6131
Best Test Accuracy: 0.7620, Val Accuracy: 0.7480, Train Accuracy: 1.0000
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8638, Train: 0.1143, Val: 0.0180, Test: 0.0350
Epoch: 2, Loss: 4.8208, Train: 0.2286, Val: 0.0740, Test: 0.1040
Epoch: 3, Loss: 4.7934, Train: 0.3357, Val: 0.1760, Test: 0.1930
Epoch: 4, Loss: 4.6954, Train: 0.4357, Val: 0.2540, Test: 0.2630
Epoch: 5, Loss: 4.6644, Train: 0.4714, Val: 0.3000, Test: 0.3270
Epoch: 6, Loss: 4.6469, Train: 0.5071, Val: 0.3240, Test: 0.3660
Epoch: 7, Loss: 4.5754, Train: 0.5214, Val: 0.3360, Test: 0.3840
Epoch: 8, Loss: 4.5411, Train: 0.5286, Val: 0.3500, Test: 0.3890
Epoch: 9, Loss: 4.5739, Train: 0.5286, Val: 0.3560, Test: 0.4090
Epoch: 10, Loss: 4.4153, Train: 0.5643, Val: 0.3780, Test: 0.4190
Epoch: 11, Loss: 4.4149, Train: 0.6143, Val: 0.3880, Test: 0.4330
Epoch: 12, Loss: 4.3599, Train: 0.6286, Val: 0.4140, Test: 0.4460
Epoch: 13, Loss: 3.9876, Train: 0.6786, Val: 0.4340, Test: 0.4740
Epoch: 14, Loss: 4.3588, Train: 0.7071, Val: 0.4420, Test: 0.4890
Epoch: 15, Loss: 4.1775, Train: 0.7286, Val: 0.4640, Test: 0.5060
Epoch: 16, Loss: 4.0034, Train: 0.7714, Val: 0.4840, Test: 0.5300
Epoch: 17, Loss: 3.9071, Train: 0.7786, Val: 0.5120, Test: 0.5580
Epoch: 18, Loss: 3.9513, Train: 0.8071, Val: 0.5380, Test: 0.5870
Epoch: 19, Loss: 4.1244, Train: 0.8143, Val: 0.5520, Test: 0.6040
Epoch: 20, Loss: 4.2560, Train: 0.8214, Val: 0.5660, Test: 0.6280
Epoch: 21, Loss: 4.0574, Train: 0.8357, Val: 0.5840, Test: 0.6430
Epoch: 22, Loss: 4.0923, Train: 0.8429, Val: 0.5860, Test: 0.6500
Epoch: 23, Loss: 3.8382, Train: 0.8429, Val: 0.5840, Test: 0.6540
Epoch: 24, Loss: 4.2381, Train: 0.8429, Val: 0.5940, Test: 0.6490
Epoch: 25, Loss: 4.0665, Train: 0.8429, Val: 0.5940, Test: 0.6500
Epoch: 26, Loss: 4.0085, Train: 0.8500, Val: 0.5900, Test: 0.6490
Epoch: 27, Loss: 3.7517, Train: 0.8500, Val: 0.5920, Test: 0.6460
Epoch: 28, Loss: 3.9506, Train: 0.8500, Val: 0.5940, Test: 0.6500
Epoch: 29, Loss: 3.7103, Train: 0.8571, Val: 0.5940, Test: 0.6490
Epoch: 30, Loss: 3.7660, Train: 0.8571, Val: 0.5980, Test: 0.6490
Epoch: 31, Loss: 3.7642, Train: 0.8571, Val: 0.6060, Test: 0.6500
Epoch: 32, Loss: 3.8154, Train: 0.8643, Val: 0.6120, Test: 0.6560
Epoch: 33, Loss: 3.8868, Train: 0.8643, Val: 0.6140, Test: 0.6630
Epoch: 34, Loss: 3.9589, Train: 0.8786, Val: 0.6140, Test: 0.6660
Epoch: 35, Loss: 3.4923, Train: 0.8857, Val: 0.6180, Test: 0.6680
Epoch: 36, Loss: 4.1085, Train: 0.9214, Val: 0.6220, Test: 0.6680
Epoch: 37, Loss: 3.8625, Train: 0.9286, Val: 0.6240, Test: 0.6720
Epoch: 38, Loss: 3.9536, Train: 0.9429, Val: 0.6320, Test: 0.6780
Epoch: 39, Loss: 3.9607, Train: 0.9500, Val: 0.6420, Test: 0.6850
Epoch: 40, Loss: 3.8818, Train: 0.9714, Val: 0.6540, Test: 0.6980
Epoch: 41, Loss: 3.6824, Train: 0.9786, Val: 0.6740, Test: 0.7210
Epoch: 42, Loss: 3.7086, Train: 0.9929, Val: 0.6900, Test: 0.7390
Epoch: 43, Loss: 3.7346, Train: 1.0000, Val: 0.7140, Test: 0.7420
Epoch: 44, Loss: 3.5620, Train: 1.0000, Val: 0.7220, Test: 0.7470
Epoch: 45, Loss: 3.6864, Train: 1.0000, Val: 0.7160, Test: 0.7480
Epoch: 46, Loss: 3.5008, Train: 1.0000, Val: 0.7160, Test: 0.7500
Epoch: 47, Loss: 3.7589, Train: 1.0000, Val: 0.7060, Test: 0.7410
Epoch: 48, Loss: 3.8655, Train: 1.0000, Val: 0.7080, Test: 0.7350
Epoch: 49, Loss: 3.9368, Train: 1.0000, Val: 0.7060, Test: 0.7230
Epoch: 50, Loss: 3.8799, Train: 1.0000, Val: 0.7100, Test: 0.7240
Epoch: 51, Loss: 3.4323, Train: 1.0000, Val: 0.7100, Test: 0.7210
Epoch: 52, Loss: 3.7572, Train: 1.0000, Val: 0.7100, Test: 0.7170
Epoch: 53, Loss: 3.7167, Train: 1.0000, Val: 0.7100, Test: 0.7210
Epoch: 54, Loss: 3.5124, Train: 1.0000, Val: 0.7140, Test: 0.7270
Epoch: 55, Loss: 3.7518, Train: 1.0000, Val: 0.7200, Test: 0.7320
Epoch: 56, Loss: 3.8590, Train: 1.0000, Val: 0.7220, Test: 0.7400
Epoch: 57, Loss: 3.8521, Train: 1.0000, Val: 0.7200, Test: 0.7440
Epoch: 58, Loss: 3.8655, Train: 1.0000, Val: 0.7220, Test: 0.7510
Epoch: 59, Loss: 3.6177, Train: 1.0000, Val: 0.7240, Test: 0.7520
Epoch: 60, Loss: 3.5577, Train: 1.0000, Val: 0.7380, Test: 0.7570
Epoch: 61, Loss: 3.3592, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 62, Loss: 3.6547, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 63, Loss: 3.5072, Train: 1.0000, Val: 0.7500, Test: 0.7610
Epoch: 64, Loss: 3.6182, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 65, Loss: 3.4942, Train: 1.0000, Val: 0.7500, Test: 0.7620
Epoch: 66, Loss: 3.7621, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 67, Loss: 3.4078, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 68, Loss: 3.5347, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 69, Loss: 3.6995, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 70, Loss: 3.8283, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 71, Loss: 3.8629, Train: 1.0000, Val: 0.7460, Test: 0.7670
Epoch: 72, Loss: 3.4133, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 73, Loss: 3.6864, Train: 1.0000, Val: 0.7480, Test: 0.7680
Epoch: 74, Loss: 3.4529, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 75, Loss: 3.5949, Train: 1.0000, Val: 0.7500, Test: 0.7700
Epoch: 76, Loss: 3.6584, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 77, Loss: 3.3118, Train: 1.0000, Val: 0.7460, Test: 0.7680
Epoch: 78, Loss: 3.9396, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 79, Loss: 3.7186, Train: 1.0000, Val: 0.7440, Test: 0.7680
Epoch: 80, Loss: 3.6546, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 81, Loss: 3.6182, Train: 1.0000, Val: 0.7440, Test: 0.7660
Epoch: 82, Loss: 3.4920, Train: 1.0000, Val: 0.7420, Test: 0.7650
Epoch: 83, Loss: 3.4874, Train: 1.0000, Val: 0.7420, Test: 0.7640
Epoch: 84, Loss: 3.6868, Train: 1.0000, Val: 0.7400, Test: 0.7640
Epoch: 85, Loss: 3.8671, Train: 1.0000, Val: 0.7400, Test: 0.7660
Epoch: 86, Loss: 3.5571, Train: 1.0000, Val: 0.7380, Test: 0.7650
Epoch: 87, Loss: 3.2790, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 88, Loss: 3.6512, Train: 1.0000, Val: 0.7400, Test: 0.7620
Epoch: 89, Loss: 3.5886, Train: 1.0000, Val: 0.7420, Test: 0.7630
Epoch: 90, Loss: 3.6824, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 91, Loss: 3.5811, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 92, Loss: 3.6148, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 93, Loss: 3.7527, Train: 1.0000, Val: 0.7480, Test: 0.7620
Epoch: 94, Loss: 3.6837, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 95, Loss: 3.8914, Train: 1.0000, Val: 0.7520, Test: 0.7620
Epoch: 96, Loss: 3.8598, Train: 1.0000, Val: 0.7540, Test: 0.7630
Epoch: 97, Loss: 3.7942, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 98, Loss: 3.4414, Train: 1.0000, Val: 0.7540, Test: 0.7620
Epoch: 99, Loss: 3.8920, Train: 1.0000, Val: 0.7540, Test: 0.7630
Epoch: 100, Loss: 3.5825, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 101, Loss: 3.7511, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 102, Loss: 3.5787, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 103, Loss: 3.3084, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 104, Loss: 3.7175, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 105, Loss: 3.7521, Train: 1.0000, Val: 0.7480, Test: 0.7630
Epoch: 106, Loss: 3.5806, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 107, Loss: 3.5442, Train: 1.0000, Val: 0.7480, Test: 0.7650
Epoch: 108, Loss: 3.7893, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 109, Loss: 3.6835, Train: 1.0000, Val: 0.7500, Test: 0.7660
Epoch: 110, Loss: 3.6831, Train: 1.0000, Val: 0.7500, Test: 0.7660
Epoch: 111, Loss: 3.7510, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 112, Loss: 3.6858, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 113, Loss: 3.6136, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 114, Loss: 3.7204, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 115, Loss: 3.8534, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 116, Loss: 3.4405, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 117, Loss: 3.5092, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 118, Loss: 3.5784, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 119, Loss: 3.6126, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 120, Loss: 3.5097, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 121, Loss: 3.8555, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 122, Loss: 3.6817, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 123, Loss: 3.4440, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 124, Loss: 3.8882, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 125, Loss: 3.6845, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 126, Loss: 3.8551, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 127, Loss: 3.3747, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 128, Loss: 3.7150, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 129, Loss: 3.7538, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 130, Loss: 3.6812, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 131, Loss: 3.5083, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 132, Loss: 3.7506, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 133, Loss: 3.6462, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 134, Loss: 3.6817, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 135, Loss: 3.9246, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 136, Loss: 3.5766, Train: 1.0000, Val: 0.7500, Test: 0.7670
Epoch: 137, Loss: 3.8916, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 138, Loss: 3.6812, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 139, Loss: 3.8543, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 140, Loss: 3.5081, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 141, Loss: 3.7526, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 142, Loss: 3.8180, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 143, Loss: 3.7853, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 144, Loss: 3.7844, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 145, Loss: 3.6460, Train: 1.0000, Val: 0.7520, Test: 0.7670
Epoch: 146, Loss: 3.4387, Train: 1.0000, Val: 0.7520, Test: 0.7670
Epoch: 147, Loss: 3.9574, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 148, Loss: 3.5416, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 149, Loss: 3.6799, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 150, Loss: 3.4723, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 151, Loss: 3.5438, Train: 1.0000, Val: 0.7520, Test: 0.7680
Epoch: 152, Loss: 3.7490, Train: 1.0000, Val: 0.7500, Test: 0.7680
Epoch: 153, Loss: 3.6457, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 154, Loss: 3.6471, Train: 1.0000, Val: 0.7520, Test: 0.7690
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 155, Loss: 3.8198, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 156, Loss: 3.7157, Train: 1.0000, Val: 0.7520, Test: 0.7700
Epoch: 157, Loss: 3.6465, Train: 1.0000, Val: 0.7520, Test: 0.7690
Epoch: 158, Loss: 3.4396, Train: 1.0000, Val: 0.7520, Test: 0.7700
Epoch: 159, Loss: 3.9210, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 160, Loss: 3.6121, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 161, Loss: 3.9590, Train: 1.0000, Val: 0.7460, Test: 0.7690
Epoch: 162, Loss: 3.7830, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 163, Loss: 4.0264, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 164, Loss: 3.7159, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 165, Loss: 3.7484, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 166, Loss: 3.9565, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 167, Loss: 3.5080, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 168, Loss: 3.6457, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 169, Loss: 3.3707, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 170, Loss: 3.8195, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 171, Loss: 3.5786, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 172, Loss: 3.4030, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 173, Loss: 3.4379, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 174, Loss: 3.7852, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 175, Loss: 3.5081, Train: 1.0000, Val: 0.7440, Test: 0.7690
Epoch: 176, Loss: 4.0604, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 177, Loss: 3.4721, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 178, Loss: 3.5082, Train: 1.0000, Val: 0.7480, Test: 0.7700
Epoch: 179, Loss: 3.6792, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 180, Loss: 3.6101, Train: 1.0000, Val: 0.7480, Test: 0.7690
Epoch: 181, Loss: 3.8524, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 182, Loss: 3.5061, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 183, Loss: 3.8866, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 184, Loss: 3.5418, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 185, Loss: 3.7142, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 186, Loss: 3.8520, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 187, Loss: 3.5748, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 188, Loss: 3.1970, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 189, Loss: 3.6791, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 190, Loss: 3.6462, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 191, Loss: 4.0584, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 192, Loss: 3.7831, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 193, Loss: 3.8537, Train: 1.0000, Val: 0.7460, Test: 0.7700
Epoch: 194, Loss: 3.7826, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 195, Loss: 3.6449, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 196, Loss: 3.7148, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 197, Loss: 3.6100, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 198, Loss: 3.8176, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 199, Loss: 3.6801, Train: 1.0000, Val: 0.7460, Test: 0.7710
Epoch: 200, Loss: 3.4382, Train: 1.0000, Val: 0.7460, Test: 0.7710
MAD:  0.6262
Best Test Accuracy: 0.7710, Val Accuracy: 0.7460, Train Accuracy: 1.0000
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8359, Train: 0.1571, Val: 0.0560, Test: 0.0540
Epoch: 2, Loss: 4.7949, Train: 0.3429, Val: 0.1660, Test: 0.1670
Epoch: 3, Loss: 4.6956, Train: 0.4429, Val: 0.2760, Test: 0.2900
Epoch: 4, Loss: 4.6770, Train: 0.5500, Val: 0.3560, Test: 0.3810
Epoch: 5, Loss: 4.5727, Train: 0.6000, Val: 0.4020, Test: 0.4440
Epoch: 6, Loss: 4.5517, Train: 0.6500, Val: 0.4240, Test: 0.4730
Epoch: 7, Loss: 4.5324, Train: 0.7143, Val: 0.4640, Test: 0.4980
Epoch: 8, Loss: 4.3657, Train: 0.7571, Val: 0.4740, Test: 0.5150
Epoch: 9, Loss: 4.4965, Train: 0.8000, Val: 0.5000, Test: 0.5410
Epoch: 10, Loss: 4.2131, Train: 0.8071, Val: 0.5080, Test: 0.5550
Epoch: 11, Loss: 4.2953, Train: 0.8143, Val: 0.5160, Test: 0.5620
Epoch: 12, Loss: 4.2566, Train: 0.8357, Val: 0.5180, Test: 0.5600
Epoch: 13, Loss: 4.0935, Train: 0.8500, Val: 0.5180, Test: 0.5610
Epoch: 14, Loss: 4.1158, Train: 0.8571, Val: 0.5200, Test: 0.5630
Epoch: 15, Loss: 3.9155, Train: 0.8643, Val: 0.5260, Test: 0.5590
Epoch: 16, Loss: 3.9383, Train: 0.8643, Val: 0.5400, Test: 0.5670
Epoch: 17, Loss: 3.9465, Train: 0.8714, Val: 0.5680, Test: 0.5910
Epoch: 18, Loss: 3.8550, Train: 0.8929, Val: 0.5800, Test: 0.6060
Epoch: 19, Loss: 3.9793, Train: 0.9214, Val: 0.5940, Test: 0.6160
Epoch: 20, Loss: 3.8118, Train: 0.9429, Val: 0.6140, Test: 0.6270
Epoch: 21, Loss: 4.0524, Train: 0.9500, Val: 0.6440, Test: 0.6490
Epoch: 22, Loss: 3.6561, Train: 0.9643, Val: 0.6580, Test: 0.6660
Epoch: 23, Loss: 3.6808, Train: 0.9857, Val: 0.6800, Test: 0.7000
Epoch: 24, Loss: 4.0614, Train: 0.9929, Val: 0.6900, Test: 0.7220
Epoch: 25, Loss: 3.8151, Train: 1.0000, Val: 0.7020, Test: 0.7340
Epoch: 26, Loss: 3.7982, Train: 1.0000, Val: 0.7220, Test: 0.7350
Epoch: 27, Loss: 4.1159, Train: 1.0000, Val: 0.7260, Test: 0.7360
Epoch: 28, Loss: 4.0972, Train: 1.0000, Val: 0.7260, Test: 0.7380
Epoch: 29, Loss: 3.7437, Train: 1.0000, Val: 0.7220, Test: 0.7400
Epoch: 30, Loss: 3.9095, Train: 1.0000, Val: 0.7280, Test: 0.7410
Epoch: 31, Loss: 3.9349, Train: 1.0000, Val: 0.7320, Test: 0.7410
Epoch: 32, Loss: 4.1522, Train: 1.0000, Val: 0.7320, Test: 0.7430
Epoch: 33, Loss: 3.8895, Train: 1.0000, Val: 0.7340, Test: 0.7430
Epoch: 34, Loss: 4.0116, Train: 1.0000, Val: 0.7300, Test: 0.7430
Epoch: 35, Loss: 3.7140, Train: 1.0000, Val: 0.7340, Test: 0.7480
Epoch: 36, Loss: 3.8112, Train: 1.0000, Val: 0.7340, Test: 0.7490
Epoch: 37, Loss: 3.7970, Train: 1.0000, Val: 0.7380, Test: 0.7540
Epoch: 38, Loss: 3.8923, Train: 1.0000, Val: 0.7420, Test: 0.7510
Epoch: 39, Loss: 3.6165, Train: 1.0000, Val: 0.7440, Test: 0.7540
Epoch: 40, Loss: 3.7431, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 41, Loss: 3.6330, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 42, Loss: 3.9774, Train: 1.0000, Val: 0.7440, Test: 0.7550
Epoch: 43, Loss: 3.8891, Train: 1.0000, Val: 0.7460, Test: 0.7540
Epoch: 44, Loss: 3.9942, Train: 1.0000, Val: 0.7460, Test: 0.7520
Epoch: 45, Loss: 3.8215, Train: 1.0000, Val: 0.7460, Test: 0.7540
Epoch: 46, Loss: 3.7827, Train: 1.0000, Val: 0.7440, Test: 0.7520
Epoch: 47, Loss: 3.8167, Train: 1.0000, Val: 0.7460, Test: 0.7510
Epoch: 48, Loss: 3.5225, Train: 1.0000, Val: 0.7500, Test: 0.7510
Epoch: 49, Loss: 3.8654, Train: 1.0000, Val: 0.7520, Test: 0.7530
Epoch: 50, Loss: 3.8887, Train: 1.0000, Val: 0.7520, Test: 0.7530
Epoch: 51, Loss: 3.6269, Train: 1.0000, Val: 0.7500, Test: 0.7540
Epoch: 52, Loss: 3.5996, Train: 1.0000, Val: 0.7500, Test: 0.7540
Epoch: 53, Loss: 4.0733, Train: 1.0000, Val: 0.7500, Test: 0.7560
Epoch: 54, Loss: 3.9456, Train: 1.0000, Val: 0.7480, Test: 0.7560
Epoch: 55, Loss: 3.8123, Train: 1.0000, Val: 0.7500, Test: 0.7540
Epoch: 56, Loss: 3.5443, Train: 1.0000, Val: 0.7460, Test: 0.7550
Epoch: 57, Loss: 3.3628, Train: 1.0000, Val: 0.7460, Test: 0.7520
Epoch: 58, Loss: 3.7705, Train: 1.0000, Val: 0.7500, Test: 0.7530
Epoch: 59, Loss: 3.5956, Train: 1.0000, Val: 0.7480, Test: 0.7510
Epoch: 60, Loss: 3.7335, Train: 1.0000, Val: 0.7480, Test: 0.7490
Epoch: 61, Loss: 3.5650, Train: 1.0000, Val: 0.7480, Test: 0.7510
Epoch: 62, Loss: 3.5935, Train: 1.0000, Val: 0.7500, Test: 0.7470
Epoch: 63, Loss: 3.6695, Train: 1.0000, Val: 0.7500, Test: 0.7460
Epoch: 64, Loss: 3.4498, Train: 1.0000, Val: 0.7480, Test: 0.7450
Epoch: 65, Loss: 3.5625, Train: 1.0000, Val: 0.7480, Test: 0.7450
Epoch: 66, Loss: 3.5998, Train: 1.0000, Val: 0.7480, Test: 0.7460
Epoch: 67, Loss: 3.4552, Train: 1.0000, Val: 0.7500, Test: 0.7460
Epoch: 68, Loss: 3.5891, Train: 1.0000, Val: 0.7540, Test: 0.7480
Epoch: 69, Loss: 3.5996, Train: 1.0000, Val: 0.7560, Test: 0.7490
Epoch: 70, Loss: 3.5646, Train: 1.0000, Val: 0.7560, Test: 0.7470
Epoch: 71, Loss: 3.7230, Train: 1.0000, Val: 0.7560, Test: 0.7470
Epoch: 72, Loss: 3.7235, Train: 1.0000, Val: 0.7540, Test: 0.7470
Epoch: 73, Loss: 3.6240, Train: 1.0000, Val: 0.7540, Test: 0.7470
Epoch: 74, Loss: 3.9014, Train: 1.0000, Val: 0.7540, Test: 0.7480
Epoch: 75, Loss: 3.5969, Train: 1.0000, Val: 0.7560, Test: 0.7490
Epoch: 76, Loss: 3.8642, Train: 1.0000, Val: 0.7560, Test: 0.7480
Epoch: 77, Loss: 3.5493, Train: 1.0000, Val: 0.7560, Test: 0.7490
Epoch: 78, Loss: 3.6649, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 79, Loss: 3.5942, Train: 1.0000, Val: 0.7540, Test: 0.7500
Epoch: 80, Loss: 3.7207, Train: 1.0000, Val: 0.7560, Test: 0.7520
Epoch: 81, Loss: 3.7300, Train: 1.0000, Val: 0.7560, Test: 0.7520
Epoch: 82, Loss: 3.9700, Train: 1.0000, Val: 0.7580, Test: 0.7520
Epoch: 83, Loss: 3.5195, Train: 1.0000, Val: 0.7560, Test: 0.7530
Epoch: 84, Loss: 3.6941, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 85, Loss: 3.8585, Train: 1.0000, Val: 0.7600, Test: 0.7530
Epoch: 86, Loss: 3.6525, Train: 1.0000, Val: 0.7580, Test: 0.7540
Epoch: 87, Loss: 3.5810, Train: 1.0000, Val: 0.7580, Test: 0.7550
Epoch: 88, Loss: 3.8234, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 89, Loss: 3.7581, Train: 1.0000, Val: 0.7560, Test: 0.7540
Epoch: 90, Loss: 3.5136, Train: 1.0000, Val: 0.7540, Test: 0.7520
Epoch: 91, Loss: 3.4795, Train: 1.0000, Val: 0.7520, Test: 0.7530
Epoch: 92, Loss: 3.6882, Train: 1.0000, Val: 0.7520, Test: 0.7520
Epoch: 93, Loss: 3.2008, Train: 1.0000, Val: 0.7520, Test: 0.7510
Epoch: 94, Loss: 3.5843, Train: 1.0000, Val: 0.7520, Test: 0.7500
Epoch: 95, Loss: 3.6151, Train: 1.0000, Val: 0.7540, Test: 0.7510
Epoch: 96, Loss: 3.8567, Train: 1.0000, Val: 0.7540, Test: 0.7510
Epoch: 97, Loss: 3.5102, Train: 1.0000, Val: 0.7560, Test: 0.7510
Epoch: 98, Loss: 3.7524, Train: 1.0000, Val: 0.7560, Test: 0.7510
Epoch: 99, Loss: 3.6187, Train: 1.0000, Val: 0.7560, Test: 0.7510
Epoch: 100, Loss: 3.7187, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 101, Loss: 3.6498, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 102, Loss: 3.6517, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 103, Loss: 3.4086, Train: 1.0000, Val: 0.7580, Test: 0.7500
Epoch: 104, Loss: 3.3078, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 105, Loss: 3.6134, Train: 1.0000, Val: 0.7580, Test: 0.7490
Epoch: 106, Loss: 4.0295, Train: 1.0000, Val: 0.7580, Test: 0.7500
Epoch: 107, Loss: 3.8560, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 108, Loss: 3.8896, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 109, Loss: 3.3727, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 110, Loss: 3.6488, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 111, Loss: 3.8929, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 112, Loss: 3.5828, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 113, Loss: 3.5442, Train: 1.0000, Val: 0.7560, Test: 0.7500
Epoch: 114, Loss: 3.6493, Train: 1.0000, Val: 0.7560, Test: 0.7490
Epoch: 115, Loss: 3.1309, Train: 1.0000, Val: 0.7560, Test: 0.7480
Epoch: 116, Loss: 3.7530, Train: 1.0000, Val: 0.7560, Test: 0.7480
Epoch: 117, Loss: 3.8885, Train: 1.0000, Val: 0.7560, Test: 0.7480
Epoch: 118, Loss: 3.7171, Train: 1.0000, Val: 0.7580, Test: 0.7490
Epoch: 119, Loss: 3.3761, Train: 1.0000, Val: 0.7600, Test: 0.7500
Epoch: 120, Loss: 3.7213, Train: 1.0000, Val: 0.7600, Test: 0.7500
Epoch: 121, Loss: 3.9587, Train: 1.0000, Val: 0.7600, Test: 0.7480
Epoch: 122, Loss: 3.4448, Train: 1.0000, Val: 0.7600, Test: 0.7460
Epoch: 123, Loss: 3.8212, Train: 1.0000, Val: 0.7600, Test: 0.7450
Epoch: 124, Loss: 3.6139, Train: 1.0000, Val: 0.7600, Test: 0.7460
Epoch: 125, Loss: 3.7518, Train: 1.0000, Val: 0.7580, Test: 0.7440
Epoch: 126, Loss: 3.5453, Train: 1.0000, Val: 0.7580, Test: 0.7440
Epoch: 127, Loss: 3.7510, Train: 1.0000, Val: 0.7600, Test: 0.7450
Epoch: 128, Loss: 3.7176, Train: 1.0000, Val: 0.7600, Test: 0.7440
Epoch: 129, Loss: 3.6814, Train: 1.0000, Val: 0.7600, Test: 0.7440
Epoch: 130, Loss: 3.4073, Train: 1.0000, Val: 0.7600, Test: 0.7430
Epoch: 131, Loss: 3.6506, Train: 1.0000, Val: 0.7600, Test: 0.7430
Epoch: 132, Loss: 3.5096, Train: 1.0000, Val: 0.7600, Test: 0.7430
Epoch: 133, Loss: 3.6810, Train: 1.0000, Val: 0.7600, Test: 0.7440
Epoch: 134, Loss: 3.8552, Train: 1.0000, Val: 0.7600, Test: 0.7460
Epoch: 135, Loss: 3.5775, Train: 1.0000, Val: 0.7600, Test: 0.7450
Epoch: 136, Loss: 3.5773, Train: 1.0000, Val: 0.7620, Test: 0.7460
Epoch: 137, Loss: 3.6815, Train: 1.0000, Val: 0.7620, Test: 0.7460
Epoch: 138, Loss: 3.7488, Train: 1.0000, Val: 0.7620, Test: 0.7460
Epoch: 139, Loss: 3.8201, Train: 1.0000, Val: 0.7620, Test: 0.7470
Epoch: 140, Loss: 3.7167, Train: 1.0000, Val: 0.7620, Test: 0.7460
Epoch: 141, Loss: 3.3019, Train: 1.0000, Val: 0.7600, Test: 0.7460
Epoch: 142, Loss: 3.7509, Train: 1.0000, Val: 0.7580, Test: 0.7470
Epoch: 143, Loss: 3.5439, Train: 1.0000, Val: 0.7580, Test: 0.7470
Epoch: 144, Loss: 3.7842, Train: 1.0000, Val: 0.7560, Test: 0.7470
Epoch: 145, Loss: 3.6126, Train: 1.0000, Val: 0.7560, Test: 0.7470
Epoch: 146, Loss: 3.6815, Train: 1.0000, Val: 0.7540, Test: 0.7470
Epoch: 147, Loss: 3.8177, Train: 1.0000, Val: 0.7540, Test: 0.7460
Epoch: 148, Loss: 3.6108, Train: 1.0000, Val: 0.7540, Test: 0.7450
Epoch: 149, Loss: 3.6116, Train: 1.0000, Val: 0.7540, Test: 0.7450
Epoch: 150, Loss: 3.6463, Train: 1.0000, Val: 0.7540, Test: 0.7460
Epoch: 151, Loss: 3.8185, Train: 1.0000, Val: 0.7540, Test: 0.7460
Epoch: 152, Loss: 3.4742, Train: 1.0000, Val: 0.7540, Test: 0.7460
Epoch: 153, Loss: 3.6809, Train: 1.0000, Val: 0.7540, Test: 0.7440
Epoch: 154, Loss: 3.8535, Train: 1.0000, Val: 0.7540, Test: 0.7450
Epoch: 155, Loss: 3.7499, Train: 1.0000, Val: 0.7540, Test: 0.7450
Epoch: 156, Loss: 3.5771, Train: 1.0000, Val: 0.7540, Test: 0.7460
Epoch: 157, Loss: 3.6108, Train: 1.0000, Val: 0.7540, Test: 0.7470
Epoch: 158, Loss: 3.5774, Train: 1.0000, Val: 0.7540, Test: 0.7480
Epoch: 159, Loss: 3.7506, Train: 1.0000, Val: 0.7540, Test: 0.7470
Epoch: 160, Loss: 3.7149, Train: 1.0000, Val: 0.7560, Test: 0.7470
Epoch: 161, Loss: 3.5431, Train: 1.0000, Val: 0.7580, Test: 0.7470
Epoch: 162, Loss: 4.0263, Train: 1.0000, Val: 0.7580, Test: 0.7470
Epoch: 163, Loss: 3.4731, Train: 1.0000, Val: 0.7580, Test: 0.7460
Epoch: 164, Loss: 3.6800, Train: 1.0000, Val: 0.7600, Test: 0.7470
Epoch: 165, Loss: 3.6803, Train: 1.0000, Val: 0.7600, Test: 0.7470
Epoch: 166, Loss: 3.5784, Train: 1.0000, Val: 0.7620, Test: 0.7470
Epoch: 167, Loss: 3.5441, Train: 1.0000, Val: 0.7620, Test: 0.7470
Epoch: 168, Loss: 3.6460, Train: 1.0000, Val: 0.7620, Test: 0.7490
Epoch: 169, Loss: 3.4382, Train: 1.0000, Val: 0.7620, Test: 0.7490
Epoch: 170, Loss: 3.6118, Train: 1.0000, Val: 0.7640, Test: 0.7510
Epoch: 171, Loss: 3.4727, Train: 1.0000, Val: 0.7640, Test: 0.7520
Epoch: 172, Loss: 3.4373, Train: 1.0000, Val: 0.7640, Test: 0.7510
Epoch: 173, Loss: 3.9557, Train: 1.0000, Val: 0.7640, Test: 0.7520
Epoch: 174, Loss: 3.6445, Train: 1.0000, Val: 0.7640, Test: 0.7520
Epoch: 175, Loss: 3.8187, Train: 1.0000, Val: 0.7640, Test: 0.7510
Epoch: 176, Loss: 3.5079, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 177, Loss: 3.5423, Train: 1.0000, Val: 0.7640, Test: 0.7510
Epoch: 178, Loss: 3.7837, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 179, Loss: 4.0257, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 180, Loss: 3.4398, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 181, Loss: 3.6808, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 182, Loss: 3.8181, Train: 1.0000, Val: 0.7640, Test: 0.7490
Epoch: 183, Loss: 3.7504, Train: 1.0000, Val: 0.7640, Test: 0.7490
Epoch: 184, Loss: 3.3690, Train: 1.0000, Val: 0.7640, Test: 0.7490
Epoch: 185, Loss: 3.5062, Train: 1.0000, Val: 0.7640, Test: 0.7490
Epoch: 186, Loss: 3.8173, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 187, Loss: 3.3340, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 188, Loss: 3.8517, Train: 1.0000, Val: 0.7640, Test: 0.7490
Epoch: 189, Loss: 3.6799, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 190, Loss: 3.8865, Train: 1.0000, Val: 0.7640, Test: 0.7500
Epoch: 191, Loss: 3.6107, Train: 1.0000, Val: 0.7640, Test: 0.7520
Epoch: 192, Loss: 3.5061, Train: 1.0000, Val: 0.7640, Test: 0.7510
Epoch: 193, Loss: 3.8516, Train: 1.0000, Val: 0.7660, Test: 0.7500
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 194, Loss: 3.4376, Train: 1.0000, Val: 0.7660, Test: 0.7490
Epoch: 195, Loss: 3.8166, Train: 1.0000, Val: 0.7660, Test: 0.7500
Epoch: 196, Loss: 3.7824, Train: 1.0000, Val: 0.7660, Test: 0.7500
Epoch: 197, Loss: 3.6450, Train: 1.0000, Val: 0.7660, Test: 0.7510
Epoch: 198, Loss: 3.6794, Train: 1.0000, Val: 0.7660, Test: 0.7520
Epoch: 199, Loss: 3.6790, Train: 1.0000, Val: 0.7660, Test: 0.7520
Epoch: 200, Loss: 3.6781, Train: 1.0000, Val: 0.7660, Test: 0.7520
MAD:  0.3947
Best Test Accuracy: 0.7560, Val Accuracy: 0.7380, Train Accuracy: 1.0000
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8587, Train: 0.1071, Val: 0.0760, Test: 0.0670
Epoch: 2, Loss: 4.8394, Train: 0.2500, Val: 0.1620, Test: 0.1510
Epoch: 3, Loss: 4.7223, Train: 0.3214, Val: 0.2140, Test: 0.2090
Epoch: 4, Loss: 4.7363, Train: 0.3929, Val: 0.2580, Test: 0.2490
Epoch: 5, Loss: 4.7051, Train: 0.4429, Val: 0.2740, Test: 0.2710
Epoch: 6, Loss: 4.5079, Train: 0.5143, Val: 0.2900, Test: 0.2910
Epoch: 7, Loss: 4.4070, Train: 0.5643, Val: 0.3100, Test: 0.3080
Epoch: 8, Loss: 4.5920, Train: 0.5929, Val: 0.3220, Test: 0.3300
Epoch: 9, Loss: 4.4630, Train: 0.6143, Val: 0.3580, Test: 0.3540
Epoch: 10, Loss: 4.3544, Train: 0.6286, Val: 0.3840, Test: 0.3770
Epoch: 11, Loss: 4.2779, Train: 0.7286, Val: 0.4100, Test: 0.4110
Epoch: 12, Loss: 4.1827, Train: 0.8000, Val: 0.4520, Test: 0.4270
Epoch: 13, Loss: 4.2194, Train: 0.8500, Val: 0.4720, Test: 0.4640
Epoch: 14, Loss: 4.2203, Train: 0.8857, Val: 0.5180, Test: 0.5060
Epoch: 15, Loss: 3.9923, Train: 0.9143, Val: 0.5360, Test: 0.5310
Epoch: 16, Loss: 3.8736, Train: 0.9357, Val: 0.5560, Test: 0.5680
Epoch: 17, Loss: 3.6304, Train: 0.9500, Val: 0.5760, Test: 0.5950
Epoch: 18, Loss: 4.0470, Train: 0.9643, Val: 0.6220, Test: 0.6240
Epoch: 19, Loss: 3.8156, Train: 0.9714, Val: 0.6420, Test: 0.6630
Epoch: 20, Loss: 3.9520, Train: 0.9857, Val: 0.6700, Test: 0.6760
Epoch: 21, Loss: 3.9357, Train: 0.9786, Val: 0.6860, Test: 0.6970
Epoch: 22, Loss: 3.7225, Train: 0.9786, Val: 0.6860, Test: 0.7090
Epoch: 23, Loss: 3.9371, Train: 0.9857, Val: 0.6820, Test: 0.7190
Epoch: 24, Loss: 3.8875, Train: 0.9857, Val: 0.6980, Test: 0.7350
Epoch: 25, Loss: 3.9067, Train: 0.9929, Val: 0.7040, Test: 0.7370
Epoch: 26, Loss: 3.8287, Train: 0.9929, Val: 0.7080, Test: 0.7430
Epoch: 27, Loss: 3.7084, Train: 0.9929, Val: 0.7060, Test: 0.7470
Epoch: 28, Loss: 3.4427, Train: 0.9929, Val: 0.7040, Test: 0.7530
Epoch: 29, Loss: 3.6251, Train: 0.9929, Val: 0.7060, Test: 0.7520
Epoch: 30, Loss: 3.7657, Train: 0.9929, Val: 0.7140, Test: 0.7530
Epoch: 31, Loss: 4.2469, Train: 0.9929, Val: 0.7180, Test: 0.7620
Epoch: 32, Loss: 3.7497, Train: 1.0000, Val: 0.7240, Test: 0.7680
Epoch: 33, Loss: 4.2070, Train: 1.0000, Val: 0.7220, Test: 0.7690
Epoch: 34, Loss: 3.7958, Train: 1.0000, Val: 0.7300, Test: 0.7700
Epoch: 35, Loss: 3.8206, Train: 1.0000, Val: 0.7300, Test: 0.7730
Epoch: 36, Loss: 3.9464, Train: 1.0000, Val: 0.7300, Test: 0.7720
Epoch: 37, Loss: 3.7907, Train: 1.0000, Val: 0.7340, Test: 0.7710
Epoch: 38, Loss: 3.6580, Train: 1.0000, Val: 0.7320, Test: 0.7740
Epoch: 39, Loss: 3.9023, Train: 1.0000, Val: 0.7300, Test: 0.7700
Epoch: 40, Loss: 3.6818, Train: 1.0000, Val: 0.7260, Test: 0.7680
Epoch: 41, Loss: 3.8315, Train: 1.0000, Val: 0.7240, Test: 0.7660
Epoch: 42, Loss: 3.9127, Train: 1.0000, Val: 0.7300, Test: 0.7630
Epoch: 43, Loss: 3.6420, Train: 1.0000, Val: 0.7260, Test: 0.7600
Epoch: 44, Loss: 3.5927, Train: 1.0000, Val: 0.7300, Test: 0.7620
Epoch: 45, Loss: 3.4445, Train: 1.0000, Val: 0.7260, Test: 0.7600
Epoch: 46, Loss: 3.5101, Train: 1.0000, Val: 0.7260, Test: 0.7580
Epoch: 47, Loss: 3.7054, Train: 1.0000, Val: 0.7280, Test: 0.7570
Epoch: 48, Loss: 3.6536, Train: 1.0000, Val: 0.7260, Test: 0.7530
Epoch: 49, Loss: 3.7442, Train: 1.0000, Val: 0.7260, Test: 0.7510
Epoch: 50, Loss: 3.9262, Train: 1.0000, Val: 0.7260, Test: 0.7510
Epoch: 51, Loss: 3.7217, Train: 1.0000, Val: 0.7320, Test: 0.7500
Epoch: 52, Loss: 3.6120, Train: 1.0000, Val: 0.7320, Test: 0.7490
Epoch: 53, Loss: 3.8130, Train: 1.0000, Val: 0.7340, Test: 0.7530
Epoch: 54, Loss: 3.7070, Train: 1.0000, Val: 0.7360, Test: 0.7530
Epoch: 55, Loss: 3.7090, Train: 1.0000, Val: 0.7360, Test: 0.7500
Epoch: 56, Loss: 3.2958, Train: 1.0000, Val: 0.7380, Test: 0.7480
Epoch: 57, Loss: 3.7328, Train: 1.0000, Val: 0.7400, Test: 0.7460
Epoch: 58, Loss: 3.9380, Train: 1.0000, Val: 0.7360, Test: 0.7450
Epoch: 59, Loss: 3.8717, Train: 1.0000, Val: 0.7360, Test: 0.7450
Epoch: 60, Loss: 3.6944, Train: 1.0000, Val: 0.7340, Test: 0.7420
Epoch: 61, Loss: 3.6934, Train: 1.0000, Val: 0.7360, Test: 0.7440
Epoch: 62, Loss: 3.7633, Train: 1.0000, Val: 0.7360, Test: 0.7410
Epoch: 63, Loss: 3.6964, Train: 1.0000, Val: 0.7360, Test: 0.7420
Epoch: 64, Loss: 3.8728, Train: 1.0000, Val: 0.7380, Test: 0.7400
Epoch: 65, Loss: 3.5273, Train: 1.0000, Val: 0.7360, Test: 0.7390
Epoch: 66, Loss: 3.6601, Train: 1.0000, Val: 0.7360, Test: 0.7410
Epoch: 67, Loss: 3.4582, Train: 1.0000, Val: 0.7380, Test: 0.7420
Epoch: 68, Loss: 3.8634, Train: 1.0000, Val: 0.7400, Test: 0.7450
Epoch: 69, Loss: 3.5916, Train: 1.0000, Val: 0.7420, Test: 0.7450
Epoch: 70, Loss: 3.7605, Train: 1.0000, Val: 0.7420, Test: 0.7440
Epoch: 71, Loss: 3.4841, Train: 1.0000, Val: 0.7380, Test: 0.7470
Epoch: 72, Loss: 3.5554, Train: 1.0000, Val: 0.7380, Test: 0.7510
Epoch: 73, Loss: 3.8248, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 74, Loss: 3.5509, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 75, Loss: 3.8264, Train: 1.0000, Val: 0.7420, Test: 0.7520
Epoch: 76, Loss: 3.8596, Train: 1.0000, Val: 0.7420, Test: 0.7540
Epoch: 77, Loss: 3.7586, Train: 1.0000, Val: 0.7420, Test: 0.7540
Epoch: 78, Loss: 3.6595, Train: 1.0000, Val: 0.7420, Test: 0.7540
Epoch: 79, Loss: 3.4102, Train: 1.0000, Val: 0.7420, Test: 0.7560
Epoch: 80, Loss: 3.3090, Train: 1.0000, Val: 0.7440, Test: 0.7550
Epoch: 81, Loss: 3.4472, Train: 1.0000, Val: 0.7460, Test: 0.7530
Epoch: 82, Loss: 4.1012, Train: 1.0000, Val: 0.7420, Test: 0.7530
Epoch: 83, Loss: 3.7886, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 84, Loss: 3.9983, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 85, Loss: 3.5176, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 86, Loss: 4.0706, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 87, Loss: 3.8596, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 88, Loss: 3.4811, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 89, Loss: 4.1360, Train: 1.0000, Val: 0.7360, Test: 0.7550
Epoch: 90, Loss: 3.5148, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 91, Loss: 3.5167, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 92, Loss: 3.7581, Train: 1.0000, Val: 0.7380, Test: 0.7570
Epoch: 93, Loss: 3.6877, Train: 1.0000, Val: 0.7380, Test: 0.7570
Epoch: 94, Loss: 3.7545, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 95, Loss: 3.6193, Train: 1.0000, Val: 0.7400, Test: 0.7580
Epoch: 96, Loss: 3.5112, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 97, Loss: 3.4780, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 98, Loss: 3.4088, Train: 1.0000, Val: 0.7400, Test: 0.7560
Epoch: 99, Loss: 3.8222, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 100, Loss: 3.6509, Train: 1.0000, Val: 0.7400, Test: 0.7570
Epoch: 101, Loss: 3.5132, Train: 1.0000, Val: 0.7400, Test: 0.7580
Epoch: 102, Loss: 3.7555, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 103, Loss: 3.6177, Train: 1.0000, Val: 0.7400, Test: 0.7590
Epoch: 104, Loss: 3.4403, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 105, Loss: 3.6851, Train: 1.0000, Val: 0.7380, Test: 0.7590
Epoch: 106, Loss: 3.8209, Train: 1.0000, Val: 0.7380, Test: 0.7580
Epoch: 107, Loss: 3.5111, Train: 1.0000, Val: 0.7380, Test: 0.7580
Epoch: 108, Loss: 3.8205, Train: 1.0000, Val: 0.7380, Test: 0.7570
Epoch: 109, Loss: 3.8551, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 110, Loss: 3.7520, Train: 1.0000, Val: 0.7380, Test: 0.7560
Epoch: 111, Loss: 3.7883, Train: 1.0000, Val: 0.7380, Test: 0.7550
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 112, Loss: 3.4409, Train: 1.0000, Val: 0.7380, Test: 0.7530
Epoch: 113, Loss: 3.6127, Train: 1.0000, Val: 0.7380, Test: 0.7520
Epoch: 114, Loss: 3.8567, Train: 1.0000, Val: 0.7360, Test: 0.7530
Epoch: 115, Loss: 3.7154, Train: 1.0000, Val: 0.7360, Test: 0.7530
Epoch: 116, Loss: 3.6485, Train: 1.0000, Val: 0.7360, Test: 0.7530
Epoch: 117, Loss: 3.2672, Train: 1.0000, Val: 0.7400, Test: 0.7550
Epoch: 118, Loss: 3.6482, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 119, Loss: 3.3721, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 120, Loss: 3.5110, Train: 1.0000, Val: 0.7420, Test: 0.7550
Epoch: 121, Loss: 3.7845, Train: 1.0000, Val: 0.7420, Test: 0.7560
Epoch: 122, Loss: 3.2678, Train: 1.0000, Val: 0.7420, Test: 0.7580
Epoch: 123, Loss: 3.5788, Train: 1.0000, Val: 0.7420, Test: 0.7590
Epoch: 124, Loss: 3.7507, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 125, Loss: 3.6811, Train: 1.0000, Val: 0.7440, Test: 0.7610
Epoch: 126, Loss: 3.6138, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 127, Loss: 3.6822, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 128, Loss: 3.2312, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 129, Loss: 4.0281, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 130, Loss: 3.6481, Train: 1.0000, Val: 0.7460, Test: 0.7600
Epoch: 131, Loss: 3.5785, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 132, Loss: 3.4404, Train: 1.0000, Val: 0.7460, Test: 0.7590
Epoch: 133, Loss: 3.6123, Train: 1.0000, Val: 0.7440, Test: 0.7590
Epoch: 134, Loss: 3.2693, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 135, Loss: 3.6123, Train: 1.0000, Val: 0.7440, Test: 0.7600
Epoch: 136, Loss: 3.9230, Train: 1.0000, Val: 0.7420, Test: 0.7610
Epoch: 137, Loss: 3.8530, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 138, Loss: 3.5104, Train: 1.0000, Val: 0.7400, Test: 0.7610
Epoch: 139, Loss: 3.5088, Train: 1.0000, Val: 0.7380, Test: 0.7610
Epoch: 140, Loss: 3.7158, Train: 1.0000, Val: 0.7340, Test: 0.7630
Epoch: 141, Loss: 3.8882, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 142, Loss: 3.6813, Train: 1.0000, Val: 0.7320, Test: 0.7650
Epoch: 143, Loss: 3.5797, Train: 1.0000, Val: 0.7320, Test: 0.7650
Epoch: 144, Loss: 3.5090, Train: 1.0000, Val: 0.7320, Test: 0.7650
Epoch: 145, Loss: 3.4396, Train: 1.0000, Val: 0.7340, Test: 0.7650
Epoch: 146, Loss: 3.5421, Train: 1.0000, Val: 0.7340, Test: 0.7640
Epoch: 147, Loss: 3.5775, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 148, Loss: 3.8175, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 149, Loss: 3.5761, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 150, Loss: 3.6120, Train: 1.0000, Val: 0.7380, Test: 0.7640
Epoch: 151, Loss: 3.6113, Train: 1.0000, Val: 0.7360, Test: 0.7640
Epoch: 152, Loss: 3.6111, Train: 1.0000, Val: 0.7360, Test: 0.7640
Epoch: 153, Loss: 3.6112, Train: 1.0000, Val: 0.7360, Test: 0.7630
Epoch: 154, Loss: 3.9557, Train: 1.0000, Val: 0.7360, Test: 0.7630
Epoch: 155, Loss: 3.6819, Train: 1.0000, Val: 0.7360, Test: 0.7650
Epoch: 156, Loss: 3.6465, Train: 1.0000, Val: 0.7360, Test: 0.7650
Epoch: 157, Loss: 3.6101, Train: 1.0000, Val: 0.7360, Test: 0.7630
Epoch: 158, Loss: 3.7495, Train: 1.0000, Val: 0.7360, Test: 0.7630
Epoch: 159, Loss: 3.6804, Train: 1.0000, Val: 0.7360, Test: 0.7620
Epoch: 160, Loss: 3.7840, Train: 1.0000, Val: 0.7360, Test: 0.7620
Epoch: 161, Loss: 3.7495, Train: 1.0000, Val: 0.7360, Test: 0.7620
Epoch: 162, Loss: 3.7147, Train: 1.0000, Val: 0.7360, Test: 0.7630
Epoch: 163, Loss: 3.6444, Train: 1.0000, Val: 0.7340, Test: 0.7630
Epoch: 164, Loss: 3.5774, Train: 1.0000, Val: 0.7340, Test: 0.7630
Epoch: 165, Loss: 3.6108, Train: 1.0000, Val: 0.7340, Test: 0.7630
Epoch: 166, Loss: 3.6460, Train: 1.0000, Val: 0.7340, Test: 0.7630
Epoch: 167, Loss: 3.7490, Train: 1.0000, Val: 0.7340, Test: 0.7630
Epoch: 168, Loss: 3.7484, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 169, Loss: 3.7844, Train: 1.0000, Val: 0.7340, Test: 0.7640
Epoch: 170, Loss: 3.5754, Train: 1.0000, Val: 0.7340, Test: 0.7640
Epoch: 171, Loss: 3.4736, Train: 1.0000, Val: 0.7340, Test: 0.7640
Epoch: 172, Loss: 3.8887, Train: 1.0000, Val: 0.7340, Test: 0.7640
Epoch: 173, Loss: 3.8528, Train: 1.0000, Val: 0.7320, Test: 0.7640
Epoch: 174, Loss: 3.8536, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 175, Loss: 3.5765, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 176, Loss: 3.4721, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 177, Loss: 3.6108, Train: 1.0000, Val: 0.7300, Test: 0.7620
Epoch: 178, Loss: 3.7484, Train: 1.0000, Val: 0.7300, Test: 0.7630
Epoch: 179, Loss: 3.8180, Train: 1.0000, Val: 0.7300, Test: 0.7630
Epoch: 180, Loss: 3.5414, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 181, Loss: 3.5079, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 182, Loss: 3.5422, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 183, Loss: 3.6803, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 184, Loss: 3.5077, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 185, Loss: 3.6803, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 186, Loss: 3.6448, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 187, Loss: 3.7481, Train: 1.0000, Val: 0.7320, Test: 0.7620
Epoch: 188, Loss: 3.7140, Train: 1.0000, Val: 0.7320, Test: 0.7640
Epoch: 189, Loss: 3.6453, Train: 1.0000, Val: 0.7320, Test: 0.7640
Epoch: 190, Loss: 3.5751, Train: 1.0000, Val: 0.7320, Test: 0.7640
Epoch: 191, Loss: 3.7838, Train: 1.0000, Val: 0.7320, Test: 0.7630
Epoch: 192, Loss: 3.6800, Train: 1.0000, Val: 0.7340, Test: 0.7610
Epoch: 193, Loss: 3.4709, Train: 1.0000, Val: 0.7340, Test: 0.7570
Epoch: 194, Loss: 3.8189, Train: 1.0000, Val: 0.7340, Test: 0.7560
Epoch: 195, Loss: 3.5771, Train: 1.0000, Val: 0.7340, Test: 0.7560
Epoch: 196, Loss: 3.9552, Train: 1.0000, Val: 0.7340, Test: 0.7560
Epoch: 197, Loss: 3.5410, Train: 1.0000, Val: 0.7340, Test: 0.7560
Epoch: 198, Loss: 3.6092, Train: 1.0000, Val: 0.7360, Test: 0.7560
Epoch: 199, Loss: 3.7835, Train: 1.0000, Val: 0.7360, Test: 0.7570
Epoch: 200, Loss: 3.5406, Train: 1.0000, Val: 0.7360, Test: 0.7570
MAD:  0.3554
Best Test Accuracy: 0.7740, Val Accuracy: 0.7320, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.7704444444444445 ± 0.004645454435637087
Average MAD:  0.5114777777777779 ± 0.07824834790982672

/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1621, Train: 0.1357, Val: 0.1060, Test: 0.1170
Epoch: 2, Loss: 4.1466, Train: 0.1857, Val: 0.1380, Test: 0.1640
Epoch: 3, Loss: 4.1372, Train: 0.2143, Val: 0.1500, Test: 0.1670
Epoch: 4, Loss: 4.1120, Train: 0.2429, Val: 0.1520, Test: 0.1720
Epoch: 5, Loss: 4.0987, Train: 0.2643, Val: 0.1600, Test: 0.1780
Epoch: 6, Loss: 4.0679, Train: 0.2643, Val: 0.1620, Test: 0.1810
Epoch: 7, Loss: 4.0412, Train: 0.2857, Val: 0.1580, Test: 0.1820
Epoch: 8, Loss: 4.0283, Train: 0.3000, Val: 0.1620, Test: 0.1870
Epoch: 9, Loss: 3.9989, Train: 0.3214, Val: 0.1700, Test: 0.1960
Epoch: 10, Loss: 3.9587, Train: 0.3286, Val: 0.1760, Test: 0.2050
Epoch: 11, Loss: 3.9167, Train: 0.3500, Val: 0.1800, Test: 0.2090
Epoch: 12, Loss: 3.9019, Train: 0.3643, Val: 0.2000, Test: 0.2280
Epoch: 13, Loss: 3.8590, Train: 0.4071, Val: 0.2080, Test: 0.2400
Epoch: 14, Loss: 3.8066, Train: 0.4214, Val: 0.2160, Test: 0.2450
Epoch: 15, Loss: 3.6714, Train: 0.4357, Val: 0.2180, Test: 0.2510
Epoch: 16, Loss: 3.7376, Train: 0.4357, Val: 0.2260, Test: 0.2680
Epoch: 17, Loss: 3.6746, Train: 0.4643, Val: 0.2300, Test: 0.2750
Epoch: 18, Loss: 3.7112, Train: 0.4643, Val: 0.2460, Test: 0.2780
Epoch: 19, Loss: 3.4308, Train: 0.4643, Val: 0.2380, Test: 0.2780
Epoch: 20, Loss: 3.5052, Train: 0.4857, Val: 0.2440, Test: 0.2810
Epoch: 21, Loss: 3.4638, Train: 0.4929, Val: 0.2620, Test: 0.2900
Epoch: 22, Loss: 3.4195, Train: 0.5500, Val: 0.2880, Test: 0.3150
Epoch: 23, Loss: 3.2956, Train: 0.5929, Val: 0.3120, Test: 0.3410
Epoch: 24, Loss: 3.4352, Train: 0.6357, Val: 0.3440, Test: 0.3770
Epoch: 25, Loss: 3.3202, Train: 0.6643, Val: 0.3840, Test: 0.4240
Epoch: 26, Loss: 3.2273, Train: 0.7000, Val: 0.4420, Test: 0.4730
Epoch: 27, Loss: 3.0777, Train: 0.7357, Val: 0.4900, Test: 0.5350
Epoch: 28, Loss: 3.1701, Train: 0.7643, Val: 0.5320, Test: 0.5680
Epoch: 29, Loss: 3.3324, Train: 0.7714, Val: 0.5700, Test: 0.5880
Epoch: 30, Loss: 2.8207, Train: 0.8143, Val: 0.6000, Test: 0.6020
Epoch: 31, Loss: 3.0918, Train: 0.8571, Val: 0.6140, Test: 0.6250
Epoch: 32, Loss: 3.0387, Train: 0.8929, Val: 0.6480, Test: 0.6480
Epoch: 33, Loss: 2.9132, Train: 0.9000, Val: 0.6800, Test: 0.6790
Epoch: 34, Loss: 3.2070, Train: 0.9286, Val: 0.7140, Test: 0.7100
Epoch: 35, Loss: 2.6322, Train: 0.9357, Val: 0.7160, Test: 0.7270
Epoch: 36, Loss: 2.7561, Train: 0.9357, Val: 0.7340, Test: 0.7340
Epoch: 37, Loss: 2.7149, Train: 0.9429, Val: 0.7400, Test: 0.7460
Epoch: 38, Loss: 2.8469, Train: 0.9500, Val: 0.7560, Test: 0.7620
Epoch: 39, Loss: 2.8654, Train: 0.9643, Val: 0.7620, Test: 0.7800
Epoch: 40, Loss: 2.7484, Train: 0.9643, Val: 0.7700, Test: 0.7910
Epoch: 41, Loss: 2.9167, Train: 0.9786, Val: 0.7800, Test: 0.8000
Epoch: 42, Loss: 2.6446, Train: 0.9786, Val: 0.7840, Test: 0.8050
Epoch: 43, Loss: 2.8981, Train: 0.9786, Val: 0.7900, Test: 0.8040
Epoch: 44, Loss: 2.6532, Train: 0.9857, Val: 0.7940, Test: 0.8050
Epoch: 45, Loss: 2.7307, Train: 0.9857, Val: 0.7980, Test: 0.8030
Epoch: 46, Loss: 2.6732, Train: 0.9857, Val: 0.7960, Test: 0.8020
Epoch: 47, Loss: 2.6332, Train: 0.9857, Val: 0.7940, Test: 0.7990
Epoch: 48, Loss: 2.2051, Train: 0.9929, Val: 0.7940, Test: 0.8010
Epoch: 49, Loss: 2.6240, Train: 0.9929, Val: 0.7940, Test: 0.7990
Epoch: 50, Loss: 2.8240, Train: 0.9929, Val: 0.7900, Test: 0.7980
MAD:  0.1021
Best Test Accuracy: 0.8050, Val Accuracy: 0.7840, Train Accuracy: 0.9786
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1517, Train: 0.1929, Val: 0.1300, Test: 0.1450
Epoch: 2, Loss: 4.1410, Train: 0.2929, Val: 0.1680, Test: 0.1780
Epoch: 3, Loss: 4.1096, Train: 0.3286, Val: 0.1980, Test: 0.2100
Epoch: 4, Loss: 4.0917, Train: 0.3714, Val: 0.2080, Test: 0.2140
Epoch: 5, Loss: 4.0644, Train: 0.4286, Val: 0.2100, Test: 0.2280
Epoch: 6, Loss: 4.0211, Train: 0.4286, Val: 0.2160, Test: 0.2290
Epoch: 7, Loss: 3.9893, Train: 0.4357, Val: 0.2200, Test: 0.2410
Epoch: 8, Loss: 3.9707, Train: 0.4500, Val: 0.2300, Test: 0.2450
Epoch: 9, Loss: 3.8840, Train: 0.4643, Val: 0.2340, Test: 0.2520
Epoch: 10, Loss: 3.8676, Train: 0.4786, Val: 0.2400, Test: 0.2520
Epoch: 11, Loss: 3.8210, Train: 0.4714, Val: 0.2400, Test: 0.2500
Epoch: 12, Loss: 3.8081, Train: 0.4643, Val: 0.2360, Test: 0.2440
Epoch: 13, Loss: 3.6697, Train: 0.4643, Val: 0.2360, Test: 0.2430
Epoch: 14, Loss: 3.6559, Train: 0.4786, Val: 0.2360, Test: 0.2540
Epoch: 15, Loss: 3.5591, Train: 0.4786, Val: 0.2520, Test: 0.2600
Epoch: 16, Loss: 3.6493, Train: 0.4857, Val: 0.2680, Test: 0.2670
Epoch: 17, Loss: 3.5128, Train: 0.4714, Val: 0.2640, Test: 0.2690
Epoch: 18, Loss: 3.4056, Train: 0.4857, Val: 0.2620, Test: 0.2800
Epoch: 19, Loss: 3.5034, Train: 0.5000, Val: 0.2680, Test: 0.2910
Epoch: 20, Loss: 3.5261, Train: 0.5000, Val: 0.2940, Test: 0.3010
Epoch: 21, Loss: 3.4816, Train: 0.5500, Val: 0.3180, Test: 0.3330
Epoch: 22, Loss: 3.1434, Train: 0.5571, Val: 0.3540, Test: 0.3600
Epoch: 23, Loss: 3.2896, Train: 0.6000, Val: 0.3800, Test: 0.3840
Epoch: 24, Loss: 3.3620, Train: 0.6143, Val: 0.4000, Test: 0.4030
Epoch: 25, Loss: 3.1505, Train: 0.6429, Val: 0.4140, Test: 0.4170
Epoch: 26, Loss: 3.2067, Train: 0.6714, Val: 0.4240, Test: 0.4270
Epoch: 27, Loss: 3.1362, Train: 0.7000, Val: 0.4260, Test: 0.4270
Epoch: 28, Loss: 2.8777, Train: 0.7357, Val: 0.4440, Test: 0.4430
Epoch: 29, Loss: 3.3837, Train: 0.7857, Val: 0.4720, Test: 0.4750
Epoch: 30, Loss: 3.0206, Train: 0.8357, Val: 0.5040, Test: 0.5030
Epoch: 31, Loss: 3.1059, Train: 0.8500, Val: 0.5400, Test: 0.5450
Epoch: 32, Loss: 3.1608, Train: 0.8929, Val: 0.6060, Test: 0.6170
Epoch: 33, Loss: 3.0501, Train: 0.9286, Val: 0.6620, Test: 0.6720
Epoch: 34, Loss: 3.0520, Train: 0.9357, Val: 0.7000, Test: 0.7240
Epoch: 35, Loss: 2.7215, Train: 0.9500, Val: 0.7380, Test: 0.7530
Epoch: 36, Loss: 3.0748, Train: 0.9571, Val: 0.7540, Test: 0.7790
Epoch: 37, Loss: 3.0980, Train: 0.9714, Val: 0.7760, Test: 0.7960
Epoch: 38, Loss: 2.8956, Train: 0.9714, Val: 0.7800, Test: 0.8020
Epoch: 39, Loss: 2.9230, Train: 0.9786, Val: 0.7760, Test: 0.8070
Epoch: 40, Loss: 2.8458, Train: 0.9857, Val: 0.7800, Test: 0.8120
Epoch: 41, Loss: 2.9084, Train: 0.9857, Val: 0.7820, Test: 0.8120
Epoch: 42, Loss: 2.6538, Train: 0.9857, Val: 0.7860, Test: 0.8090
Epoch: 43, Loss: 2.8157, Train: 0.9857, Val: 0.7860, Test: 0.8090
Epoch: 44, Loss: 2.6206, Train: 0.9857, Val: 0.7820, Test: 0.8080
Epoch: 45, Loss: 2.7856, Train: 0.9857, Val: 0.7800, Test: 0.8060
Epoch: 46, Loss: 2.4985, Train: 0.9857, Val: 0.7800, Test: 0.8070
Epoch: 47, Loss: 2.8885, Train: 0.9857, Val: 0.7800, Test: 0.8100
Epoch: 48, Loss: 2.7861, Train: 0.9857, Val: 0.7840, Test: 0.8120
Epoch: 49, Loss: 2.6588, Train: 0.9857, Val: 0.7900, Test: 0.8140
Epoch: 50, Loss: 2.6501, Train: 0.9929, Val: 0.7940, Test: 0.8140
MAD:  0.1499
Best Test Accuracy: 0.8140, Val Accuracy: 0.7900, Train Accuracy: 0.9857
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1578, Train: 0.0429, Val: 0.0400, Test: 0.0420
Epoch: 2, Loss: 4.1441, Train: 0.2143, Val: 0.1400, Test: 0.1520
Epoch: 3, Loss: 4.1356, Train: 0.3000, Val: 0.2320, Test: 0.2430
Epoch: 4, Loss: 4.0989, Train: 0.3857, Val: 0.2640, Test: 0.2710
Epoch: 5, Loss: 4.0756, Train: 0.4214, Val: 0.2920, Test: 0.3020
Epoch: 6, Loss: 4.0681, Train: 0.4500, Val: 0.3160, Test: 0.3130
Epoch: 7, Loss: 4.0219, Train: 0.4714, Val: 0.3300, Test: 0.3230
Epoch: 8, Loss: 4.0040, Train: 0.4929, Val: 0.3460, Test: 0.3320
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 9, Loss: 3.9626, Train: 0.4857, Val: 0.3340, Test: 0.3220
Epoch: 10, Loss: 3.9732, Train: 0.4714, Val: 0.3240, Test: 0.3140
Epoch: 11, Loss: 3.8992, Train: 0.4786, Val: 0.3160, Test: 0.3110
Epoch: 12, Loss: 3.8580, Train: 0.4857, Val: 0.3120, Test: 0.3130
Epoch: 13, Loss: 3.8020, Train: 0.4857, Val: 0.3140, Test: 0.3130
Epoch: 14, Loss: 3.8184, Train: 0.4857, Val: 0.3240, Test: 0.3190
Epoch: 15, Loss: 3.7849, Train: 0.5000, Val: 0.3200, Test: 0.3270
Epoch: 16, Loss: 3.7522, Train: 0.5357, Val: 0.3240, Test: 0.3340
Epoch: 17, Loss: 3.6796, Train: 0.5643, Val: 0.3300, Test: 0.3350
Epoch: 18, Loss: 3.6224, Train: 0.5857, Val: 0.3400, Test: 0.3460
Epoch: 19, Loss: 3.6898, Train: 0.6000, Val: 0.3660, Test: 0.3630
Epoch: 20, Loss: 3.5174, Train: 0.6143, Val: 0.3840, Test: 0.3830
Epoch: 21, Loss: 3.6223, Train: 0.6571, Val: 0.4120, Test: 0.4040
Epoch: 22, Loss: 3.4319, Train: 0.6643, Val: 0.4500, Test: 0.4380
Epoch: 23, Loss: 3.2471, Train: 0.6929, Val: 0.4660, Test: 0.4730
Epoch: 24, Loss: 3.4571, Train: 0.7071, Val: 0.4840, Test: 0.4930
Epoch: 25, Loss: 3.2229, Train: 0.7214, Val: 0.4940, Test: 0.5160
Epoch: 26, Loss: 3.1941, Train: 0.7571, Val: 0.5240, Test: 0.5390
Epoch: 27, Loss: 3.2412, Train: 0.7857, Val: 0.5560, Test: 0.5690
Epoch: 28, Loss: 3.3543, Train: 0.8071, Val: 0.5760, Test: 0.5960
Epoch: 29, Loss: 3.0930, Train: 0.8143, Val: 0.6220, Test: 0.6390
Epoch: 30, Loss: 3.2589, Train: 0.8500, Val: 0.6540, Test: 0.6740
Epoch: 31, Loss: 2.7858, Train: 0.8571, Val: 0.6820, Test: 0.6940
Epoch: 32, Loss: 3.1676, Train: 0.8714, Val: 0.6960, Test: 0.7240
Epoch: 33, Loss: 3.0415, Train: 0.8929, Val: 0.7120, Test: 0.7330
Epoch: 34, Loss: 2.8440, Train: 0.9286, Val: 0.7300, Test: 0.7470
Epoch: 35, Loss: 3.1364, Train: 0.9571, Val: 0.7720, Test: 0.7600
Epoch: 36, Loss: 3.0457, Train: 0.9571, Val: 0.7760, Test: 0.7700
Epoch: 37, Loss: 2.7336, Train: 0.9643, Val: 0.7860, Test: 0.7770
Epoch: 38, Loss: 2.8026, Train: 0.9643, Val: 0.7840, Test: 0.7760
Epoch: 39, Loss: 2.7808, Train: 0.9643, Val: 0.7980, Test: 0.7820
Epoch: 40, Loss: 2.7096, Train: 0.9714, Val: 0.8000, Test: 0.7890
Epoch: 41, Loss: 2.7185, Train: 0.9786, Val: 0.7940, Test: 0.7950
Epoch: 42, Loss: 2.8217, Train: 0.9786, Val: 0.7940, Test: 0.7990
Epoch: 43, Loss: 2.6768, Train: 0.9786, Val: 0.7880, Test: 0.7970
Epoch: 44, Loss: 2.5965, Train: 0.9714, Val: 0.7860, Test: 0.7950
Epoch: 45, Loss: 2.6253, Train: 0.9857, Val: 0.7840, Test: 0.7950
Epoch: 46, Loss: 2.5533, Train: 0.9857, Val: 0.7860, Test: 0.8000
Epoch: 47, Loss: 2.8945, Train: 0.9857, Val: 0.7900, Test: 0.8020
Epoch: 48, Loss: 2.6569, Train: 0.9857, Val: 0.7920, Test: 0.8010
Epoch: 49, Loss: 2.7295, Train: 0.9786, Val: 0.7900, Test: 0.8050
Epoch: 50, Loss: 2.7250, Train: 0.9786, Val: 0.7900, Test: 0.8050
MAD:  0.1678
Best Test Accuracy: 0.8050, Val Accuracy: 0.7900, Train Accuracy: 0.9786
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1618, Train: 0.0571, Val: 0.0420, Test: 0.0390
Epoch: 2, Loss: 4.1467, Train: 0.2000, Val: 0.1300, Test: 0.1110
Epoch: 3, Loss: 4.1260, Train: 0.3071, Val: 0.2200, Test: 0.2300
Epoch: 4, Loss: 4.1092, Train: 0.4500, Val: 0.2940, Test: 0.3170
Epoch: 5, Loss: 4.0863, Train: 0.5214, Val: 0.3840, Test: 0.4090
Epoch: 6, Loss: 4.0523, Train: 0.5500, Val: 0.4200, Test: 0.4560
Epoch: 7, Loss: 4.0442, Train: 0.5643, Val: 0.4300, Test: 0.4790
Epoch: 8, Loss: 3.9959, Train: 0.5571, Val: 0.4340, Test: 0.4770
Epoch: 9, Loss: 3.9577, Train: 0.5643, Val: 0.4420, Test: 0.4770
Epoch: 10, Loss: 3.9180, Train: 0.5643, Val: 0.4440, Test: 0.4750
Epoch: 11, Loss: 3.9285, Train: 0.5643, Val: 0.4480, Test: 0.4800
Epoch: 12, Loss: 3.8269, Train: 0.5643, Val: 0.4560, Test: 0.4800
Epoch: 13, Loss: 3.7818, Train: 0.5786, Val: 0.4660, Test: 0.4880
Epoch: 14, Loss: 3.8516, Train: 0.5786, Val: 0.4740, Test: 0.4980
Epoch: 15, Loss: 3.6589, Train: 0.5857, Val: 0.4780, Test: 0.5020
Epoch: 16, Loss: 3.7857, Train: 0.6000, Val: 0.4840, Test: 0.5020
Epoch: 17, Loss: 3.6510, Train: 0.6000, Val: 0.4860, Test: 0.5060
Epoch: 18, Loss: 3.5633, Train: 0.6071, Val: 0.4860, Test: 0.5130
Epoch: 19, Loss: 3.5896, Train: 0.6286, Val: 0.4900, Test: 0.5130
Epoch: 20, Loss: 3.5127, Train: 0.6286, Val: 0.4940, Test: 0.5210
Epoch: 21, Loss: 3.4730, Train: 0.6429, Val: 0.5060, Test: 0.5250
Epoch: 22, Loss: 3.4249, Train: 0.6571, Val: 0.5080, Test: 0.5340
Epoch: 23, Loss: 3.2673, Train: 0.6786, Val: 0.5140, Test: 0.5380
Epoch: 24, Loss: 3.3432, Train: 0.6857, Val: 0.5200, Test: 0.5480
Epoch: 25, Loss: 3.2719, Train: 0.7000, Val: 0.5340, Test: 0.5540
Epoch: 26, Loss: 3.1555, Train: 0.7357, Val: 0.5460, Test: 0.5710
Epoch: 27, Loss: 3.2766, Train: 0.7500, Val: 0.5620, Test: 0.5960
Epoch: 28, Loss: 3.4419, Train: 0.7714, Val: 0.5740, Test: 0.6170
Epoch: 29, Loss: 3.1822, Train: 0.7929, Val: 0.6020, Test: 0.6350
Epoch: 30, Loss: 2.8891, Train: 0.8214, Val: 0.6260, Test: 0.6640
Epoch: 31, Loss: 2.9947, Train: 0.8714, Val: 0.6540, Test: 0.6830
Epoch: 32, Loss: 3.0253, Train: 0.8857, Val: 0.6780, Test: 0.6990
Epoch: 33, Loss: 2.9155, Train: 0.9000, Val: 0.7040, Test: 0.7260
Epoch: 34, Loss: 3.0088, Train: 0.9214, Val: 0.7200, Test: 0.7480
Epoch: 35, Loss: 2.9997, Train: 0.9286, Val: 0.7420, Test: 0.7720
Epoch: 36, Loss: 3.0541, Train: 0.9357, Val: 0.7500, Test: 0.7750
Epoch: 37, Loss: 2.6622, Train: 0.9571, Val: 0.7580, Test: 0.7750
Epoch: 38, Loss: 2.9801, Train: 0.9643, Val: 0.7580, Test: 0.7780
Epoch: 39, Loss: 3.1659, Train: 0.9714, Val: 0.7640, Test: 0.7770
Epoch: 40, Loss: 2.9360, Train: 0.9714, Val: 0.7660, Test: 0.7810
Epoch: 41, Loss: 3.0075, Train: 0.9714, Val: 0.7720, Test: 0.7780
Epoch: 42, Loss: 2.8352, Train: 0.9857, Val: 0.7660, Test: 0.7780
Epoch: 43, Loss: 2.9838, Train: 0.9857, Val: 0.7740, Test: 0.7820
Epoch: 44, Loss: 2.7333, Train: 0.9857, Val: 0.7820, Test: 0.7860
Epoch: 45, Loss: 2.7086, Train: 0.9857, Val: 0.7800, Test: 0.7900
Epoch: 46, Loss: 2.6198, Train: 0.9857, Val: 0.7860, Test: 0.7890
Epoch: 47, Loss: 2.6197, Train: 0.9857, Val: 0.7900, Test: 0.7880
Epoch: 48, Loss: 2.6544, Train: 0.9857, Val: 0.7920, Test: 0.7910
Epoch: 49, Loss: 2.6181, Train: 0.9857, Val: 0.7920, Test: 0.7920
Epoch: 50, Loss: 2.9305, Train: 0.9857, Val: 0.7920, Test: 0.7910
MAD:  0.1822
Best Test Accuracy: 0.7920, Val Accuracy: 0.7920, Train Accuracy: 0.9857
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1633, Train: 0.1071, Val: 0.0800, Test: 0.0750
Epoch: 2, Loss: 4.1504, Train: 0.1643, Val: 0.1380, Test: 0.1400
Epoch: 3, Loss: 4.1437, Train: 0.2571, Val: 0.1840, Test: 0.1850
Epoch: 4, Loss: 4.1247, Train: 0.3214, Val: 0.2320, Test: 0.2230
Epoch: 5, Loss: 4.0976, Train: 0.4071, Val: 0.2700, Test: 0.2720
Epoch: 6, Loss: 4.0879, Train: 0.4357, Val: 0.2900, Test: 0.2970
Epoch: 7, Loss: 4.0414, Train: 0.5071, Val: 0.3180, Test: 0.3180
Epoch: 8, Loss: 3.9992, Train: 0.5714, Val: 0.3280, Test: 0.3400
Epoch: 9, Loss: 4.0050, Train: 0.6214, Val: 0.3560, Test: 0.3780
Epoch: 10, Loss: 3.9560, Train: 0.6571, Val: 0.3840, Test: 0.4070
Epoch: 11, Loss: 3.9550, Train: 0.6786, Val: 0.4140, Test: 0.4380
Epoch: 12, Loss: 3.8842, Train: 0.6857, Val: 0.4280, Test: 0.4620
Epoch: 13, Loss: 3.8613, Train: 0.6929, Val: 0.4420, Test: 0.4790
Epoch: 14, Loss: 3.8867, Train: 0.7214, Val: 0.4560, Test: 0.4870
Epoch: 15, Loss: 3.7925, Train: 0.7143, Val: 0.4520, Test: 0.4860
Epoch: 16, Loss: 3.6729, Train: 0.7071, Val: 0.4480, Test: 0.4790
Epoch: 17, Loss: 3.6808, Train: 0.6714, Val: 0.4260, Test: 0.4640
Epoch: 18, Loss: 3.6787, Train: 0.6786, Val: 0.4220, Test: 0.4530
Epoch: 19, Loss: 3.5608, Train: 0.6643, Val: 0.4220, Test: 0.4550
Epoch: 20, Loss: 3.5040, Train: 0.6714, Val: 0.4200, Test: 0.4540
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 21, Loss: 3.6136, Train: 0.6714, Val: 0.4200, Test: 0.4550
Epoch: 22, Loss: 3.4311, Train: 0.6929, Val: 0.4380, Test: 0.4640
Epoch: 23, Loss: 3.4246, Train: 0.6929, Val: 0.4440, Test: 0.4660
Epoch: 24, Loss: 3.3752, Train: 0.6929, Val: 0.4620, Test: 0.4950
Epoch: 25, Loss: 3.4012, Train: 0.7071, Val: 0.4920, Test: 0.5150
Epoch: 26, Loss: 3.3156, Train: 0.7357, Val: 0.5220, Test: 0.5420
Epoch: 27, Loss: 3.4185, Train: 0.7429, Val: 0.5400, Test: 0.5660
Epoch: 28, Loss: 3.2530, Train: 0.7714, Val: 0.5660, Test: 0.5860
Epoch: 29, Loss: 3.3710, Train: 0.7714, Val: 0.5760, Test: 0.6040
Epoch: 30, Loss: 3.2558, Train: 0.8143, Val: 0.5980, Test: 0.6180
Epoch: 31, Loss: 2.9076, Train: 0.8429, Val: 0.6000, Test: 0.6390
Epoch: 32, Loss: 3.2952, Train: 0.8500, Val: 0.6140, Test: 0.6550
Epoch: 33, Loss: 2.8904, Train: 0.8643, Val: 0.6220, Test: 0.6630
Epoch: 34, Loss: 3.1489, Train: 0.8857, Val: 0.6340, Test: 0.6720
Epoch: 35, Loss: 3.1327, Train: 0.9000, Val: 0.6460, Test: 0.6800
Epoch: 36, Loss: 3.0455, Train: 0.9000, Val: 0.6680, Test: 0.6890
Epoch: 37, Loss: 2.9957, Train: 0.9214, Val: 0.6860, Test: 0.7010
Epoch: 38, Loss: 3.0088, Train: 0.9429, Val: 0.6960, Test: 0.7090
Epoch: 39, Loss: 2.7672, Train: 0.9429, Val: 0.7120, Test: 0.7240
Epoch: 40, Loss: 3.1060, Train: 0.9429, Val: 0.7240, Test: 0.7380
Epoch: 41, Loss: 2.8674, Train: 0.9571, Val: 0.7260, Test: 0.7480
Epoch: 42, Loss: 2.6450, Train: 0.9571, Val: 0.7340, Test: 0.7550
Epoch: 43, Loss: 2.5625, Train: 0.9643, Val: 0.7420, Test: 0.7650
Epoch: 44, Loss: 2.7302, Train: 0.9643, Val: 0.7480, Test: 0.7750
Epoch: 45, Loss: 2.3173, Train: 0.9643, Val: 0.7620, Test: 0.7830
Epoch: 46, Loss: 2.8079, Train: 0.9643, Val: 0.7620, Test: 0.7870
Epoch: 47, Loss: 2.6714, Train: 0.9643, Val: 0.7660, Test: 0.7960
Epoch: 48, Loss: 2.8716, Train: 0.9714, Val: 0.7680, Test: 0.7990
Epoch: 49, Loss: 2.6133, Train: 0.9643, Val: 0.7720, Test: 0.7990
Epoch: 50, Loss: 2.6886, Train: 0.9643, Val: 0.7740, Test: 0.8010
MAD:  0.1635
Best Test Accuracy: 0.8010, Val Accuracy: 0.7740, Train Accuracy: 0.9643
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1581, Train: 0.0786, Val: 0.0420, Test: 0.0540
Epoch: 2, Loss: 4.1438, Train: 0.1929, Val: 0.0760, Test: 0.0980
Epoch: 3, Loss: 4.1140, Train: 0.2357, Val: 0.0940, Test: 0.1230
Epoch: 4, Loss: 4.0981, Train: 0.2571, Val: 0.0940, Test: 0.1270
Epoch: 5, Loss: 4.0583, Train: 0.2571, Val: 0.0960, Test: 0.1210
Epoch: 6, Loss: 4.0147, Train: 0.2643, Val: 0.0940, Test: 0.1240
Epoch: 7, Loss: 3.9916, Train: 0.2500, Val: 0.0980, Test: 0.1220
Epoch: 8, Loss: 3.9494, Train: 0.2571, Val: 0.0960, Test: 0.1230
Epoch: 9, Loss: 3.9569, Train: 0.2643, Val: 0.0960, Test: 0.1270
Epoch: 10, Loss: 3.8736, Train: 0.2571, Val: 0.0980, Test: 0.1320
Epoch: 11, Loss: 3.8361, Train: 0.2857, Val: 0.0960, Test: 0.1360
Epoch: 12, Loss: 3.7382, Train: 0.3000, Val: 0.0960, Test: 0.1380
Epoch: 13, Loss: 3.6980, Train: 0.3071, Val: 0.0960, Test: 0.1390
Epoch: 14, Loss: 3.6519, Train: 0.3357, Val: 0.1020, Test: 0.1420
Epoch: 15, Loss: 3.6297, Train: 0.3357, Val: 0.1120, Test: 0.1430
Epoch: 16, Loss: 3.6559, Train: 0.3714, Val: 0.1140, Test: 0.1460
Epoch: 17, Loss: 3.5628, Train: 0.3714, Val: 0.1220, Test: 0.1480
Epoch: 18, Loss: 3.6546, Train: 0.3857, Val: 0.1340, Test: 0.1560
Epoch: 19, Loss: 3.4773, Train: 0.4214, Val: 0.1540, Test: 0.1730
Epoch: 20, Loss: 3.5391, Train: 0.4714, Val: 0.1760, Test: 0.2030
Epoch: 21, Loss: 3.3983, Train: 0.5143, Val: 0.2080, Test: 0.2280
Epoch: 22, Loss: 3.5196, Train: 0.5857, Val: 0.2520, Test: 0.2720
Epoch: 23, Loss: 3.4014, Train: 0.6929, Val: 0.3460, Test: 0.3480
Epoch: 24, Loss: 3.3145, Train: 0.7929, Val: 0.4180, Test: 0.4280
Epoch: 25, Loss: 3.3140, Train: 0.8286, Val: 0.5080, Test: 0.5130
Epoch: 26, Loss: 3.0834, Train: 0.8714, Val: 0.5760, Test: 0.5910
Epoch: 27, Loss: 3.2009, Train: 0.8929, Val: 0.6260, Test: 0.6380
Epoch: 28, Loss: 3.0647, Train: 0.9286, Val: 0.6860, Test: 0.6920
Epoch: 29, Loss: 3.1571, Train: 0.9357, Val: 0.7260, Test: 0.7330
Epoch: 30, Loss: 3.1798, Train: 0.9571, Val: 0.7600, Test: 0.7670
Epoch: 31, Loss: 2.9365, Train: 0.9571, Val: 0.7740, Test: 0.7810
Epoch: 32, Loss: 3.0218, Train: 0.9714, Val: 0.7900, Test: 0.7920
Epoch: 33, Loss: 2.9255, Train: 0.9714, Val: 0.7940, Test: 0.8010
Epoch: 34, Loss: 3.0218, Train: 0.9714, Val: 0.7960, Test: 0.8070
Epoch: 35, Loss: 2.9374, Train: 0.9714, Val: 0.7960, Test: 0.8110
Epoch: 36, Loss: 3.1614, Train: 0.9714, Val: 0.7920, Test: 0.8120
Epoch: 37, Loss: 2.8985, Train: 0.9786, Val: 0.7900, Test: 0.8040
Epoch: 38, Loss: 2.7292, Train: 0.9786, Val: 0.7880, Test: 0.7990
Epoch: 39, Loss: 2.8124, Train: 0.9714, Val: 0.7840, Test: 0.7940
Epoch: 40, Loss: 2.9922, Train: 0.9714, Val: 0.7800, Test: 0.7970
Epoch: 41, Loss: 2.8590, Train: 0.9714, Val: 0.7720, Test: 0.7960
Epoch: 42, Loss: 2.9013, Train: 0.9714, Val: 0.7720, Test: 0.7960
Epoch: 43, Loss: 2.5594, Train: 0.9714, Val: 0.7660, Test: 0.7950
Epoch: 44, Loss: 2.5667, Train: 0.9857, Val: 0.7720, Test: 0.8000
Epoch: 45, Loss: 2.8668, Train: 0.9857, Val: 0.7760, Test: 0.8010
Epoch: 46, Loss: 2.7066, Train: 0.9857, Val: 0.7780, Test: 0.7990
Epoch: 47, Loss: 2.3062, Train: 0.9857, Val: 0.7800, Test: 0.8060
Epoch: 48, Loss: 2.7341, Train: 0.9857, Val: 0.7800, Test: 0.8070
Epoch: 49, Loss: 2.4609, Train: 0.9857, Val: 0.7800, Test: 0.8060
Epoch: 50, Loss: 2.5893, Train: 0.9857, Val: 0.7800, Test: 0.8040
MAD:  0.056
Best Test Accuracy: 0.8120, Val Accuracy: 0.7920, Train Accuracy: 0.9714
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1577, Train: 0.1214, Val: 0.0500, Test: 0.0620
Epoch: 2, Loss: 4.1487, Train: 0.3071, Val: 0.1540, Test: 0.1690
Epoch: 3, Loss: 4.1372, Train: 0.4286, Val: 0.2560, Test: 0.2720
Epoch: 4, Loss: 4.1163, Train: 0.4929, Val: 0.3740, Test: 0.3770
Epoch: 5, Loss: 4.0840, Train: 0.5286, Val: 0.4220, Test: 0.4300
Epoch: 6, Loss: 4.0778, Train: 0.5643, Val: 0.4520, Test: 0.4620
Epoch: 7, Loss: 4.0300, Train: 0.5786, Val: 0.4640, Test: 0.4730
Epoch: 8, Loss: 4.0091, Train: 0.5857, Val: 0.4760, Test: 0.4890
Epoch: 9, Loss: 3.9559, Train: 0.6000, Val: 0.5040, Test: 0.4990
Epoch: 10, Loss: 3.9531, Train: 0.6143, Val: 0.5200, Test: 0.5110
Epoch: 11, Loss: 3.9274, Train: 0.6143, Val: 0.5260, Test: 0.5270
Epoch: 12, Loss: 3.9306, Train: 0.6429, Val: 0.5340, Test: 0.5370
Epoch: 13, Loss: 3.8205, Train: 0.6429, Val: 0.5360, Test: 0.5500
Epoch: 14, Loss: 3.8092, Train: 0.6571, Val: 0.5340, Test: 0.5530
Epoch: 15, Loss: 3.8145, Train: 0.6571, Val: 0.5380, Test: 0.5560
Epoch: 16, Loss: 3.6860, Train: 0.6643, Val: 0.5380, Test: 0.5620
Epoch: 17, Loss: 3.7535, Train: 0.6643, Val: 0.5400, Test: 0.5660
Epoch: 18, Loss: 3.5705, Train: 0.6643, Val: 0.5540, Test: 0.5690
Epoch: 19, Loss: 3.5543, Train: 0.6643, Val: 0.5600, Test: 0.5720
Epoch: 20, Loss: 3.5333, Train: 0.6643, Val: 0.5640, Test: 0.5730
Epoch: 21, Loss: 3.6495, Train: 0.6786, Val: 0.5760, Test: 0.5790
Epoch: 22, Loss: 3.5055, Train: 0.6786, Val: 0.5820, Test: 0.5870
Epoch: 23, Loss: 3.4217, Train: 0.6786, Val: 0.5880, Test: 0.5920
Epoch: 24, Loss: 3.3159, Train: 0.6857, Val: 0.5960, Test: 0.5960
Epoch: 25, Loss: 3.2948, Train: 0.7000, Val: 0.6000, Test: 0.5980
Epoch: 26, Loss: 3.3011, Train: 0.7071, Val: 0.6100, Test: 0.6090
Epoch: 27, Loss: 3.0994, Train: 0.7286, Val: 0.6180, Test: 0.6290
Epoch: 28, Loss: 3.2902, Train: 0.7786, Val: 0.6300, Test: 0.6400
Epoch: 29, Loss: 3.0713, Train: 0.8000, Val: 0.6480, Test: 0.6510
Epoch: 30, Loss: 3.0639, Train: 0.8214, Val: 0.6600, Test: 0.6660
Epoch: 31, Loss: 3.0823, Train: 0.8714, Val: 0.6720, Test: 0.6870
Epoch: 32, Loss: 3.1105, Train: 0.9000, Val: 0.6920, Test: 0.7180
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 33, Loss: 3.1933, Train: 0.9214, Val: 0.7240, Test: 0.7510
Epoch: 34, Loss: 2.8797, Train: 0.9357, Val: 0.7340, Test: 0.7660
Epoch: 35, Loss: 2.7976, Train: 0.9500, Val: 0.7660, Test: 0.7750
Epoch: 36, Loss: 2.7828, Train: 0.9500, Val: 0.7700, Test: 0.7830
Epoch: 37, Loss: 2.7337, Train: 0.9714, Val: 0.7660, Test: 0.7890
Epoch: 38, Loss: 3.0202, Train: 0.9714, Val: 0.7700, Test: 0.7910
Epoch: 39, Loss: 3.3152, Train: 0.9643, Val: 0.7640, Test: 0.7960
Epoch: 40, Loss: 3.1357, Train: 0.9786, Val: 0.7660, Test: 0.7930
Epoch: 41, Loss: 2.8552, Train: 0.9786, Val: 0.7600, Test: 0.7880
Epoch: 42, Loss: 3.0015, Train: 0.9786, Val: 0.7560, Test: 0.7840
Epoch: 43, Loss: 3.0703, Train: 0.9786, Val: 0.7480, Test: 0.7770
Epoch: 44, Loss: 2.8315, Train: 0.9786, Val: 0.7460, Test: 0.7770
Epoch: 45, Loss: 2.7112, Train: 0.9786, Val: 0.7420, Test: 0.7800
Epoch: 46, Loss: 2.7733, Train: 0.9786, Val: 0.7420, Test: 0.7810
Epoch: 47, Loss: 2.8718, Train: 0.9786, Val: 0.7460, Test: 0.7810
Epoch: 48, Loss: 2.8961, Train: 0.9786, Val: 0.7540, Test: 0.7810
Epoch: 49, Loss: 2.3285, Train: 0.9786, Val: 0.7520, Test: 0.7820
Epoch: 50, Loss: 2.5315, Train: 0.9786, Val: 0.7620, Test: 0.7840
MAD:  0.0624
Best Test Accuracy: 0.7960, Val Accuracy: 0.7640, Train Accuracy: 0.9643
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1613, Train: 0.1143, Val: 0.1000, Test: 0.0840
Epoch: 2, Loss: 4.1314, Train: 0.2571, Val: 0.1440, Test: 0.1440
Epoch: 3, Loss: 4.1226, Train: 0.3000, Val: 0.1700, Test: 0.1720
Epoch: 4, Loss: 4.0915, Train: 0.3000, Val: 0.1840, Test: 0.1850
Epoch: 5, Loss: 4.0745, Train: 0.3214, Val: 0.1980, Test: 0.1940
Epoch: 6, Loss: 4.0283, Train: 0.3500, Val: 0.2040, Test: 0.1980
Epoch: 7, Loss: 3.9947, Train: 0.3643, Val: 0.2060, Test: 0.2070
Epoch: 8, Loss: 3.9935, Train: 0.3786, Val: 0.2080, Test: 0.2120
Epoch: 9, Loss: 3.9489, Train: 0.3857, Val: 0.2100, Test: 0.2150
Epoch: 10, Loss: 3.9400, Train: 0.4000, Val: 0.2100, Test: 0.2180
Epoch: 11, Loss: 3.9349, Train: 0.3929, Val: 0.2100, Test: 0.2180
Epoch: 12, Loss: 3.8333, Train: 0.3857, Val: 0.2100, Test: 0.2170
Epoch: 13, Loss: 3.7466, Train: 0.3786, Val: 0.2080, Test: 0.2160
Epoch: 14, Loss: 3.7513, Train: 0.3786, Val: 0.2080, Test: 0.2170
Epoch: 15, Loss: 3.7123, Train: 0.3714, Val: 0.2060, Test: 0.2160
Epoch: 16, Loss: 3.5924, Train: 0.3643, Val: 0.2040, Test: 0.2140
Epoch: 17, Loss: 3.5713, Train: 0.3571, Val: 0.2040, Test: 0.2110
Epoch: 18, Loss: 3.6370, Train: 0.3571, Val: 0.2020, Test: 0.2110
Epoch: 19, Loss: 3.5113, Train: 0.3571, Val: 0.2020, Test: 0.2110
Epoch: 20, Loss: 3.2511, Train: 0.3571, Val: 0.2020, Test: 0.2130
Epoch: 21, Loss: 3.4393, Train: 0.3571, Val: 0.2020, Test: 0.2130
Epoch: 22, Loss: 3.3898, Train: 0.3643, Val: 0.2080, Test: 0.2160
Epoch: 23, Loss: 3.4720, Train: 0.4071, Val: 0.2140, Test: 0.2260
Epoch: 24, Loss: 3.0502, Train: 0.4357, Val: 0.2160, Test: 0.2330
Epoch: 25, Loss: 3.2723, Train: 0.4643, Val: 0.2320, Test: 0.2440
Epoch: 26, Loss: 3.2800, Train: 0.5071, Val: 0.2580, Test: 0.2640
Epoch: 27, Loss: 3.2426, Train: 0.5786, Val: 0.3020, Test: 0.2900
Epoch: 28, Loss: 3.5335, Train: 0.6429, Val: 0.3640, Test: 0.3390
Epoch: 29, Loss: 3.1750, Train: 0.7714, Val: 0.4240, Test: 0.4170
Epoch: 30, Loss: 3.1501, Train: 0.8214, Val: 0.4780, Test: 0.4740
Epoch: 31, Loss: 3.1483, Train: 0.8857, Val: 0.5420, Test: 0.5560
Epoch: 32, Loss: 2.9541, Train: 0.9429, Val: 0.6380, Test: 0.6340
Epoch: 33, Loss: 2.9700, Train: 0.9714, Val: 0.6980, Test: 0.7120
Epoch: 34, Loss: 3.0193, Train: 0.9714, Val: 0.7280, Test: 0.7410
Epoch: 35, Loss: 2.8990, Train: 0.9786, Val: 0.7400, Test: 0.7580
Epoch: 36, Loss: 3.1645, Train: 0.9786, Val: 0.7520, Test: 0.7760
Epoch: 37, Loss: 3.0508, Train: 0.9786, Val: 0.7740, Test: 0.7870
Epoch: 38, Loss: 3.0945, Train: 0.9714, Val: 0.7860, Test: 0.7910
Epoch: 39, Loss: 2.9816, Train: 0.9714, Val: 0.7880, Test: 0.7860
Epoch: 40, Loss: 2.8290, Train: 0.9643, Val: 0.7880, Test: 0.7870
Epoch: 41, Loss: 2.9704, Train: 0.9643, Val: 0.7900, Test: 0.7850
Epoch: 42, Loss: 2.7537, Train: 0.9714, Val: 0.7920, Test: 0.7880
Epoch: 43, Loss: 2.8233, Train: 0.9714, Val: 0.7900, Test: 0.7880
Epoch: 44, Loss: 2.7551, Train: 0.9714, Val: 0.7880, Test: 0.7880
Epoch: 45, Loss: 2.8585, Train: 0.9714, Val: 0.7820, Test: 0.7880
Epoch: 46, Loss: 2.7635, Train: 0.9714, Val: 0.7760, Test: 0.7880
Epoch: 47, Loss: 2.8836, Train: 0.9714, Val: 0.7760, Test: 0.7870
Epoch: 48, Loss: 2.6066, Train: 0.9714, Val: 0.7720, Test: 0.7830
Epoch: 49, Loss: 2.8150, Train: 0.9714, Val: 0.7700, Test: 0.7880
Epoch: 50, Loss: 2.7274, Train: 0.9786, Val: 0.7700, Test: 0.7900
MAD:  0.065
Best Test Accuracy: 0.7910, Val Accuracy: 0.7860, Train Accuracy: 0.9714
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1635, Train: 0.1357, Val: 0.0580, Test: 0.0630
Epoch: 2, Loss: 4.1434, Train: 0.3643, Val: 0.2040, Test: 0.2260
Epoch: 3, Loss: 4.1228, Train: 0.4429, Val: 0.2780, Test: 0.2960
Epoch: 4, Loss: 4.1039, Train: 0.4857, Val: 0.3080, Test: 0.3090
Epoch: 5, Loss: 4.0731, Train: 0.5214, Val: 0.3180, Test: 0.3140
Epoch: 6, Loss: 4.0408, Train: 0.5143, Val: 0.3220, Test: 0.3200
Epoch: 7, Loss: 4.0123, Train: 0.5214, Val: 0.3020, Test: 0.3180
Epoch: 8, Loss: 3.9572, Train: 0.5071, Val: 0.3000, Test: 0.3190
Epoch: 9, Loss: 3.9098, Train: 0.5071, Val: 0.3020, Test: 0.3230
Epoch: 10, Loss: 3.9387, Train: 0.5357, Val: 0.3100, Test: 0.3310
Epoch: 11, Loss: 3.8436, Train: 0.5643, Val: 0.3380, Test: 0.3560
Epoch: 12, Loss: 3.8391, Train: 0.6071, Val: 0.3640, Test: 0.4010
Epoch: 13, Loss: 3.7986, Train: 0.6357, Val: 0.4200, Test: 0.4440
Epoch: 14, Loss: 3.6923, Train: 0.6286, Val: 0.4380, Test: 0.4680
Epoch: 15, Loss: 3.6960, Train: 0.6429, Val: 0.4620, Test: 0.4920
Epoch: 16, Loss: 3.6549, Train: 0.6857, Val: 0.5000, Test: 0.5110
Epoch: 17, Loss: 3.4890, Train: 0.7071, Val: 0.5200, Test: 0.5330
Epoch: 18, Loss: 3.5429, Train: 0.7071, Val: 0.5300, Test: 0.5520
Epoch: 19, Loss: 3.5675, Train: 0.7143, Val: 0.5420, Test: 0.5610
Epoch: 20, Loss: 3.3486, Train: 0.7214, Val: 0.5440, Test: 0.5640
Epoch: 21, Loss: 3.4302, Train: 0.7429, Val: 0.5480, Test: 0.5720
Epoch: 22, Loss: 3.3306, Train: 0.7500, Val: 0.5640, Test: 0.5790
Epoch: 23, Loss: 3.3900, Train: 0.7643, Val: 0.5740, Test: 0.5930
Epoch: 24, Loss: 3.3988, Train: 0.8071, Val: 0.5880, Test: 0.6090
Epoch: 25, Loss: 3.2220, Train: 0.8214, Val: 0.6000, Test: 0.6220
Epoch: 26, Loss: 3.2844, Train: 0.8357, Val: 0.6200, Test: 0.6510
Epoch: 27, Loss: 3.3942, Train: 0.8571, Val: 0.6460, Test: 0.6830
Epoch: 28, Loss: 3.1982, Train: 0.8929, Val: 0.6800, Test: 0.6900
Epoch: 29, Loss: 3.2228, Train: 0.9071, Val: 0.6920, Test: 0.7130
Epoch: 30, Loss: 2.9584, Train: 0.9214, Val: 0.7060, Test: 0.7290
Epoch: 31, Loss: 3.0347, Train: 0.9357, Val: 0.7160, Test: 0.7400
Epoch: 32, Loss: 3.4178, Train: 0.9429, Val: 0.7260, Test: 0.7590
Epoch: 33, Loss: 3.0419, Train: 0.9500, Val: 0.7340, Test: 0.7670
Epoch: 34, Loss: 3.2083, Train: 0.9571, Val: 0.7480, Test: 0.7670
Epoch: 35, Loss: 3.0616, Train: 0.9714, Val: 0.7560, Test: 0.7720
Epoch: 36, Loss: 3.1177, Train: 0.9714, Val: 0.7560, Test: 0.7760
Epoch: 37, Loss: 2.9624, Train: 0.9857, Val: 0.7420, Test: 0.7810
Epoch: 38, Loss: 2.9286, Train: 0.9786, Val: 0.7500, Test: 0.7830
Epoch: 39, Loss: 2.5350, Train: 0.9643, Val: 0.7600, Test: 0.7850
Epoch: 40, Loss: 2.9350, Train: 0.9643, Val: 0.7680, Test: 0.7960
Epoch: 41, Loss: 3.0637, Train: 0.9643, Val: 0.7800, Test: 0.8000
Epoch: 42, Loss: 2.7904, Train: 0.9643, Val: 0.7840, Test: 0.8060
Epoch: 43, Loss: 2.6970, Train: 0.9643, Val: 0.7880, Test: 0.8090
Epoch: 44, Loss: 2.8286, Train: 0.9643, Val: 0.7860, Test: 0.8140
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 45, Loss: 2.6281, Train: 0.9643, Val: 0.7880, Test: 0.8160
Epoch: 46, Loss: 2.7066, Train: 0.9571, Val: 0.7860, Test: 0.8150
Epoch: 47, Loss: 2.6307, Train: 0.9643, Val: 0.7840, Test: 0.8150
Epoch: 48, Loss: 2.9903, Train: 0.9714, Val: 0.7840, Test: 0.8180
Epoch: 49, Loss: 2.4523, Train: 0.9714, Val: 0.7840, Test: 0.8230
Epoch: 50, Loss: 2.8565, Train: 0.9714, Val: 0.7860, Test: 0.8210
MAD:  0.1697
Best Test Accuracy: 0.8230, Val Accuracy: 0.7840, Train Accuracy: 0.9714
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0): ParallelGNNBlock(
      (conv1): GCNConv(64, 64)
      (conv2): GCNConv(64, 64)
    )
    (1): GCNConv(64, 64)
  )
  (proj): Linear(in_features=1433, out_features=64, bias=True)
  (mlp): Linear(in_features=64, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.1629, Train: 0.2071, Val: 0.1360, Test: 0.1490
Epoch: 2, Loss: 4.1327, Train: 0.2857, Val: 0.1680, Test: 0.1900
Epoch: 3, Loss: 4.1197, Train: 0.3000, Val: 0.1780, Test: 0.1940
Epoch: 4, Loss: 4.0941, Train: 0.3071, Val: 0.1820, Test: 0.2030
Epoch: 5, Loss: 4.0597, Train: 0.3214, Val: 0.1820, Test: 0.2090
Epoch: 6, Loss: 4.0288, Train: 0.3500, Val: 0.1920, Test: 0.2150
Epoch: 7, Loss: 4.0020, Train: 0.3643, Val: 0.1960, Test: 0.2180
Epoch: 8, Loss: 3.9635, Train: 0.3714, Val: 0.2020, Test: 0.2220
Epoch: 9, Loss: 3.9399, Train: 0.3857, Val: 0.2020, Test: 0.2240
Epoch: 10, Loss: 3.9136, Train: 0.3857, Val: 0.2020, Test: 0.2270
Epoch: 11, Loss: 3.8759, Train: 0.3929, Val: 0.2020, Test: 0.2290
Epoch: 12, Loss: 3.9064, Train: 0.4000, Val: 0.2100, Test: 0.2340
Epoch: 13, Loss: 3.8481, Train: 0.4143, Val: 0.2100, Test: 0.2380
Epoch: 14, Loss: 3.6970, Train: 0.4286, Val: 0.2140, Test: 0.2410
Epoch: 15, Loss: 3.7395, Train: 0.4500, Val: 0.2200, Test: 0.2530
Epoch: 16, Loss: 3.7268, Train: 0.4643, Val: 0.2300, Test: 0.2640
Epoch: 17, Loss: 3.6470, Train: 0.4714, Val: 0.2440, Test: 0.2700
Epoch: 18, Loss: 3.6856, Train: 0.4857, Val: 0.2500, Test: 0.2740
Epoch: 19, Loss: 3.4356, Train: 0.4786, Val: 0.2580, Test: 0.2720
Epoch: 20, Loss: 3.6086, Train: 0.5071, Val: 0.2720, Test: 0.2810
Epoch: 21, Loss: 3.7337, Train: 0.5357, Val: 0.2760, Test: 0.3060
Epoch: 22, Loss: 3.4758, Train: 0.5857, Val: 0.2940, Test: 0.3230
Epoch: 23, Loss: 3.4667, Train: 0.5929, Val: 0.3220, Test: 0.3490
Epoch: 24, Loss: 3.0649, Train: 0.6071, Val: 0.3360, Test: 0.3640
Epoch: 25, Loss: 3.2505, Train: 0.6786, Val: 0.3760, Test: 0.3870
Epoch: 26, Loss: 3.6592, Train: 0.7143, Val: 0.4120, Test: 0.4100
Epoch: 27, Loss: 3.3596, Train: 0.7643, Val: 0.4520, Test: 0.4390
Epoch: 28, Loss: 3.4141, Train: 0.7714, Val: 0.4780, Test: 0.4820
Epoch: 29, Loss: 3.2579, Train: 0.7786, Val: 0.5140, Test: 0.5170
Epoch: 30, Loss: 3.2481, Train: 0.8000, Val: 0.5380, Test: 0.5400
Epoch: 31, Loss: 3.1193, Train: 0.8143, Val: 0.5500, Test: 0.5600
Epoch: 32, Loss: 3.1120, Train: 0.8429, Val: 0.5640, Test: 0.5720
Epoch: 33, Loss: 3.0507, Train: 0.8429, Val: 0.5700, Test: 0.5780
Epoch: 34, Loss: 2.9904, Train: 0.8429, Val: 0.5720, Test: 0.5780
Epoch: 35, Loss: 3.2243, Train: 0.8500, Val: 0.5700, Test: 0.5850
Epoch: 36, Loss: 2.9488, Train: 0.8429, Val: 0.5740, Test: 0.5800
Epoch: 37, Loss: 3.0900, Train: 0.8429, Val: 0.5800, Test: 0.5790
Epoch: 38, Loss: 3.2162, Train: 0.8429, Val: 0.5820, Test: 0.5760
Epoch: 39, Loss: 3.0568, Train: 0.8429, Val: 0.5820, Test: 0.5750
Epoch: 40, Loss: 2.9083, Train: 0.8500, Val: 0.5780, Test: 0.5780
Epoch: 41, Loss: 2.9039, Train: 0.8500, Val: 0.5760, Test: 0.5760
Epoch: 42, Loss: 3.0088, Train: 0.8429, Val: 0.5760, Test: 0.5750
Epoch: 43, Loss: 2.8449, Train: 0.8429, Val: 0.5740, Test: 0.5730
Epoch: 44, Loss: 3.0359, Train: 0.8429, Val: 0.5740, Test: 0.5750
Epoch: 45, Loss: 3.2078, Train: 0.8429, Val: 0.5760, Test: 0.5790
Epoch: 46, Loss: 2.7809, Train: 0.8429, Val: 0.5760, Test: 0.5800
Epoch: 47, Loss: 2.8694, Train: 0.8429, Val: 0.5780, Test: 0.5740
Epoch: 48, Loss: 2.9231, Train: 0.8429, Val: 0.5780, Test: 0.5750
Epoch: 49, Loss: 2.8310, Train: 0.8500, Val: 0.5860, Test: 0.5780
Epoch: 50, Loss: 2.8613, Train: 0.8571, Val: 0.5860, Test: 0.5800
MAD:  0.0428
Best Test Accuracy: 0.5850, Val Accuracy: 0.5700, Train Accuracy: 0.8500
Training completed.
Average Test Accuracy:  0.7824000000000001 ± 0.06648939765105413
Average MAD:  0.11614 ± 0.05289616621268501

/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9868, Train: 0.2786, Val: 0.1580, Test: 0.1620
Epoch: 2, Loss: 1.8976, Train: 0.5857, Val: 0.3680, Test: 0.3710
Epoch: 3, Loss: 1.8225, Train: 0.7786, Val: 0.4820, Test: 0.5370
Epoch: 4, Loss: 1.7588, Train: 0.8357, Val: 0.5560, Test: 0.6240
Epoch: 5, Loss: 1.6899, Train: 0.8929, Val: 0.5820, Test: 0.6590
Epoch: 6, Loss: 1.6184, Train: 0.9286, Val: 0.6280, Test: 0.7020
Epoch: 7, Loss: 1.5628, Train: 0.9500, Val: 0.6720, Test: 0.7300
Epoch: 8, Loss: 1.4773, Train: 0.9571, Val: 0.7020, Test: 0.7470
Epoch: 9, Loss: 1.3712, Train: 0.9643, Val: 0.7300, Test: 0.7650
Epoch: 10, Loss: 1.2896, Train: 0.9714, Val: 0.7380, Test: 0.7800
Epoch: 11, Loss: 1.2051, Train: 0.9786, Val: 0.7500, Test: 0.7850
Epoch: 12, Loss: 1.0921, Train: 0.9786, Val: 0.7660, Test: 0.7980
Epoch: 13, Loss: 1.0601, Train: 0.9786, Val: 0.7720, Test: 0.8040
Epoch: 14, Loss: 0.9035, Train: 0.9714, Val: 0.7760, Test: 0.8050
Epoch: 15, Loss: 0.8006, Train: 0.9714, Val: 0.7900, Test: 0.8110
Epoch: 16, Loss: 0.7640, Train: 0.9714, Val: 0.7920, Test: 0.8150
Epoch: 17, Loss: 0.6467, Train: 0.9714, Val: 0.7800, Test: 0.8100
Epoch: 18, Loss: 0.5399, Train: 0.9714, Val: 0.7800, Test: 0.8090
Epoch: 19, Loss: 0.4998, Train: 0.9786, Val: 0.7800, Test: 0.8090
Epoch: 20, Loss: 0.4466, Train: 0.9786, Val: 0.7820, Test: 0.8050
Epoch: 21, Loss: 0.3432, Train: 0.9857, Val: 0.7720, Test: 0.8030
Epoch: 22, Loss: 0.3189, Train: 0.9857, Val: 0.7740, Test: 0.8020
Epoch: 23, Loss: 0.2568, Train: 0.9929, Val: 0.7760, Test: 0.8030
Epoch: 24, Loss: 0.2092, Train: 0.9929, Val: 0.7800, Test: 0.7990
Epoch: 25, Loss: 0.2348, Train: 0.9929, Val: 0.7820, Test: 0.7980
Epoch: 26, Loss: 0.1981, Train: 0.9929, Val: 0.7820, Test: 0.7940
Epoch: 27, Loss: 0.1690, Train: 0.9929, Val: 0.7840, Test: 0.7950
Epoch: 28, Loss: 0.1255, Train: 0.9929, Val: 0.7860, Test: 0.7940
Epoch: 29, Loss: 0.1313, Train: 0.9929, Val: 0.7820, Test: 0.7960
Epoch: 30, Loss: 0.1010, Train: 0.9929, Val: 0.7820, Test: 0.7960
Epoch: 31, Loss: 0.0881, Train: 0.9929, Val: 0.7860, Test: 0.7920
Epoch: 32, Loss: 0.0813, Train: 1.0000, Val: 0.7920, Test: 0.7930
Epoch: 33, Loss: 0.0802, Train: 1.0000, Val: 0.7860, Test: 0.7950
Epoch: 34, Loss: 0.0866, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 35, Loss: 0.0834, Train: 1.0000, Val: 0.7680, Test: 0.7830
Epoch: 36, Loss: 0.0653, Train: 1.0000, Val: 0.7720, Test: 0.7810
Epoch: 37, Loss: 0.0588, Train: 1.0000, Val: 0.7780, Test: 0.7840
Epoch: 38, Loss: 0.0450, Train: 1.0000, Val: 0.7820, Test: 0.7890
Epoch: 39, Loss: 0.0296, Train: 1.0000, Val: 0.7800, Test: 0.7900
Epoch: 40, Loss: 0.0610, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 41, Loss: 0.0407, Train: 1.0000, Val: 0.7860, Test: 0.7930
Epoch: 42, Loss: 0.0303, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 43, Loss: 0.0237, Train: 1.0000, Val: 0.7780, Test: 0.7920
Epoch: 44, Loss: 0.0353, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 45, Loss: 0.0173, Train: 1.0000, Val: 0.7720, Test: 0.7960
Epoch: 46, Loss: 0.0243, Train: 1.0000, Val: 0.7720, Test: 0.7960
Epoch: 47, Loss: 0.0234, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 48, Loss: 0.0282, Train: 1.0000, Val: 0.7880, Test: 0.8060
Epoch: 49, Loss: 0.0247, Train: 1.0000, Val: 0.7880, Test: 0.8100
Epoch: 50, Loss: 0.0176, Train: 1.0000, Val: 0.7880, Test: 0.8120
Epoch: 51, Loss: 0.0223, Train: 1.0000, Val: 0.7840, Test: 0.8080
Epoch: 52, Loss: 0.0142, Train: 1.0000, Val: 0.7840, Test: 0.8040
Epoch: 53, Loss: 0.0135, Train: 1.0000, Val: 0.7800, Test: 0.7990
Epoch: 54, Loss: 0.0229, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 55, Loss: 0.0260, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 56, Loss: 0.0171, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 57, Loss: 0.0065, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 58, Loss: 0.0095, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 59, Loss: 0.0091, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 60, Loss: 0.0132, Train: 1.0000, Val: 0.7520, Test: 0.7720
Epoch: 61, Loss: 0.0174, Train: 1.0000, Val: 0.7540, Test: 0.7730
Epoch: 62, Loss: 0.0199, Train: 1.0000, Val: 0.7580, Test: 0.7780
Epoch: 63, Loss: 0.0256, Train: 1.0000, Val: 0.7600, Test: 0.7860
Epoch: 64, Loss: 0.0151, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 65, Loss: 0.0080, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 66, Loss: 0.0105, Train: 1.0000, Val: 0.7780, Test: 0.7990
Epoch: 67, Loss: 0.0048, Train: 1.0000, Val: 0.7860, Test: 0.8000
Epoch: 68, Loss: 0.0106, Train: 1.0000, Val: 0.7880, Test: 0.8050
Epoch: 69, Loss: 0.0096, Train: 1.0000, Val: 0.7880, Test: 0.8050
Epoch: 70, Loss: 0.0076, Train: 1.0000, Val: 0.7880, Test: 0.8040
Epoch: 71, Loss: 0.0107, Train: 1.0000, Val: 0.7860, Test: 0.8050
Epoch: 72, Loss: 0.0113, Train: 1.0000, Val: 0.7880, Test: 0.8000
Epoch: 73, Loss: 0.0123, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 74, Loss: 0.0084, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 75, Loss: 0.0205, Train: 1.0000, Val: 0.7820, Test: 0.7980
Epoch: 76, Loss: 0.0066, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 77, Loss: 0.0097, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 78, Loss: 0.0088, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 79, Loss: 0.0080, Train: 1.0000, Val: 0.7620, Test: 0.7820
Epoch: 80, Loss: 0.0050, Train: 1.0000, Val: 0.7620, Test: 0.7800
Epoch: 81, Loss: 0.0199, Train: 1.0000, Val: 0.7600, Test: 0.7820
Epoch: 82, Loss: 0.0263, Train: 1.0000, Val: 0.7580, Test: 0.7840
Epoch: 83, Loss: 0.0074, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 84, Loss: 0.0083, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 85, Loss: 0.0041, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 86, Loss: 0.0137, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 87, Loss: 0.0066, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 88, Loss: 0.0070, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 89, Loss: 0.0048, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 90, Loss: 0.0110, Train: 1.0000, Val: 0.7720, Test: 0.7840
Epoch: 91, Loss: 0.0073, Train: 1.0000, Val: 0.7720, Test: 0.7850
Epoch: 92, Loss: 0.0134, Train: 1.0000, Val: 0.7660, Test: 0.7830
Epoch: 93, Loss: 0.0053, Train: 1.0000, Val: 0.7660, Test: 0.7830
Epoch: 94, Loss: 0.0058, Train: 1.0000, Val: 0.7660, Test: 0.7850
Epoch: 95, Loss: 0.0055, Train: 1.0000, Val: 0.7620, Test: 0.7870
Epoch: 96, Loss: 0.0060, Train: 1.0000, Val: 0.7580, Test: 0.7860
Epoch: 97, Loss: 0.0092, Train: 1.0000, Val: 0.7580, Test: 0.7850
Epoch: 98, Loss: 0.0044, Train: 1.0000, Val: 0.7560, Test: 0.7840
Epoch: 99, Loss: 0.0125, Train: 1.0000, Val: 0.7560, Test: 0.7790
Epoch: 100, Loss: 0.0093, Train: 1.0000, Val: 0.7600, Test: 0.7820
MAD:  0.9069
Best Test Accuracy: 0.8150, Val Accuracy: 0.7920, Train Accuracy: 0.9714
Training completed.
Seed:  1
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9584, Train: 0.3643, Val: 0.1680, Test: 0.2240
Epoch: 2, Loss: 1.8749, Train: 0.7143, Val: 0.3820, Test: 0.4840
Epoch: 3, Loss: 1.8241, Train: 0.8714, Val: 0.5300, Test: 0.6210
Epoch: 4, Loss: 1.7593, Train: 0.9071, Val: 0.6080, Test: 0.6500
Epoch: 5, Loss: 1.6911, Train: 0.9000, Val: 0.6180, Test: 0.6510
Epoch: 6, Loss: 1.6212, Train: 0.9214, Val: 0.6280, Test: 0.6570
Epoch: 7, Loss: 1.5273, Train: 0.9214, Val: 0.6480, Test: 0.6650
Epoch: 8, Loss: 1.4555, Train: 0.9286, Val: 0.6680, Test: 0.6990
Epoch: 9, Loss: 1.3597, Train: 0.9286, Val: 0.7080, Test: 0.7260
Epoch: 10, Loss: 1.2830, Train: 0.9571, Val: 0.7180, Test: 0.7360
Epoch: 11, Loss: 1.1565, Train: 0.9714, Val: 0.7360, Test: 0.7500
Epoch: 12, Loss: 1.0404, Train: 0.9714, Val: 0.7360, Test: 0.7620
Epoch: 13, Loss: 0.9786, Train: 0.9786, Val: 0.7460, Test: 0.7820
Epoch: 14, Loss: 0.8352, Train: 0.9786, Val: 0.7600, Test: 0.7920
Epoch: 15, Loss: 0.7721, Train: 0.9786, Val: 0.7620, Test: 0.8020
Epoch: 16, Loss: 0.6673, Train: 0.9786, Val: 0.7660, Test: 0.8030
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 17, Loss: 0.6050, Train: 0.9786, Val: 0.7660, Test: 0.8020
Epoch: 18, Loss: 0.4852, Train: 0.9786, Val: 0.7720, Test: 0.8010
Epoch: 19, Loss: 0.4656, Train: 0.9786, Val: 0.7740, Test: 0.8000
Epoch: 20, Loss: 0.3940, Train: 0.9786, Val: 0.7740, Test: 0.8010
Epoch: 21, Loss: 0.3211, Train: 0.9786, Val: 0.7700, Test: 0.8000
Epoch: 22, Loss: 0.3009, Train: 0.9929, Val: 0.7660, Test: 0.7990
Epoch: 23, Loss: 0.2942, Train: 0.9929, Val: 0.7660, Test: 0.7990
Epoch: 24, Loss: 0.2299, Train: 0.9929, Val: 0.7700, Test: 0.7980
Epoch: 25, Loss: 0.1880, Train: 0.9929, Val: 0.7720, Test: 0.7990
Epoch: 26, Loss: 0.1640, Train: 0.9929, Val: 0.7740, Test: 0.7990
Epoch: 27, Loss: 0.1794, Train: 0.9929, Val: 0.7660, Test: 0.8080
Epoch: 28, Loss: 0.1358, Train: 1.0000, Val: 0.7680, Test: 0.8100
Epoch: 29, Loss: 0.1470, Train: 1.0000, Val: 0.7720, Test: 0.8100
Epoch: 30, Loss: 0.1040, Train: 1.0000, Val: 0.7720, Test: 0.8080
Epoch: 31, Loss: 0.0789, Train: 1.0000, Val: 0.7680, Test: 0.8020
Epoch: 32, Loss: 0.0914, Train: 1.0000, Val: 0.7640, Test: 0.7970
Epoch: 33, Loss: 0.0807, Train: 1.0000, Val: 0.7660, Test: 0.7960
Epoch: 34, Loss: 0.0764, Train: 1.0000, Val: 0.7620, Test: 0.7970
Epoch: 35, Loss: 0.0520, Train: 1.0000, Val: 0.7620, Test: 0.7940
Epoch: 36, Loss: 0.0520, Train: 1.0000, Val: 0.7580, Test: 0.7910
Epoch: 37, Loss: 0.0683, Train: 1.0000, Val: 0.7580, Test: 0.7830
Epoch: 38, Loss: 0.0456, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 39, Loss: 0.0429, Train: 1.0000, Val: 0.7580, Test: 0.7840
Epoch: 40, Loss: 0.0241, Train: 1.0000, Val: 0.7560, Test: 0.7870
Epoch: 41, Loss: 0.0380, Train: 1.0000, Val: 0.7600, Test: 0.7920
Epoch: 42, Loss: 0.0193, Train: 1.0000, Val: 0.7640, Test: 0.7950
Epoch: 43, Loss: 0.0259, Train: 1.0000, Val: 0.7660, Test: 0.7970
Epoch: 44, Loss: 0.0225, Train: 1.0000, Val: 0.7700, Test: 0.7960
Epoch: 45, Loss: 0.0205, Train: 1.0000, Val: 0.7700, Test: 0.7980
Epoch: 46, Loss: 0.0299, Train: 1.0000, Val: 0.7700, Test: 0.7990
Epoch: 47, Loss: 0.0357, Train: 1.0000, Val: 0.7700, Test: 0.8000
Epoch: 48, Loss: 0.0144, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 49, Loss: 0.0164, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 50, Loss: 0.0147, Train: 1.0000, Val: 0.7660, Test: 0.7940
Epoch: 51, Loss: 0.0157, Train: 1.0000, Val: 0.7640, Test: 0.7950
Epoch: 52, Loss: 0.0229, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 53, Loss: 0.0202, Train: 1.0000, Val: 0.7600, Test: 0.7920
Epoch: 54, Loss: 0.0195, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 55, Loss: 0.0072, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 56, Loss: 0.0070, Train: 1.0000, Val: 0.7560, Test: 0.7810
Epoch: 57, Loss: 0.0176, Train: 1.0000, Val: 0.7560, Test: 0.7820
Epoch: 58, Loss: 0.0130, Train: 1.0000, Val: 0.7540, Test: 0.7820
Epoch: 59, Loss: 0.0071, Train: 1.0000, Val: 0.7520, Test: 0.7810
Epoch: 60, Loss: 0.0157, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 61, Loss: 0.0149, Train: 1.0000, Val: 0.7540, Test: 0.7870
Epoch: 62, Loss: 0.0142, Train: 1.0000, Val: 0.7620, Test: 0.7850
Epoch: 63, Loss: 0.0103, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 64, Loss: 0.0102, Train: 1.0000, Val: 0.7740, Test: 0.7840
Epoch: 65, Loss: 0.0151, Train: 1.0000, Val: 0.7720, Test: 0.7790
Epoch: 66, Loss: 0.0451, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 67, Loss: 0.0134, Train: 1.0000, Val: 0.7620, Test: 0.7880
Epoch: 68, Loss: 0.0070, Train: 1.0000, Val: 0.7500, Test: 0.7900
Epoch: 69, Loss: 0.0208, Train: 1.0000, Val: 0.7500, Test: 0.7930
Epoch: 70, Loss: 0.0097, Train: 1.0000, Val: 0.7480, Test: 0.7930
Epoch: 71, Loss: 0.0063, Train: 1.0000, Val: 0.7500, Test: 0.7900
Epoch: 72, Loss: 0.0101, Train: 1.0000, Val: 0.7520, Test: 0.7900
Epoch: 73, Loss: 0.0112, Train: 1.0000, Val: 0.7540, Test: 0.7910
Epoch: 74, Loss: 0.0111, Train: 1.0000, Val: 0.7480, Test: 0.7920
Epoch: 75, Loss: 0.0142, Train: 1.0000, Val: 0.7480, Test: 0.7920
Epoch: 76, Loss: 0.0066, Train: 1.0000, Val: 0.7460, Test: 0.7890
Epoch: 77, Loss: 0.0081, Train: 1.0000, Val: 0.7460, Test: 0.7870
Epoch: 78, Loss: 0.0225, Train: 1.0000, Val: 0.7480, Test: 0.7900
Epoch: 79, Loss: 0.0096, Train: 1.0000, Val: 0.7500, Test: 0.7910
Epoch: 80, Loss: 0.0065, Train: 1.0000, Val: 0.7540, Test: 0.7940
Epoch: 81, Loss: 0.0048, Train: 1.0000, Val: 0.7560, Test: 0.7920
Epoch: 82, Loss: 0.0093, Train: 1.0000, Val: 0.7600, Test: 0.7930
Epoch: 83, Loss: 0.0165, Train: 1.0000, Val: 0.7640, Test: 0.7980
Epoch: 84, Loss: 0.0059, Train: 1.0000, Val: 0.7660, Test: 0.7980
Epoch: 85, Loss: 0.0072, Train: 1.0000, Val: 0.7640, Test: 0.8000
Epoch: 86, Loss: 0.0087, Train: 1.0000, Val: 0.7660, Test: 0.7980
Epoch: 87, Loss: 0.0073, Train: 1.0000, Val: 0.7660, Test: 0.7950
Epoch: 88, Loss: 0.0067, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 89, Loss: 0.0044, Train: 1.0000, Val: 0.7620, Test: 0.7920
Epoch: 90, Loss: 0.0108, Train: 1.0000, Val: 0.7600, Test: 0.7910
Epoch: 91, Loss: 0.0102, Train: 1.0000, Val: 0.7580, Test: 0.7900
Epoch: 92, Loss: 0.0104, Train: 1.0000, Val: 0.7540, Test: 0.7850
Epoch: 93, Loss: 0.0058, Train: 1.0000, Val: 0.7520, Test: 0.7830
Epoch: 94, Loss: 0.0055, Train: 1.0000, Val: 0.7500, Test: 0.7820
Epoch: 95, Loss: 0.0046, Train: 1.0000, Val: 0.7500, Test: 0.7810
Epoch: 96, Loss: 0.0062, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 97, Loss: 0.0049, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 98, Loss: 0.0107, Train: 1.0000, Val: 0.7580, Test: 0.7810
Epoch: 99, Loss: 0.0072, Train: 1.0000, Val: 0.7600, Test: 0.7820
Epoch: 100, Loss: 0.0081, Train: 1.0000, Val: 0.7580, Test: 0.7820
MAD:  0.8756
Best Test Accuracy: 0.8100, Val Accuracy: 0.7680, Train Accuracy: 1.0000
Training completed.
Seed:  2
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9584, Train: 0.3286, Val: 0.2180, Test: 0.2060
Epoch: 2, Loss: 1.8864, Train: 0.5714, Val: 0.3620, Test: 0.3320
Epoch: 3, Loss: 1.8708, Train: 0.7643, Val: 0.4780, Test: 0.4880
Epoch: 4, Loss: 1.7684, Train: 0.8571, Val: 0.5420, Test: 0.5970
Epoch: 5, Loss: 1.6997, Train: 0.9357, Val: 0.6120, Test: 0.6560
Epoch: 6, Loss: 1.5977, Train: 0.9714, Val: 0.6500, Test: 0.6750
Epoch: 7, Loss: 1.5525, Train: 0.9786, Val: 0.6800, Test: 0.7070
Epoch: 8, Loss: 1.4209, Train: 0.9714, Val: 0.7000, Test: 0.7290
Epoch: 9, Loss: 1.3664, Train: 0.9714, Val: 0.7200, Test: 0.7440
Epoch: 10, Loss: 1.2489, Train: 0.9714, Val: 0.7280, Test: 0.7600
Epoch: 11, Loss: 1.1700, Train: 0.9714, Val: 0.7500, Test: 0.7730
Epoch: 12, Loss: 1.0221, Train: 0.9714, Val: 0.7620, Test: 0.7800
Epoch: 13, Loss: 0.9504, Train: 0.9714, Val: 0.7700, Test: 0.7890
Epoch: 14, Loss: 0.8527, Train: 0.9714, Val: 0.7700, Test: 0.7920
Epoch: 15, Loss: 0.7952, Train: 0.9786, Val: 0.7760, Test: 0.7930
Epoch: 16, Loss: 0.6600, Train: 0.9786, Val: 0.7820, Test: 0.7970
Epoch: 17, Loss: 0.6087, Train: 0.9857, Val: 0.7840, Test: 0.8000
Epoch: 18, Loss: 0.5206, Train: 0.9857, Val: 0.7840, Test: 0.8000
Epoch: 19, Loss: 0.4491, Train: 0.9857, Val: 0.7840, Test: 0.8030
Epoch: 20, Loss: 0.4201, Train: 0.9929, Val: 0.7820, Test: 0.8040
Epoch: 21, Loss: 0.3188, Train: 0.9929, Val: 0.7840, Test: 0.8090
Epoch: 22, Loss: 0.3109, Train: 0.9929, Val: 0.7840, Test: 0.8080
Epoch: 23, Loss: 0.2527, Train: 0.9929, Val: 0.7820, Test: 0.8090
Epoch: 24, Loss: 0.2218, Train: 0.9929, Val: 0.7860, Test: 0.8090
Epoch: 25, Loss: 0.1690, Train: 0.9929, Val: 0.7780, Test: 0.8090
Epoch: 26, Loss: 0.1533, Train: 0.9857, Val: 0.7800, Test: 0.8060
Epoch: 27, Loss: 0.1495, Train: 0.9929, Val: 0.7860, Test: 0.8100
Epoch: 28, Loss: 0.1221, Train: 0.9929, Val: 0.7860, Test: 0.8120
Epoch: 29, Loss: 0.1035, Train: 1.0000, Val: 0.7860, Test: 0.8090
Epoch: 30, Loss: 0.1201, Train: 1.0000, Val: 0.7840, Test: 0.8030
Epoch: 31, Loss: 0.0665, Train: 1.0000, Val: 0.7840, Test: 0.8030
Epoch: 32, Loss: 0.0715, Train: 1.0000, Val: 0.7840, Test: 0.8030
Epoch: 33, Loss: 0.0645, Train: 1.0000, Val: 0.7860, Test: 0.8060
Epoch: 34, Loss: 0.0668, Train: 1.0000, Val: 0.7860, Test: 0.8080
Epoch: 35, Loss: 0.0438, Train: 1.0000, Val: 0.7820, Test: 0.8070
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 36, Loss: 0.0533, Train: 1.0000, Val: 0.7840, Test: 0.8030
Epoch: 37, Loss: 0.0360, Train: 1.0000, Val: 0.7800, Test: 0.8050
Epoch: 38, Loss: 0.0334, Train: 1.0000, Val: 0.7720, Test: 0.8020
Epoch: 39, Loss: 0.0514, Train: 1.0000, Val: 0.7720, Test: 0.8000
Epoch: 40, Loss: 0.0380, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 41, Loss: 0.0289, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 42, Loss: 0.0211, Train: 1.0000, Val: 0.7620, Test: 0.7860
Epoch: 43, Loss: 0.0188, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 44, Loss: 0.0282, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 45, Loss: 0.0188, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 46, Loss: 0.0440, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 47, Loss: 0.0297, Train: 1.0000, Val: 0.7700, Test: 0.7920
Epoch: 48, Loss: 0.0165, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 49, Loss: 0.0158, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 50, Loss: 0.0269, Train: 1.0000, Val: 0.7640, Test: 0.7880
Epoch: 51, Loss: 0.0167, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 52, Loss: 0.0179, Train: 1.0000, Val: 0.7660, Test: 0.7870
Epoch: 53, Loss: 0.0170, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 54, Loss: 0.0162, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 55, Loss: 0.0375, Train: 1.0000, Val: 0.7660, Test: 0.7880
Epoch: 56, Loss: 0.0148, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 57, Loss: 0.0124, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 58, Loss: 0.0114, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 59, Loss: 0.0233, Train: 1.0000, Val: 0.7820, Test: 0.7920
Epoch: 60, Loss: 0.0152, Train: 1.0000, Val: 0.7820, Test: 0.7910
Epoch: 61, Loss: 0.0132, Train: 1.0000, Val: 0.7820, Test: 0.7930
Epoch: 62, Loss: 0.0111, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 63, Loss: 0.0125, Train: 1.0000, Val: 0.7700, Test: 0.7920
Epoch: 64, Loss: 0.0204, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 65, Loss: 0.0094, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 66, Loss: 0.0153, Train: 1.0000, Val: 0.7600, Test: 0.7920
Epoch: 67, Loss: 0.0170, Train: 1.0000, Val: 0.7620, Test: 0.7990
Epoch: 68, Loss: 0.0078, Train: 1.0000, Val: 0.7720, Test: 0.8040
Epoch: 69, Loss: 0.0091, Train: 1.0000, Val: 0.7780, Test: 0.8080
Epoch: 70, Loss: 0.0103, Train: 1.0000, Val: 0.7800, Test: 0.8030
Epoch: 71, Loss: 0.0071, Train: 1.0000, Val: 0.7840, Test: 0.7990
Epoch: 72, Loss: 0.0084, Train: 1.0000, Val: 0.7820, Test: 0.8010
Epoch: 73, Loss: 0.0052, Train: 1.0000, Val: 0.7820, Test: 0.8020
Epoch: 74, Loss: 0.0130, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 75, Loss: 0.0093, Train: 1.0000, Val: 0.7820, Test: 0.8030
Epoch: 76, Loss: 0.0154, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 77, Loss: 0.0076, Train: 1.0000, Val: 0.7780, Test: 0.7950
Epoch: 78, Loss: 0.0046, Train: 1.0000, Val: 0.7760, Test: 0.7950
Epoch: 79, Loss: 0.0068, Train: 1.0000, Val: 0.7680, Test: 0.7930
Epoch: 80, Loss: 0.0060, Train: 1.0000, Val: 0.7600, Test: 0.7870
Epoch: 81, Loss: 0.0059, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 82, Loss: 0.0044, Train: 1.0000, Val: 0.7580, Test: 0.7860
Epoch: 83, Loss: 0.0055, Train: 1.0000, Val: 0.7560, Test: 0.7880
Epoch: 84, Loss: 0.0111, Train: 1.0000, Val: 0.7560, Test: 0.7870
Epoch: 85, Loss: 0.0040, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 86, Loss: 0.0047, Train: 1.0000, Val: 0.7600, Test: 0.7890
Epoch: 87, Loss: 0.0083, Train: 1.0000, Val: 0.7620, Test: 0.7940
Epoch: 88, Loss: 0.0118, Train: 1.0000, Val: 0.7620, Test: 0.7960
Epoch: 89, Loss: 0.0048, Train: 1.0000, Val: 0.7680, Test: 0.7990
Epoch: 90, Loss: 0.0071, Train: 1.0000, Val: 0.7760, Test: 0.8010
Epoch: 91, Loss: 0.0139, Train: 1.0000, Val: 0.7780, Test: 0.8020
Epoch: 92, Loss: 0.0047, Train: 1.0000, Val: 0.7820, Test: 0.8040
Epoch: 93, Loss: 0.0098, Train: 1.0000, Val: 0.7820, Test: 0.8020
Epoch: 94, Loss: 0.0109, Train: 1.0000, Val: 0.7800, Test: 0.8030
Epoch: 95, Loss: 0.0079, Train: 1.0000, Val: 0.7800, Test: 0.8020
Epoch: 96, Loss: 0.0093, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 97, Loss: 0.0083, Train: 1.0000, Val: 0.7820, Test: 0.8020
Epoch: 98, Loss: 0.0070, Train: 1.0000, Val: 0.7820, Test: 0.8000
Epoch: 99, Loss: 0.0075, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 100, Loss: 0.0054, Train: 1.0000, Val: 0.7840, Test: 0.7990
MAD:  0.9412
Best Test Accuracy: 0.8120, Val Accuracy: 0.7860, Train Accuracy: 0.9929
Training completed.
Seed:  3
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9811, Train: 0.2214, Val: 0.0940, Test: 0.1050
Epoch: 2, Loss: 1.8917, Train: 0.5286, Val: 0.2620, Test: 0.2790
Epoch: 3, Loss: 1.7963, Train: 0.8286, Val: 0.4380, Test: 0.4610
Epoch: 4, Loss: 1.7725, Train: 0.9214, Val: 0.5640, Test: 0.5820
Epoch: 5, Loss: 1.7326, Train: 0.9571, Val: 0.6560, Test: 0.6610
Epoch: 6, Loss: 1.6191, Train: 0.9643, Val: 0.6880, Test: 0.7060
Epoch: 7, Loss: 1.5280, Train: 0.9786, Val: 0.7000, Test: 0.7370
Epoch: 8, Loss: 1.4835, Train: 0.9786, Val: 0.7160, Test: 0.7480
Epoch: 9, Loss: 1.3731, Train: 0.9786, Val: 0.7260, Test: 0.7560
Epoch: 10, Loss: 1.2860, Train: 0.9714, Val: 0.7320, Test: 0.7640
Epoch: 11, Loss: 1.2198, Train: 0.9714, Val: 0.7440, Test: 0.7700
Epoch: 12, Loss: 1.1090, Train: 0.9786, Val: 0.7560, Test: 0.7770
Epoch: 13, Loss: 1.0072, Train: 0.9786, Val: 0.7660, Test: 0.7750
Epoch: 14, Loss: 0.8758, Train: 0.9857, Val: 0.7720, Test: 0.7760
Epoch: 15, Loss: 0.7640, Train: 0.9857, Val: 0.7740, Test: 0.7780
Epoch: 16, Loss: 0.7179, Train: 0.9857, Val: 0.7740, Test: 0.7880
Epoch: 17, Loss: 0.6171, Train: 0.9857, Val: 0.7760, Test: 0.7940
Epoch: 18, Loss: 0.5316, Train: 0.9857, Val: 0.7820, Test: 0.8000
Epoch: 19, Loss: 0.4749, Train: 0.9857, Val: 0.7840, Test: 0.8030
Epoch: 20, Loss: 0.4355, Train: 0.9857, Val: 0.7800, Test: 0.8060
Epoch: 21, Loss: 0.3838, Train: 0.9857, Val: 0.7740, Test: 0.7990
Epoch: 22, Loss: 0.3475, Train: 0.9857, Val: 0.7760, Test: 0.8020
Epoch: 23, Loss: 0.2785, Train: 0.9857, Val: 0.7760, Test: 0.8090
Epoch: 24, Loss: 0.2491, Train: 0.9857, Val: 0.7920, Test: 0.8080
Epoch: 25, Loss: 0.2105, Train: 0.9929, Val: 0.7940, Test: 0.8110
Epoch: 26, Loss: 0.1883, Train: 0.9929, Val: 0.7980, Test: 0.8130
Epoch: 27, Loss: 0.1377, Train: 1.0000, Val: 0.7980, Test: 0.8090
Epoch: 28, Loss: 0.1770, Train: 1.0000, Val: 0.7940, Test: 0.8070
Epoch: 29, Loss: 0.1390, Train: 1.0000, Val: 0.7900, Test: 0.8050
Epoch: 30, Loss: 0.1110, Train: 0.9929, Val: 0.7920, Test: 0.8030
Epoch: 31, Loss: 0.0863, Train: 0.9929, Val: 0.7920, Test: 0.8020
Epoch: 32, Loss: 0.0801, Train: 1.0000, Val: 0.7920, Test: 0.8050
Epoch: 33, Loss: 0.0973, Train: 1.0000, Val: 0.7920, Test: 0.8050
Epoch: 34, Loss: 0.0885, Train: 1.0000, Val: 0.7920, Test: 0.8030
Epoch: 35, Loss: 0.0672, Train: 1.0000, Val: 0.7920, Test: 0.8020
Epoch: 36, Loss: 0.0657, Train: 1.0000, Val: 0.7820, Test: 0.8030
Epoch: 37, Loss: 0.0488, Train: 1.0000, Val: 0.7760, Test: 0.8000
Epoch: 38, Loss: 0.0675, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 39, Loss: 0.0530, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 40, Loss: 0.0352, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 41, Loss: 0.0346, Train: 1.0000, Val: 0.7760, Test: 0.7960
Epoch: 42, Loss: 0.0290, Train: 1.0000, Val: 0.7740, Test: 0.7900
Epoch: 43, Loss: 0.0361, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 44, Loss: 0.0406, Train: 0.9929, Val: 0.7720, Test: 0.7890
Epoch: 45, Loss: 0.0404, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 46, Loss: 0.0387, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 47, Loss: 0.0334, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 48, Loss: 0.0172, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 49, Loss: 0.0231, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 50, Loss: 0.0177, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 51, Loss: 0.0167, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 52, Loss: 0.0144, Train: 1.0000, Val: 0.7740, Test: 0.7920
Epoch: 53, Loss: 0.0160, Train: 1.0000, Val: 0.7740, Test: 0.7920
Epoch: 54, Loss: 0.0273, Train: 1.0000, Val: 0.7800, Test: 0.7940
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 55, Loss: 0.0180, Train: 1.0000, Val: 0.7800, Test: 0.7910
Epoch: 56, Loss: 0.0139, Train: 1.0000, Val: 0.7780, Test: 0.7910
Epoch: 57, Loss: 0.0162, Train: 1.0000, Val: 0.7760, Test: 0.7910
Epoch: 58, Loss: 0.0122, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 59, Loss: 0.0187, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 60, Loss: 0.0112, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 61, Loss: 0.0175, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 62, Loss: 0.0174, Train: 1.0000, Val: 0.7640, Test: 0.7840
Epoch: 63, Loss: 0.0138, Train: 1.0000, Val: 0.7640, Test: 0.7830
Epoch: 64, Loss: 0.0236, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 65, Loss: 0.0227, Train: 1.0000, Val: 0.7740, Test: 0.7810
Epoch: 66, Loss: 0.0155, Train: 1.0000, Val: 0.7740, Test: 0.7780
Epoch: 67, Loss: 0.0139, Train: 1.0000, Val: 0.7780, Test: 0.7790
Epoch: 68, Loss: 0.0221, Train: 1.0000, Val: 0.7780, Test: 0.7830
Epoch: 69, Loss: 0.0126, Train: 1.0000, Val: 0.7800, Test: 0.7880
Epoch: 70, Loss: 0.0125, Train: 1.0000, Val: 0.7780, Test: 0.7920
Epoch: 71, Loss: 0.0135, Train: 1.0000, Val: 0.7760, Test: 0.7950
Epoch: 72, Loss: 0.0059, Train: 1.0000, Val: 0.7760, Test: 0.7940
Epoch: 73, Loss: 0.0167, Train: 1.0000, Val: 0.7720, Test: 0.7880
Epoch: 74, Loss: 0.0096, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 75, Loss: 0.0141, Train: 1.0000, Val: 0.7720, Test: 0.7860
Epoch: 76, Loss: 0.0077, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 77, Loss: 0.0088, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 78, Loss: 0.0072, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 79, Loss: 0.0057, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 80, Loss: 0.0096, Train: 1.0000, Val: 0.7660, Test: 0.7940
Epoch: 81, Loss: 0.0062, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 82, Loss: 0.0138, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 83, Loss: 0.0070, Train: 1.0000, Val: 0.7780, Test: 0.7910
Epoch: 84, Loss: 0.0086, Train: 1.0000, Val: 0.7760, Test: 0.7910
Epoch: 85, Loss: 0.0060, Train: 1.0000, Val: 0.7780, Test: 0.7900
Epoch: 86, Loss: 0.0090, Train: 1.0000, Val: 0.7760, Test: 0.7910
Epoch: 87, Loss: 0.0040, Train: 1.0000, Val: 0.7760, Test: 0.7900
Epoch: 88, Loss: 0.0090, Train: 1.0000, Val: 0.7700, Test: 0.7850
Epoch: 89, Loss: 0.0072, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 90, Loss: 0.0098, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 91, Loss: 0.0123, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 92, Loss: 0.0060, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 93, Loss: 0.0083, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 94, Loss: 0.0073, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 95, Loss: 0.0098, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 96, Loss: 0.0063, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 97, Loss: 0.0108, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 98, Loss: 0.0072, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 99, Loss: 0.0233, Train: 1.0000, Val: 0.7740, Test: 0.7970
Epoch: 100, Loss: 0.0132, Train: 1.0000, Val: 0.7780, Test: 0.7920
MAD:  0.9424
Best Test Accuracy: 0.8130, Val Accuracy: 0.7980, Train Accuracy: 0.9929
Training completed.
Seed:  4
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9488, Train: 0.3929, Val: 0.1920, Test: 0.2150
Epoch: 2, Loss: 1.9176, Train: 0.5286, Val: 0.2720, Test: 0.3020
Epoch: 3, Loss: 1.8280, Train: 0.7071, Val: 0.3440, Test: 0.3650
Epoch: 4, Loss: 1.7545, Train: 0.8000, Val: 0.4280, Test: 0.4580
Epoch: 5, Loss: 1.7069, Train: 0.8786, Val: 0.5120, Test: 0.5310
Epoch: 6, Loss: 1.6287, Train: 0.8857, Val: 0.5680, Test: 0.5870
Epoch: 7, Loss: 1.5414, Train: 0.9286, Val: 0.5980, Test: 0.6240
Epoch: 8, Loss: 1.4835, Train: 0.9286, Val: 0.6240, Test: 0.6550
Epoch: 9, Loss: 1.3794, Train: 0.9500, Val: 0.6640, Test: 0.6850
Epoch: 10, Loss: 1.2807, Train: 0.9571, Val: 0.6920, Test: 0.7230
Epoch: 11, Loss: 1.2002, Train: 0.9643, Val: 0.7280, Test: 0.7380
Epoch: 12, Loss: 1.1164, Train: 0.9786, Val: 0.7360, Test: 0.7540
Epoch: 13, Loss: 1.0397, Train: 0.9786, Val: 0.7560, Test: 0.7660
Epoch: 14, Loss: 0.8953, Train: 0.9786, Val: 0.7740, Test: 0.7770
Epoch: 15, Loss: 0.8065, Train: 0.9857, Val: 0.7800, Test: 0.7880
Epoch: 16, Loss: 0.7440, Train: 0.9857, Val: 0.7820, Test: 0.7910
Epoch: 17, Loss: 0.6425, Train: 0.9857, Val: 0.7860, Test: 0.7980
Epoch: 18, Loss: 0.5801, Train: 0.9857, Val: 0.7900, Test: 0.8020
Epoch: 19, Loss: 0.4931, Train: 0.9857, Val: 0.7860, Test: 0.8050
Epoch: 20, Loss: 0.4484, Train: 0.9857, Val: 0.7860, Test: 0.8070
Epoch: 21, Loss: 0.3577, Train: 0.9857, Val: 0.7860, Test: 0.8080
Epoch: 22, Loss: 0.3337, Train: 0.9857, Val: 0.7880, Test: 0.8120
Epoch: 23, Loss: 0.2649, Train: 0.9857, Val: 0.7840, Test: 0.8130
Epoch: 24, Loss: 0.2881, Train: 0.9857, Val: 0.7820, Test: 0.8110
Epoch: 25, Loss: 0.2164, Train: 1.0000, Val: 0.7820, Test: 0.8080
Epoch: 26, Loss: 0.2033, Train: 1.0000, Val: 0.7900, Test: 0.8150
Epoch: 27, Loss: 0.1739, Train: 1.0000, Val: 0.7980, Test: 0.8120
Epoch: 28, Loss: 0.1791, Train: 1.0000, Val: 0.7980, Test: 0.8180
Epoch: 29, Loss: 0.1121, Train: 1.0000, Val: 0.7960, Test: 0.8180
Epoch: 30, Loss: 0.1157, Train: 0.9929, Val: 0.7980, Test: 0.8160
Epoch: 31, Loss: 0.1128, Train: 0.9929, Val: 0.7940, Test: 0.8150
Epoch: 32, Loss: 0.0805, Train: 0.9929, Val: 0.7920, Test: 0.8110
Epoch: 33, Loss: 0.0750, Train: 1.0000, Val: 0.7980, Test: 0.8060
Epoch: 34, Loss: 0.0907, Train: 1.0000, Val: 0.7900, Test: 0.8010
Epoch: 35, Loss: 0.0739, Train: 1.0000, Val: 0.7860, Test: 0.7930
Epoch: 36, Loss: 0.0642, Train: 1.0000, Val: 0.7760, Test: 0.7880
Epoch: 37, Loss: 0.0963, Train: 1.0000, Val: 0.7680, Test: 0.7820
Epoch: 38, Loss: 0.0542, Train: 1.0000, Val: 0.7680, Test: 0.7820
Epoch: 39, Loss: 0.0808, Train: 1.0000, Val: 0.7700, Test: 0.7870
Epoch: 40, Loss: 0.0417, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 41, Loss: 0.0299, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 42, Loss: 0.0209, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 43, Loss: 0.0343, Train: 1.0000, Val: 0.7900, Test: 0.8010
Epoch: 44, Loss: 0.0237, Train: 1.0000, Val: 0.7920, Test: 0.8030
Epoch: 45, Loss: 0.0323, Train: 1.0000, Val: 0.7860, Test: 0.8080
Epoch: 46, Loss: 0.0326, Train: 1.0000, Val: 0.7860, Test: 0.8100
Epoch: 47, Loss: 0.0252, Train: 1.0000, Val: 0.7880, Test: 0.8070
Epoch: 48, Loss: 0.0190, Train: 1.0000, Val: 0.7860, Test: 0.8060
Epoch: 49, Loss: 0.0164, Train: 1.0000, Val: 0.7780, Test: 0.8040
Epoch: 50, Loss: 0.0192, Train: 1.0000, Val: 0.7780, Test: 0.8010
Epoch: 51, Loss: 0.0166, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 52, Loss: 0.0114, Train: 1.0000, Val: 0.7820, Test: 0.7930
Epoch: 53, Loss: 0.0157, Train: 1.0000, Val: 0.7800, Test: 0.7920
Epoch: 54, Loss: 0.0189, Train: 1.0000, Val: 0.7780, Test: 0.7890
Epoch: 55, Loss: 0.0172, Train: 1.0000, Val: 0.7780, Test: 0.7900
Epoch: 56, Loss: 0.0138, Train: 1.0000, Val: 0.7860, Test: 0.7890
Epoch: 57, Loss: 0.0258, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 58, Loss: 0.0076, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 59, Loss: 0.0124, Train: 1.0000, Val: 0.7860, Test: 0.7930
Epoch: 60, Loss: 0.0171, Train: 1.0000, Val: 0.7780, Test: 0.7910
Epoch: 61, Loss: 0.0235, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 62, Loss: 0.0158, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 63, Loss: 0.0109, Train: 1.0000, Val: 0.7860, Test: 0.7930
Epoch: 64, Loss: 0.0161, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 65, Loss: 0.0092, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 66, Loss: 0.0112, Train: 1.0000, Val: 0.7760, Test: 0.7910
Epoch: 67, Loss: 0.0068, Train: 1.0000, Val: 0.7760, Test: 0.7910
Epoch: 68, Loss: 0.0123, Train: 1.0000, Val: 0.7800, Test: 0.7920
Epoch: 69, Loss: 0.0081, Train: 1.0000, Val: 0.7820, Test: 0.7920
Epoch: 70, Loss: 0.0107, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 71, Loss: 0.0077, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 72, Loss: 0.0081, Train: 1.0000, Val: 0.7840, Test: 0.8010
Epoch: 73, Loss: 0.0127, Train: 1.0000, Val: 0.7860, Test: 0.8050
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 74, Loss: 0.0119, Train: 1.0000, Val: 0.7880, Test: 0.8070
Epoch: 75, Loss: 0.0110, Train: 1.0000, Val: 0.7880, Test: 0.7990
Epoch: 76, Loss: 0.0171, Train: 1.0000, Val: 0.7880, Test: 0.8010
Epoch: 77, Loss: 0.0083, Train: 1.0000, Val: 0.7900, Test: 0.7990
Epoch: 78, Loss: 0.0120, Train: 1.0000, Val: 0.7880, Test: 0.8010
Epoch: 79, Loss: 0.0125, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 80, Loss: 0.0045, Train: 1.0000, Val: 0.7860, Test: 0.8000
Epoch: 81, Loss: 0.0138, Train: 1.0000, Val: 0.7780, Test: 0.7990
Epoch: 82, Loss: 0.0074, Train: 1.0000, Val: 0.7780, Test: 0.7980
Epoch: 83, Loss: 0.0094, Train: 1.0000, Val: 0.7700, Test: 0.7970
Epoch: 84, Loss: 0.0079, Train: 1.0000, Val: 0.7680, Test: 0.7960
Epoch: 85, Loss: 0.0079, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 86, Loss: 0.0098, Train: 1.0000, Val: 0.7660, Test: 0.7930
Epoch: 87, Loss: 0.0085, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 88, Loss: 0.0087, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 89, Loss: 0.0065, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 90, Loss: 0.0128, Train: 1.0000, Val: 0.7740, Test: 0.7900
Epoch: 91, Loss: 0.0079, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 92, Loss: 0.0099, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 93, Loss: 0.0262, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 94, Loss: 0.0078, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 95, Loss: 0.0052, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 96, Loss: 0.0106, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 97, Loss: 0.0070, Train: 1.0000, Val: 0.7760, Test: 0.7920
Epoch: 98, Loss: 0.0137, Train: 1.0000, Val: 0.7780, Test: 0.7920
Epoch: 99, Loss: 0.0132, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 100, Loss: 0.0145, Train: 1.0000, Val: 0.7780, Test: 0.7920
MAD:  0.8747
Best Test Accuracy: 0.8180, Val Accuracy: 0.7980, Train Accuracy: 1.0000
Training completed.
Seed:  5
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9776, Train: 0.3143, Val: 0.2460, Test: 0.2740
Epoch: 2, Loss: 1.8691, Train: 0.5000, Val: 0.3420, Test: 0.3940
Epoch: 3, Loss: 1.8160, Train: 0.6786, Val: 0.4560, Test: 0.4970
Epoch: 4, Loss: 1.7558, Train: 0.7714, Val: 0.5520, Test: 0.5850
Epoch: 5, Loss: 1.6993, Train: 0.8357, Val: 0.5980, Test: 0.6320
Epoch: 6, Loss: 1.6074, Train: 0.8786, Val: 0.6480, Test: 0.6800
Epoch: 7, Loss: 1.5502, Train: 0.8929, Val: 0.6760, Test: 0.7020
Epoch: 8, Loss: 1.4755, Train: 0.9000, Val: 0.6900, Test: 0.7200
Epoch: 9, Loss: 1.3640, Train: 0.9143, Val: 0.7040, Test: 0.7280
Epoch: 10, Loss: 1.2285, Train: 0.9429, Val: 0.7140, Test: 0.7410
Epoch: 11, Loss: 1.1830, Train: 0.9500, Val: 0.7240, Test: 0.7560
Epoch: 12, Loss: 1.0813, Train: 0.9500, Val: 0.7480, Test: 0.7730
Epoch: 13, Loss: 1.0005, Train: 0.9571, Val: 0.7560, Test: 0.7830
Epoch: 14, Loss: 0.9009, Train: 0.9643, Val: 0.7780, Test: 0.7900
Epoch: 15, Loss: 0.8169, Train: 0.9857, Val: 0.7860, Test: 0.8010
Epoch: 16, Loss: 0.6832, Train: 0.9857, Val: 0.7940, Test: 0.8070
Epoch: 17, Loss: 0.6035, Train: 0.9857, Val: 0.7940, Test: 0.8040
Epoch: 18, Loss: 0.5186, Train: 0.9857, Val: 0.7940, Test: 0.8040
Epoch: 19, Loss: 0.4274, Train: 0.9857, Val: 0.8020, Test: 0.8090
Epoch: 20, Loss: 0.3934, Train: 0.9929, Val: 0.7980, Test: 0.8060
Epoch: 21, Loss: 0.3395, Train: 0.9929, Val: 0.7940, Test: 0.8070
Epoch: 22, Loss: 0.3286, Train: 0.9929, Val: 0.7940, Test: 0.8070
Epoch: 23, Loss: 0.2633, Train: 0.9929, Val: 0.7940, Test: 0.8020
Epoch: 24, Loss: 0.2130, Train: 0.9929, Val: 0.7920, Test: 0.8070
Epoch: 25, Loss: 0.1787, Train: 0.9929, Val: 0.7940, Test: 0.8120
Epoch: 26, Loss: 0.1627, Train: 0.9929, Val: 0.7980, Test: 0.8140
Epoch: 27, Loss: 0.1292, Train: 0.9929, Val: 0.7900, Test: 0.8070
Epoch: 28, Loss: 0.1115, Train: 1.0000, Val: 0.7920, Test: 0.8100
Epoch: 29, Loss: 0.1326, Train: 1.0000, Val: 0.7940, Test: 0.8080
Epoch: 30, Loss: 0.0994, Train: 1.0000, Val: 0.7940, Test: 0.8080
Epoch: 31, Loss: 0.0873, Train: 1.0000, Val: 0.7940, Test: 0.8050
Epoch: 32, Loss: 0.0901, Train: 1.0000, Val: 0.7920, Test: 0.8040
Epoch: 33, Loss: 0.0767, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 34, Loss: 0.0509, Train: 1.0000, Val: 0.7780, Test: 0.8000
Epoch: 35, Loss: 0.0618, Train: 1.0000, Val: 0.7800, Test: 0.8030
Epoch: 36, Loss: 0.0476, Train: 1.0000, Val: 0.7820, Test: 0.8020
Epoch: 37, Loss: 0.0574, Train: 1.0000, Val: 0.7740, Test: 0.8000
Epoch: 38, Loss: 0.0500, Train: 1.0000, Val: 0.7800, Test: 0.8010
Epoch: 39, Loss: 0.0425, Train: 1.0000, Val: 0.7800, Test: 0.7990
Epoch: 40, Loss: 0.0367, Train: 1.0000, Val: 0.7780, Test: 0.7980
Epoch: 41, Loss: 0.0258, Train: 1.0000, Val: 0.7820, Test: 0.7980
Epoch: 42, Loss: 0.0351, Train: 1.0000, Val: 0.7780, Test: 0.7950
Epoch: 43, Loss: 0.0302, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 44, Loss: 0.0176, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 45, Loss: 0.0266, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 46, Loss: 0.0142, Train: 1.0000, Val: 0.7700, Test: 0.7840
Epoch: 47, Loss: 0.0392, Train: 1.0000, Val: 0.7720, Test: 0.7840
Epoch: 48, Loss: 0.0220, Train: 1.0000, Val: 0.7620, Test: 0.7810
Epoch: 49, Loss: 0.0167, Train: 1.0000, Val: 0.7620, Test: 0.7830
Epoch: 50, Loss: 0.0159, Train: 1.0000, Val: 0.7580, Test: 0.7820
Epoch: 51, Loss: 0.0171, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 52, Loss: 0.0161, Train: 1.0000, Val: 0.7620, Test: 0.7830
Epoch: 53, Loss: 0.0122, Train: 1.0000, Val: 0.7680, Test: 0.7840
Epoch: 54, Loss: 0.0223, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 55, Loss: 0.0113, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 56, Loss: 0.0107, Train: 1.0000, Val: 0.7800, Test: 0.7980
Epoch: 57, Loss: 0.0145, Train: 1.0000, Val: 0.7760, Test: 0.7950
Epoch: 58, Loss: 0.0061, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 59, Loss: 0.0074, Train: 1.0000, Val: 0.7860, Test: 0.8000
Epoch: 60, Loss: 0.0157, Train: 1.0000, Val: 0.7880, Test: 0.7990
Epoch: 61, Loss: 0.0121, Train: 1.0000, Val: 0.7900, Test: 0.8010
Epoch: 62, Loss: 0.0162, Train: 1.0000, Val: 0.7820, Test: 0.8000
Epoch: 63, Loss: 0.0077, Train: 1.0000, Val: 0.7800, Test: 0.7960
Epoch: 64, Loss: 0.0177, Train: 1.0000, Val: 0.7800, Test: 0.7940
Epoch: 65, Loss: 0.0162, Train: 1.0000, Val: 0.7720, Test: 0.7890
Epoch: 66, Loss: 0.0123, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 67, Loss: 0.0098, Train: 1.0000, Val: 0.7660, Test: 0.7850
Epoch: 68, Loss: 0.0086, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 69, Loss: 0.0132, Train: 1.0000, Val: 0.7620, Test: 0.7830
Epoch: 70, Loss: 0.0105, Train: 1.0000, Val: 0.7600, Test: 0.7900
Epoch: 71, Loss: 0.0153, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 72, Loss: 0.0130, Train: 1.0000, Val: 0.7700, Test: 0.7910
Epoch: 73, Loss: 0.0095, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 74, Loss: 0.0306, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 75, Loss: 0.0141, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 76, Loss: 0.0102, Train: 1.0000, Val: 0.7620, Test: 0.7870
Epoch: 77, Loss: 0.0047, Train: 1.0000, Val: 0.7520, Test: 0.7830
Epoch: 78, Loss: 0.0063, Train: 1.0000, Val: 0.7540, Test: 0.7800
Epoch: 79, Loss: 0.0091, Train: 1.0000, Val: 0.7520, Test: 0.7800
Epoch: 80, Loss: 0.0160, Train: 1.0000, Val: 0.7560, Test: 0.7850
Epoch: 81, Loss: 0.0157, Train: 1.0000, Val: 0.7580, Test: 0.7860
Epoch: 82, Loss: 0.0164, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 83, Loss: 0.0203, Train: 1.0000, Val: 0.7580, Test: 0.7930
Epoch: 84, Loss: 0.0058, Train: 1.0000, Val: 0.7600, Test: 0.7940
Epoch: 85, Loss: 0.0049, Train: 1.0000, Val: 0.7660, Test: 0.7960
Epoch: 86, Loss: 0.0078, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 87, Loss: 0.0029, Train: 1.0000, Val: 0.7760, Test: 0.7990
Epoch: 88, Loss: 0.0063, Train: 1.0000, Val: 0.7760, Test: 0.8010
Epoch: 89, Loss: 0.0046, Train: 1.0000, Val: 0.7780, Test: 0.8030
Epoch: 90, Loss: 0.0032, Train: 1.0000, Val: 0.7800, Test: 0.8060
Epoch: 91, Loss: 0.0107, Train: 1.0000, Val: 0.7820, Test: 0.8040
Epoch: 92, Loss: 0.0050, Train: 1.0000, Val: 0.7840, Test: 0.8040
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 93, Loss: 0.0052, Train: 1.0000, Val: 0.7860, Test: 0.8020
Epoch: 94, Loss: 0.0080, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 95, Loss: 0.0052, Train: 1.0000, Val: 0.7860, Test: 0.8030
Epoch: 96, Loss: 0.0105, Train: 1.0000, Val: 0.7880, Test: 0.8040
Epoch: 97, Loss: 0.0076, Train: 1.0000, Val: 0.7860, Test: 0.8020
Epoch: 98, Loss: 0.0068, Train: 1.0000, Val: 0.7860, Test: 0.8020
Epoch: 99, Loss: 0.0096, Train: 1.0000, Val: 0.7860, Test: 0.7980
Epoch: 100, Loss: 0.0087, Train: 1.0000, Val: 0.7860, Test: 0.7960
MAD:  0.9182
Best Test Accuracy: 0.8140, Val Accuracy: 0.7980, Train Accuracy: 0.9929
Training completed.
Seed:  6
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9647, Train: 0.3571, Val: 0.2240, Test: 0.2530
Epoch: 2, Loss: 1.9209, Train: 0.5857, Val: 0.3520, Test: 0.3560
Epoch: 3, Loss: 1.8315, Train: 0.7571, Val: 0.4540, Test: 0.4550
Epoch: 4, Loss: 1.7676, Train: 0.9000, Val: 0.5300, Test: 0.5440
Epoch: 5, Loss: 1.6943, Train: 0.9071, Val: 0.5840, Test: 0.6190
Epoch: 6, Loss: 1.6203, Train: 0.9214, Val: 0.6340, Test: 0.6640
Epoch: 7, Loss: 1.5521, Train: 0.9214, Val: 0.6700, Test: 0.6960
Epoch: 8, Loss: 1.4926, Train: 0.9357, Val: 0.7000, Test: 0.7240
Epoch: 9, Loss: 1.3666, Train: 0.9500, Val: 0.7360, Test: 0.7340
Epoch: 10, Loss: 1.2373, Train: 0.9571, Val: 0.7520, Test: 0.7540
Epoch: 11, Loss: 1.2150, Train: 0.9786, Val: 0.7640, Test: 0.7670
Epoch: 12, Loss: 1.0515, Train: 0.9786, Val: 0.7700, Test: 0.7760
Epoch: 13, Loss: 0.9805, Train: 0.9786, Val: 0.7700, Test: 0.7870
Epoch: 14, Loss: 0.8528, Train: 0.9714, Val: 0.7860, Test: 0.7940
Epoch: 15, Loss: 0.7780, Train: 0.9857, Val: 0.7900, Test: 0.8060
Epoch: 16, Loss: 0.7033, Train: 0.9857, Val: 0.7960, Test: 0.8130
Epoch: 17, Loss: 0.6248, Train: 0.9857, Val: 0.7960, Test: 0.8180
Epoch: 18, Loss: 0.5128, Train: 0.9857, Val: 0.7940, Test: 0.8240
Epoch: 19, Loss: 0.4821, Train: 0.9857, Val: 0.7940, Test: 0.8260
Epoch: 20, Loss: 0.3807, Train: 0.9857, Val: 0.7920, Test: 0.8220
Epoch: 21, Loss: 0.3796, Train: 0.9857, Val: 0.7940, Test: 0.8190
Epoch: 22, Loss: 0.3080, Train: 0.9857, Val: 0.7960, Test: 0.8140
Epoch: 23, Loss: 0.2326, Train: 0.9929, Val: 0.8020, Test: 0.8130
Epoch: 24, Loss: 0.2468, Train: 0.9929, Val: 0.8000, Test: 0.8070
Epoch: 25, Loss: 0.1931, Train: 0.9929, Val: 0.7980, Test: 0.8070
Epoch: 26, Loss: 0.1684, Train: 0.9929, Val: 0.8000, Test: 0.8010
Epoch: 27, Loss: 0.1464, Train: 0.9929, Val: 0.7900, Test: 0.8000
Epoch: 28, Loss: 0.1089, Train: 0.9929, Val: 0.7880, Test: 0.7950
Epoch: 29, Loss: 0.1232, Train: 0.9929, Val: 0.7840, Test: 0.7950
Epoch: 30, Loss: 0.1057, Train: 0.9929, Val: 0.7800, Test: 0.7960
Epoch: 31, Loss: 0.1029, Train: 1.0000, Val: 0.7860, Test: 0.8030
Epoch: 32, Loss: 0.0609, Train: 1.0000, Val: 0.7880, Test: 0.8080
Epoch: 33, Loss: 0.0759, Train: 1.0000, Val: 0.7960, Test: 0.8020
Epoch: 34, Loss: 0.1029, Train: 1.0000, Val: 0.7960, Test: 0.7980
Epoch: 35, Loss: 0.0513, Train: 1.0000, Val: 0.8020, Test: 0.7980
Epoch: 36, Loss: 0.0494, Train: 1.0000, Val: 0.7920, Test: 0.7970
Epoch: 37, Loss: 0.0378, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 38, Loss: 0.0368, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 39, Loss: 0.0391, Train: 1.0000, Val: 0.7660, Test: 0.7950
Epoch: 40, Loss: 0.0318, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 41, Loss: 0.0257, Train: 1.0000, Val: 0.7660, Test: 0.7920
Epoch: 42, Loss: 0.0258, Train: 1.0000, Val: 0.7620, Test: 0.7920
Epoch: 43, Loss: 0.0324, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 44, Loss: 0.0256, Train: 1.0000, Val: 0.7780, Test: 0.7970
Epoch: 45, Loss: 0.0376, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 46, Loss: 0.0153, Train: 1.0000, Val: 0.7940, Test: 0.8010
Epoch: 47, Loss: 0.0308, Train: 1.0000, Val: 0.7960, Test: 0.8000
Epoch: 48, Loss: 0.0232, Train: 1.0000, Val: 0.8040, Test: 0.8050
Epoch: 49, Loss: 0.0159, Train: 1.0000, Val: 0.8060, Test: 0.8070
Epoch: 50, Loss: 0.0248, Train: 1.0000, Val: 0.8000, Test: 0.8060
Epoch: 51, Loss: 0.0130, Train: 1.0000, Val: 0.7920, Test: 0.8040
Epoch: 52, Loss: 0.0135, Train: 1.0000, Val: 0.7880, Test: 0.8040
Epoch: 53, Loss: 0.0133, Train: 1.0000, Val: 0.7840, Test: 0.8030
Epoch: 54, Loss: 0.0119, Train: 1.0000, Val: 0.7740, Test: 0.8000
Epoch: 55, Loss: 0.0338, Train: 1.0000, Val: 0.7700, Test: 0.7960
Epoch: 56, Loss: 0.0145, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 57, Loss: 0.0152, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 58, Loss: 0.0151, Train: 1.0000, Val: 0.7680, Test: 0.7940
Epoch: 59, Loss: 0.0253, Train: 1.0000, Val: 0.7780, Test: 0.7920
Epoch: 60, Loss: 0.0114, Train: 1.0000, Val: 0.7900, Test: 0.7950
Epoch: 61, Loss: 0.0066, Train: 1.0000, Val: 0.7900, Test: 0.7920
Epoch: 62, Loss: 0.0107, Train: 1.0000, Val: 0.7920, Test: 0.7950
Epoch: 63, Loss: 0.0069, Train: 1.0000, Val: 0.7880, Test: 0.7920
Epoch: 64, Loss: 0.0090, Train: 1.0000, Val: 0.7860, Test: 0.7880
Epoch: 65, Loss: 0.0094, Train: 1.0000, Val: 0.7840, Test: 0.7870
Epoch: 66, Loss: 0.0110, Train: 1.0000, Val: 0.7840, Test: 0.7900
Epoch: 67, Loss: 0.0163, Train: 1.0000, Val: 0.7820, Test: 0.7930
Epoch: 68, Loss: 0.0126, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 69, Loss: 0.0081, Train: 1.0000, Val: 0.7720, Test: 0.7900
Epoch: 70, Loss: 0.0170, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 71, Loss: 0.0085, Train: 1.0000, Val: 0.7600, Test: 0.7880
Epoch: 72, Loss: 0.0091, Train: 1.0000, Val: 0.7500, Test: 0.7870
Epoch: 73, Loss: 0.0074, Train: 1.0000, Val: 0.7480, Test: 0.7860
Epoch: 74, Loss: 0.0111, Train: 1.0000, Val: 0.7520, Test: 0.7870
Epoch: 75, Loss: 0.0072, Train: 1.0000, Val: 0.7500, Test: 0.7890
Epoch: 76, Loss: 0.0058, Train: 1.0000, Val: 0.7500, Test: 0.7900
Epoch: 77, Loss: 0.0182, Train: 1.0000, Val: 0.7640, Test: 0.7940
Epoch: 78, Loss: 0.0046, Train: 1.0000, Val: 0.7740, Test: 0.7910
Epoch: 79, Loss: 0.0059, Train: 1.0000, Val: 0.7780, Test: 0.7950
Epoch: 80, Loss: 0.0106, Train: 1.0000, Val: 0.7860, Test: 0.7960
Epoch: 81, Loss: 0.0068, Train: 1.0000, Val: 0.7880, Test: 0.7950
Epoch: 82, Loss: 0.0038, Train: 1.0000, Val: 0.7900, Test: 0.7980
Epoch: 83, Loss: 0.0052, Train: 1.0000, Val: 0.7920, Test: 0.7990
Epoch: 84, Loss: 0.0144, Train: 1.0000, Val: 0.7900, Test: 0.8010
Epoch: 85, Loss: 0.0134, Train: 1.0000, Val: 0.7880, Test: 0.7990
Epoch: 86, Loss: 0.0105, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 87, Loss: 0.0122, Train: 1.0000, Val: 0.7740, Test: 0.7970
Epoch: 88, Loss: 0.0069, Train: 1.0000, Val: 0.7720, Test: 0.7960
Epoch: 89, Loss: 0.0059, Train: 1.0000, Val: 0.7680, Test: 0.7950
Epoch: 90, Loss: 0.0062, Train: 1.0000, Val: 0.7620, Test: 0.7950
Epoch: 91, Loss: 0.0044, Train: 1.0000, Val: 0.7600, Test: 0.7940
Epoch: 92, Loss: 0.0085, Train: 1.0000, Val: 0.7580, Test: 0.7930
Epoch: 93, Loss: 0.0061, Train: 1.0000, Val: 0.7580, Test: 0.7930
Epoch: 94, Loss: 0.0080, Train: 1.0000, Val: 0.7620, Test: 0.7940
Epoch: 95, Loss: 0.0064, Train: 1.0000, Val: 0.7680, Test: 0.7900
Epoch: 96, Loss: 0.0056, Train: 1.0000, Val: 0.7720, Test: 0.7930
Epoch: 97, Loss: 0.0075, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 98, Loss: 0.0089, Train: 1.0000, Val: 0.7800, Test: 0.7930
Epoch: 99, Loss: 0.0087, Train: 1.0000, Val: 0.7880, Test: 0.7950
Epoch: 100, Loss: 0.0055, Train: 1.0000, Val: 0.7900, Test: 0.7970
MAD:  0.9242
Best Test Accuracy: 0.8260, Val Accuracy: 0.7940, Train Accuracy: 0.9857
Training completed.
Seed:  7
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9589, Train: 0.3071, Val: 0.1760, Test: 0.1870
Epoch: 2, Loss: 1.9002, Train: 0.5500, Val: 0.2760, Test: 0.2880
Epoch: 3, Loss: 1.8375, Train: 0.7429, Val: 0.4080, Test: 0.4390
Epoch: 4, Loss: 1.7730, Train: 0.8571, Val: 0.5040, Test: 0.5310
Epoch: 5, Loss: 1.7303, Train: 0.9071, Val: 0.5860, Test: 0.6020
Epoch: 6, Loss: 1.6696, Train: 0.9143, Val: 0.6380, Test: 0.6540
Epoch: 7, Loss: 1.5641, Train: 0.9214, Val: 0.6580, Test: 0.6920
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 8, Loss: 1.4724, Train: 0.9500, Val: 0.6820, Test: 0.7120
Epoch: 9, Loss: 1.3915, Train: 0.9571, Val: 0.6940, Test: 0.7230
Epoch: 10, Loss: 1.3462, Train: 0.9643, Val: 0.7140, Test: 0.7320
Epoch: 11, Loss: 1.2162, Train: 0.9643, Val: 0.7280, Test: 0.7470
Epoch: 12, Loss: 1.1133, Train: 0.9643, Val: 0.7340, Test: 0.7540
Epoch: 13, Loss: 1.0192, Train: 0.9643, Val: 0.7440, Test: 0.7630
Epoch: 14, Loss: 0.9155, Train: 0.9714, Val: 0.7520, Test: 0.7700
Epoch: 15, Loss: 0.8249, Train: 0.9714, Val: 0.7580, Test: 0.7770
Epoch: 16, Loss: 0.7345, Train: 0.9714, Val: 0.7680, Test: 0.7960
Epoch: 17, Loss: 0.6285, Train: 0.9857, Val: 0.7740, Test: 0.8080
Epoch: 18, Loss: 0.5658, Train: 0.9857, Val: 0.7780, Test: 0.8090
Epoch: 19, Loss: 0.5160, Train: 0.9857, Val: 0.7780, Test: 0.8070
Epoch: 20, Loss: 0.4630, Train: 0.9857, Val: 0.7780, Test: 0.8050
Epoch: 21, Loss: 0.3583, Train: 0.9857, Val: 0.7740, Test: 0.8020
Epoch: 22, Loss: 0.3375, Train: 0.9857, Val: 0.7760, Test: 0.8030
Epoch: 23, Loss: 0.3023, Train: 0.9857, Val: 0.7700, Test: 0.7950
Epoch: 24, Loss: 0.2556, Train: 0.9857, Val: 0.7620, Test: 0.7900
Epoch: 25, Loss: 0.2415, Train: 0.9857, Val: 0.7620, Test: 0.7880
Epoch: 26, Loss: 0.2108, Train: 0.9857, Val: 0.7660, Test: 0.7910
Epoch: 27, Loss: 0.1702, Train: 0.9929, Val: 0.7740, Test: 0.7990
Epoch: 28, Loss: 0.1712, Train: 0.9929, Val: 0.7780, Test: 0.8020
Epoch: 29, Loss: 0.1330, Train: 1.0000, Val: 0.7780, Test: 0.8020
Epoch: 30, Loss: 0.1052, Train: 1.0000, Val: 0.7800, Test: 0.8050
Epoch: 31, Loss: 0.1108, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 32, Loss: 0.0820, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 33, Loss: 0.0781, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 34, Loss: 0.0699, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 35, Loss: 0.0701, Train: 1.0000, Val: 0.7660, Test: 0.7870
Epoch: 36, Loss: 0.0527, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 37, Loss: 0.0475, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 38, Loss: 0.0598, Train: 1.0000, Val: 0.7680, Test: 0.7860
Epoch: 39, Loss: 0.0382, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 40, Loss: 0.0283, Train: 1.0000, Val: 0.7680, Test: 0.7850
Epoch: 41, Loss: 0.0447, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 42, Loss: 0.0461, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 43, Loss: 0.0307, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 44, Loss: 0.0366, Train: 1.0000, Val: 0.7700, Test: 0.7890
Epoch: 45, Loss: 0.0275, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 46, Loss: 0.0214, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 47, Loss: 0.0184, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 48, Loss: 0.0267, Train: 1.0000, Val: 0.7700, Test: 0.7920
Epoch: 49, Loss: 0.0203, Train: 1.0000, Val: 0.7760, Test: 0.7970
Epoch: 50, Loss: 0.0150, Train: 1.0000, Val: 0.7820, Test: 0.7990
Epoch: 51, Loss: 0.0146, Train: 1.0000, Val: 0.7840, Test: 0.7970
Epoch: 52, Loss: 0.0236, Train: 1.0000, Val: 0.7840, Test: 0.7960
Epoch: 53, Loss: 0.0175, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 54, Loss: 0.0273, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 55, Loss: 0.0139, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 56, Loss: 0.0138, Train: 1.0000, Val: 0.7720, Test: 0.7960
Epoch: 57, Loss: 0.0228, Train: 1.0000, Val: 0.7700, Test: 0.7940
Epoch: 58, Loss: 0.0275, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 59, Loss: 0.0120, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 60, Loss: 0.0103, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 61, Loss: 0.0068, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 62, Loss: 0.0067, Train: 1.0000, Val: 0.7660, Test: 0.7850
Epoch: 63, Loss: 0.0151, Train: 1.0000, Val: 0.7680, Test: 0.7860
Epoch: 64, Loss: 0.0121, Train: 1.0000, Val: 0.7640, Test: 0.7850
Epoch: 65, Loss: 0.0122, Train: 1.0000, Val: 0.7700, Test: 0.7830
Epoch: 66, Loss: 0.0126, Train: 1.0000, Val: 0.7720, Test: 0.7860
Epoch: 67, Loss: 0.0091, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 68, Loss: 0.0103, Train: 1.0000, Val: 0.7680, Test: 0.7910
Epoch: 69, Loss: 0.0113, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 70, Loss: 0.0062, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 71, Loss: 0.0110, Train: 1.0000, Val: 0.7700, Test: 0.7890
Epoch: 72, Loss: 0.0085, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 73, Loss: 0.0134, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 74, Loss: 0.0090, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 75, Loss: 0.0133, Train: 1.0000, Val: 0.7760, Test: 0.7920
Epoch: 76, Loss: 0.0082, Train: 1.0000, Val: 0.7720, Test: 0.7940
Epoch: 77, Loss: 0.0077, Train: 1.0000, Val: 0.7740, Test: 0.7970
Epoch: 78, Loss: 0.0059, Train: 1.0000, Val: 0.7780, Test: 0.7990
Epoch: 79, Loss: 0.0117, Train: 1.0000, Val: 0.7760, Test: 0.8000
Epoch: 80, Loss: 0.0098, Train: 1.0000, Val: 0.7760, Test: 0.7980
Epoch: 81, Loss: 0.0095, Train: 1.0000, Val: 0.7740, Test: 0.7960
Epoch: 82, Loss: 0.0196, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 83, Loss: 0.0092, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 84, Loss: 0.0054, Train: 1.0000, Val: 0.7740, Test: 0.7930
Epoch: 85, Loss: 0.0124, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 86, Loss: 0.0062, Train: 1.0000, Val: 0.7720, Test: 0.7950
Epoch: 87, Loss: 0.0098, Train: 1.0000, Val: 0.7700, Test: 0.7930
Epoch: 88, Loss: 0.0092, Train: 1.0000, Val: 0.7640, Test: 0.7880
Epoch: 89, Loss: 0.0055, Train: 1.0000, Val: 0.7620, Test: 0.7880
Epoch: 90, Loss: 0.0102, Train: 1.0000, Val: 0.7660, Test: 0.7900
Epoch: 91, Loss: 0.0101, Train: 1.0000, Val: 0.7660, Test: 0.7870
Epoch: 92, Loss: 0.0055, Train: 1.0000, Val: 0.7660, Test: 0.7890
Epoch: 93, Loss: 0.0090, Train: 1.0000, Val: 0.7700, Test: 0.7890
Epoch: 94, Loss: 0.0068, Train: 1.0000, Val: 0.7720, Test: 0.7910
Epoch: 95, Loss: 0.0102, Train: 1.0000, Val: 0.7760, Test: 0.7940
Epoch: 96, Loss: 0.0060, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 97, Loss: 0.0105, Train: 1.0000, Val: 0.7820, Test: 0.8030
Epoch: 98, Loss: 0.0054, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 99, Loss: 0.0076, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 100, Loss: 0.0068, Train: 1.0000, Val: 0.7860, Test: 0.7990
MAD:  0.8837
Best Test Accuracy: 0.8090, Val Accuracy: 0.7780, Train Accuracy: 0.9857
Training completed.
Seed:  8
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9616, Train: 0.2786, Val: 0.2020, Test: 0.2130
Epoch: 2, Loss: 1.8785, Train: 0.5714, Val: 0.4080, Test: 0.4030
Epoch: 3, Loss: 1.8351, Train: 0.7357, Val: 0.5180, Test: 0.5350
Epoch: 4, Loss: 1.7468, Train: 0.8286, Val: 0.5780, Test: 0.6150
Epoch: 5, Loss: 1.6956, Train: 0.8714, Val: 0.6360, Test: 0.6570
Epoch: 6, Loss: 1.6472, Train: 0.8786, Val: 0.6840, Test: 0.6890
Epoch: 7, Loss: 1.5283, Train: 0.9000, Val: 0.7080, Test: 0.7080
Epoch: 8, Loss: 1.4658, Train: 0.9429, Val: 0.7320, Test: 0.7360
Epoch: 9, Loss: 1.3627, Train: 0.9571, Val: 0.7460, Test: 0.7530
Epoch: 10, Loss: 1.2813, Train: 0.9643, Val: 0.7600, Test: 0.7640
Epoch: 11, Loss: 1.1815, Train: 0.9786, Val: 0.7640, Test: 0.7790
Epoch: 12, Loss: 1.0742, Train: 0.9786, Val: 0.7620, Test: 0.7880
Epoch: 13, Loss: 0.9516, Train: 0.9786, Val: 0.7740, Test: 0.7930
Epoch: 14, Loss: 0.8573, Train: 0.9786, Val: 0.7800, Test: 0.7970
Epoch: 15, Loss: 0.7640, Train: 0.9786, Val: 0.7840, Test: 0.8000
Epoch: 16, Loss: 0.6617, Train: 0.9857, Val: 0.7860, Test: 0.7960
Epoch: 17, Loss: 0.5874, Train: 0.9857, Val: 0.7880, Test: 0.8010
Epoch: 18, Loss: 0.4770, Train: 0.9857, Val: 0.7880, Test: 0.8010
Epoch: 19, Loss: 0.4610, Train: 0.9929, Val: 0.7840, Test: 0.8020
Epoch: 20, Loss: 0.3593, Train: 0.9929, Val: 0.7820, Test: 0.8060
Epoch: 21, Loss: 0.3408, Train: 0.9929, Val: 0.7820, Test: 0.7980
Epoch: 22, Loss: 0.2806, Train: 0.9929, Val: 0.7780, Test: 0.7960
Epoch: 23, Loss: 0.2733, Train: 0.9929, Val: 0.7800, Test: 0.7950
Epoch: 24, Loss: 0.1939, Train: 0.9929, Val: 0.7760, Test: 0.7940
Epoch: 25, Loss: 0.1666, Train: 0.9929, Val: 0.7820, Test: 0.7940
Epoch: 26, Loss: 0.1882, Train: 0.9929, Val: 0.7880, Test: 0.7970
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 27, Loss: 0.1664, Train: 1.0000, Val: 0.7940, Test: 0.8020
Epoch: 28, Loss: 0.1284, Train: 1.0000, Val: 0.7920, Test: 0.8010
Epoch: 29, Loss: 0.1245, Train: 1.0000, Val: 0.7900, Test: 0.8010
Epoch: 30, Loss: 0.1166, Train: 1.0000, Val: 0.7740, Test: 0.8050
Epoch: 31, Loss: 0.0812, Train: 1.0000, Val: 0.7740, Test: 0.7980
Epoch: 32, Loss: 0.0865, Train: 1.0000, Val: 0.7700, Test: 0.7950
Epoch: 33, Loss: 0.0806, Train: 1.0000, Val: 0.7620, Test: 0.7900
Epoch: 34, Loss: 0.0927, Train: 1.0000, Val: 0.7640, Test: 0.7860
Epoch: 35, Loss: 0.0591, Train: 1.0000, Val: 0.7660, Test: 0.7820
Epoch: 36, Loss: 0.0389, Train: 1.0000, Val: 0.7700, Test: 0.7800
Epoch: 37, Loss: 0.0456, Train: 1.0000, Val: 0.7780, Test: 0.7800
Epoch: 38, Loss: 0.0414, Train: 1.0000, Val: 0.7800, Test: 0.7820
Epoch: 39, Loss: 0.0313, Train: 0.9929, Val: 0.7840, Test: 0.7810
Epoch: 40, Loss: 0.0351, Train: 0.9857, Val: 0.7860, Test: 0.7830
Epoch: 41, Loss: 0.0447, Train: 0.9857, Val: 0.7860, Test: 0.7840
Epoch: 42, Loss: 0.0356, Train: 1.0000, Val: 0.7800, Test: 0.7840
Epoch: 43, Loss: 0.0234, Train: 1.0000, Val: 0.7760, Test: 0.7860
Epoch: 44, Loss: 0.0484, Train: 1.0000, Val: 0.7620, Test: 0.7850
Epoch: 45, Loss: 0.0152, Train: 1.0000, Val: 0.7600, Test: 0.7770
Epoch: 46, Loss: 0.0388, Train: 1.0000, Val: 0.7560, Test: 0.7710
Epoch: 47, Loss: 0.0270, Train: 1.0000, Val: 0.7600, Test: 0.7710
Epoch: 48, Loss: 0.0366, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 49, Loss: 0.0220, Train: 1.0000, Val: 0.7560, Test: 0.7770
Epoch: 50, Loss: 0.0209, Train: 1.0000, Val: 0.7680, Test: 0.7760
Epoch: 51, Loss: 0.0116, Train: 1.0000, Val: 0.7680, Test: 0.7800
Epoch: 52, Loss: 0.0201, Train: 1.0000, Val: 0.7740, Test: 0.7850
Epoch: 53, Loss: 0.0163, Train: 1.0000, Val: 0.7700, Test: 0.7870
Epoch: 54, Loss: 0.0138, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 55, Loss: 0.0146, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 56, Loss: 0.0167, Train: 1.0000, Val: 0.7800, Test: 0.7920
Epoch: 57, Loss: 0.0097, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 58, Loss: 0.0121, Train: 1.0000, Val: 0.7840, Test: 0.7970
Epoch: 59, Loss: 0.0131, Train: 1.0000, Val: 0.7860, Test: 0.7980
Epoch: 60, Loss: 0.0093, Train: 1.0000, Val: 0.7840, Test: 0.7970
Epoch: 61, Loss: 0.0153, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 62, Loss: 0.0101, Train: 1.0000, Val: 0.7820, Test: 0.7960
Epoch: 63, Loss: 0.0189, Train: 1.0000, Val: 0.7840, Test: 0.7960
Epoch: 64, Loss: 0.0098, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 65, Loss: 0.0121, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 66, Loss: 0.0115, Train: 1.0000, Val: 0.7620, Test: 0.7910
Epoch: 67, Loss: 0.0072, Train: 1.0000, Val: 0.7620, Test: 0.7900
Epoch: 68, Loss: 0.0083, Train: 1.0000, Val: 0.7600, Test: 0.7890
Epoch: 69, Loss: 0.0090, Train: 1.0000, Val: 0.7600, Test: 0.7860
Epoch: 70, Loss: 0.0110, Train: 1.0000, Val: 0.7620, Test: 0.7860
Epoch: 71, Loss: 0.0123, Train: 1.0000, Val: 0.7620, Test: 0.7860
Epoch: 72, Loss: 0.0087, Train: 1.0000, Val: 0.7640, Test: 0.7820
Epoch: 73, Loss: 0.0059, Train: 1.0000, Val: 0.7720, Test: 0.7810
Epoch: 74, Loss: 0.0119, Train: 1.0000, Val: 0.7740, Test: 0.7810
Epoch: 75, Loss: 0.0085, Train: 1.0000, Val: 0.7840, Test: 0.7800
Epoch: 76, Loss: 0.0390, Train: 1.0000, Val: 0.7840, Test: 0.7800
Epoch: 77, Loss: 0.0128, Train: 1.0000, Val: 0.7680, Test: 0.7810
Epoch: 78, Loss: 0.0078, Train: 1.0000, Val: 0.7620, Test: 0.7810
Epoch: 79, Loss: 0.0099, Train: 1.0000, Val: 0.7540, Test: 0.7830
Epoch: 80, Loss: 0.0119, Train: 1.0000, Val: 0.7540, Test: 0.7790
Epoch: 81, Loss: 0.0102, Train: 1.0000, Val: 0.7540, Test: 0.7760
Epoch: 82, Loss: 0.0080, Train: 1.0000, Val: 0.7560, Test: 0.7760
Epoch: 83, Loss: 0.0116, Train: 1.0000, Val: 0.7540, Test: 0.7760
Epoch: 84, Loss: 0.0112, Train: 1.0000, Val: 0.7520, Test: 0.7750
Epoch: 85, Loss: 0.0116, Train: 1.0000, Val: 0.7500, Test: 0.7730
Epoch: 86, Loss: 0.0098, Train: 1.0000, Val: 0.7500, Test: 0.7760
Epoch: 87, Loss: 0.0071, Train: 1.0000, Val: 0.7540, Test: 0.7780
Epoch: 88, Loss: 0.0080, Train: 1.0000, Val: 0.7540, Test: 0.7800
Epoch: 89, Loss: 0.0115, Train: 1.0000, Val: 0.7600, Test: 0.7850
Epoch: 90, Loss: 0.0071, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 91, Loss: 0.0039, Train: 1.0000, Val: 0.7700, Test: 0.7950
Epoch: 92, Loss: 0.0105, Train: 1.0000, Val: 0.7720, Test: 0.7950
Epoch: 93, Loss: 0.0065, Train: 1.0000, Val: 0.7780, Test: 0.7950
Epoch: 94, Loss: 0.0165, Train: 1.0000, Val: 0.7820, Test: 0.7970
Epoch: 95, Loss: 0.0082, Train: 1.0000, Val: 0.7780, Test: 0.7990
Epoch: 96, Loss: 0.0045, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 97, Loss: 0.0053, Train: 1.0000, Val: 0.7760, Test: 0.7990
Epoch: 98, Loss: 0.0100, Train: 1.0000, Val: 0.7700, Test: 0.7980
Epoch: 99, Loss: 0.0031, Train: 1.0000, Val: 0.7660, Test: 0.7970
Epoch: 100, Loss: 0.0089, Train: 1.0000, Val: 0.7680, Test: 0.7970
MAD:  0.9645
Best Test Accuracy: 0.8060, Val Accuracy: 0.7820, Train Accuracy: 0.9929
Training completed.
Seed:  9
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-2): 2 x GCNConv(128, 128)
    (3): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9555, Train: 0.3357, Val: 0.2260, Test: 0.2650
Epoch: 2, Loss: 1.8850, Train: 0.7143, Val: 0.3720, Test: 0.4200
Epoch: 3, Loss: 1.8353, Train: 0.8429, Val: 0.4780, Test: 0.5020
Epoch: 4, Loss: 1.7293, Train: 0.8643, Val: 0.5340, Test: 0.5550
Epoch: 5, Loss: 1.7072, Train: 0.8857, Val: 0.5780, Test: 0.5910
Epoch: 6, Loss: 1.6278, Train: 0.9143, Val: 0.6100, Test: 0.6230
Epoch: 7, Loss: 1.5440, Train: 0.9286, Val: 0.6360, Test: 0.6420
Epoch: 8, Loss: 1.4398, Train: 0.9429, Val: 0.6720, Test: 0.6670
Epoch: 9, Loss: 1.3319, Train: 0.9643, Val: 0.6820, Test: 0.6890
Epoch: 10, Loss: 1.2555, Train: 0.9786, Val: 0.6940, Test: 0.7110
Epoch: 11, Loss: 1.1676, Train: 0.9786, Val: 0.7240, Test: 0.7360
Epoch: 12, Loss: 1.0261, Train: 0.9786, Val: 0.7460, Test: 0.7550
Epoch: 13, Loss: 0.9725, Train: 0.9786, Val: 0.7600, Test: 0.7710
Epoch: 14, Loss: 0.8649, Train: 0.9786, Val: 0.7680, Test: 0.7740
Epoch: 15, Loss: 0.7696, Train: 0.9786, Val: 0.7700, Test: 0.7800
Epoch: 16, Loss: 0.6702, Train: 0.9929, Val: 0.7720, Test: 0.7880
Epoch: 17, Loss: 0.6113, Train: 0.9929, Val: 0.7740, Test: 0.7900
Epoch: 18, Loss: 0.4687, Train: 0.9929, Val: 0.7720, Test: 0.7920
Epoch: 19, Loss: 0.4262, Train: 0.9929, Val: 0.7660, Test: 0.7960
Epoch: 20, Loss: 0.3666, Train: 0.9929, Val: 0.7700, Test: 0.7970
Epoch: 21, Loss: 0.3128, Train: 0.9857, Val: 0.7740, Test: 0.7940
Epoch: 22, Loss: 0.2453, Train: 0.9857, Val: 0.7740, Test: 0.8020
Epoch: 23, Loss: 0.2615, Train: 0.9857, Val: 0.7760, Test: 0.8060
Epoch: 24, Loss: 0.2431, Train: 0.9929, Val: 0.7820, Test: 0.8080
Epoch: 25, Loss: 0.2222, Train: 1.0000, Val: 0.7840, Test: 0.8090
Epoch: 26, Loss: 0.1713, Train: 1.0000, Val: 0.7840, Test: 0.8080
Epoch: 27, Loss: 0.1082, Train: 1.0000, Val: 0.7840, Test: 0.8100
Epoch: 28, Loss: 0.1288, Train: 1.0000, Val: 0.7820, Test: 0.8110
Epoch: 29, Loss: 0.0868, Train: 1.0000, Val: 0.7880, Test: 0.8080
Epoch: 30, Loss: 0.0821, Train: 1.0000, Val: 0.7820, Test: 0.8040
Epoch: 31, Loss: 0.0843, Train: 1.0000, Val: 0.7860, Test: 0.8030
Epoch: 32, Loss: 0.0770, Train: 1.0000, Val: 0.7880, Test: 0.8030
Epoch: 33, Loss: 0.0540, Train: 1.0000, Val: 0.7900, Test: 0.8020
Epoch: 34, Loss: 0.0605, Train: 1.0000, Val: 0.7840, Test: 0.8000
Epoch: 35, Loss: 0.0662, Train: 1.0000, Val: 0.7800, Test: 0.8020
Epoch: 36, Loss: 0.0496, Train: 1.0000, Val: 0.7820, Test: 0.7980
Epoch: 37, Loss: 0.0540, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 38, Loss: 0.0412, Train: 1.0000, Val: 0.7820, Test: 0.7970
Epoch: 39, Loss: 0.0303, Train: 0.9929, Val: 0.7840, Test: 0.7970
Epoch: 40, Loss: 0.0245, Train: 0.9929, Val: 0.7840, Test: 0.7950
Epoch: 41, Loss: 0.0320, Train: 0.9929, Val: 0.7840, Test: 0.7970
Epoch: 42, Loss: 0.0442, Train: 1.0000, Val: 0.7860, Test: 0.7930
Epoch: 43, Loss: 0.0190, Train: 1.0000, Val: 0.7840, Test: 0.7990
Epoch: 44, Loss: 0.0219, Train: 1.0000, Val: 0.7800, Test: 0.7920
Epoch: 45, Loss: 0.0280, Train: 1.0000, Val: 0.7720, Test: 0.7870
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 46, Loss: 0.0441, Train: 1.0000, Val: 0.7640, Test: 0.7800
Epoch: 47, Loss: 0.0168, Train: 1.0000, Val: 0.7620, Test: 0.7790
Epoch: 48, Loss: 0.0455, Train: 1.0000, Val: 0.7660, Test: 0.7850
Epoch: 49, Loss: 0.0322, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 50, Loss: 0.0195, Train: 1.0000, Val: 0.7740, Test: 0.7840
Epoch: 51, Loss: 0.0139, Train: 1.0000, Val: 0.7760, Test: 0.7880
Epoch: 52, Loss: 0.0257, Train: 1.0000, Val: 0.7820, Test: 0.7930
Epoch: 53, Loss: 0.0208, Train: 1.0000, Val: 0.7820, Test: 0.7950
Epoch: 54, Loss: 0.0154, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 55, Loss: 0.0107, Train: 1.0000, Val: 0.7880, Test: 0.7990
Epoch: 56, Loss: 0.0293, Train: 1.0000, Val: 0.7840, Test: 0.7990
Epoch: 57, Loss: 0.0141, Train: 1.0000, Val: 0.7820, Test: 0.7970
Epoch: 58, Loss: 0.0180, Train: 1.0000, Val: 0.7800, Test: 0.7950
Epoch: 59, Loss: 0.0059, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 60, Loss: 0.0101, Train: 1.0000, Val: 0.7700, Test: 0.7880
Epoch: 61, Loss: 0.0121, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 62, Loss: 0.0103, Train: 1.0000, Val: 0.7720, Test: 0.7860
Epoch: 63, Loss: 0.0071, Train: 1.0000, Val: 0.7720, Test: 0.7870
Epoch: 64, Loss: 0.0174, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 65, Loss: 0.0101, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 66, Loss: 0.0107, Train: 1.0000, Val: 0.7780, Test: 0.7960
Epoch: 67, Loss: 0.0110, Train: 1.0000, Val: 0.7920, Test: 0.8020
Epoch: 68, Loss: 0.0085, Train: 1.0000, Val: 0.7900, Test: 0.8020
Epoch: 69, Loss: 0.0104, Train: 1.0000, Val: 0.7940, Test: 0.8010
Epoch: 70, Loss: 0.0078, Train: 1.0000, Val: 0.8000, Test: 0.8010
Epoch: 71, Loss: 0.0110, Train: 1.0000, Val: 0.8000, Test: 0.8030
Epoch: 72, Loss: 0.0065, Train: 1.0000, Val: 0.8020, Test: 0.8020
Epoch: 73, Loss: 0.0141, Train: 1.0000, Val: 0.7960, Test: 0.8080
Epoch: 74, Loss: 0.0068, Train: 1.0000, Val: 0.7880, Test: 0.8090
Epoch: 75, Loss: 0.0049, Train: 1.0000, Val: 0.7820, Test: 0.8070
Epoch: 76, Loss: 0.0111, Train: 1.0000, Val: 0.7720, Test: 0.8020
Epoch: 77, Loss: 0.0090, Train: 1.0000, Val: 0.7760, Test: 0.8030
Epoch: 78, Loss: 0.0082, Train: 1.0000, Val: 0.7760, Test: 0.8050
Epoch: 79, Loss: 0.0053, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 80, Loss: 0.0128, Train: 1.0000, Val: 0.7800, Test: 0.7980
Epoch: 81, Loss: 0.0129, Train: 1.0000, Val: 0.7840, Test: 0.7970
Epoch: 82, Loss: 0.0082, Train: 1.0000, Val: 0.7860, Test: 0.8000
Epoch: 83, Loss: 0.0109, Train: 1.0000, Val: 0.7880, Test: 0.8020
Epoch: 84, Loss: 0.0057, Train: 1.0000, Val: 0.7840, Test: 0.8050
Epoch: 85, Loss: 0.0080, Train: 1.0000, Val: 0.7840, Test: 0.8050
Epoch: 86, Loss: 0.0081, Train: 1.0000, Val: 0.7860, Test: 0.8060
Epoch: 87, Loss: 0.0052, Train: 1.0000, Val: 0.7920, Test: 0.8050
Epoch: 88, Loss: 0.0103, Train: 1.0000, Val: 0.7900, Test: 0.8020
Epoch: 89, Loss: 0.0050, Train: 1.0000, Val: 0.7900, Test: 0.8040
Epoch: 90, Loss: 0.0109, Train: 1.0000, Val: 0.7900, Test: 0.8060
Epoch: 91, Loss: 0.0037, Train: 1.0000, Val: 0.7860, Test: 0.8050
Epoch: 92, Loss: 0.0185, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 93, Loss: 0.0077, Train: 1.0000, Val: 0.7820, Test: 0.8000
Epoch: 94, Loss: 0.0077, Train: 1.0000, Val: 0.7800, Test: 0.7970
Epoch: 95, Loss: 0.0069, Train: 1.0000, Val: 0.7760, Test: 0.7960
Epoch: 96, Loss: 0.0050, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 97, Loss: 0.0043, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 98, Loss: 0.0075, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 99, Loss: 0.0051, Train: 1.0000, Val: 0.7760, Test: 0.7860
Epoch: 100, Loss: 0.0069, Train: 1.0000, Val: 0.7780, Test: 0.7870
MAD:  0.893
Best Test Accuracy: 0.8110, Val Accuracy: 0.7820, Train Accuracy: 1.0000
Training completed.
Average Test Accuracy:  0.8134 ± 0.005257375771237931
Average MAD:  0.9124399999999999 ± 0.029364168641390135

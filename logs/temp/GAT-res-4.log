/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0124, Train: 0.2214, Val: 0.1180, Test: 0.1210
Epoch: 2, Loss: 1.8804, Train: 0.3929, Val: 0.1900, Test: 0.2080
Epoch: 3, Loss: 1.8316, Train: 0.6071, Val: 0.3380, Test: 0.3390
Epoch: 4, Loss: 1.7478, Train: 0.7643, Val: 0.4520, Test: 0.4550
Epoch: 5, Loss: 1.7138, Train: 0.8714, Val: 0.5440, Test: 0.5720
Epoch: 6, Loss: 1.6393, Train: 0.9143, Val: 0.6000, Test: 0.6280
Epoch: 7, Loss: 1.5564, Train: 0.9286, Val: 0.6540, Test: 0.6710
Epoch: 8, Loss: 1.5005, Train: 0.9429, Val: 0.6860, Test: 0.6990
Epoch: 9, Loss: 1.4046, Train: 0.9500, Val: 0.6940, Test: 0.7130
Epoch: 10, Loss: 1.3749, Train: 0.9571, Val: 0.7200, Test: 0.7290
Epoch: 11, Loss: 1.2424, Train: 0.9714, Val: 0.7240, Test: 0.7260
Epoch: 12, Loss: 1.2056, Train: 0.9643, Val: 0.7240, Test: 0.7250
Epoch: 13, Loss: 1.0811, Train: 0.9643, Val: 0.7260, Test: 0.7280
Epoch: 14, Loss: 0.9992, Train: 0.9714, Val: 0.7300, Test: 0.7310
Epoch: 15, Loss: 0.8807, Train: 0.9714, Val: 0.7320, Test: 0.7350
Epoch: 16, Loss: 0.8605, Train: 0.9714, Val: 0.7420, Test: 0.7470
Epoch: 17, Loss: 0.7604, Train: 0.9714, Val: 0.7560, Test: 0.7460
Epoch: 18, Loss: 0.6899, Train: 0.9714, Val: 0.7520, Test: 0.7480
Epoch: 19, Loss: 0.5597, Train: 0.9786, Val: 0.7500, Test: 0.7480
Epoch: 20, Loss: 0.5320, Train: 0.9857, Val: 0.7520, Test: 0.7570
Epoch: 21, Loss: 0.4876, Train: 0.9929, Val: 0.7480, Test: 0.7550
Epoch: 22, Loss: 0.4025, Train: 0.9929, Val: 0.7380, Test: 0.7590
Epoch: 23, Loss: 0.3961, Train: 0.9929, Val: 0.7360, Test: 0.7600
Epoch: 24, Loss: 0.3568, Train: 0.9929, Val: 0.7440, Test: 0.7630
Epoch: 25, Loss: 0.2862, Train: 0.9929, Val: 0.7500, Test: 0.7680
Epoch: 26, Loss: 0.2386, Train: 0.9929, Val: 0.7540, Test: 0.7720
Epoch: 27, Loss: 0.2747, Train: 0.9929, Val: 0.7440, Test: 0.7760
Epoch: 28, Loss: 0.2511, Train: 0.9929, Val: 0.7420, Test: 0.7660
Epoch: 29, Loss: 0.2501, Train: 0.9929, Val: 0.7460, Test: 0.7650
Epoch: 30, Loss: 0.1739, Train: 0.9929, Val: 0.7420, Test: 0.7670
Epoch: 31, Loss: 0.1966, Train: 0.9929, Val: 0.7440, Test: 0.7660
Epoch: 32, Loss: 0.1315, Train: 0.9929, Val: 0.7440, Test: 0.7640
Epoch: 33, Loss: 0.1735, Train: 0.9929, Val: 0.7440, Test: 0.7670
Epoch: 34, Loss: 0.1234, Train: 0.9929, Val: 0.7460, Test: 0.7670
Epoch: 35, Loss: 0.2144, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 36, Loss: 0.1131, Train: 1.0000, Val: 0.7440, Test: 0.7700
Epoch: 37, Loss: 0.1026, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 38, Loss: 0.0986, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 39, Loss: 0.0918, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 40, Loss: 0.0798, Train: 1.0000, Val: 0.7600, Test: 0.7840
Epoch: 41, Loss: 0.0637, Train: 1.0000, Val: 0.7620, Test: 0.7900
Epoch: 42, Loss: 0.0537, Train: 1.0000, Val: 0.7620, Test: 0.7910
Epoch: 43, Loss: 0.0516, Train: 1.0000, Val: 0.7620, Test: 0.7940
Epoch: 44, Loss: 0.0981, Train: 1.0000, Val: 0.7600, Test: 0.7920
Epoch: 45, Loss: 0.0477, Train: 1.0000, Val: 0.7560, Test: 0.7900
Epoch: 46, Loss: 0.0317, Train: 1.0000, Val: 0.7560, Test: 0.7900
Epoch: 47, Loss: 0.0381, Train: 1.0000, Val: 0.7480, Test: 0.7850
Epoch: 48, Loss: 0.0725, Train: 1.0000, Val: 0.7520, Test: 0.7860
Epoch: 49, Loss: 0.0398, Train: 1.0000, Val: 0.7540, Test: 0.7840
Epoch: 50, Loss: 0.0407, Train: 1.0000, Val: 0.7560, Test: 0.7840
MAD:  0.7979
Best Test Accuracy: 0.7940, Val Accuracy: 0.7620, Train Accuracy: 1.0000
Training completed.
Seed:  1
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9955, Train: 0.4500, Val: 0.3140, Test: 0.2960
Epoch: 2, Loss: 1.8796, Train: 0.7000, Val: 0.4920, Test: 0.4780
Epoch: 3, Loss: 1.8380, Train: 0.8071, Val: 0.6120, Test: 0.6060
Epoch: 4, Loss: 1.7567, Train: 0.8643, Val: 0.7040, Test: 0.6750
Epoch: 5, Loss: 1.7149, Train: 0.8929, Val: 0.7440, Test: 0.7070
Epoch: 6, Loss: 1.6339, Train: 0.9286, Val: 0.7540, Test: 0.7410
Epoch: 7, Loss: 1.5508, Train: 0.9286, Val: 0.7600, Test: 0.7600
Epoch: 8, Loss: 1.4877, Train: 0.9429, Val: 0.7740, Test: 0.7830
Epoch: 9, Loss: 1.4067, Train: 0.9500, Val: 0.7800, Test: 0.7900
Epoch: 10, Loss: 1.3671, Train: 0.9500, Val: 0.7880, Test: 0.8010
Epoch: 11, Loss: 1.2151, Train: 0.9643, Val: 0.7940, Test: 0.8030
Epoch: 12, Loss: 1.1660, Train: 0.9643, Val: 0.7900, Test: 0.8040
Epoch: 13, Loss: 1.0018, Train: 0.9643, Val: 0.7960, Test: 0.7990
Epoch: 14, Loss: 0.9395, Train: 0.9714, Val: 0.8000, Test: 0.7980
Epoch: 15, Loss: 0.8365, Train: 0.9786, Val: 0.7960, Test: 0.7950
Epoch: 16, Loss: 0.7390, Train: 0.9786, Val: 0.7980, Test: 0.7930
Epoch: 17, Loss: 0.6655, Train: 0.9786, Val: 0.7980, Test: 0.7970
Epoch: 18, Loss: 0.6244, Train: 0.9786, Val: 0.8000, Test: 0.8010
Epoch: 19, Loss: 0.4804, Train: 0.9786, Val: 0.8060, Test: 0.7990
Epoch: 20, Loss: 0.4838, Train: 0.9857, Val: 0.8100, Test: 0.8050
Epoch: 21, Loss: 0.4336, Train: 0.9929, Val: 0.8060, Test: 0.8080
Epoch: 22, Loss: 0.3703, Train: 0.9929, Val: 0.8140, Test: 0.8090
Epoch: 23, Loss: 0.2967, Train: 0.9929, Val: 0.8140, Test: 0.8130
Epoch: 24, Loss: 0.3464, Train: 0.9929, Val: 0.8120, Test: 0.8110
Epoch: 25, Loss: 0.2064, Train: 0.9929, Val: 0.8020, Test: 0.8050
Epoch: 26, Loss: 0.2085, Train: 0.9929, Val: 0.7920, Test: 0.8020
Epoch: 27, Loss: 0.2522, Train: 0.9929, Val: 0.7860, Test: 0.7950
Epoch: 28, Loss: 0.1821, Train: 0.9929, Val: 0.7780, Test: 0.7880
Epoch: 29, Loss: 0.1871, Train: 0.9929, Val: 0.7760, Test: 0.7820
Epoch: 30, Loss: 0.1864, Train: 0.9929, Val: 0.7700, Test: 0.7790
Epoch: 31, Loss: 0.1542, Train: 0.9929, Val: 0.7620, Test: 0.7770
Epoch: 32, Loss: 0.1288, Train: 0.9929, Val: 0.7600, Test: 0.7790
Epoch: 33, Loss: 0.0905, Train: 0.9929, Val: 0.7600, Test: 0.7830
Epoch: 34, Loss: 0.0874, Train: 0.9929, Val: 0.7660, Test: 0.7840
Epoch: 35, Loss: 0.0869, Train: 1.0000, Val: 0.7700, Test: 0.7860
Epoch: 36, Loss: 0.1164, Train: 1.0000, Val: 0.7760, Test: 0.7870
Epoch: 37, Loss: 0.0881, Train: 1.0000, Val: 0.7820, Test: 0.7920
Epoch: 38, Loss: 0.0750, Train: 1.0000, Val: 0.7780, Test: 0.7930
Epoch: 39, Loss: 0.1187, Train: 1.0000, Val: 0.7740, Test: 0.7950
Epoch: 40, Loss: 0.0807, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 41, Loss: 0.0678, Train: 1.0000, Val: 0.7640, Test: 0.7880
Epoch: 42, Loss: 0.0477, Train: 1.0000, Val: 0.7640, Test: 0.7880
Epoch: 43, Loss: 0.0608, Train: 1.0000, Val: 0.7640, Test: 0.7890
Epoch: 44, Loss: 0.0647, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 45, Loss: 0.0478, Train: 1.0000, Val: 0.7680, Test: 0.7880
Epoch: 46, Loss: 0.0444, Train: 1.0000, Val: 0.7660, Test: 0.7860
Epoch: 47, Loss: 0.0415, Train: 1.0000, Val: 0.7640, Test: 0.7870
Epoch: 48, Loss: 0.0421, Train: 1.0000, Val: 0.7620, Test: 0.7880
Epoch: 49, Loss: 0.0301, Train: 1.0000, Val: 0.7620, Test: 0.7880
Epoch: 50, Loss: 0.0360, Train: 1.0000, Val: 0.7660, Test: 0.7880
MAD:  0.8466
Best Test Accuracy: 0.8130, Val Accuracy: 0.8140, Train Accuracy: 0.9929
Training completed.
Seed:  2
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0092, Train: 0.3071, Val: 0.2680, Test: 0.2440
Epoch: 2, Loss: 1.9195, Train: 0.4357, Val: 0.3740, Test: 0.3640
Epoch: 3, Loss: 1.8406, Train: 0.7071, Val: 0.5040, Test: 0.5070
Epoch: 4, Loss: 1.8100, Train: 0.8643, Val: 0.5920, Test: 0.6200
Epoch: 5, Loss: 1.7344, Train: 0.9214, Val: 0.6300, Test: 0.6750
Epoch: 6, Loss: 1.6375, Train: 0.9357, Val: 0.6580, Test: 0.6990
Epoch: 7, Loss: 1.5699, Train: 0.9643, Val: 0.6920, Test: 0.7260
Epoch: 8, Loss: 1.4529, Train: 0.9714, Val: 0.7140, Test: 0.7420
Epoch: 9, Loss: 1.4094, Train: 0.9714, Val: 0.7080, Test: 0.7520
Epoch: 10, Loss: 1.3666, Train: 0.9714, Val: 0.7200, Test: 0.7610
Epoch: 11, Loss: 1.2607, Train: 0.9857, Val: 0.7340, Test: 0.7660
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 12, Loss: 1.1139, Train: 0.9857, Val: 0.7460, Test: 0.7770
Epoch: 13, Loss: 1.0511, Train: 0.9786, Val: 0.7580, Test: 0.7910
Epoch: 14, Loss: 0.9435, Train: 0.9786, Val: 0.7600, Test: 0.7960
Epoch: 15, Loss: 0.8332, Train: 0.9786, Val: 0.7680, Test: 0.7990
Epoch: 16, Loss: 0.7774, Train: 0.9786, Val: 0.7860, Test: 0.7980
Epoch: 17, Loss: 0.6491, Train: 0.9786, Val: 0.7880, Test: 0.7970
Epoch: 18, Loss: 0.5816, Train: 0.9929, Val: 0.7780, Test: 0.7870
Epoch: 19, Loss: 0.5606, Train: 0.9929, Val: 0.7760, Test: 0.7820
Epoch: 20, Loss: 0.5156, Train: 0.9929, Val: 0.7720, Test: 0.7810
Epoch: 21, Loss: 0.3525, Train: 0.9857, Val: 0.7700, Test: 0.7780
Epoch: 22, Loss: 0.3833, Train: 0.9786, Val: 0.7760, Test: 0.7780
Epoch: 23, Loss: 0.3292, Train: 0.9786, Val: 0.7800, Test: 0.7770
Epoch: 24, Loss: 0.2862, Train: 0.9786, Val: 0.7760, Test: 0.7820
Epoch: 25, Loss: 0.2920, Train: 0.9857, Val: 0.7600, Test: 0.7750
Epoch: 26, Loss: 0.2762, Train: 0.9857, Val: 0.7540, Test: 0.7650
Epoch: 27, Loss: 0.2523, Train: 0.9857, Val: 0.7540, Test: 0.7590
Epoch: 28, Loss: 0.1775, Train: 0.9929, Val: 0.7540, Test: 0.7610
Epoch: 29, Loss: 0.1906, Train: 0.9929, Val: 0.7500, Test: 0.7550
Epoch: 30, Loss: 0.1979, Train: 0.9929, Val: 0.7500, Test: 0.7560
Epoch: 31, Loss: 0.1832, Train: 0.9929, Val: 0.7580, Test: 0.7610
Epoch: 32, Loss: 0.1450, Train: 0.9929, Val: 0.7620, Test: 0.7630
Epoch: 33, Loss: 0.1995, Train: 0.9929, Val: 0.7600, Test: 0.7630
Epoch: 34, Loss: 0.1084, Train: 0.9929, Val: 0.7620, Test: 0.7670
Epoch: 35, Loss: 0.1584, Train: 0.9929, Val: 0.7620, Test: 0.7610
Epoch: 36, Loss: 0.0983, Train: 0.9929, Val: 0.7640, Test: 0.7630
Epoch: 37, Loss: 0.1537, Train: 0.9929, Val: 0.7660, Test: 0.7650
Epoch: 38, Loss: 0.0834, Train: 0.9929, Val: 0.7640, Test: 0.7660
Epoch: 39, Loss: 0.0942, Train: 1.0000, Val: 0.7700, Test: 0.7700
Epoch: 40, Loss: 0.1149, Train: 1.0000, Val: 0.7720, Test: 0.7750
Epoch: 41, Loss: 0.0637, Train: 1.0000, Val: 0.7680, Test: 0.7760
Epoch: 42, Loss: 0.1247, Train: 0.9929, Val: 0.7740, Test: 0.7780
Epoch: 43, Loss: 0.0707, Train: 0.9929, Val: 0.7720, Test: 0.7780
Epoch: 44, Loss: 0.0596, Train: 0.9929, Val: 0.7720, Test: 0.7820
Epoch: 45, Loss: 0.0610, Train: 0.9929, Val: 0.7700, Test: 0.7820
Epoch: 46, Loss: 0.0686, Train: 0.9929, Val: 0.7680, Test: 0.7800
Epoch: 47, Loss: 0.0525, Train: 0.9929, Val: 0.7620, Test: 0.7790
Epoch: 48, Loss: 0.0704, Train: 0.9929, Val: 0.7540, Test: 0.7730
Epoch: 49, Loss: 0.0593, Train: 1.0000, Val: 0.7540, Test: 0.7700
Epoch: 50, Loss: 0.0569, Train: 1.0000, Val: 0.7520, Test: 0.7660
MAD:  0.9266
Best Test Accuracy: 0.7990, Val Accuracy: 0.7680, Train Accuracy: 0.9786
Training completed.
Seed:  3
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.0000, Train: 0.3571, Val: 0.1780, Test: 0.1760
Epoch: 2, Loss: 1.9282, Train: 0.5643, Val: 0.3160, Test: 0.3400
Epoch: 3, Loss: 1.8185, Train: 0.7214, Val: 0.4240, Test: 0.4400
Epoch: 4, Loss: 1.7446, Train: 0.8357, Val: 0.4980, Test: 0.5120
Epoch: 5, Loss: 1.6872, Train: 0.9071, Val: 0.5720, Test: 0.5910
Epoch: 6, Loss: 1.6124, Train: 0.9571, Val: 0.6280, Test: 0.6340
Epoch: 7, Loss: 1.5499, Train: 0.9643, Val: 0.6500, Test: 0.6810
Epoch: 8, Loss: 1.4746, Train: 0.9714, Val: 0.6800, Test: 0.7170
Epoch: 9, Loss: 1.3151, Train: 0.9786, Val: 0.7060, Test: 0.7320
Epoch: 10, Loss: 1.2770, Train: 0.9714, Val: 0.7340, Test: 0.7450
Epoch: 11, Loss: 1.1518, Train: 0.9714, Val: 0.7440, Test: 0.7540
Epoch: 12, Loss: 1.0250, Train: 0.9714, Val: 0.7620, Test: 0.7600
Epoch: 13, Loss: 0.9686, Train: 0.9786, Val: 0.7720, Test: 0.7640
Epoch: 14, Loss: 0.8518, Train: 0.9857, Val: 0.7760, Test: 0.7630
Epoch: 15, Loss: 0.8317, Train: 0.9857, Val: 0.7720, Test: 0.7670
Epoch: 16, Loss: 0.6740, Train: 0.9857, Val: 0.7760, Test: 0.7730
Epoch: 17, Loss: 0.6293, Train: 0.9857, Val: 0.7720, Test: 0.7810
Epoch: 18, Loss: 0.5506, Train: 0.9857, Val: 0.7660, Test: 0.7810
Epoch: 19, Loss: 0.4992, Train: 0.9857, Val: 0.7640, Test: 0.7850
Epoch: 20, Loss: 0.4873, Train: 0.9857, Val: 0.7660, Test: 0.7840
Epoch: 21, Loss: 0.4115, Train: 0.9857, Val: 0.7640, Test: 0.7840
Epoch: 22, Loss: 0.3545, Train: 0.9857, Val: 0.7480, Test: 0.7810
Epoch: 23, Loss: 0.3510, Train: 0.9857, Val: 0.7460, Test: 0.7800
Epoch: 24, Loss: 0.2676, Train: 0.9857, Val: 0.7480, Test: 0.7810
Epoch: 25, Loss: 0.2333, Train: 0.9857, Val: 0.7540, Test: 0.7820
Epoch: 26, Loss: 0.2473, Train: 0.9857, Val: 0.7660, Test: 0.7910
Epoch: 27, Loss: 0.1770, Train: 0.9857, Val: 0.7660, Test: 0.7880
Epoch: 28, Loss: 0.1595, Train: 0.9929, Val: 0.7700, Test: 0.7870
Epoch: 29, Loss: 0.1708, Train: 0.9929, Val: 0.7740, Test: 0.7830
Epoch: 30, Loss: 0.1257, Train: 0.9929, Val: 0.7760, Test: 0.7840
Epoch: 31, Loss: 0.1215, Train: 0.9929, Val: 0.7760, Test: 0.7850
Epoch: 32, Loss: 0.1300, Train: 0.9929, Val: 0.7740, Test: 0.7850
Epoch: 33, Loss: 0.1671, Train: 0.9929, Val: 0.7720, Test: 0.7800
Epoch: 34, Loss: 0.0951, Train: 0.9929, Val: 0.7680, Test: 0.7760
Epoch: 35, Loss: 0.0712, Train: 0.9929, Val: 0.7680, Test: 0.7780
Epoch: 36, Loss: 0.0980, Train: 0.9929, Val: 0.7660, Test: 0.7790
Epoch: 37, Loss: 0.1084, Train: 0.9929, Val: 0.7660, Test: 0.7800
Epoch: 38, Loss: 0.1026, Train: 0.9929, Val: 0.7660, Test: 0.7830
Epoch: 39, Loss: 0.0676, Train: 0.9929, Val: 0.7720, Test: 0.7850
Epoch: 40, Loss: 0.0626, Train: 1.0000, Val: 0.7740, Test: 0.7870
Epoch: 41, Loss: 0.0558, Train: 1.0000, Val: 0.7760, Test: 0.7930
Epoch: 42, Loss: 0.0614, Train: 1.0000, Val: 0.7760, Test: 0.7950
Epoch: 43, Loss: 0.0593, Train: 1.0000, Val: 0.7820, Test: 0.7940
Epoch: 44, Loss: 0.0443, Train: 1.0000, Val: 0.7740, Test: 0.7940
Epoch: 45, Loss: 0.0630, Train: 1.0000, Val: 0.7720, Test: 0.7920
Epoch: 46, Loss: 0.0354, Train: 1.0000, Val: 0.7680, Test: 0.7920
Epoch: 47, Loss: 0.0555, Train: 1.0000, Val: 0.7640, Test: 0.7900
Epoch: 48, Loss: 0.0344, Train: 1.0000, Val: 0.7640, Test: 0.7910
Epoch: 49, Loss: 0.0432, Train: 1.0000, Val: 0.7660, Test: 0.7880
Epoch: 50, Loss: 0.0286, Train: 1.0000, Val: 0.7600, Test: 0.7850
MAD:  0.7888
Best Test Accuracy: 0.7950, Val Accuracy: 0.7760, Train Accuracy: 1.0000
Training completed.
Seed:  4
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9728, Train: 0.3071, Val: 0.2020, Test: 0.2060
Epoch: 2, Loss: 1.9015, Train: 0.5429, Val: 0.3240, Test: 0.3370
Epoch: 3, Loss: 1.8391, Train: 0.6929, Val: 0.4440, Test: 0.4590
Epoch: 4, Loss: 1.7405, Train: 0.8214, Val: 0.5260, Test: 0.5500
Epoch: 5, Loss: 1.7100, Train: 0.8714, Val: 0.6040, Test: 0.6010
Epoch: 6, Loss: 1.6620, Train: 0.8929, Val: 0.6500, Test: 0.6390
Epoch: 7, Loss: 1.5648, Train: 0.9071, Val: 0.6800, Test: 0.6610
Epoch: 8, Loss: 1.5044, Train: 0.9214, Val: 0.7000, Test: 0.6850
Epoch: 9, Loss: 1.3818, Train: 0.9286, Val: 0.7080, Test: 0.7030
Epoch: 10, Loss: 1.3338, Train: 0.9429, Val: 0.7320, Test: 0.7200
Epoch: 11, Loss: 1.2458, Train: 0.9500, Val: 0.7420, Test: 0.7430
Epoch: 12, Loss: 1.1311, Train: 0.9429, Val: 0.7540, Test: 0.7520
Epoch: 13, Loss: 1.0464, Train: 0.9500, Val: 0.7620, Test: 0.7570
Epoch: 14, Loss: 0.9866, Train: 0.9500, Val: 0.7660, Test: 0.7610
Epoch: 15, Loss: 0.8994, Train: 0.9571, Val: 0.7720, Test: 0.7700
Epoch: 16, Loss: 0.8427, Train: 0.9643, Val: 0.7720, Test: 0.7720
Epoch: 17, Loss: 0.6754, Train: 0.9786, Val: 0.7800, Test: 0.7830
Epoch: 18, Loss: 0.6056, Train: 0.9786, Val: 0.7920, Test: 0.7830
Epoch: 19, Loss: 0.5412, Train: 0.9857, Val: 0.7920, Test: 0.7870
Epoch: 20, Loss: 0.4780, Train: 0.9857, Val: 0.7940, Test: 0.7970
Epoch: 21, Loss: 0.4968, Train: 0.9857, Val: 0.7960, Test: 0.8040
Epoch: 22, Loss: 0.4202, Train: 0.9857, Val: 0.7880, Test: 0.8050
Epoch: 23, Loss: 0.3552, Train: 0.9929, Val: 0.7900, Test: 0.8100
Epoch: 24, Loss: 0.3435, Train: 0.9929, Val: 0.7880, Test: 0.8100
Epoch: 25, Loss: 0.3542, Train: 0.9929, Val: 0.7960, Test: 0.8100
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 26, Loss: 0.2381, Train: 0.9929, Val: 0.7960, Test: 0.8040
Epoch: 27, Loss: 0.2418, Train: 0.9929, Val: 0.7920, Test: 0.8000
Epoch: 28, Loss: 0.2059, Train: 0.9857, Val: 0.7900, Test: 0.8010
Epoch: 29, Loss: 0.2170, Train: 0.9929, Val: 0.7940, Test: 0.7990
Epoch: 30, Loss: 0.1773, Train: 0.9929, Val: 0.7920, Test: 0.8090
Epoch: 31, Loss: 0.1495, Train: 0.9929, Val: 0.7880, Test: 0.8110
Epoch: 32, Loss: 0.1527, Train: 0.9929, Val: 0.7880, Test: 0.8060
Epoch: 33, Loss: 0.1276, Train: 0.9929, Val: 0.7880, Test: 0.8040
Epoch: 34, Loss: 0.1061, Train: 0.9929, Val: 0.7880, Test: 0.8060
Epoch: 35, Loss: 0.0944, Train: 0.9929, Val: 0.7880, Test: 0.8080
Epoch: 36, Loss: 0.1033, Train: 0.9929, Val: 0.7900, Test: 0.8140
Epoch: 37, Loss: 0.0742, Train: 0.9929, Val: 0.7880, Test: 0.8150
Epoch: 38, Loss: 0.0986, Train: 0.9929, Val: 0.7900, Test: 0.8140
Epoch: 39, Loss: 0.0695, Train: 0.9929, Val: 0.7880, Test: 0.8170
Epoch: 40, Loss: 0.0637, Train: 0.9929, Val: 0.7860, Test: 0.8170
Epoch: 41, Loss: 0.0801, Train: 0.9929, Val: 0.7800, Test: 0.8170
Epoch: 42, Loss: 0.0690, Train: 0.9929, Val: 0.7800, Test: 0.8120
Epoch: 43, Loss: 0.0563, Train: 0.9929, Val: 0.7800, Test: 0.8020
Epoch: 44, Loss: 0.0517, Train: 0.9929, Val: 0.7760, Test: 0.7960
Epoch: 45, Loss: 0.0710, Train: 0.9929, Val: 0.7780, Test: 0.7990
Epoch: 46, Loss: 0.0571, Train: 0.9929, Val: 0.7800, Test: 0.7960
Epoch: 47, Loss: 0.0787, Train: 1.0000, Val: 0.7840, Test: 0.8010
Epoch: 48, Loss: 0.0449, Train: 1.0000, Val: 0.7860, Test: 0.8030
Epoch: 49, Loss: 0.0466, Train: 1.0000, Val: 0.7840, Test: 0.8050
Epoch: 50, Loss: 0.0294, Train: 1.0000, Val: 0.7940, Test: 0.8010
MAD:  0.8367
Best Test Accuracy: 0.8170, Val Accuracy: 0.7880, Train Accuracy: 0.9929
Training completed.
Seed:  5
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9760, Train: 0.2857, Val: 0.2000, Test: 0.2180
Epoch: 2, Loss: 1.9098, Train: 0.5571, Val: 0.3160, Test: 0.3630
Epoch: 3, Loss: 1.7902, Train: 0.8214, Val: 0.4780, Test: 0.5220
Epoch: 4, Loss: 1.7628, Train: 0.8786, Val: 0.5680, Test: 0.6070
Epoch: 5, Loss: 1.6488, Train: 0.9286, Val: 0.6040, Test: 0.6480
Epoch: 6, Loss: 1.6049, Train: 0.9643, Val: 0.6560, Test: 0.6810
Epoch: 7, Loss: 1.5416, Train: 0.9571, Val: 0.6780, Test: 0.6980
Epoch: 8, Loss: 1.4542, Train: 0.9571, Val: 0.6960, Test: 0.7160
Epoch: 9, Loss: 1.3578, Train: 0.9571, Val: 0.7160, Test: 0.7280
Epoch: 10, Loss: 1.2461, Train: 0.9714, Val: 0.7260, Test: 0.7380
Epoch: 11, Loss: 1.1577, Train: 0.9714, Val: 0.7420, Test: 0.7470
Epoch: 12, Loss: 1.0834, Train: 0.9714, Val: 0.7480, Test: 0.7620
Epoch: 13, Loss: 0.9278, Train: 0.9786, Val: 0.7540, Test: 0.7660
Epoch: 14, Loss: 0.8564, Train: 0.9786, Val: 0.7560, Test: 0.7710
Epoch: 15, Loss: 0.7668, Train: 0.9786, Val: 0.7580, Test: 0.7750
Epoch: 16, Loss: 0.6188, Train: 0.9786, Val: 0.7640, Test: 0.7860
Epoch: 17, Loss: 0.6122, Train: 0.9786, Val: 0.7660, Test: 0.7860
Epoch: 18, Loss: 0.4769, Train: 0.9857, Val: 0.7680, Test: 0.7870
Epoch: 19, Loss: 0.4251, Train: 0.9786, Val: 0.7740, Test: 0.7880
Epoch: 20, Loss: 0.4238, Train: 0.9786, Val: 0.7680, Test: 0.7870
Epoch: 21, Loss: 0.3859, Train: 0.9786, Val: 0.7680, Test: 0.7840
Epoch: 22, Loss: 0.3107, Train: 0.9857, Val: 0.7660, Test: 0.7850
Epoch: 23, Loss: 0.2788, Train: 0.9857, Val: 0.7680, Test: 0.7830
Epoch: 24, Loss: 0.2502, Train: 0.9857, Val: 0.7560, Test: 0.7790
Epoch: 25, Loss: 0.2201, Train: 0.9857, Val: 0.7500, Test: 0.7730
Epoch: 26, Loss: 0.1775, Train: 0.9857, Val: 0.7500, Test: 0.7660
Epoch: 27, Loss: 0.2033, Train: 0.9857, Val: 0.7520, Test: 0.7690
Epoch: 28, Loss: 0.1816, Train: 0.9857, Val: 0.7500, Test: 0.7690
Epoch: 29, Loss: 0.1657, Train: 0.9857, Val: 0.7620, Test: 0.7710
Epoch: 30, Loss: 0.1323, Train: 0.9929, Val: 0.7660, Test: 0.7770
Epoch: 31, Loss: 0.1747, Train: 0.9929, Val: 0.7680, Test: 0.7820
Epoch: 32, Loss: 0.1052, Train: 1.0000, Val: 0.7720, Test: 0.7840
Epoch: 33, Loss: 0.0932, Train: 1.0000, Val: 0.7720, Test: 0.7840
Epoch: 34, Loss: 0.1316, Train: 1.0000, Val: 0.7680, Test: 0.7870
Epoch: 35, Loss: 0.1571, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 36, Loss: 0.0785, Train: 1.0000, Val: 0.7680, Test: 0.7890
Epoch: 37, Loss: 0.0608, Train: 1.0000, Val: 0.7580, Test: 0.7920
Epoch: 38, Loss: 0.0767, Train: 1.0000, Val: 0.7540, Test: 0.7870
Epoch: 39, Loss: 0.0590, Train: 1.0000, Val: 0.7580, Test: 0.7820
Epoch: 40, Loss: 0.0363, Train: 1.0000, Val: 0.7580, Test: 0.7810
Epoch: 41, Loss: 0.0419, Train: 1.0000, Val: 0.7580, Test: 0.7790
Epoch: 42, Loss: 0.0511, Train: 1.0000, Val: 0.7540, Test: 0.7770
Epoch: 43, Loss: 0.0419, Train: 1.0000, Val: 0.7520, Test: 0.7740
Epoch: 44, Loss: 0.0425, Train: 1.0000, Val: 0.7520, Test: 0.7730
Epoch: 45, Loss: 0.0273, Train: 1.0000, Val: 0.7500, Test: 0.7740
Epoch: 46, Loss: 0.1294, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 47, Loss: 0.0291, Train: 1.0000, Val: 0.7620, Test: 0.7760
Epoch: 48, Loss: 0.0510, Train: 1.0000, Val: 0.7640, Test: 0.7770
Epoch: 49, Loss: 0.0393, Train: 1.0000, Val: 0.7660, Test: 0.7780
Epoch: 50, Loss: 0.0434, Train: 1.0000, Val: 0.7720, Test: 0.7790
MAD:  0.8517
Best Test Accuracy: 0.7920, Val Accuracy: 0.7580, Train Accuracy: 1.0000
Training completed.
Seed:  6
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9944, Train: 0.2571, Val: 0.3420, Test: 0.3600
Epoch: 2, Loss: 1.9227, Train: 0.4714, Val: 0.4540, Test: 0.4650
Epoch: 3, Loss: 1.8822, Train: 0.7286, Val: 0.5760, Test: 0.5770
Epoch: 4, Loss: 1.7807, Train: 0.7857, Val: 0.6260, Test: 0.6470
Epoch: 5, Loss: 1.7396, Train: 0.8286, Val: 0.6640, Test: 0.6690
Epoch: 6, Loss: 1.6557, Train: 0.8571, Val: 0.6900, Test: 0.6900
Epoch: 7, Loss: 1.6125, Train: 0.8857, Val: 0.7000, Test: 0.6920
Epoch: 8, Loss: 1.5260, Train: 0.8857, Val: 0.6960, Test: 0.6900
Epoch: 9, Loss: 1.4405, Train: 0.9000, Val: 0.7060, Test: 0.6930
Epoch: 10, Loss: 1.3818, Train: 0.9071, Val: 0.7180, Test: 0.7140
Epoch: 11, Loss: 1.2443, Train: 0.9214, Val: 0.7340, Test: 0.7300
Epoch: 12, Loss: 1.1716, Train: 0.9357, Val: 0.7460, Test: 0.7500
Epoch: 13, Loss: 1.1045, Train: 0.9500, Val: 0.7540, Test: 0.7660
Epoch: 14, Loss: 0.9729, Train: 0.9571, Val: 0.7600, Test: 0.7780
Epoch: 15, Loss: 0.9323, Train: 0.9643, Val: 0.7680, Test: 0.7830
Epoch: 16, Loss: 0.7948, Train: 0.9643, Val: 0.7680, Test: 0.7840
Epoch: 17, Loss: 0.7263, Train: 0.9643, Val: 0.7600, Test: 0.7810
Epoch: 18, Loss: 0.6791, Train: 0.9714, Val: 0.7640, Test: 0.7820
Epoch: 19, Loss: 0.5761, Train: 0.9714, Val: 0.7580, Test: 0.7820
Epoch: 20, Loss: 0.4742, Train: 0.9786, Val: 0.7680, Test: 0.7880
Epoch: 21, Loss: 0.4844, Train: 0.9929, Val: 0.7700, Test: 0.7890
Epoch: 22, Loss: 0.3977, Train: 0.9929, Val: 0.7720, Test: 0.7940
Epoch: 23, Loss: 0.3996, Train: 0.9929, Val: 0.7720, Test: 0.8000
Epoch: 24, Loss: 0.2708, Train: 0.9929, Val: 0.7620, Test: 0.8000
Epoch: 25, Loss: 0.2884, Train: 0.9929, Val: 0.7640, Test: 0.7950
Epoch: 26, Loss: 0.2363, Train: 0.9929, Val: 0.7660, Test: 0.7860
Epoch: 27, Loss: 0.2809, Train: 1.0000, Val: 0.7560, Test: 0.7830
Epoch: 28, Loss: 0.2280, Train: 1.0000, Val: 0.7580, Test: 0.7750
Epoch: 29, Loss: 0.1725, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 30, Loss: 0.1671, Train: 1.0000, Val: 0.7500, Test: 0.7690
Epoch: 31, Loss: 0.1911, Train: 1.0000, Val: 0.7520, Test: 0.7760
Epoch: 32, Loss: 0.1596, Train: 1.0000, Val: 0.7520, Test: 0.7730
Epoch: 33, Loss: 0.1087, Train: 1.0000, Val: 0.7500, Test: 0.7780
Epoch: 34, Loss: 0.1129, Train: 1.0000, Val: 0.7480, Test: 0.7830
Epoch: 35, Loss: 0.1216, Train: 1.0000, Val: 0.7500, Test: 0.7850
Epoch: 36, Loss: 0.1271, Train: 1.0000, Val: 0.7600, Test: 0.7910
Epoch: 37, Loss: 0.1144, Train: 1.0000, Val: 0.7620, Test: 0.7910
Epoch: 38, Loss: 0.0830, Train: 1.0000, Val: 0.7640, Test: 0.7930
Epoch: 39, Loss: 0.1014, Train: 1.0000, Val: 0.7640, Test: 0.7920
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 40, Loss: 0.0639, Train: 1.0000, Val: 0.7620, Test: 0.7890
Epoch: 41, Loss: 0.0884, Train: 1.0000, Val: 0.7580, Test: 0.7880
Epoch: 42, Loss: 0.0547, Train: 1.0000, Val: 0.7580, Test: 0.7850
Epoch: 43, Loss: 0.0676, Train: 1.0000, Val: 0.7580, Test: 0.7830
Epoch: 44, Loss: 0.0591, Train: 1.0000, Val: 0.7580, Test: 0.7890
Epoch: 45, Loss: 0.0564, Train: 1.0000, Val: 0.7560, Test: 0.7790
Epoch: 46, Loss: 0.0896, Train: 1.0000, Val: 0.7600, Test: 0.7800
Epoch: 47, Loss: 0.0439, Train: 1.0000, Val: 0.7580, Test: 0.7800
Epoch: 48, Loss: 0.0539, Train: 1.0000, Val: 0.7600, Test: 0.7760
Epoch: 49, Loss: 0.0408, Train: 1.0000, Val: 0.7620, Test: 0.7720
Epoch: 50, Loss: 0.0431, Train: 1.0000, Val: 0.7580, Test: 0.7700
MAD:  0.8992
Best Test Accuracy: 0.8000, Val Accuracy: 0.7720, Train Accuracy: 0.9929
Training completed.
Seed:  7
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9849, Train: 0.3429, Val: 0.2720, Test: 0.2780
Epoch: 2, Loss: 1.9274, Train: 0.6643, Val: 0.4420, Test: 0.4450
Epoch: 3, Loss: 1.8370, Train: 0.8429, Val: 0.5600, Test: 0.5660
Epoch: 4, Loss: 1.7722, Train: 0.8929, Val: 0.6300, Test: 0.6390
Epoch: 5, Loss: 1.7297, Train: 0.9429, Val: 0.6580, Test: 0.6860
Epoch: 6, Loss: 1.6402, Train: 0.9571, Val: 0.7020, Test: 0.7160
Epoch: 7, Loss: 1.5408, Train: 0.9571, Val: 0.7180, Test: 0.7480
Epoch: 8, Loss: 1.4674, Train: 0.9643, Val: 0.7480, Test: 0.7650
Epoch: 9, Loss: 1.3793, Train: 0.9643, Val: 0.7620, Test: 0.7760
Epoch: 10, Loss: 1.2935, Train: 0.9643, Val: 0.7700, Test: 0.7800
Epoch: 11, Loss: 1.2189, Train: 0.9714, Val: 0.7780, Test: 0.7910
Epoch: 12, Loss: 1.0733, Train: 0.9714, Val: 0.7820, Test: 0.7890
Epoch: 13, Loss: 0.9820, Train: 0.9714, Val: 0.7860, Test: 0.7890
Epoch: 14, Loss: 0.9193, Train: 0.9714, Val: 0.7820, Test: 0.8010
Epoch: 15, Loss: 0.8346, Train: 0.9714, Val: 0.7840, Test: 0.8050
Epoch: 16, Loss: 0.7720, Train: 0.9714, Val: 0.7900, Test: 0.8020
Epoch: 17, Loss: 0.6706, Train: 0.9714, Val: 0.7940, Test: 0.8020
Epoch: 18, Loss: 0.6343, Train: 0.9786, Val: 0.7940, Test: 0.8080
Epoch: 19, Loss: 0.5717, Train: 0.9786, Val: 0.7920, Test: 0.8080
Epoch: 20, Loss: 0.4998, Train: 0.9786, Val: 0.7860, Test: 0.8080
Epoch: 21, Loss: 0.3962, Train: 0.9786, Val: 0.7840, Test: 0.8090
Epoch: 22, Loss: 0.3536, Train: 0.9786, Val: 0.7860, Test: 0.8070
Epoch: 23, Loss: 0.3330, Train: 0.9786, Val: 0.7920, Test: 0.8120
Epoch: 24, Loss: 0.3553, Train: 0.9857, Val: 0.7900, Test: 0.8130
Epoch: 25, Loss: 0.2662, Train: 0.9857, Val: 0.7900, Test: 0.8120
Epoch: 26, Loss: 0.1999, Train: 0.9929, Val: 0.7980, Test: 0.8140
Epoch: 27, Loss: 0.2426, Train: 1.0000, Val: 0.8060, Test: 0.8180
Epoch: 28, Loss: 0.2342, Train: 1.0000, Val: 0.8080, Test: 0.8130
Epoch: 29, Loss: 0.1865, Train: 1.0000, Val: 0.8120, Test: 0.8170
Epoch: 30, Loss: 0.1404, Train: 1.0000, Val: 0.8100, Test: 0.8110
Epoch: 31, Loss: 0.1429, Train: 1.0000, Val: 0.8040, Test: 0.8110
Epoch: 32, Loss: 0.1120, Train: 1.0000, Val: 0.8000, Test: 0.8020
Epoch: 33, Loss: 0.1134, Train: 1.0000, Val: 0.8000, Test: 0.8010
Epoch: 34, Loss: 0.1175, Train: 1.0000, Val: 0.8000, Test: 0.8070
Epoch: 35, Loss: 0.0887, Train: 1.0000, Val: 0.7940, Test: 0.8090
Epoch: 36, Loss: 0.0868, Train: 1.0000, Val: 0.7940, Test: 0.8130
Epoch: 37, Loss: 0.0981, Train: 1.0000, Val: 0.8000, Test: 0.8100
Epoch: 38, Loss: 0.1238, Train: 1.0000, Val: 0.7920, Test: 0.8100
Epoch: 39, Loss: 0.0808, Train: 1.0000, Val: 0.7900, Test: 0.8120
Epoch: 40, Loss: 0.0626, Train: 1.0000, Val: 0.7920, Test: 0.8080
Epoch: 41, Loss: 0.0783, Train: 1.0000, Val: 0.7920, Test: 0.8080
Epoch: 42, Loss: 0.0420, Train: 0.9929, Val: 0.7920, Test: 0.8070
Epoch: 43, Loss: 0.0929, Train: 0.9929, Val: 0.7980, Test: 0.8070
Epoch: 44, Loss: 0.0690, Train: 0.9857, Val: 0.7980, Test: 0.8040
Epoch: 45, Loss: 0.0600, Train: 0.9857, Val: 0.8000, Test: 0.8000
Epoch: 46, Loss: 0.0616, Train: 0.9857, Val: 0.8000, Test: 0.7990
Epoch: 47, Loss: 0.1134, Train: 0.9857, Val: 0.7960, Test: 0.7970
Epoch: 48, Loss: 0.0649, Train: 0.9929, Val: 0.7940, Test: 0.7970
Epoch: 49, Loss: 0.0460, Train: 1.0000, Val: 0.7920, Test: 0.7990
Epoch: 50, Loss: 0.0390, Train: 1.0000, Val: 0.7900, Test: 0.7980
MAD:  0.8434
Best Test Accuracy: 0.8180, Val Accuracy: 0.8060, Train Accuracy: 1.0000
Training completed.
Seed:  8
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9901, Train: 0.3214, Val: 0.1880, Test: 0.1810
Epoch: 2, Loss: 1.9075, Train: 0.6286, Val: 0.3440, Test: 0.3580
Epoch: 3, Loss: 1.8163, Train: 0.8000, Val: 0.5320, Test: 0.5340
Epoch: 4, Loss: 1.7390, Train: 0.9071, Val: 0.6500, Test: 0.6410
Epoch: 5, Loss: 1.6882, Train: 0.9429, Val: 0.6880, Test: 0.6860
Epoch: 6, Loss: 1.6264, Train: 0.9429, Val: 0.7060, Test: 0.7090
Epoch: 7, Loss: 1.5413, Train: 0.9500, Val: 0.7200, Test: 0.7140
Epoch: 8, Loss: 1.4523, Train: 0.9429, Val: 0.7360, Test: 0.7190
Epoch: 9, Loss: 1.3340, Train: 0.9429, Val: 0.7460, Test: 0.7250
Epoch: 10, Loss: 1.2706, Train: 0.9357, Val: 0.7480, Test: 0.7340
Epoch: 11, Loss: 1.1443, Train: 0.9357, Val: 0.7500, Test: 0.7380
Epoch: 12, Loss: 1.0139, Train: 0.9429, Val: 0.7600, Test: 0.7470
Epoch: 13, Loss: 0.9553, Train: 0.9429, Val: 0.7720, Test: 0.7580
Epoch: 14, Loss: 0.8838, Train: 0.9429, Val: 0.7800, Test: 0.7650
Epoch: 15, Loss: 0.7117, Train: 0.9429, Val: 0.7900, Test: 0.7680
Epoch: 16, Loss: 0.6598, Train: 0.9429, Val: 0.7940, Test: 0.7780
Epoch: 17, Loss: 0.6409, Train: 0.9429, Val: 0.8020, Test: 0.7860
Epoch: 18, Loss: 0.5384, Train: 0.9643, Val: 0.8160, Test: 0.8000
Epoch: 19, Loss: 0.4329, Train: 0.9643, Val: 0.8200, Test: 0.8090
Epoch: 20, Loss: 0.4667, Train: 0.9714, Val: 0.8180, Test: 0.8120
Epoch: 21, Loss: 0.4009, Train: 0.9786, Val: 0.8080, Test: 0.8080
Epoch: 22, Loss: 0.3298, Train: 0.9786, Val: 0.8060, Test: 0.8080
Epoch: 23, Loss: 0.2977, Train: 0.9857, Val: 0.8080, Test: 0.8080
Epoch: 24, Loss: 0.2808, Train: 0.9786, Val: 0.8160, Test: 0.8100
Epoch: 25, Loss: 0.2314, Train: 0.9786, Val: 0.8140, Test: 0.8110
Epoch: 26, Loss: 0.2427, Train: 0.9857, Val: 0.8140, Test: 0.8080
Epoch: 27, Loss: 0.2042, Train: 0.9857, Val: 0.8140, Test: 0.8110
Epoch: 28, Loss: 0.2217, Train: 0.9857, Val: 0.8140, Test: 0.8130
Epoch: 29, Loss: 0.1492, Train: 1.0000, Val: 0.8100, Test: 0.8130
Epoch: 30, Loss: 0.1252, Train: 1.0000, Val: 0.8060, Test: 0.8120
Epoch: 31, Loss: 0.1350, Train: 1.0000, Val: 0.8040, Test: 0.8150
Epoch: 32, Loss: 0.1546, Train: 1.0000, Val: 0.8060, Test: 0.8130
Epoch: 33, Loss: 0.1538, Train: 1.0000, Val: 0.8120, Test: 0.8140
Epoch: 34, Loss: 0.0782, Train: 1.0000, Val: 0.8120, Test: 0.8150
Epoch: 35, Loss: 0.1250, Train: 1.0000, Val: 0.8140, Test: 0.8160
Epoch: 36, Loss: 0.1029, Train: 1.0000, Val: 0.8100, Test: 0.8130
Epoch: 37, Loss: 0.0744, Train: 1.0000, Val: 0.8060, Test: 0.8130
Epoch: 38, Loss: 0.0530, Train: 1.0000, Val: 0.8020, Test: 0.8090
Epoch: 39, Loss: 0.0534, Train: 1.0000, Val: 0.7960, Test: 0.8070
Epoch: 40, Loss: 0.0959, Train: 1.0000, Val: 0.7980, Test: 0.8040
Epoch: 41, Loss: 0.0515, Train: 1.0000, Val: 0.7880, Test: 0.7970
Epoch: 42, Loss: 0.0772, Train: 1.0000, Val: 0.7800, Test: 0.8000
Epoch: 43, Loss: 0.0648, Train: 1.0000, Val: 0.7820, Test: 0.8020
Epoch: 44, Loss: 0.0503, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 45, Loss: 0.0362, Train: 1.0000, Val: 0.7860, Test: 0.7970
Epoch: 46, Loss: 0.0555, Train: 1.0000, Val: 0.7840, Test: 0.7990
Epoch: 47, Loss: 0.0244, Train: 1.0000, Val: 0.7860, Test: 0.8010
Epoch: 48, Loss: 0.0422, Train: 1.0000, Val: 0.7880, Test: 0.8020
Epoch: 49, Loss: 0.0383, Train: 1.0000, Val: 0.7900, Test: 0.8010
Epoch: 50, Loss: 0.0433, Train: 1.0000, Val: 0.7880, Test: 0.8010
MAD:  0.8405
Best Test Accuracy: 0.8160, Val Accuracy: 0.8140, Train Accuracy: 1.0000
Training completed.
Seed:  9
/root/code/DIR/DIR-GNN/train/cora.py:421: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
GAT(
  (convs): ModuleList(
    (0): GATConv(1433, 128, heads=1)
    (1-2): 2 x GATConv(128, 128, heads=1)
    (3): GATConv(128, 7, heads=1)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 1.9990, Train: 0.3571, Val: 0.2640, Test: 0.2500
Epoch: 2, Loss: 1.9110, Train: 0.5500, Val: 0.4180, Test: 0.4280
Epoch: 3, Loss: 1.8250, Train: 0.7643, Val: 0.5380, Test: 0.5660
Epoch: 4, Loss: 1.7582, Train: 0.8857, Val: 0.6420, Test: 0.6490
Epoch: 5, Loss: 1.7005, Train: 0.9357, Val: 0.6860, Test: 0.7020
Epoch: 6, Loss: 1.6180, Train: 0.9500, Val: 0.7260, Test: 0.7230
Epoch: 7, Loss: 1.5626, Train: 0.9571, Val: 0.7540, Test: 0.7490
Epoch: 8, Loss: 1.4757, Train: 0.9714, Val: 0.7540, Test: 0.7590
Epoch: 9, Loss: 1.4521, Train: 0.9643, Val: 0.7640, Test: 0.7710
Epoch: 10, Loss: 1.3123, Train: 0.9500, Val: 0.7620, Test: 0.7740
Epoch: 11, Loss: 1.1865, Train: 0.9500, Val: 0.7680, Test: 0.7780
Epoch: 12, Loss: 1.1268, Train: 0.9500, Val: 0.7740, Test: 0.7850
Epoch: 13, Loss: 0.9978, Train: 0.9500, Val: 0.7820, Test: 0.7910
Epoch: 14, Loss: 0.9228, Train: 0.9571, Val: 0.7880, Test: 0.7960
Epoch: 15, Loss: 0.8394, Train: 0.9643, Val: 0.8020, Test: 0.8070
Epoch: 16, Loss: 0.7751, Train: 0.9714, Val: 0.8120, Test: 0.8140
Epoch: 17, Loss: 0.6267, Train: 0.9500, Val: 0.8040, Test: 0.8230
Epoch: 18, Loss: 0.5670, Train: 0.9500, Val: 0.8040, Test: 0.8220
Epoch: 19, Loss: 0.5399, Train: 0.9643, Val: 0.8000, Test: 0.8150
Epoch: 20, Loss: 0.4811, Train: 0.9643, Val: 0.8000, Test: 0.8180
Epoch: 21, Loss: 0.3597, Train: 0.9714, Val: 0.8040, Test: 0.8170
Epoch: 22, Loss: 0.3299, Train: 0.9786, Val: 0.8100, Test: 0.8140
Epoch: 23, Loss: 0.2778, Train: 0.9786, Val: 0.8120, Test: 0.8130
Epoch: 24, Loss: 0.2893, Train: 0.9857, Val: 0.8140, Test: 0.8100
Epoch: 25, Loss: 0.2190, Train: 0.9857, Val: 0.8040, Test: 0.8070
Epoch: 26, Loss: 0.2181, Train: 0.9857, Val: 0.8020, Test: 0.8070
Epoch: 27, Loss: 0.1722, Train: 0.9857, Val: 0.8080, Test: 0.8060
Epoch: 28, Loss: 0.1709, Train: 0.9857, Val: 0.8100, Test: 0.8060
Epoch: 29, Loss: 0.1475, Train: 0.9857, Val: 0.8000, Test: 0.8010
Epoch: 30, Loss: 0.1479, Train: 0.9929, Val: 0.8020, Test: 0.7990
Epoch: 31, Loss: 0.1459, Train: 0.9857, Val: 0.7840, Test: 0.7980
Epoch: 32, Loss: 0.1210, Train: 0.9857, Val: 0.7740, Test: 0.7970
Epoch: 33, Loss: 0.1038, Train: 0.9786, Val: 0.7660, Test: 0.7980
Epoch: 34, Loss: 0.1084, Train: 0.9786, Val: 0.7640, Test: 0.7930
Epoch: 35, Loss: 0.1066, Train: 0.9714, Val: 0.7620, Test: 0.7890
Epoch: 36, Loss: 0.0622, Train: 0.9786, Val: 0.7640, Test: 0.7910
Epoch: 37, Loss: 0.0926, Train: 0.9857, Val: 0.7680, Test: 0.7940
Epoch: 38, Loss: 0.1003, Train: 0.9857, Val: 0.7800, Test: 0.8010
Epoch: 39, Loss: 0.0792, Train: 0.9929, Val: 0.7700, Test: 0.7920
Epoch: 40, Loss: 0.0554, Train: 1.0000, Val: 0.7660, Test: 0.7840
Epoch: 41, Loss: 0.1071, Train: 1.0000, Val: 0.7600, Test: 0.7810
Epoch: 42, Loss: 0.0724, Train: 1.0000, Val: 0.7560, Test: 0.7740
Epoch: 43, Loss: 0.0502, Train: 1.0000, Val: 0.7580, Test: 0.7720
Epoch: 44, Loss: 0.0502, Train: 1.0000, Val: 0.7540, Test: 0.7750
Epoch: 45, Loss: 0.0453, Train: 1.0000, Val: 0.7560, Test: 0.7750
Epoch: 46, Loss: 0.0378, Train: 1.0000, Val: 0.7620, Test: 0.7780
Epoch: 47, Loss: 0.0382, Train: 1.0000, Val: 0.7600, Test: 0.7830
Epoch: 48, Loss: 0.0415, Train: 1.0000, Val: 0.7700, Test: 0.7900
Epoch: 49, Loss: 0.0470, Train: 1.0000, Val: 0.7740, Test: 0.7920
Epoch: 50, Loss: 0.0463, Train: 1.0000, Val: 0.7780, Test: 0.7980
MAD:  0.9206
Best Test Accuracy: 0.8230, Val Accuracy: 0.8040, Train Accuracy: 0.9500
Training completed.
Average Test Accuracy:  0.8067 ± 0.011153923076657784
Average MAD:  0.8552 ± 0.04448276070569361

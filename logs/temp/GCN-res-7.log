/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.3274, Train: 0.1571, Val: 0.0960, Test: 0.0950
Epoch: 2, Loss: 2.0056, Train: 0.2857, Val: 0.2260, Test: 0.2260
Epoch: 3, Loss: 2.0291, Train: 0.3714, Val: 0.2860, Test: 0.3020
Epoch: 4, Loss: 2.0956, Train: 0.4929, Val: 0.2900, Test: 0.3190
Epoch: 5, Loss: 1.9889, Train: 0.5000, Val: 0.3260, Test: 0.3290
Epoch: 6, Loss: 1.9673, Train: 0.4857, Val: 0.3340, Test: 0.3280
Epoch: 7, Loss: 1.8809, Train: 0.4929, Val: 0.3140, Test: 0.3180
Epoch: 8, Loss: 1.8535, Train: 0.5143, Val: 0.3060, Test: 0.3100
Epoch: 9, Loss: 1.8875, Train: 0.5643, Val: 0.3240, Test: 0.3450
Epoch: 10, Loss: 1.8869, Train: 0.6714, Val: 0.3980, Test: 0.3980
Epoch: 11, Loss: 1.7868, Train: 0.7500, Val: 0.4700, Test: 0.4550
Epoch: 12, Loss: 1.7895, Train: 0.8214, Val: 0.5380, Test: 0.5120
Epoch: 13, Loss: 1.6886, Train: 0.8714, Val: 0.5920, Test: 0.5650
Epoch: 14, Loss: 1.7307, Train: 0.8786, Val: 0.6440, Test: 0.6250
Epoch: 15, Loss: 1.6229, Train: 0.9000, Val: 0.6680, Test: 0.6610
Epoch: 16, Loss: 1.6456, Train: 0.9000, Val: 0.6940, Test: 0.6930
Epoch: 17, Loss: 1.6341, Train: 0.9071, Val: 0.7120, Test: 0.7080
Epoch: 18, Loss: 1.5531, Train: 0.9071, Val: 0.7260, Test: 0.7190
Epoch: 19, Loss: 1.5486, Train: 0.9000, Val: 0.7280, Test: 0.7250
Epoch: 20, Loss: 1.4087, Train: 0.8929, Val: 0.7240, Test: 0.7130
Epoch: 21, Loss: 1.3713, Train: 0.8929, Val: 0.7260, Test: 0.7190
Epoch: 22, Loss: 1.2975, Train: 0.8929, Val: 0.7360, Test: 0.7250
Epoch: 23, Loss: 1.2899, Train: 0.8929, Val: 0.7460, Test: 0.7440
Epoch: 24, Loss: 1.2482, Train: 0.9214, Val: 0.7580, Test: 0.7580
Epoch: 25, Loss: 1.1222, Train: 0.9357, Val: 0.7680, Test: 0.7700
Epoch: 26, Loss: 1.0610, Train: 0.9357, Val: 0.7760, Test: 0.7750
Epoch: 27, Loss: 0.9856, Train: 0.9357, Val: 0.7760, Test: 0.7800
Epoch: 28, Loss: 0.9271, Train: 0.9357, Val: 0.7840, Test: 0.7840
Epoch: 29, Loss: 0.8221, Train: 0.9429, Val: 0.7740, Test: 0.7900
Epoch: 30, Loss: 0.7688, Train: 0.9429, Val: 0.7760, Test: 0.7920
Epoch: 31, Loss: 0.7698, Train: 0.9429, Val: 0.7720, Test: 0.7930
Epoch: 32, Loss: 0.5592, Train: 0.9429, Val: 0.7680, Test: 0.7840
Epoch: 33, Loss: 0.6535, Train: 0.9500, Val: 0.7640, Test: 0.7820
Epoch: 34, Loss: 0.6832, Train: 0.9500, Val: 0.7740, Test: 0.7890
Epoch: 35, Loss: 0.5411, Train: 0.9714, Val: 0.7860, Test: 0.7930
Epoch: 36, Loss: 0.5239, Train: 0.9643, Val: 0.7920, Test: 0.7970
Epoch: 37, Loss: 0.4794, Train: 0.9643, Val: 0.7900, Test: 0.8020
Epoch: 38, Loss: 0.3787, Train: 0.9786, Val: 0.7960, Test: 0.8040
Epoch: 39, Loss: 0.4847, Train: 0.9857, Val: 0.7960, Test: 0.8070
Epoch: 40, Loss: 0.2962, Train: 0.9857, Val: 0.7860, Test: 0.8060
Epoch: 41, Loss: 0.3310, Train: 0.9857, Val: 0.7820, Test: 0.8040
Epoch: 42, Loss: 0.4097, Train: 0.9857, Val: 0.7760, Test: 0.7940
Epoch: 43, Loss: 0.2792, Train: 0.9857, Val: 0.7720, Test: 0.7910
Epoch: 44, Loss: 0.2330, Train: 0.9857, Val: 0.7700, Test: 0.7950
Epoch: 45, Loss: 0.2541, Train: 0.9929, Val: 0.7740, Test: 0.7940
Epoch: 46, Loss: 0.2643, Train: 1.0000, Val: 0.7820, Test: 0.7920
Epoch: 47, Loss: 0.2181, Train: 0.9929, Val: 0.7760, Test: 0.7950
Epoch: 48, Loss: 0.1597, Train: 0.9857, Val: 0.7720, Test: 0.7930
Epoch: 49, Loss: 0.1345, Train: 0.9857, Val: 0.7740, Test: 0.7900
Epoch: 50, Loss: 0.1719, Train: 0.9857, Val: 0.7720, Test: 0.7920
MAD:  0.8104
Best Test Accuracy: 0.8070, Val Accuracy: 0.7960, Train Accuracy: 0.9857
Training completed.
Seed:  1
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.2574, Train: 0.1500, Val: 0.1720, Test: 0.1480
Epoch: 2, Loss: 2.1562, Train: 0.3714, Val: 0.1760, Test: 0.1970
Epoch: 3, Loss: 2.0297, Train: 0.4357, Val: 0.2080, Test: 0.2250
Epoch: 4, Loss: 2.0623, Train: 0.4571, Val: 0.2280, Test: 0.2500
Epoch: 5, Loss: 1.9522, Train: 0.4571, Val: 0.2300, Test: 0.2540
Epoch: 6, Loss: 1.8968, Train: 0.4571, Val: 0.2440, Test: 0.2600
Epoch: 7, Loss: 1.9285, Train: 0.5000, Val: 0.2560, Test: 0.2770
Epoch: 8, Loss: 1.9031, Train: 0.5643, Val: 0.2860, Test: 0.3210
Epoch: 9, Loss: 1.8277, Train: 0.6571, Val: 0.3540, Test: 0.3880
Epoch: 10, Loss: 1.8180, Train: 0.7357, Val: 0.4100, Test: 0.4580
Epoch: 11, Loss: 1.7878, Train: 0.8286, Val: 0.4960, Test: 0.5550
Epoch: 12, Loss: 1.7509, Train: 0.8571, Val: 0.6020, Test: 0.6380
Epoch: 13, Loss: 1.6780, Train: 0.8929, Val: 0.6540, Test: 0.6950
Epoch: 14, Loss: 1.6809, Train: 0.9071, Val: 0.6680, Test: 0.7290
Epoch: 15, Loss: 1.6681, Train: 0.9286, Val: 0.6900, Test: 0.7380
Epoch: 16, Loss: 1.5095, Train: 0.9357, Val: 0.7020, Test: 0.7510
Epoch: 17, Loss: 1.5447, Train: 0.9429, Val: 0.7120, Test: 0.7530
Epoch: 18, Loss: 1.4408, Train: 0.9429, Val: 0.7140, Test: 0.7570
Epoch: 19, Loss: 1.4292, Train: 0.9500, Val: 0.7180, Test: 0.7610
Epoch: 20, Loss: 1.3743, Train: 0.9429, Val: 0.7240, Test: 0.7640
Epoch: 21, Loss: 1.2905, Train: 0.9429, Val: 0.7240, Test: 0.7660
Epoch: 22, Loss: 1.2881, Train: 0.9429, Val: 0.7180, Test: 0.7640
Epoch: 23, Loss: 1.1113, Train: 0.9357, Val: 0.7160, Test: 0.7630
Epoch: 24, Loss: 1.0920, Train: 0.9429, Val: 0.7120, Test: 0.7530
Epoch: 25, Loss: 1.0673, Train: 0.9429, Val: 0.7120, Test: 0.7470
Epoch: 26, Loss: 0.9841, Train: 0.9500, Val: 0.7120, Test: 0.7460
Epoch: 27, Loss: 0.8690, Train: 0.9500, Val: 0.7180, Test: 0.7560
Epoch: 28, Loss: 0.8317, Train: 0.9500, Val: 0.7300, Test: 0.7680
Epoch: 29, Loss: 0.6973, Train: 0.9571, Val: 0.7420, Test: 0.7790
Epoch: 30, Loss: 0.7760, Train: 0.9500, Val: 0.7660, Test: 0.7930
Epoch: 31, Loss: 0.7034, Train: 0.9643, Val: 0.7640, Test: 0.7950
Epoch: 32, Loss: 0.6764, Train: 0.9714, Val: 0.7680, Test: 0.8020
Epoch: 33, Loss: 0.4691, Train: 0.9714, Val: 0.7760, Test: 0.8150
Epoch: 34, Loss: 0.5763, Train: 0.9714, Val: 0.7840, Test: 0.8150
Epoch: 35, Loss: 0.5134, Train: 0.9786, Val: 0.7860, Test: 0.8180
Epoch: 36, Loss: 0.3677, Train: 0.9714, Val: 0.7860, Test: 0.8130
Epoch: 37, Loss: 0.3085, Train: 0.9929, Val: 0.7840, Test: 0.8150
Epoch: 38, Loss: 0.3524, Train: 0.9929, Val: 0.7860, Test: 0.8170
Epoch: 39, Loss: 0.3707, Train: 0.9929, Val: 0.7860, Test: 0.8140
Epoch: 40, Loss: 0.2841, Train: 1.0000, Val: 0.7820, Test: 0.8110
Epoch: 41, Loss: 0.2387, Train: 0.9929, Val: 0.7840, Test: 0.8120
Epoch: 42, Loss: 0.1991, Train: 0.9929, Val: 0.7860, Test: 0.8140
Epoch: 43, Loss: 0.2083, Train: 0.9929, Val: 0.7840, Test: 0.8100
Epoch: 44, Loss: 0.2279, Train: 0.9929, Val: 0.7880, Test: 0.8170
Epoch: 45, Loss: 0.1599, Train: 1.0000, Val: 0.7860, Test: 0.8140
Epoch: 46, Loss: 0.1222, Train: 1.0000, Val: 0.7880, Test: 0.8120
Epoch: 47, Loss: 0.1349, Train: 1.0000, Val: 0.7840, Test: 0.8100
Epoch: 48, Loss: 0.2090, Train: 1.0000, Val: 0.7820, Test: 0.8060
Epoch: 49, Loss: 0.1358, Train: 1.0000, Val: 0.7840, Test: 0.8020
Epoch: 50, Loss: 0.1518, Train: 0.9929, Val: 0.7860, Test: 0.8060
MAD:  0.9558
Best Test Accuracy: 0.8180, Val Accuracy: 0.7860, Train Accuracy: 0.9786
Training completed.
Seed:  2
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.4038, Train: 0.2286, Val: 0.2120, Test: 0.1970
Epoch: 2, Loss: 2.1827, Train: 0.2143, Val: 0.1460, Test: 0.1570
Epoch: 3, Loss: 2.0415, Train: 0.2357, Val: 0.1500, Test: 0.1430
Epoch: 4, Loss: 2.0409, Train: 0.2714, Val: 0.1660, Test: 0.1770
Epoch: 5, Loss: 1.9803, Train: 0.3500, Val: 0.2120, Test: 0.2090
Epoch: 6, Loss: 1.8475, Train: 0.4000, Val: 0.2580, Test: 0.2620
Epoch: 7, Loss: 1.8969, Train: 0.5286, Val: 0.2940, Test: 0.3100
Epoch: 8, Loss: 1.9270, Train: 0.6071, Val: 0.3500, Test: 0.3640
Epoch: 9, Loss: 1.8282, Train: 0.6429, Val: 0.4280, Test: 0.4490
Epoch: 10, Loss: 1.6956, Train: 0.7357, Val: 0.5080, Test: 0.5320
Epoch: 11, Loss: 1.8233, Train: 0.7786, Val: 0.5760, Test: 0.6020
Epoch: 12, Loss: 1.7413, Train: 0.8214, Val: 0.6520, Test: 0.6600
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 13, Loss: 1.6890, Train: 0.8643, Val: 0.6800, Test: 0.7040
Epoch: 14, Loss: 1.6498, Train: 0.8643, Val: 0.6920, Test: 0.7350
Epoch: 15, Loss: 1.5811, Train: 0.8714, Val: 0.6940, Test: 0.7540
Epoch: 16, Loss: 1.5136, Train: 0.8786, Val: 0.7120, Test: 0.7510
Epoch: 17, Loss: 1.5596, Train: 0.8857, Val: 0.7120, Test: 0.7530
Epoch: 18, Loss: 1.4374, Train: 0.8786, Val: 0.7160, Test: 0.7520
Epoch: 19, Loss: 1.4528, Train: 0.8714, Val: 0.7160, Test: 0.7510
Epoch: 20, Loss: 1.4142, Train: 0.8571, Val: 0.7340, Test: 0.7480
Epoch: 21, Loss: 1.3834, Train: 0.8643, Val: 0.7240, Test: 0.7500
Epoch: 22, Loss: 1.3072, Train: 0.8857, Val: 0.7320, Test: 0.7550
Epoch: 23, Loss: 1.1131, Train: 0.8857, Val: 0.7360, Test: 0.7500
Epoch: 24, Loss: 1.1841, Train: 0.9000, Val: 0.7480, Test: 0.7550
Epoch: 25, Loss: 1.0315, Train: 0.9000, Val: 0.7540, Test: 0.7560
Epoch: 26, Loss: 1.0060, Train: 0.9143, Val: 0.7560, Test: 0.7660
Epoch: 27, Loss: 0.9896, Train: 0.9286, Val: 0.7640, Test: 0.7710
Epoch: 28, Loss: 0.9284, Train: 0.9286, Val: 0.7720, Test: 0.7780
Epoch: 29, Loss: 0.8108, Train: 0.9429, Val: 0.7800, Test: 0.7820
Epoch: 30, Loss: 0.6991, Train: 0.9500, Val: 0.7860, Test: 0.7830
Epoch: 31, Loss: 0.7183, Train: 0.9571, Val: 0.7880, Test: 0.7910
Epoch: 32, Loss: 0.7021, Train: 0.9571, Val: 0.7900, Test: 0.7890
Epoch: 33, Loss: 0.5811, Train: 0.9643, Val: 0.7860, Test: 0.7880
Epoch: 34, Loss: 0.5596, Train: 0.9643, Val: 0.7960, Test: 0.7960
Epoch: 35, Loss: 0.5677, Train: 0.9643, Val: 0.7980, Test: 0.7980
Epoch: 36, Loss: 0.4496, Train: 0.9786, Val: 0.7900, Test: 0.8070
Epoch: 37, Loss: 0.4174, Train: 0.9857, Val: 0.7980, Test: 0.8140
Epoch: 38, Loss: 0.4753, Train: 0.9857, Val: 0.7960, Test: 0.8070
Epoch: 39, Loss: 0.4357, Train: 0.9857, Val: 0.7980, Test: 0.8040
Epoch: 40, Loss: 0.3374, Train: 0.9857, Val: 0.8040, Test: 0.8090
Epoch: 41, Loss: 0.4111, Train: 0.9857, Val: 0.7980, Test: 0.8030
Epoch: 42, Loss: 0.2891, Train: 0.9857, Val: 0.7980, Test: 0.8000
Epoch: 43, Loss: 0.2444, Train: 0.9857, Val: 0.7920, Test: 0.8010
Epoch: 44, Loss: 0.2370, Train: 0.9857, Val: 0.7900, Test: 0.8000
Epoch: 45, Loss: 0.2094, Train: 0.9929, Val: 0.7880, Test: 0.7980
Epoch: 46, Loss: 0.2118, Train: 0.9929, Val: 0.7900, Test: 0.8030
Epoch: 47, Loss: 0.2414, Train: 0.9929, Val: 0.7920, Test: 0.8040
Epoch: 48, Loss: 0.2000, Train: 0.9929, Val: 0.7980, Test: 0.8030
Epoch: 49, Loss: 0.1534, Train: 0.9929, Val: 0.8020, Test: 0.8050
Epoch: 50, Loss: 0.1515, Train: 0.9929, Val: 0.7880, Test: 0.8060
MAD:  0.8226
Best Test Accuracy: 0.8140, Val Accuracy: 0.7980, Train Accuracy: 0.9857
Training completed.
Seed:  3
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.3430, Train: 0.1571, Val: 0.1440, Test: 0.1310
Epoch: 2, Loss: 2.2025, Train: 0.3429, Val: 0.2380, Test: 0.2400
Epoch: 3, Loss: 2.1179, Train: 0.4071, Val: 0.2720, Test: 0.2570
Epoch: 4, Loss: 2.1022, Train: 0.4714, Val: 0.2820, Test: 0.2830
Epoch: 5, Loss: 1.9111, Train: 0.4714, Val: 0.2940, Test: 0.3000
Epoch: 6, Loss: 1.9085, Train: 0.4714, Val: 0.3200, Test: 0.3240
Epoch: 7, Loss: 1.8142, Train: 0.4786, Val: 0.3340, Test: 0.3410
Epoch: 8, Loss: 1.8415, Train: 0.4857, Val: 0.3520, Test: 0.3610
Epoch: 9, Loss: 1.9235, Train: 0.5857, Val: 0.3760, Test: 0.4030
Epoch: 10, Loss: 1.7485, Train: 0.6571, Val: 0.4400, Test: 0.4560
Epoch: 11, Loss: 1.7283, Train: 0.7643, Val: 0.4820, Test: 0.5190
Epoch: 12, Loss: 1.6928, Train: 0.8214, Val: 0.5200, Test: 0.5960
Epoch: 13, Loss: 1.7649, Train: 0.8714, Val: 0.5920, Test: 0.6530
Epoch: 14, Loss: 1.6502, Train: 0.9000, Val: 0.6400, Test: 0.7050
Epoch: 15, Loss: 1.6550, Train: 0.9071, Val: 0.6780, Test: 0.7330
Epoch: 16, Loss: 1.5140, Train: 0.9071, Val: 0.6880, Test: 0.7440
Epoch: 17, Loss: 1.5032, Train: 0.9214, Val: 0.6960, Test: 0.7400
Epoch: 18, Loss: 1.3846, Train: 0.9286, Val: 0.7040, Test: 0.7460
Epoch: 19, Loss: 1.2989, Train: 0.9286, Val: 0.7060, Test: 0.7540
Epoch: 20, Loss: 1.2515, Train: 0.9214, Val: 0.7100, Test: 0.7620
Epoch: 21, Loss: 1.2589, Train: 0.9214, Val: 0.7180, Test: 0.7640
Epoch: 22, Loss: 1.2935, Train: 0.9214, Val: 0.7200, Test: 0.7720
Epoch: 23, Loss: 1.1161, Train: 0.9429, Val: 0.7360, Test: 0.7770
Epoch: 24, Loss: 1.0706, Train: 0.9429, Val: 0.7460, Test: 0.7880
Epoch: 25, Loss: 0.9486, Train: 0.9571, Val: 0.7500, Test: 0.8000
Epoch: 26, Loss: 0.8362, Train: 0.9571, Val: 0.7640, Test: 0.8080
Epoch: 27, Loss: 0.8469, Train: 0.9429, Val: 0.7700, Test: 0.8200
Epoch: 28, Loss: 0.8025, Train: 0.9429, Val: 0.7800, Test: 0.8220
Epoch: 29, Loss: 0.7636, Train: 0.9571, Val: 0.7800, Test: 0.8250
Epoch: 30, Loss: 0.6165, Train: 0.9571, Val: 0.7800, Test: 0.8240
Epoch: 31, Loss: 0.6202, Train: 0.9643, Val: 0.7800, Test: 0.8210
Epoch: 32, Loss: 0.5403, Train: 0.9643, Val: 0.7740, Test: 0.8150
Epoch: 33, Loss: 0.5174, Train: 0.9714, Val: 0.7800, Test: 0.8150
Epoch: 34, Loss: 0.4898, Train: 0.9786, Val: 0.7900, Test: 0.8170
Epoch: 35, Loss: 0.5703, Train: 0.9714, Val: 0.7900, Test: 0.8210
Epoch: 36, Loss: 0.4233, Train: 0.9786, Val: 0.7940, Test: 0.8180
Epoch: 37, Loss: 0.4381, Train: 0.9786, Val: 0.7940, Test: 0.8170
Epoch: 38, Loss: 0.2081, Train: 0.9786, Val: 0.7960, Test: 0.8170
Epoch: 39, Loss: 0.1956, Train: 0.9786, Val: 0.7960, Test: 0.8140
Epoch: 40, Loss: 0.3416, Train: 0.9786, Val: 0.7940, Test: 0.8110
Epoch: 41, Loss: 0.3468, Train: 0.9857, Val: 0.7900, Test: 0.8020
Epoch: 42, Loss: 0.2735, Train: 0.9857, Val: 0.7900, Test: 0.8010
Epoch: 43, Loss: 0.2730, Train: 1.0000, Val: 0.7820, Test: 0.8050
Epoch: 44, Loss: 0.2525, Train: 1.0000, Val: 0.7720, Test: 0.8030
Epoch: 45, Loss: 0.2388, Train: 0.9929, Val: 0.7720, Test: 0.7980
Epoch: 46, Loss: 0.2242, Train: 1.0000, Val: 0.7740, Test: 0.7970
Epoch: 47, Loss: 0.1732, Train: 0.9929, Val: 0.7780, Test: 0.7940
Epoch: 48, Loss: 0.1819, Train: 0.9929, Val: 0.7780, Test: 0.7950
Epoch: 49, Loss: 0.1600, Train: 0.9857, Val: 0.7820, Test: 0.7920
Epoch: 50, Loss: 0.1170, Train: 0.9857, Val: 0.7800, Test: 0.7920
MAD:  0.8922
Best Test Accuracy: 0.8250, Val Accuracy: 0.7800, Train Accuracy: 0.9571
Training completed.
Seed:  4
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.3483, Train: 0.1714, Val: 0.1020, Test: 0.0910
Epoch: 2, Loss: 2.1178, Train: 0.3143, Val: 0.2360, Test: 0.2770
Epoch: 3, Loss: 1.9382, Train: 0.2857, Val: 0.3020, Test: 0.3410
Epoch: 4, Loss: 1.9274, Train: 0.3000, Val: 0.3260, Test: 0.3690
Epoch: 5, Loss: 1.9790, Train: 0.3429, Val: 0.3540, Test: 0.3790
Epoch: 6, Loss: 1.8901, Train: 0.4429, Val: 0.3900, Test: 0.4170
Epoch: 7, Loss: 1.9639, Train: 0.5429, Val: 0.4720, Test: 0.4800
Epoch: 8, Loss: 1.8654, Train: 0.6143, Val: 0.5440, Test: 0.5320
Epoch: 9, Loss: 1.8787, Train: 0.6857, Val: 0.5980, Test: 0.5790
Epoch: 10, Loss: 1.7607, Train: 0.7143, Val: 0.6380, Test: 0.6190
Epoch: 11, Loss: 1.7923, Train: 0.7429, Val: 0.6580, Test: 0.6350
Epoch: 12, Loss: 1.7244, Train: 0.7786, Val: 0.6580, Test: 0.6460
Epoch: 13, Loss: 1.6098, Train: 0.7929, Val: 0.6600, Test: 0.6570
Epoch: 14, Loss: 1.7165, Train: 0.8214, Val: 0.6620, Test: 0.6590
Epoch: 15, Loss: 1.6751, Train: 0.8286, Val: 0.6700, Test: 0.6750
Epoch: 16, Loss: 1.6005, Train: 0.8286, Val: 0.6840, Test: 0.6890
Epoch: 17, Loss: 1.7021, Train: 0.8500, Val: 0.6980, Test: 0.7010
Epoch: 18, Loss: 1.5438, Train: 0.8714, Val: 0.7060, Test: 0.7060
Epoch: 19, Loss: 1.4278, Train: 0.8857, Val: 0.7060, Test: 0.7150
Epoch: 20, Loss: 1.4205, Train: 0.9000, Val: 0.7140, Test: 0.7260
Epoch: 21, Loss: 1.3438, Train: 0.9214, Val: 0.7160, Test: 0.7340
Epoch: 22, Loss: 1.2684, Train: 0.9286, Val: 0.7320, Test: 0.7400
Epoch: 23, Loss: 1.2348, Train: 0.9286, Val: 0.7480, Test: 0.7550
Epoch: 24, Loss: 1.2660, Train: 0.9286, Val: 0.7600, Test: 0.7640
Epoch: 25, Loss: 1.1592, Train: 0.9357, Val: 0.7740, Test: 0.7790
Epoch: 26, Loss: 1.0354, Train: 0.9357, Val: 0.7840, Test: 0.7860
Epoch: 27, Loss: 0.9956, Train: 0.9429, Val: 0.7920, Test: 0.7920
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 28, Loss: 0.9142, Train: 0.9500, Val: 0.7960, Test: 0.7920
Epoch: 29, Loss: 0.9589, Train: 0.9571, Val: 0.7900, Test: 0.7870
Epoch: 30, Loss: 0.7374, Train: 0.9571, Val: 0.7900, Test: 0.7810
Epoch: 31, Loss: 0.6960, Train: 0.9500, Val: 0.7860, Test: 0.7830
Epoch: 32, Loss: 0.7616, Train: 0.9500, Val: 0.7840, Test: 0.7900
Epoch: 33, Loss: 0.6939, Train: 0.9429, Val: 0.7880, Test: 0.7940
Epoch: 34, Loss: 0.6013, Train: 0.9500, Val: 0.7920, Test: 0.7950
Epoch: 35, Loss: 0.5706, Train: 0.9500, Val: 0.8000, Test: 0.7970
Epoch: 36, Loss: 0.4781, Train: 0.9500, Val: 0.8060, Test: 0.8000
Epoch: 37, Loss: 0.6187, Train: 0.9500, Val: 0.8080, Test: 0.8100
Epoch: 38, Loss: 0.4360, Train: 0.9571, Val: 0.8040, Test: 0.8170
Epoch: 39, Loss: 0.3812, Train: 0.9643, Val: 0.8080, Test: 0.8170
Epoch: 40, Loss: 0.3993, Train: 0.9786, Val: 0.8120, Test: 0.8100
Epoch: 41, Loss: 0.3974, Train: 0.9786, Val: 0.8080, Test: 0.8040
Epoch: 42, Loss: 0.2386, Train: 0.9857, Val: 0.8020, Test: 0.8030
Epoch: 43, Loss: 0.2777, Train: 0.9857, Val: 0.7980, Test: 0.7950
Epoch: 44, Loss: 0.3480, Train: 0.9857, Val: 0.7940, Test: 0.7890
Epoch: 45, Loss: 0.2417, Train: 0.9857, Val: 0.7960, Test: 0.7900
Epoch: 46, Loss: 0.2574, Train: 0.9786, Val: 0.7980, Test: 0.7900
Epoch: 47, Loss: 0.3300, Train: 0.9857, Val: 0.7940, Test: 0.7910
Epoch: 48, Loss: 0.4057, Train: 0.9857, Val: 0.7880, Test: 0.7910
Epoch: 49, Loss: 0.1407, Train: 0.9786, Val: 0.7920, Test: 0.7910
Epoch: 50, Loss: 0.2009, Train: 0.9714, Val: 0.7920, Test: 0.7910
MAD:  0.8545
Best Test Accuracy: 0.8170, Val Accuracy: 0.8040, Train Accuracy: 0.9571
Training completed.
Seed:  5
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.3702, Train: 0.1643, Val: 0.1320, Test: 0.1020
Epoch: 2, Loss: 2.1498, Train: 0.3214, Val: 0.1980, Test: 0.1760
Epoch: 3, Loss: 2.0576, Train: 0.3857, Val: 0.2440, Test: 0.2320
Epoch: 4, Loss: 1.9226, Train: 0.5071, Val: 0.3400, Test: 0.3170
Epoch: 5, Loss: 1.7985, Train: 0.5714, Val: 0.3920, Test: 0.3570
Epoch: 6, Loss: 1.8815, Train: 0.6429, Val: 0.4440, Test: 0.4170
Epoch: 7, Loss: 1.8687, Train: 0.7000, Val: 0.4740, Test: 0.4630
Epoch: 8, Loss: 1.7522, Train: 0.7643, Val: 0.5140, Test: 0.5090
Epoch: 9, Loss: 1.7333, Train: 0.7929, Val: 0.5540, Test: 0.5630
Epoch: 10, Loss: 1.6569, Train: 0.8643, Val: 0.5960, Test: 0.6030
Epoch: 11, Loss: 1.5960, Train: 0.8714, Val: 0.6320, Test: 0.6320
Epoch: 12, Loss: 1.6495, Train: 0.8857, Val: 0.6520, Test: 0.6580
Epoch: 13, Loss: 1.5904, Train: 0.9000, Val: 0.6680, Test: 0.6740
Epoch: 14, Loss: 1.5767, Train: 0.9071, Val: 0.6720, Test: 0.6900
Epoch: 15, Loss: 1.5402, Train: 0.9071, Val: 0.6700, Test: 0.6890
Epoch: 16, Loss: 1.4747, Train: 0.9143, Val: 0.6680, Test: 0.6810
Epoch: 17, Loss: 1.3176, Train: 0.9143, Val: 0.6600, Test: 0.6670
Epoch: 18, Loss: 1.3697, Train: 0.8929, Val: 0.6540, Test: 0.6550
Epoch: 19, Loss: 1.2655, Train: 0.9000, Val: 0.6580, Test: 0.6560
Epoch: 20, Loss: 1.2034, Train: 0.9071, Val: 0.6640, Test: 0.6660
Epoch: 21, Loss: 1.0803, Train: 0.9071, Val: 0.6840, Test: 0.6790
Epoch: 22, Loss: 1.0646, Train: 0.9071, Val: 0.6920, Test: 0.6980
Epoch: 23, Loss: 1.0604, Train: 0.9143, Val: 0.7100, Test: 0.7170
Epoch: 24, Loss: 0.9161, Train: 0.9143, Val: 0.7220, Test: 0.7290
Epoch: 25, Loss: 0.7976, Train: 0.9286, Val: 0.7360, Test: 0.7440
Epoch: 26, Loss: 0.8032, Train: 0.9357, Val: 0.7580, Test: 0.7480
Epoch: 27, Loss: 0.7877, Train: 0.9357, Val: 0.7640, Test: 0.7520
Epoch: 28, Loss: 0.6228, Train: 0.9429, Val: 0.7780, Test: 0.7590
Epoch: 29, Loss: 0.5684, Train: 0.9500, Val: 0.7800, Test: 0.7640
Epoch: 30, Loss: 0.5407, Train: 0.9429, Val: 0.7800, Test: 0.7670
Epoch: 31, Loss: 0.4303, Train: 0.9500, Val: 0.7820, Test: 0.7780
Epoch: 32, Loss: 0.4183, Train: 0.9500, Val: 0.7960, Test: 0.7820
Epoch: 33, Loss: 0.4843, Train: 0.9571, Val: 0.7940, Test: 0.7920
Epoch: 34, Loss: 0.4680, Train: 0.9714, Val: 0.7860, Test: 0.7860
Epoch: 35, Loss: 0.4401, Train: 0.9714, Val: 0.7900, Test: 0.7980
Epoch: 36, Loss: 0.3993, Train: 0.9786, Val: 0.7920, Test: 0.8040
Epoch: 37, Loss: 0.2629, Train: 0.9786, Val: 0.7920, Test: 0.8030
Epoch: 38, Loss: 0.2489, Train: 0.9857, Val: 0.7920, Test: 0.7980
Epoch: 39, Loss: 0.2534, Train: 0.9857, Val: 0.7880, Test: 0.7920
Epoch: 40, Loss: 0.3182, Train: 0.9857, Val: 0.7900, Test: 0.7870
Epoch: 41, Loss: 0.2861, Train: 0.9857, Val: 0.7940, Test: 0.7830
Epoch: 42, Loss: 0.2038, Train: 0.9857, Val: 0.7900, Test: 0.7790
Epoch: 43, Loss: 0.1707, Train: 0.9929, Val: 0.7760, Test: 0.7750
Epoch: 44, Loss: 0.2495, Train: 0.9929, Val: 0.7700, Test: 0.7770
Epoch: 45, Loss: 0.2308, Train: 0.9857, Val: 0.7780, Test: 0.7850
Epoch: 46, Loss: 0.1818, Train: 0.9929, Val: 0.7880, Test: 0.7890
Epoch: 47, Loss: 0.1970, Train: 0.9929, Val: 0.7940, Test: 0.7890
Epoch: 48, Loss: 0.2086, Train: 0.9929, Val: 0.7880, Test: 0.7960
Epoch: 49, Loss: 0.1472, Train: 0.9929, Val: 0.7860, Test: 0.7960
Epoch: 50, Loss: 0.1295, Train: 0.9929, Val: 0.7880, Test: 0.7930
MAD:  0.8478
Best Test Accuracy: 0.8040, Val Accuracy: 0.7920, Train Accuracy: 0.9786
Training completed.
Seed:  6
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.3847, Train: 0.2500, Val: 0.2320, Test: 0.2490
Epoch: 2, Loss: 2.1519, Train: 0.2214, Val: 0.1920, Test: 0.2050
Epoch: 3, Loss: 2.0760, Train: 0.2714, Val: 0.2100, Test: 0.2200
Epoch: 4, Loss: 1.9756, Train: 0.3429, Val: 0.2140, Test: 0.2300
Epoch: 5, Loss: 1.9819, Train: 0.3429, Val: 0.1840, Test: 0.2050
Epoch: 6, Loss: 1.9339, Train: 0.3500, Val: 0.1540, Test: 0.1740
Epoch: 7, Loss: 1.9440, Train: 0.3571, Val: 0.1780, Test: 0.1840
Epoch: 8, Loss: 1.9083, Train: 0.4000, Val: 0.1960, Test: 0.2000
Epoch: 9, Loss: 1.7902, Train: 0.4500, Val: 0.2340, Test: 0.2440
Epoch: 10, Loss: 1.8331, Train: 0.4857, Val: 0.3020, Test: 0.2970
Epoch: 11, Loss: 1.8121, Train: 0.5714, Val: 0.3960, Test: 0.3730
Epoch: 12, Loss: 1.7575, Train: 0.6786, Val: 0.4840, Test: 0.4650
Epoch: 13, Loss: 1.7197, Train: 0.7357, Val: 0.5640, Test: 0.5480
Epoch: 14, Loss: 1.6698, Train: 0.7714, Val: 0.5900, Test: 0.5910
Epoch: 15, Loss: 1.6676, Train: 0.7857, Val: 0.6060, Test: 0.6120
Epoch: 16, Loss: 1.6071, Train: 0.7857, Val: 0.6120, Test: 0.6280
Epoch: 17, Loss: 1.5532, Train: 0.7929, Val: 0.6260, Test: 0.6530
Epoch: 18, Loss: 1.5543, Train: 0.8071, Val: 0.6300, Test: 0.6600
Epoch: 19, Loss: 1.4046, Train: 0.8214, Val: 0.6360, Test: 0.6740
Epoch: 20, Loss: 1.4271, Train: 0.8357, Val: 0.6500, Test: 0.6780
Epoch: 21, Loss: 1.3467, Train: 0.8429, Val: 0.6560, Test: 0.6850
Epoch: 22, Loss: 1.3045, Train: 0.8571, Val: 0.6720, Test: 0.6970
Epoch: 23, Loss: 1.2188, Train: 0.8500, Val: 0.6740, Test: 0.7050
Epoch: 24, Loss: 1.0964, Train: 0.8571, Val: 0.6760, Test: 0.7140
Epoch: 25, Loss: 1.0017, Train: 0.8643, Val: 0.6860, Test: 0.7210
Epoch: 26, Loss: 1.1299, Train: 0.8929, Val: 0.6960, Test: 0.7350
Epoch: 27, Loss: 0.8761, Train: 0.9071, Val: 0.7100, Test: 0.7540
Epoch: 28, Loss: 0.8444, Train: 0.9357, Val: 0.7380, Test: 0.7620
Epoch: 29, Loss: 0.7221, Train: 0.9214, Val: 0.7540, Test: 0.7810
Epoch: 30, Loss: 0.7595, Train: 0.9214, Val: 0.7560, Test: 0.7920
Epoch: 31, Loss: 0.7251, Train: 0.9214, Val: 0.7720, Test: 0.7960
Epoch: 32, Loss: 0.6088, Train: 0.9357, Val: 0.7700, Test: 0.7980
Epoch: 33, Loss: 0.6967, Train: 0.9429, Val: 0.7680, Test: 0.8010
Epoch: 34, Loss: 0.6238, Train: 0.9571, Val: 0.7660, Test: 0.7990
Epoch: 35, Loss: 0.5597, Train: 0.9571, Val: 0.7600, Test: 0.7940
Epoch: 36, Loss: 0.3994, Train: 0.9571, Val: 0.7660, Test: 0.7930
Epoch: 37, Loss: 0.5818, Train: 0.9571, Val: 0.7640, Test: 0.7910
Epoch: 38, Loss: 0.4978, Train: 0.9643, Val: 0.7680, Test: 0.7910
Epoch: 39, Loss: 0.4685, Train: 0.9643, Val: 0.7600, Test: 0.7890
Epoch: 40, Loss: 0.3697, Train: 0.9714, Val: 0.7760, Test: 0.7940
Epoch: 41, Loss: 0.3470, Train: 0.9714, Val: 0.7840, Test: 0.8030
Epoch: 42, Loss: 0.3144, Train: 0.9786, Val: 0.7880, Test: 0.8040
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 43, Loss: 0.2447, Train: 0.9857, Val: 0.7920, Test: 0.8080
Epoch: 44, Loss: 0.3245, Train: 0.9857, Val: 0.7880, Test: 0.8000
Epoch: 45, Loss: 0.2321, Train: 0.9857, Val: 0.7880, Test: 0.7930
Epoch: 46, Loss: 0.2051, Train: 1.0000, Val: 0.7840, Test: 0.7840
Epoch: 47, Loss: 0.1810, Train: 0.9929, Val: 0.7800, Test: 0.7790
Epoch: 48, Loss: 0.2230, Train: 1.0000, Val: 0.7760, Test: 0.7810
Epoch: 49, Loss: 0.1986, Train: 0.9929, Val: 0.7820, Test: 0.7830
Epoch: 50, Loss: 0.1731, Train: 0.9857, Val: 0.7820, Test: 0.7850
MAD:  0.9136
Best Test Accuracy: 0.8080, Val Accuracy: 0.7920, Train Accuracy: 0.9857
Training completed.
Seed:  7
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.2907, Train: 0.1714, Val: 0.1900, Test: 0.1710
Epoch: 2, Loss: 2.0067, Train: 0.2357, Val: 0.1400, Test: 0.1490
Epoch: 3, Loss: 2.1004, Train: 0.2000, Val: 0.0980, Test: 0.1120
Epoch: 4, Loss: 1.9519, Train: 0.1786, Val: 0.0860, Test: 0.0990
Epoch: 5, Loss: 1.9351, Train: 0.1857, Val: 0.0780, Test: 0.0990
Epoch: 6, Loss: 1.8912, Train: 0.2143, Val: 0.0860, Test: 0.1120
Epoch: 7, Loss: 1.9308, Train: 0.2500, Val: 0.1000, Test: 0.1230
Epoch: 8, Loss: 1.9341, Train: 0.2929, Val: 0.1260, Test: 0.1450
Epoch: 9, Loss: 1.8465, Train: 0.3786, Val: 0.1720, Test: 0.2080
Epoch: 10, Loss: 1.8119, Train: 0.5429, Val: 0.3040, Test: 0.3100
Epoch: 11, Loss: 1.8308, Train: 0.7000, Val: 0.4540, Test: 0.4600
Epoch: 12, Loss: 1.6986, Train: 0.7714, Val: 0.5540, Test: 0.5700
Epoch: 13, Loss: 1.7430, Train: 0.8286, Val: 0.6440, Test: 0.6450
Epoch: 14, Loss: 1.6948, Train: 0.8571, Val: 0.6540, Test: 0.6810
Epoch: 15, Loss: 1.6911, Train: 0.8643, Val: 0.6700, Test: 0.7090
Epoch: 16, Loss: 1.6428, Train: 0.8929, Val: 0.6880, Test: 0.7340
Epoch: 17, Loss: 1.6764, Train: 0.8929, Val: 0.6940, Test: 0.7420
Epoch: 18, Loss: 1.5921, Train: 0.8929, Val: 0.7020, Test: 0.7390
Epoch: 19, Loss: 1.4588, Train: 0.8929, Val: 0.7140, Test: 0.7450
Epoch: 20, Loss: 1.4357, Train: 0.8929, Val: 0.7220, Test: 0.7460
Epoch: 21, Loss: 1.3565, Train: 0.8929, Val: 0.7260, Test: 0.7510
Epoch: 22, Loss: 1.3186, Train: 0.9071, Val: 0.7320, Test: 0.7560
Epoch: 23, Loss: 1.2721, Train: 0.9143, Val: 0.7280, Test: 0.7600
Epoch: 24, Loss: 1.1529, Train: 0.9143, Val: 0.7320, Test: 0.7550
Epoch: 25, Loss: 1.1611, Train: 0.9357, Val: 0.7400, Test: 0.7560
Epoch: 26, Loss: 0.9760, Train: 0.9500, Val: 0.7520, Test: 0.7570
Epoch: 27, Loss: 0.9506, Train: 0.9500, Val: 0.7620, Test: 0.7630
Epoch: 28, Loss: 0.9611, Train: 0.9571, Val: 0.7700, Test: 0.7720
Epoch: 29, Loss: 0.9801, Train: 0.9571, Val: 0.7680, Test: 0.7790
Epoch: 30, Loss: 0.8681, Train: 0.9643, Val: 0.7680, Test: 0.7850
Epoch: 31, Loss: 0.7535, Train: 0.9643, Val: 0.7780, Test: 0.7960
Epoch: 32, Loss: 0.6373, Train: 0.9571, Val: 0.7820, Test: 0.8040
Epoch: 33, Loss: 0.6394, Train: 0.9643, Val: 0.7880, Test: 0.8100
Epoch: 34, Loss: 0.6258, Train: 0.9714, Val: 0.7840, Test: 0.8190
Epoch: 35, Loss: 0.5907, Train: 0.9714, Val: 0.7880, Test: 0.8170
Epoch: 36, Loss: 0.4788, Train: 0.9714, Val: 0.7860, Test: 0.8190
Epoch: 37, Loss: 0.4885, Train: 0.9643, Val: 0.7880, Test: 0.8200
Epoch: 38, Loss: 0.4131, Train: 0.9714, Val: 0.7900, Test: 0.8190
Epoch: 39, Loss: 0.3914, Train: 0.9857, Val: 0.7860, Test: 0.8150
Epoch: 40, Loss: 0.4921, Train: 0.9857, Val: 0.7800, Test: 0.8110
Epoch: 41, Loss: 0.3475, Train: 0.9857, Val: 0.7740, Test: 0.8040
Epoch: 42, Loss: 0.3600, Train: 0.9929, Val: 0.7740, Test: 0.8000
Epoch: 43, Loss: 0.2948, Train: 0.9929, Val: 0.7820, Test: 0.7990
Epoch: 44, Loss: 0.1972, Train: 1.0000, Val: 0.7780, Test: 0.7940
Epoch: 45, Loss: 0.2100, Train: 0.9929, Val: 0.7880, Test: 0.7870
Epoch: 46, Loss: 0.2295, Train: 0.9929, Val: 0.7780, Test: 0.7790
Epoch: 47, Loss: 0.2568, Train: 0.9929, Val: 0.7840, Test: 0.7850
Epoch: 48, Loss: 0.2107, Train: 1.0000, Val: 0.7840, Test: 0.7930
Epoch: 49, Loss: 0.2264, Train: 1.0000, Val: 0.7820, Test: 0.7900
Epoch: 50, Loss: 0.1310, Train: 0.9929, Val: 0.7820, Test: 0.7990
MAD:  0.8838
Best Test Accuracy: 0.8200, Val Accuracy: 0.7880, Train Accuracy: 0.9643
Training completed.
Seed:  8
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.7290, Train: 0.2500, Val: 0.2060, Test: 0.2020
Epoch: 2, Loss: 2.0719, Train: 0.1714, Val: 0.1360, Test: 0.1230
Epoch: 3, Loss: 2.0885, Train: 0.1571, Val: 0.1180, Test: 0.1090
Epoch: 4, Loss: 1.9543, Train: 0.1857, Val: 0.1200, Test: 0.1040
Epoch: 5, Loss: 2.0193, Train: 0.2071, Val: 0.1220, Test: 0.1110
Epoch: 6, Loss: 1.9920, Train: 0.2500, Val: 0.1420, Test: 0.1420
Epoch: 7, Loss: 1.9608, Train: 0.3786, Val: 0.1920, Test: 0.2060
Epoch: 8, Loss: 1.8676, Train: 0.4857, Val: 0.2420, Test: 0.2660
Epoch: 9, Loss: 1.8367, Train: 0.5000, Val: 0.2380, Test: 0.2830
Epoch: 10, Loss: 1.9014, Train: 0.5286, Val: 0.2420, Test: 0.2980
Epoch: 11, Loss: 1.8613, Train: 0.5714, Val: 0.3000, Test: 0.3330
Epoch: 12, Loss: 1.8015, Train: 0.5929, Val: 0.3220, Test: 0.3650
Epoch: 13, Loss: 1.8312, Train: 0.6643, Val: 0.3700, Test: 0.4290
Epoch: 14, Loss: 1.7244, Train: 0.7000, Val: 0.4200, Test: 0.4740
Epoch: 15, Loss: 1.7794, Train: 0.7214, Val: 0.4840, Test: 0.5230
Epoch: 16, Loss: 1.7351, Train: 0.7357, Val: 0.5200, Test: 0.5670
Epoch: 17, Loss: 1.7100, Train: 0.7786, Val: 0.5500, Test: 0.6010
Epoch: 18, Loss: 1.7004, Train: 0.8071, Val: 0.6020, Test: 0.6400
Epoch: 19, Loss: 1.6444, Train: 0.8714, Val: 0.6400, Test: 0.6770
Epoch: 20, Loss: 1.5796, Train: 0.8929, Val: 0.6740, Test: 0.7100
Epoch: 21, Loss: 1.5010, Train: 0.9071, Val: 0.6880, Test: 0.7360
Epoch: 22, Loss: 1.5786, Train: 0.9214, Val: 0.7100, Test: 0.7550
Epoch: 23, Loss: 1.4668, Train: 0.9143, Val: 0.7260, Test: 0.7580
Epoch: 24, Loss: 1.4270, Train: 0.9143, Val: 0.7280, Test: 0.7560
Epoch: 25, Loss: 1.3709, Train: 0.9143, Val: 0.7280, Test: 0.7610
Epoch: 26, Loss: 1.3157, Train: 0.9071, Val: 0.7300, Test: 0.7640
Epoch: 27, Loss: 1.2790, Train: 0.9000, Val: 0.7300, Test: 0.7720
Epoch: 28, Loss: 1.3807, Train: 0.9143, Val: 0.7380, Test: 0.7690
Epoch: 29, Loss: 1.1179, Train: 0.9357, Val: 0.7420, Test: 0.7750
Epoch: 30, Loss: 1.0575, Train: 0.9429, Val: 0.7500, Test: 0.7780
Epoch: 31, Loss: 1.0436, Train: 0.9429, Val: 0.7600, Test: 0.7930
Epoch: 32, Loss: 0.9488, Train: 0.9357, Val: 0.7580, Test: 0.7920
Epoch: 33, Loss: 0.8541, Train: 0.9429, Val: 0.7640, Test: 0.7970
Epoch: 34, Loss: 0.8429, Train: 0.9429, Val: 0.7680, Test: 0.7980
Epoch: 35, Loss: 0.7362, Train: 0.9429, Val: 0.7620, Test: 0.7930
Epoch: 36, Loss: 0.7251, Train: 0.9429, Val: 0.7640, Test: 0.7950
Epoch: 37, Loss: 0.6090, Train: 0.9429, Val: 0.7580, Test: 0.7880
Epoch: 38, Loss: 0.5108, Train: 0.9714, Val: 0.7480, Test: 0.7810
Epoch: 39, Loss: 0.4532, Train: 0.9714, Val: 0.7520, Test: 0.7820
Epoch: 40, Loss: 0.5225, Train: 0.9714, Val: 0.7640, Test: 0.7880
Epoch: 41, Loss: 0.4507, Train: 0.9786, Val: 0.7740, Test: 0.7910
Epoch: 42, Loss: 0.4538, Train: 0.9786, Val: 0.7780, Test: 0.7990
Epoch: 43, Loss: 0.3013, Train: 0.9786, Val: 0.7840, Test: 0.7980
Epoch: 44, Loss: 0.4464, Train: 0.9714, Val: 0.7840, Test: 0.8010
Epoch: 45, Loss: 0.2797, Train: 0.9714, Val: 0.7860, Test: 0.8040
Epoch: 46, Loss: 0.2505, Train: 0.9786, Val: 0.7860, Test: 0.8000
Epoch: 47, Loss: 0.3177, Train: 0.9714, Val: 0.7880, Test: 0.7930
Epoch: 48, Loss: 0.3034, Train: 0.9857, Val: 0.7900, Test: 0.7880
Epoch: 49, Loss: 0.3313, Train: 0.9857, Val: 0.7820, Test: 0.7870
Epoch: 50, Loss: 0.2720, Train: 0.9929, Val: 0.7800, Test: 0.7830
MAD:  0.925
Best Test Accuracy: 0.8040, Val Accuracy: 0.7860, Train Accuracy: 0.9714
Training completed.
Seed:  9
GCN(
  (convs): ModuleList(
    (0): GCNConv(1433, 128)
    (1-5): 5 x GCNConv(128, 128)
    (6): GCNConv(128, 7)
  )
  (residual_fc): Linear(in_features=1433, out_features=128, bias=True)
)
Epoch: 1, Loss: 2.1091, Train: 0.1929, Val: 0.1960, Test: 0.2270
Epoch: 2, Loss: 2.1208, Train: 0.2786, Val: 0.2680, Test: 0.2760
/root/code/DIR/DIR-GNN/train/cora.py:393: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 3, Loss: 2.0689, Train: 0.3429, Val: 0.3260, Test: 0.3350
Epoch: 4, Loss: 2.0080, Train: 0.4714, Val: 0.4000, Test: 0.4080
Epoch: 5, Loss: 1.9685, Train: 0.5214, Val: 0.4020, Test: 0.4080
Epoch: 6, Loss: 1.9662, Train: 0.4857, Val: 0.3880, Test: 0.4170
Epoch: 7, Loss: 1.8861, Train: 0.5214, Val: 0.4120, Test: 0.4440
Epoch: 8, Loss: 1.8445, Train: 0.5714, Val: 0.4340, Test: 0.4750
Epoch: 9, Loss: 1.8774, Train: 0.6071, Val: 0.4680, Test: 0.5110
Epoch: 10, Loss: 1.7527, Train: 0.6929, Val: 0.5300, Test: 0.5570
Epoch: 11, Loss: 1.8001, Train: 0.7286, Val: 0.5700, Test: 0.6040
Epoch: 12, Loss: 1.6993, Train: 0.7786, Val: 0.5960, Test: 0.6360
Epoch: 13, Loss: 1.6667, Train: 0.8357, Val: 0.6080, Test: 0.6560
Epoch: 14, Loss: 1.7114, Train: 0.8643, Val: 0.6200, Test: 0.6730
Epoch: 15, Loss: 1.6743, Train: 0.8929, Val: 0.6380, Test: 0.6780
Epoch: 16, Loss: 1.5909, Train: 0.9000, Val: 0.6440, Test: 0.6830
Epoch: 17, Loss: 1.6352, Train: 0.9000, Val: 0.6740, Test: 0.6910
Epoch: 18, Loss: 1.5596, Train: 0.9071, Val: 0.6760, Test: 0.7010
Epoch: 19, Loss: 1.4827, Train: 0.9000, Val: 0.6860, Test: 0.6980
Epoch: 20, Loss: 1.4120, Train: 0.9000, Val: 0.7020, Test: 0.7120
Epoch: 21, Loss: 1.4162, Train: 0.9000, Val: 0.7120, Test: 0.7260
Epoch: 22, Loss: 1.3642, Train: 0.9000, Val: 0.7220, Test: 0.7370
Epoch: 23, Loss: 1.3178, Train: 0.9143, Val: 0.7260, Test: 0.7540
Epoch: 24, Loss: 1.1929, Train: 0.9286, Val: 0.7380, Test: 0.7640
Epoch: 25, Loss: 1.2434, Train: 0.9286, Val: 0.7440, Test: 0.7690
Epoch: 26, Loss: 1.0769, Train: 0.9286, Val: 0.7500, Test: 0.7770
Epoch: 27, Loss: 1.1539, Train: 0.9286, Val: 0.7420, Test: 0.7820
Epoch: 28, Loss: 0.8764, Train: 0.9286, Val: 0.7460, Test: 0.7880
Epoch: 29, Loss: 0.9857, Train: 0.9357, Val: 0.7480, Test: 0.7880
Epoch: 30, Loss: 0.9008, Train: 0.9357, Val: 0.7540, Test: 0.7910
Epoch: 31, Loss: 0.7415, Train: 0.9429, Val: 0.7600, Test: 0.7950
Epoch: 32, Loss: 0.6953, Train: 0.9500, Val: 0.7540, Test: 0.7900
Epoch: 33, Loss: 0.7860, Train: 0.9500, Val: 0.7620, Test: 0.7930
Epoch: 34, Loss: 0.4868, Train: 0.9429, Val: 0.7760, Test: 0.7900
Epoch: 35, Loss: 0.6500, Train: 0.9500, Val: 0.7760, Test: 0.7890
Epoch: 36, Loss: 0.5840, Train: 0.9500, Val: 0.7740, Test: 0.7910
Epoch: 37, Loss: 0.4657, Train: 0.9643, Val: 0.7700, Test: 0.7970
Epoch: 38, Loss: 0.5258, Train: 0.9714, Val: 0.7760, Test: 0.7930
Epoch: 39, Loss: 0.4637, Train: 0.9643, Val: 0.7700, Test: 0.7960
Epoch: 40, Loss: 0.3539, Train: 0.9714, Val: 0.7700, Test: 0.7970
Epoch: 41, Loss: 0.4746, Train: 0.9786, Val: 0.7680, Test: 0.7940
Epoch: 42, Loss: 0.3662, Train: 0.9857, Val: 0.7660, Test: 0.7980
Epoch: 43, Loss: 0.2881, Train: 0.9786, Val: 0.7740, Test: 0.8000
Epoch: 44, Loss: 0.3461, Train: 0.9857, Val: 0.7740, Test: 0.7950
Epoch: 45, Loss: 0.2979, Train: 0.9857, Val: 0.7720, Test: 0.7910
Epoch: 46, Loss: 0.2364, Train: 0.9929, Val: 0.7760, Test: 0.7930
Epoch: 47, Loss: 0.1966, Train: 0.9929, Val: 0.7800, Test: 0.7940
Epoch: 48, Loss: 0.2937, Train: 0.9929, Val: 0.7820, Test: 0.7870
Epoch: 49, Loss: 0.1740, Train: 0.9929, Val: 0.7760, Test: 0.7850
Epoch: 50, Loss: 0.1788, Train: 1.0000, Val: 0.7640, Test: 0.7760
MAD:  0.7768
Best Test Accuracy: 0.8000, Val Accuracy: 0.7740, Train Accuracy: 0.9786
Training completed.
Average Test Accuracy:  0.8117000000000001 ± 0.00781088983407136
Average MAD:  0.8682500000000001 ± 0.053064588003677166

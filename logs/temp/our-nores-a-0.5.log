/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Seed:  0
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8534, Train: 0.2071, Val: 0.1560, Test: 0.1320
Epoch: 2, Loss: 4.8470, Train: 0.2429, Val: 0.1740, Test: 0.1700
Epoch: 3, Loss: 4.8349, Train: 0.2286, Val: 0.1580, Test: 0.1490
Epoch: 4, Loss: 4.8033, Train: 0.2071, Val: 0.1500, Test: 0.1400
Epoch: 5, Loss: 4.8070, Train: 0.2071, Val: 0.1460, Test: 0.1290
Epoch: 6, Loss: 4.7686, Train: 0.2214, Val: 0.1420, Test: 0.1230
Epoch: 7, Loss: 4.7761, Train: 0.2286, Val: 0.1440, Test: 0.1260
Epoch: 8, Loss: 4.7493, Train: 0.2500, Val: 0.1520, Test: 0.1380
Epoch: 9, Loss: 4.7373, Train: 0.2786, Val: 0.1640, Test: 0.1510
Epoch: 10, Loss: 4.7053, Train: 0.2857, Val: 0.1660, Test: 0.1590
Epoch: 11, Loss: 4.6106, Train: 0.2786, Val: 0.1600, Test: 0.1560
Epoch: 12, Loss: 4.6380, Train: 0.2857, Val: 0.1620, Test: 0.1570
Epoch: 13, Loss: 4.5821, Train: 0.2857, Val: 0.1680, Test: 0.1560
Epoch: 14, Loss: 4.5409, Train: 0.2786, Val: 0.1600, Test: 0.1550
Epoch: 15, Loss: 4.5088, Train: 0.2786, Val: 0.1660, Test: 0.1560
Epoch: 16, Loss: 4.3859, Train: 0.3071, Val: 0.1800, Test: 0.1670
Epoch: 17, Loss: 4.3528, Train: 0.3214, Val: 0.1960, Test: 0.1900
Epoch: 18, Loss: 4.3288, Train: 0.3500, Val: 0.2140, Test: 0.2130
Epoch: 19, Loss: 4.2622, Train: 0.4000, Val: 0.2320, Test: 0.2540
Epoch: 20, Loss: 4.2589, Train: 0.4286, Val: 0.2600, Test: 0.2760
Epoch: 21, Loss: 4.4826, Train: 0.5286, Val: 0.3500, Test: 0.3600
Epoch: 22, Loss: 4.3532, Train: 0.6286, Val: 0.5180, Test: 0.5330
Epoch: 23, Loss: 4.3804, Train: 0.7500, Val: 0.6140, Test: 0.6400
Epoch: 24, Loss: 4.1175, Train: 0.7143, Val: 0.6280, Test: 0.6490
Epoch: 25, Loss: 4.3517, Train: 0.7071, Val: 0.6180, Test: 0.6470
Epoch: 26, Loss: 4.2028, Train: 0.7143, Val: 0.6060, Test: 0.6490
Epoch: 27, Loss: 4.2219, Train: 0.7214, Val: 0.6220, Test: 0.6430
Epoch: 28, Loss: 4.1839, Train: 0.7286, Val: 0.6040, Test: 0.6370
Epoch: 29, Loss: 4.0232, Train: 0.8214, Val: 0.6600, Test: 0.6830
Epoch: 30, Loss: 4.0824, Train: 0.8571, Val: 0.7080, Test: 0.7160
Epoch: 31, Loss: 4.3951, Train: 0.8929, Val: 0.7180, Test: 0.7350
Epoch: 32, Loss: 4.0773, Train: 0.9071, Val: 0.7320, Test: 0.7500
Epoch: 33, Loss: 4.2610, Train: 0.9000, Val: 0.7360, Test: 0.7400
Epoch: 34, Loss: 3.8583, Train: 0.9071, Val: 0.7360, Test: 0.7380
Epoch: 35, Loss: 4.3008, Train: 0.9071, Val: 0.7240, Test: 0.7360
Epoch: 36, Loss: 4.0253, Train: 0.9143, Val: 0.7420, Test: 0.7500
Epoch: 37, Loss: 4.2056, Train: 0.9143, Val: 0.7540, Test: 0.7610
Epoch: 38, Loss: 3.9307, Train: 0.9143, Val: 0.7660, Test: 0.7720
Epoch: 39, Loss: 4.1699, Train: 0.9214, Val: 0.7700, Test: 0.7770
Epoch: 40, Loss: 3.9590, Train: 0.9286, Val: 0.7760, Test: 0.7790
Epoch: 41, Loss: 4.0343, Train: 0.9286, Val: 0.7800, Test: 0.7810
Epoch: 42, Loss: 4.1858, Train: 0.9286, Val: 0.7800, Test: 0.7850
Epoch: 43, Loss: 3.7360, Train: 0.9357, Val: 0.7800, Test: 0.7850
Epoch: 44, Loss: 4.0079, Train: 0.9357, Val: 0.7820, Test: 0.7880
Epoch: 45, Loss: 4.0984, Train: 0.9429, Val: 0.7840, Test: 0.7940
Epoch: 46, Loss: 3.8753, Train: 0.9571, Val: 0.7900, Test: 0.7980
Epoch: 47, Loss: 3.9608, Train: 0.9500, Val: 0.7840, Test: 0.7990
Epoch: 48, Loss: 3.5488, Train: 0.9500, Val: 0.7900, Test: 0.8020
Epoch: 49, Loss: 3.8419, Train: 0.9571, Val: 0.7900, Test: 0.8070
Epoch: 50, Loss: 4.1471, Train: 0.9571, Val: 0.7920, Test: 0.8130
MAD:  0.2199
Best Test Accuracy: 0.8130, Val Accuracy: 0.7920, Train Accuracy: 0.9571
Training completed.
Seed:  1
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8521, Train: 0.2357, Val: 0.1500, Test: 0.1680
Epoch: 2, Loss: 4.8338, Train: 0.3143, Val: 0.1640, Test: 0.2080
Epoch: 3, Loss: 4.8203, Train: 0.3286, Val: 0.2020, Test: 0.2290
Epoch: 4, Loss: 4.7964, Train: 0.3429, Val: 0.2240, Test: 0.2490
Epoch: 5, Loss: 4.7697, Train: 0.3500, Val: 0.2380, Test: 0.2620
Epoch: 6, Loss: 4.7674, Train: 0.3643, Val: 0.2600, Test: 0.2770
Epoch: 7, Loss: 4.7725, Train: 0.3714, Val: 0.2820, Test: 0.2990
Epoch: 8, Loss: 4.6842, Train: 0.3929, Val: 0.3200, Test: 0.3170
Epoch: 9, Loss: 4.6885, Train: 0.4071, Val: 0.3340, Test: 0.3250
Epoch: 10, Loss: 4.6527, Train: 0.3929, Val: 0.3200, Test: 0.3170
Epoch: 11, Loss: 4.5735, Train: 0.3786, Val: 0.3060, Test: 0.3070
Epoch: 12, Loss: 4.5897, Train: 0.3714, Val: 0.3040, Test: 0.3050
Epoch: 13, Loss: 4.5552, Train: 0.3786, Val: 0.3080, Test: 0.3200
Epoch: 14, Loss: 4.3541, Train: 0.3714, Val: 0.3060, Test: 0.3120
Epoch: 15, Loss: 4.5546, Train: 0.3857, Val: 0.3220, Test: 0.3280
Epoch: 16, Loss: 4.3735, Train: 0.4000, Val: 0.3460, Test: 0.3550
Epoch: 17, Loss: 4.3140, Train: 0.4143, Val: 0.3780, Test: 0.3970
Epoch: 18, Loss: 4.4593, Train: 0.4857, Val: 0.4220, Test: 0.4470
Epoch: 19, Loss: 4.3782, Train: 0.5500, Val: 0.4700, Test: 0.4900
Epoch: 20, Loss: 4.3043, Train: 0.6071, Val: 0.5160, Test: 0.5250
Epoch: 21, Loss: 4.3316, Train: 0.6857, Val: 0.5440, Test: 0.5830
Epoch: 22, Loss: 4.0936, Train: 0.7000, Val: 0.5740, Test: 0.5970
Epoch: 23, Loss: 4.2028, Train: 0.7071, Val: 0.5580, Test: 0.5860
Epoch: 24, Loss: 4.2840, Train: 0.7214, Val: 0.5480, Test: 0.5610
Epoch: 25, Loss: 4.0812, Train: 0.7071, Val: 0.5440, Test: 0.5580
Epoch: 26, Loss: 4.0633, Train: 0.7571, Val: 0.5560, Test: 0.5820
Epoch: 27, Loss: 4.1332, Train: 0.7714, Val: 0.5720, Test: 0.5990
Epoch: 28, Loss: 4.2566, Train: 0.7857, Val: 0.6020, Test: 0.6170
Epoch: 29, Loss: 4.0505, Train: 0.7929, Val: 0.6260, Test: 0.6420
Epoch: 30, Loss: 3.9460, Train: 0.8071, Val: 0.6620, Test: 0.6540
Epoch: 31, Loss: 4.0460, Train: 0.8357, Val: 0.6980, Test: 0.6870
Epoch: 32, Loss: 4.3142, Train: 0.8786, Val: 0.7300, Test: 0.7440
Epoch: 33, Loss: 4.0511, Train: 0.9000, Val: 0.7480, Test: 0.7630
Epoch: 34, Loss: 4.1243, Train: 0.8929, Val: 0.7440, Test: 0.7600
Epoch: 35, Loss: 3.8862, Train: 0.8929, Val: 0.7300, Test: 0.7620
Epoch: 36, Loss: 4.0700, Train: 0.9071, Val: 0.7320, Test: 0.7610
Epoch: 37, Loss: 4.2225, Train: 0.9071, Val: 0.7440, Test: 0.7660
Epoch: 38, Loss: 4.0600, Train: 0.8929, Val: 0.7540, Test: 0.7780
Epoch: 39, Loss: 3.9237, Train: 0.9143, Val: 0.7700, Test: 0.7920
Epoch: 40, Loss: 4.2077, Train: 0.9143, Val: 0.7720, Test: 0.7960
Epoch: 41, Loss: 4.1035, Train: 0.9214, Val: 0.7700, Test: 0.8020
Epoch: 42, Loss: 3.6713, Train: 0.9143, Val: 0.7760, Test: 0.8020
Epoch: 43, Loss: 3.9442, Train: 0.9143, Val: 0.7720, Test: 0.8030
Epoch: 44, Loss: 4.2663, Train: 0.9143, Val: 0.7760, Test: 0.8000
Epoch: 45, Loss: 3.7947, Train: 0.9214, Val: 0.7740, Test: 0.7980
Epoch: 46, Loss: 4.2871, Train: 0.9429, Val: 0.7740, Test: 0.7970
Epoch: 47, Loss: 3.7246, Train: 0.9429, Val: 0.7740, Test: 0.8010
Epoch: 48, Loss: 4.1234, Train: 0.9429, Val: 0.7720, Test: 0.7980
Epoch: 49, Loss: 3.7737, Train: 0.9357, Val: 0.7740, Test: 0.8010
Epoch: 50, Loss: 3.6208, Train: 0.9357, Val: 0.7720, Test: 0.7960
MAD:  0.1127
Best Test Accuracy: 0.8030, Val Accuracy: 0.7720, Train Accuracy: 0.9143
Training completed.
Seed:  2
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8547, Train: 0.1143, Val: 0.0800, Test: 0.0750
Epoch: 2, Loss: 4.8463, Train: 0.2714, Val: 0.2260, Test: 0.2460
Epoch: 3, Loss: 4.8392, Train: 0.4500, Val: 0.3580, Test: 0.3730
Epoch: 4, Loss: 4.8078, Train: 0.5214, Val: 0.4060, Test: 0.4150
Epoch: 5, Loss: 4.8018, Train: 0.5143, Val: 0.3700, Test: 0.3890
Epoch: 6, Loss: 4.7868, Train: 0.4929, Val: 0.3460, Test: 0.3680
Epoch: 7, Loss: 4.7804, Train: 0.4929, Val: 0.3100, Test: 0.3410
Epoch: 8, Loss: 4.7374, Train: 0.5143, Val: 0.3220, Test: 0.3490
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 9, Loss: 4.7090, Train: 0.5143, Val: 0.3520, Test: 0.3580
Epoch: 10, Loss: 4.6546, Train: 0.5071, Val: 0.3700, Test: 0.3870
Epoch: 11, Loss: 4.7185, Train: 0.4786, Val: 0.3440, Test: 0.3740
Epoch: 12, Loss: 4.5857, Train: 0.4571, Val: 0.3040, Test: 0.3280
Epoch: 13, Loss: 4.5883, Train: 0.4286, Val: 0.2520, Test: 0.2760
Epoch: 14, Loss: 4.5994, Train: 0.4000, Val: 0.1940, Test: 0.2360
Epoch: 15, Loss: 4.4506, Train: 0.3786, Val: 0.1580, Test: 0.1990
Epoch: 16, Loss: 4.4271, Train: 0.3643, Val: 0.1500, Test: 0.1870
Epoch: 17, Loss: 4.2924, Train: 0.3929, Val: 0.1560, Test: 0.2060
Epoch: 18, Loss: 4.4539, Train: 0.4357, Val: 0.2620, Test: 0.2920
Epoch: 19, Loss: 4.3990, Train: 0.4643, Val: 0.3160, Test: 0.3590
Epoch: 20, Loss: 4.1351, Train: 0.5071, Val: 0.3900, Test: 0.4430
Epoch: 21, Loss: 4.3001, Train: 0.5857, Val: 0.4460, Test: 0.4850
Epoch: 22, Loss: 4.2483, Train: 0.7071, Val: 0.5380, Test: 0.5780
Epoch: 23, Loss: 4.5370, Train: 0.7857, Val: 0.6480, Test: 0.6880
Epoch: 24, Loss: 4.3069, Train: 0.8143, Val: 0.6800, Test: 0.7170
Epoch: 25, Loss: 4.2181, Train: 0.8071, Val: 0.6820, Test: 0.6980
Epoch: 26, Loss: 4.1813, Train: 0.8071, Val: 0.6280, Test: 0.6470
Epoch: 27, Loss: 4.1152, Train: 0.7786, Val: 0.5920, Test: 0.6080
Epoch: 28, Loss: 4.3836, Train: 0.7500, Val: 0.5640, Test: 0.5740
Epoch: 29, Loss: 4.1428, Train: 0.7571, Val: 0.5520, Test: 0.5640
Epoch: 30, Loss: 4.1036, Train: 0.7714, Val: 0.5620, Test: 0.5710
Epoch: 31, Loss: 4.0978, Train: 0.8286, Val: 0.6020, Test: 0.5960
Epoch: 32, Loss: 4.1750, Train: 0.8500, Val: 0.6140, Test: 0.6190
Epoch: 33, Loss: 4.1405, Train: 0.8571, Val: 0.6000, Test: 0.6190
Epoch: 34, Loss: 4.0698, Train: 0.8643, Val: 0.5900, Test: 0.6220
Epoch: 35, Loss: 4.2212, Train: 0.8571, Val: 0.5920, Test: 0.6140
Epoch: 36, Loss: 4.1113, Train: 0.8857, Val: 0.6020, Test: 0.6160
Epoch: 37, Loss: 4.1903, Train: 0.8857, Val: 0.6180, Test: 0.6400
Epoch: 38, Loss: 4.0356, Train: 0.9000, Val: 0.6460, Test: 0.6560
Epoch: 39, Loss: 4.2359, Train: 0.9000, Val: 0.6820, Test: 0.6750
Epoch: 40, Loss: 3.8720, Train: 0.9000, Val: 0.7080, Test: 0.7050
Epoch: 41, Loss: 4.0204, Train: 0.9214, Val: 0.7340, Test: 0.7260
Epoch: 42, Loss: 3.9214, Train: 0.9286, Val: 0.7600, Test: 0.7470
Epoch: 43, Loss: 4.0314, Train: 0.9500, Val: 0.7820, Test: 0.7620
Epoch: 44, Loss: 4.2470, Train: 0.9571, Val: 0.7900, Test: 0.7870
Epoch: 45, Loss: 4.0922, Train: 0.9500, Val: 0.8000, Test: 0.8020
Epoch: 46, Loss: 3.9834, Train: 0.9571, Val: 0.8000, Test: 0.8060
Epoch: 47, Loss: 3.9781, Train: 0.9571, Val: 0.8060, Test: 0.8090
Epoch: 48, Loss: 4.0851, Train: 0.9571, Val: 0.7980, Test: 0.8060
Epoch: 49, Loss: 3.8749, Train: 0.9571, Val: 0.7940, Test: 0.8030
Epoch: 50, Loss: 3.9474, Train: 0.9429, Val: 0.7880, Test: 0.8010
MAD:  0.1577
Best Test Accuracy: 0.8090, Val Accuracy: 0.8060, Train Accuracy: 0.9571
Training completed.
Seed:  3
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8525, Train: 0.1929, Val: 0.1240, Test: 0.1220
Epoch: 2, Loss: 4.8488, Train: 0.2500, Val: 0.1720, Test: 0.1720
Epoch: 3, Loss: 4.8392, Train: 0.2571, Val: 0.1940, Test: 0.1870
Epoch: 4, Loss: 4.8337, Train: 0.2857, Val: 0.2160, Test: 0.2200
Epoch: 5, Loss: 4.8194, Train: 0.3357, Val: 0.2520, Test: 0.2620
Epoch: 6, Loss: 4.7788, Train: 0.3786, Val: 0.2720, Test: 0.2780
Epoch: 7, Loss: 4.7758, Train: 0.4000, Val: 0.2760, Test: 0.2890
Epoch: 8, Loss: 4.7478, Train: 0.4071, Val: 0.2840, Test: 0.2990
Epoch: 9, Loss: 4.7427, Train: 0.4000, Val: 0.3020, Test: 0.3110
Epoch: 10, Loss: 4.7388, Train: 0.3929, Val: 0.3140, Test: 0.3190
Epoch: 11, Loss: 4.6750, Train: 0.4143, Val: 0.3100, Test: 0.3220
Epoch: 12, Loss: 4.6681, Train: 0.4214, Val: 0.3080, Test: 0.3170
Epoch: 13, Loss: 4.5628, Train: 0.4143, Val: 0.3140, Test: 0.3210
Epoch: 14, Loss: 4.6960, Train: 0.4214, Val: 0.3160, Test: 0.3180
Epoch: 15, Loss: 4.4065, Train: 0.4286, Val: 0.3140, Test: 0.3210
Epoch: 16, Loss: 4.4884, Train: 0.4429, Val: 0.3280, Test: 0.3310
Epoch: 17, Loss: 4.5826, Train: 0.4857, Val: 0.3520, Test: 0.3570
Epoch: 18, Loss: 4.4006, Train: 0.5286, Val: 0.3700, Test: 0.3770
Epoch: 19, Loss: 4.3360, Train: 0.5571, Val: 0.3820, Test: 0.3890
Epoch: 20, Loss: 4.3046, Train: 0.5643, Val: 0.3740, Test: 0.3930
Epoch: 21, Loss: 4.5822, Train: 0.5714, Val: 0.3780, Test: 0.3980
Epoch: 22, Loss: 4.4389, Train: 0.5857, Val: 0.3880, Test: 0.4030
Epoch: 23, Loss: 4.2461, Train: 0.6143, Val: 0.3900, Test: 0.4060
Epoch: 24, Loss: 4.5921, Train: 0.6500, Val: 0.4080, Test: 0.4270
Epoch: 25, Loss: 4.3115, Train: 0.7071, Val: 0.5000, Test: 0.5090
Epoch: 26, Loss: 3.9318, Train: 0.7429, Val: 0.5700, Test: 0.5910
Epoch: 27, Loss: 4.2270, Train: 0.7643, Val: 0.6160, Test: 0.6470
Epoch: 28, Loss: 4.1524, Train: 0.7714, Val: 0.6340, Test: 0.6570
Epoch: 29, Loss: 3.9879, Train: 0.7857, Val: 0.6500, Test: 0.6670
Epoch: 30, Loss: 4.1935, Train: 0.7929, Val: 0.6460, Test: 0.6820
Epoch: 31, Loss: 4.1898, Train: 0.8071, Val: 0.6460, Test: 0.6810
Epoch: 32, Loss: 4.2778, Train: 0.8071, Val: 0.6480, Test: 0.6860
Epoch: 33, Loss: 4.1577, Train: 0.8071, Val: 0.6540, Test: 0.6870
Epoch: 34, Loss: 4.1230, Train: 0.8143, Val: 0.6600, Test: 0.6900
Epoch: 35, Loss: 4.2283, Train: 0.8143, Val: 0.6600, Test: 0.6870
Epoch: 36, Loss: 4.0631, Train: 0.8429, Val: 0.6600, Test: 0.6930
Epoch: 37, Loss: 4.1316, Train: 0.8857, Val: 0.7040, Test: 0.7290
Epoch: 38, Loss: 4.1874, Train: 0.9000, Val: 0.7100, Test: 0.7310
Epoch: 39, Loss: 4.2371, Train: 0.9071, Val: 0.7180, Test: 0.7210
Epoch: 40, Loss: 4.0532, Train: 0.9071, Val: 0.6920, Test: 0.7190
Epoch: 41, Loss: 4.0644, Train: 0.9143, Val: 0.6760, Test: 0.7120
Epoch: 42, Loss: 4.0165, Train: 0.9143, Val: 0.6760, Test: 0.6980
Epoch: 43, Loss: 3.9265, Train: 0.9214, Val: 0.6800, Test: 0.7050
Epoch: 44, Loss: 3.8355, Train: 0.9286, Val: 0.6900, Test: 0.7150
Epoch: 45, Loss: 4.1622, Train: 0.9286, Val: 0.7180, Test: 0.7290
Epoch: 46, Loss: 4.1003, Train: 0.9357, Val: 0.7380, Test: 0.7450
Epoch: 47, Loss: 3.8758, Train: 0.9357, Val: 0.7560, Test: 0.7520
Epoch: 48, Loss: 3.9335, Train: 0.9500, Val: 0.7520, Test: 0.7620
Epoch: 49, Loss: 3.9704, Train: 0.9500, Val: 0.7620, Test: 0.7660
Epoch: 50, Loss: 3.7846, Train: 0.9571, Val: 0.7720, Test: 0.7700
MAD:  0.216
Best Test Accuracy: 0.7700, Val Accuracy: 0.7720, Train Accuracy: 0.9571
Training completed.
Seed:  4
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8534, Train: 0.2143, Val: 0.1400, Test: 0.1600
Epoch: 2, Loss: 4.8339, Train: 0.4500, Val: 0.3140, Test: 0.3040
Epoch: 3, Loss: 4.8245, Train: 0.5071, Val: 0.3880, Test: 0.3590
Epoch: 4, Loss: 4.8164, Train: 0.4857, Val: 0.3960, Test: 0.3570
Epoch: 5, Loss: 4.7874, Train: 0.4500, Val: 0.3800, Test: 0.3540
Epoch: 6, Loss: 4.7614, Train: 0.4571, Val: 0.3680, Test: 0.3460
Epoch: 7, Loss: 4.7154, Train: 0.4786, Val: 0.3540, Test: 0.3480
Epoch: 8, Loss: 4.6903, Train: 0.4714, Val: 0.3560, Test: 0.3560
Epoch: 9, Loss: 4.6541, Train: 0.4786, Val: 0.3720, Test: 0.3660
Epoch: 10, Loss: 4.6540, Train: 0.4714, Val: 0.3840, Test: 0.3700
Epoch: 11, Loss: 4.6297, Train: 0.4929, Val: 0.3780, Test: 0.3730
Epoch: 12, Loss: 4.5335, Train: 0.4929, Val: 0.3740, Test: 0.3690
Epoch: 13, Loss: 4.4246, Train: 0.4929, Val: 0.3660, Test: 0.3660
Epoch: 14, Loss: 4.4230, Train: 0.5000, Val: 0.3640, Test: 0.3660
Epoch: 15, Loss: 4.4259, Train: 0.4857, Val: 0.3720, Test: 0.3670
Epoch: 16, Loss: 4.4814, Train: 0.5071, Val: 0.3840, Test: 0.3890
Epoch: 17, Loss: 4.3424, Train: 0.5500, Val: 0.4040, Test: 0.4160
Epoch: 18, Loss: 4.4623, Train: 0.6286, Val: 0.4800, Test: 0.4880
Epoch: 19, Loss: 4.2055, Train: 0.6857, Val: 0.5680, Test: 0.6030
Epoch: 20, Loss: 4.3937, Train: 0.7143, Val: 0.6100, Test: 0.6460
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 21, Loss: 4.4829, Train: 0.7286, Val: 0.6420, Test: 0.6640
Epoch: 22, Loss: 4.2339, Train: 0.7429, Val: 0.6420, Test: 0.6570
Epoch: 23, Loss: 4.2380, Train: 0.7571, Val: 0.6360, Test: 0.6600
Epoch: 24, Loss: 4.1542, Train: 0.7571, Val: 0.6220, Test: 0.6630
Epoch: 25, Loss: 4.2993, Train: 0.7571, Val: 0.5840, Test: 0.6270
Epoch: 26, Loss: 4.1555, Train: 0.7643, Val: 0.5640, Test: 0.5980
Epoch: 27, Loss: 4.4000, Train: 0.8357, Val: 0.6220, Test: 0.6460
Epoch: 28, Loss: 4.0165, Train: 0.8500, Val: 0.6540, Test: 0.6950
Epoch: 29, Loss: 3.9193, Train: 0.8714, Val: 0.6840, Test: 0.7150
Epoch: 30, Loss: 3.9690, Train: 0.8714, Val: 0.7040, Test: 0.7190
Epoch: 31, Loss: 4.2720, Train: 0.8714, Val: 0.7060, Test: 0.7240
Epoch: 32, Loss: 4.2198, Train: 0.8857, Val: 0.7280, Test: 0.7400
Epoch: 33, Loss: 3.9945, Train: 0.9000, Val: 0.7480, Test: 0.7690
Epoch: 34, Loss: 4.1741, Train: 0.9143, Val: 0.7880, Test: 0.7980
Epoch: 35, Loss: 3.9898, Train: 0.9286, Val: 0.7960, Test: 0.8050
Epoch: 36, Loss: 4.0605, Train: 0.9214, Val: 0.7900, Test: 0.8040
Epoch: 37, Loss: 4.0999, Train: 0.9000, Val: 0.7760, Test: 0.8000
Epoch: 38, Loss: 4.0327, Train: 0.9143, Val: 0.7640, Test: 0.7930
Epoch: 39, Loss: 3.8255, Train: 0.9071, Val: 0.7640, Test: 0.7940
Epoch: 40, Loss: 3.7233, Train: 0.9143, Val: 0.7660, Test: 0.7920
Epoch: 41, Loss: 4.0475, Train: 0.9071, Val: 0.7500, Test: 0.7710
Epoch: 42, Loss: 4.0821, Train: 0.9000, Val: 0.7440, Test: 0.7620
Epoch: 43, Loss: 4.1263, Train: 0.9000, Val: 0.7440, Test: 0.7620
Epoch: 44, Loss: 4.2571, Train: 0.9214, Val: 0.7520, Test: 0.7670
Epoch: 45, Loss: 4.1173, Train: 0.9214, Val: 0.7720, Test: 0.7800
Epoch: 46, Loss: 3.9551, Train: 0.9357, Val: 0.7820, Test: 0.7850
Epoch: 47, Loss: 4.0688, Train: 0.9357, Val: 0.7840, Test: 0.7930
Epoch: 48, Loss: 3.9405, Train: 0.9429, Val: 0.7920, Test: 0.7980
Epoch: 49, Loss: 3.9995, Train: 0.9571, Val: 0.7940, Test: 0.7990
Epoch: 50, Loss: 3.7261, Train: 0.9571, Val: 0.7920, Test: 0.8030
MAD:  0.0711
Best Test Accuracy: 0.8050, Val Accuracy: 0.7960, Train Accuracy: 0.9286
Training completed.
Seed:  5
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8506, Train: 0.2357, Val: 0.1580, Test: 0.1610
Epoch: 2, Loss: 4.8391, Train: 0.2643, Val: 0.1640, Test: 0.1770
Epoch: 3, Loss: 4.8361, Train: 0.3143, Val: 0.1840, Test: 0.1990
Epoch: 4, Loss: 4.7966, Train: 0.3214, Val: 0.1860, Test: 0.2070
Epoch: 5, Loss: 4.8015, Train: 0.3214, Val: 0.1700, Test: 0.1920
Epoch: 6, Loss: 4.7346, Train: 0.2786, Val: 0.1620, Test: 0.1690
Epoch: 7, Loss: 4.7705, Train: 0.2571, Val: 0.1500, Test: 0.1550
Epoch: 8, Loss: 4.7481, Train: 0.2571, Val: 0.1460, Test: 0.1510
Epoch: 9, Loss: 4.6688, Train: 0.2500, Val: 0.1340, Test: 0.1420
Epoch: 10, Loss: 4.6765, Train: 0.2429, Val: 0.1340, Test: 0.1410
Epoch: 11, Loss: 4.6603, Train: 0.2500, Val: 0.1320, Test: 0.1350
Epoch: 12, Loss: 4.6830, Train: 0.2500, Val: 0.1320, Test: 0.1360
Epoch: 13, Loss: 4.6079, Train: 0.2500, Val: 0.1400, Test: 0.1420
Epoch: 14, Loss: 4.5791, Train: 0.2571, Val: 0.1520, Test: 0.1560
Epoch: 15, Loss: 4.4213, Train: 0.2786, Val: 0.1660, Test: 0.1670
Epoch: 16, Loss: 4.4440, Train: 0.3071, Val: 0.1820, Test: 0.1800
Epoch: 17, Loss: 4.3546, Train: 0.3571, Val: 0.2060, Test: 0.2100
Epoch: 18, Loss: 4.3830, Train: 0.4500, Val: 0.2520, Test: 0.2800
Epoch: 19, Loss: 4.4447, Train: 0.5357, Val: 0.3640, Test: 0.4000
Epoch: 20, Loss: 4.5153, Train: 0.6000, Val: 0.4740, Test: 0.5130
Epoch: 21, Loss: 4.2911, Train: 0.6357, Val: 0.5260, Test: 0.5640
Epoch: 22, Loss: 4.3905, Train: 0.6929, Val: 0.5780, Test: 0.5950
Epoch: 23, Loss: 4.2189, Train: 0.7357, Val: 0.6180, Test: 0.6360
Epoch: 24, Loss: 4.3947, Train: 0.7500, Val: 0.6100, Test: 0.6330
Epoch: 25, Loss: 4.3700, Train: 0.7500, Val: 0.5840, Test: 0.6040
Epoch: 26, Loss: 4.3259, Train: 0.7357, Val: 0.5500, Test: 0.5730
Epoch: 27, Loss: 4.3409, Train: 0.7286, Val: 0.5100, Test: 0.5490
Epoch: 28, Loss: 4.3225, Train: 0.7286, Val: 0.5080, Test: 0.5240
Epoch: 29, Loss: 4.0886, Train: 0.7357, Val: 0.5140, Test: 0.5220
Epoch: 30, Loss: 4.3367, Train: 0.7571, Val: 0.5400, Test: 0.5490
Epoch: 31, Loss: 4.1175, Train: 0.7857, Val: 0.5740, Test: 0.5750
Epoch: 32, Loss: 4.3358, Train: 0.7786, Val: 0.6080, Test: 0.6260
Epoch: 33, Loss: 4.1408, Train: 0.8286, Val: 0.6500, Test: 0.6740
Epoch: 34, Loss: 4.2565, Train: 0.8786, Val: 0.6760, Test: 0.6950
Epoch: 35, Loss: 4.0369, Train: 0.8857, Val: 0.6760, Test: 0.6980
Epoch: 36, Loss: 4.2314, Train: 0.8786, Val: 0.6900, Test: 0.6980
Epoch: 37, Loss: 4.3335, Train: 0.8786, Val: 0.6960, Test: 0.6960
Epoch: 38, Loss: 4.1208, Train: 0.8929, Val: 0.7180, Test: 0.7120
Epoch: 39, Loss: 4.0634, Train: 0.9000, Val: 0.7360, Test: 0.7380
Epoch: 40, Loss: 3.9986, Train: 0.9214, Val: 0.7380, Test: 0.7580
Epoch: 41, Loss: 3.8794, Train: 0.9286, Val: 0.7380, Test: 0.7740
Epoch: 42, Loss: 4.1458, Train: 0.9286, Val: 0.7440, Test: 0.7780
Epoch: 43, Loss: 4.0417, Train: 0.9214, Val: 0.7520, Test: 0.7730
Epoch: 44, Loss: 3.8529, Train: 0.9143, Val: 0.7600, Test: 0.7900
Epoch: 45, Loss: 4.0103, Train: 0.9214, Val: 0.7640, Test: 0.7860
Epoch: 46, Loss: 4.1220, Train: 0.9214, Val: 0.7680, Test: 0.7860
Epoch: 47, Loss: 3.9761, Train: 0.9214, Val: 0.7840, Test: 0.7900
Epoch: 48, Loss: 4.0733, Train: 0.9214, Val: 0.7820, Test: 0.7940
Epoch: 49, Loss: 4.4495, Train: 0.9357, Val: 0.7900, Test: 0.8020
Epoch: 50, Loss: 3.8756, Train: 0.9357, Val: 0.7880, Test: 0.7990
MAD:  0.1623
Best Test Accuracy: 0.8020, Val Accuracy: 0.7900, Train Accuracy: 0.9357
Training completed.
Seed:  6
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8482, Train: 0.3214, Val: 0.1460, Test: 0.1670
Epoch: 2, Loss: 4.8287, Train: 0.3929, Val: 0.1920, Test: 0.2350
Epoch: 3, Loss: 4.8103, Train: 0.4429, Val: 0.2340, Test: 0.2730
Epoch: 4, Loss: 4.7953, Train: 0.5143, Val: 0.2780, Test: 0.3190
Epoch: 5, Loss: 4.7922, Train: 0.5643, Val: 0.3300, Test: 0.3610
Epoch: 6, Loss: 4.7415, Train: 0.5786, Val: 0.3480, Test: 0.3780
Epoch: 7, Loss: 4.6900, Train: 0.5786, Val: 0.3420, Test: 0.3680
Epoch: 8, Loss: 4.6746, Train: 0.5643, Val: 0.3320, Test: 0.3570
Epoch: 9, Loss: 4.6595, Train: 0.5643, Val: 0.3280, Test: 0.3540
Epoch: 10, Loss: 4.6462, Train: 0.5429, Val: 0.3200, Test: 0.3450
Epoch: 11, Loss: 4.6058, Train: 0.5500, Val: 0.3240, Test: 0.3430
Epoch: 12, Loss: 4.4960, Train: 0.5429, Val: 0.3220, Test: 0.3420
Epoch: 13, Loss: 4.4854, Train: 0.5429, Val: 0.3240, Test: 0.3420
Epoch: 14, Loss: 4.2848, Train: 0.5571, Val: 0.3300, Test: 0.3490
Epoch: 15, Loss: 4.4503, Train: 0.5857, Val: 0.3340, Test: 0.3550
Epoch: 16, Loss: 4.4898, Train: 0.6214, Val: 0.3780, Test: 0.3870
Epoch: 17, Loss: 4.3192, Train: 0.6571, Val: 0.3940, Test: 0.4070
Epoch: 18, Loss: 4.2113, Train: 0.6500, Val: 0.3920, Test: 0.4240
Epoch: 19, Loss: 4.1534, Train: 0.6571, Val: 0.3840, Test: 0.3960
Epoch: 20, Loss: 4.1135, Train: 0.6929, Val: 0.4120, Test: 0.4160
Epoch: 21, Loss: 4.3988, Train: 0.8214, Val: 0.5060, Test: 0.5280
Epoch: 22, Loss: 4.2241, Train: 0.8643, Val: 0.5800, Test: 0.6100
Epoch: 23, Loss: 4.3427, Train: 0.8571, Val: 0.5920, Test: 0.6010
Epoch: 24, Loss: 4.2182, Train: 0.8143, Val: 0.5700, Test: 0.5850
Epoch: 25, Loss: 4.1497, Train: 0.8214, Val: 0.5700, Test: 0.5770
Epoch: 26, Loss: 4.3702, Train: 0.8357, Val: 0.5920, Test: 0.5860
Epoch: 27, Loss: 4.3758, Train: 0.8714, Val: 0.6540, Test: 0.6290
Epoch: 28, Loss: 4.3026, Train: 0.8929, Val: 0.6940, Test: 0.6840
Epoch: 29, Loss: 4.2802, Train: 0.9071, Val: 0.7400, Test: 0.7350
Epoch: 30, Loss: 4.3041, Train: 0.9357, Val: 0.7560, Test: 0.7610
Epoch: 31, Loss: 4.0476, Train: 0.9500, Val: 0.7740, Test: 0.7930
Epoch: 32, Loss: 4.3239, Train: 0.9429, Val: 0.7780, Test: 0.8110
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 33, Loss: 3.9990, Train: 0.9214, Val: 0.7840, Test: 0.8060
Epoch: 34, Loss: 4.1846, Train: 0.9143, Val: 0.7860, Test: 0.8030
Epoch: 35, Loss: 4.0676, Train: 0.9000, Val: 0.7900, Test: 0.8110
Epoch: 36, Loss: 4.0321, Train: 0.9071, Val: 0.7920, Test: 0.8130
Epoch: 37, Loss: 4.0603, Train: 0.9000, Val: 0.7860, Test: 0.8140
Epoch: 38, Loss: 3.9562, Train: 0.9000, Val: 0.7860, Test: 0.8070
Epoch: 39, Loss: 3.9966, Train: 0.9071, Val: 0.7860, Test: 0.8070
Epoch: 40, Loss: 4.0108, Train: 0.9071, Val: 0.7860, Test: 0.8030
Epoch: 41, Loss: 3.8861, Train: 0.9071, Val: 0.7740, Test: 0.7980
Epoch: 42, Loss: 3.9337, Train: 0.9071, Val: 0.7780, Test: 0.8010
Epoch: 43, Loss: 4.2246, Train: 0.9143, Val: 0.7820, Test: 0.7930
Epoch: 44, Loss: 4.1028, Train: 0.9143, Val: 0.7920, Test: 0.7880
Epoch: 45, Loss: 3.8431, Train: 0.9143, Val: 0.7940, Test: 0.7830
Epoch: 46, Loss: 3.8042, Train: 0.9286, Val: 0.7860, Test: 0.7800
Epoch: 47, Loss: 3.9542, Train: 0.9357, Val: 0.7880, Test: 0.7830
Epoch: 48, Loss: 4.1431, Train: 0.9429, Val: 0.7960, Test: 0.7750
Epoch: 49, Loss: 4.1423, Train: 0.9429, Val: 0.7900, Test: 0.7710
Epoch: 50, Loss: 3.7901, Train: 0.9429, Val: 0.7900, Test: 0.7720
MAD:  0.1012
Best Test Accuracy: 0.8140, Val Accuracy: 0.7860, Train Accuracy: 0.9000
Training completed.
Seed:  7
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8492, Train: 0.2071, Val: 0.1520, Test: 0.1590
Epoch: 2, Loss: 4.8408, Train: 0.2643, Val: 0.1840, Test: 0.1940
Epoch: 3, Loss: 4.8344, Train: 0.2357, Val: 0.1840, Test: 0.1930
Epoch: 4, Loss: 4.8231, Train: 0.2429, Val: 0.1700, Test: 0.1910
Epoch: 5, Loss: 4.7873, Train: 0.3286, Val: 0.2080, Test: 0.2190
Epoch: 6, Loss: 4.7787, Train: 0.3000, Val: 0.2100, Test: 0.2330
Epoch: 7, Loss: 4.7370, Train: 0.3071, Val: 0.1920, Test: 0.2170
Epoch: 8, Loss: 4.6655, Train: 0.2786, Val: 0.1780, Test: 0.1990
Epoch: 9, Loss: 4.6194, Train: 0.2500, Val: 0.1700, Test: 0.1820
Epoch: 10, Loss: 4.6403, Train: 0.2429, Val: 0.1560, Test: 0.1670
Epoch: 11, Loss: 4.6443, Train: 0.2286, Val: 0.1480, Test: 0.1550
Epoch: 12, Loss: 4.6076, Train: 0.2357, Val: 0.1560, Test: 0.1600
Epoch: 13, Loss: 4.4792, Train: 0.2500, Val: 0.1580, Test: 0.1730
Epoch: 14, Loss: 4.5211, Train: 0.2500, Val: 0.1580, Test: 0.1730
Epoch: 15, Loss: 4.3318, Train: 0.2714, Val: 0.1580, Test: 0.1760
Epoch: 16, Loss: 4.3286, Train: 0.2714, Val: 0.1740, Test: 0.1910
Epoch: 17, Loss: 4.4586, Train: 0.3286, Val: 0.1980, Test: 0.2200
Epoch: 18, Loss: 4.4843, Train: 0.3571, Val: 0.2460, Test: 0.2580
Epoch: 19, Loss: 4.4350, Train: 0.4000, Val: 0.2640, Test: 0.2590
Epoch: 20, Loss: 4.2482, Train: 0.4500, Val: 0.2960, Test: 0.2730
Epoch: 21, Loss: 4.2768, Train: 0.5571, Val: 0.3440, Test: 0.3250
Epoch: 22, Loss: 4.2055, Train: 0.5929, Val: 0.3880, Test: 0.4020
Epoch: 23, Loss: 4.1408, Train: 0.7929, Val: 0.5680, Test: 0.5670
Epoch: 24, Loss: 4.4199, Train: 0.8929, Val: 0.6740, Test: 0.7040
Epoch: 25, Loss: 4.4196, Train: 0.8571, Val: 0.6760, Test: 0.6840
Epoch: 26, Loss: 4.2294, Train: 0.8714, Val: 0.6460, Test: 0.6380
Epoch: 27, Loss: 4.1088, Train: 0.8357, Val: 0.6000, Test: 0.6030
Epoch: 28, Loss: 4.3317, Train: 0.8214, Val: 0.5600, Test: 0.5820
Epoch: 29, Loss: 4.3557, Train: 0.8000, Val: 0.5440, Test: 0.5750
Epoch: 30, Loss: 4.1624, Train: 0.8357, Val: 0.5860, Test: 0.6100
Epoch: 31, Loss: 4.2203, Train: 0.8643, Val: 0.6240, Test: 0.6460
Epoch: 32, Loss: 4.1714, Train: 0.8929, Val: 0.6460, Test: 0.6790
Epoch: 33, Loss: 4.2112, Train: 0.9000, Val: 0.6700, Test: 0.7120
Epoch: 34, Loss: 4.0791, Train: 0.9000, Val: 0.7120, Test: 0.7480
Epoch: 35, Loss: 4.2474, Train: 0.8857, Val: 0.7340, Test: 0.7580
Epoch: 36, Loss: 3.9533, Train: 0.9143, Val: 0.7520, Test: 0.7680
Epoch: 37, Loss: 3.9693, Train: 0.9214, Val: 0.7700, Test: 0.7840
Epoch: 38, Loss: 4.0782, Train: 0.9214, Val: 0.7720, Test: 0.7940
Epoch: 39, Loss: 4.2547, Train: 0.9357, Val: 0.7700, Test: 0.8000
Epoch: 40, Loss: 4.0894, Train: 0.9357, Val: 0.7740, Test: 0.7960
Epoch: 41, Loss: 3.9481, Train: 0.9429, Val: 0.7620, Test: 0.7960
Epoch: 42, Loss: 4.0753, Train: 0.9357, Val: 0.7600, Test: 0.7960
Epoch: 43, Loss: 3.9402, Train: 0.9429, Val: 0.7660, Test: 0.7960
Epoch: 44, Loss: 3.9430, Train: 0.9500, Val: 0.7640, Test: 0.8010
Epoch: 45, Loss: 4.0187, Train: 0.9500, Val: 0.7700, Test: 0.8010
Epoch: 46, Loss: 3.9772, Train: 0.9357, Val: 0.7720, Test: 0.8010
Epoch: 47, Loss: 4.0201, Train: 0.9286, Val: 0.7680, Test: 0.8050
Epoch: 48, Loss: 4.1168, Train: 0.9357, Val: 0.7800, Test: 0.8090
Epoch: 49, Loss: 3.7618, Train: 0.9357, Val: 0.7840, Test: 0.8100
Epoch: 50, Loss: 3.9356, Train: 0.9357, Val: 0.7780, Test: 0.8100
MAD:  0.151
Best Test Accuracy: 0.8100, Val Accuracy: 0.7840, Train Accuracy: 0.9357
Training completed.
Seed:  8
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8542, Train: 0.2143, Val: 0.1840, Test: 0.1920
Epoch: 2, Loss: 4.8415, Train: 0.1643, Val: 0.1400, Test: 0.1470
Epoch: 3, Loss: 4.8330, Train: 0.1571, Val: 0.1240, Test: 0.1320
Epoch: 4, Loss: 4.8161, Train: 0.1571, Val: 0.1220, Test: 0.1300
Epoch: 5, Loss: 4.8036, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 6, Loss: 4.7982, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 7, Loss: 4.7567, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 8, Loss: 4.7534, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 9, Loss: 4.7245, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 10, Loss: 4.7301, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 11, Loss: 4.7082, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 12, Loss: 4.7222, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 13, Loss: 4.6092, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 14, Loss: 4.7519, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 15, Loss: 4.5683, Train: 0.1500, Val: 0.1220, Test: 0.1300
Epoch: 16, Loss: 4.6099, Train: 0.1786, Val: 0.1240, Test: 0.1340
Epoch: 17, Loss: 4.6305, Train: 0.2000, Val: 0.1240, Test: 0.1370
Epoch: 18, Loss: 4.5166, Train: 0.2000, Val: 0.1240, Test: 0.1400
Epoch: 19, Loss: 4.5881, Train: 0.2286, Val: 0.1280, Test: 0.1510
Epoch: 20, Loss: 4.5852, Train: 0.2714, Val: 0.1500, Test: 0.1680
Epoch: 21, Loss: 4.5601, Train: 0.3429, Val: 0.2200, Test: 0.2320
Epoch: 22, Loss: 4.5148, Train: 0.4786, Val: 0.3820, Test: 0.4100
Epoch: 23, Loss: 4.4837, Train: 0.6000, Val: 0.5320, Test: 0.5580
Epoch: 24, Loss: 4.3437, Train: 0.6500, Val: 0.6500, Test: 0.6560
Epoch: 25, Loss: 4.4249, Train: 0.6571, Val: 0.6540, Test: 0.6700
Epoch: 26, Loss: 4.2944, Train: 0.6571, Val: 0.6360, Test: 0.6620
Epoch: 27, Loss: 4.5933, Train: 0.6500, Val: 0.6160, Test: 0.6490
Epoch: 28, Loss: 4.3756, Train: 0.6571, Val: 0.6320, Test: 0.6540
Epoch: 29, Loss: 4.4377, Train: 0.6571, Val: 0.6420, Test: 0.6620
Epoch: 30, Loss: 4.4434, Train: 0.6643, Val: 0.6480, Test: 0.6710
Epoch: 31, Loss: 4.3513, Train: 0.6714, Val: 0.6480, Test: 0.6750
Epoch: 32, Loss: 4.2752, Train: 0.6714, Val: 0.6540, Test: 0.6770
Epoch: 33, Loss: 4.2059, Train: 0.6714, Val: 0.6540, Test: 0.6730
Epoch: 34, Loss: 4.4047, Train: 0.6714, Val: 0.6600, Test: 0.6640
Epoch: 35, Loss: 4.2210, Train: 0.6714, Val: 0.6440, Test: 0.6490
Epoch: 36, Loss: 4.4445, Train: 0.6786, Val: 0.6420, Test: 0.6420
Epoch: 37, Loss: 4.4271, Train: 0.6786, Val: 0.6440, Test: 0.6450
Epoch: 38, Loss: 4.2212, Train: 0.6786, Val: 0.6480, Test: 0.6610
Epoch: 39, Loss: 4.3797, Train: 0.6786, Val: 0.6460, Test: 0.6640
Epoch: 40, Loss: 4.2223, Train: 0.6714, Val: 0.6560, Test: 0.6790
Epoch: 41, Loss: 4.4349, Train: 0.6714, Val: 0.6620, Test: 0.6820
Epoch: 42, Loss: 4.2933, Train: 0.6786, Val: 0.6660, Test: 0.6810
Epoch: 43, Loss: 4.2392, Train: 0.6786, Val: 0.6680, Test: 0.6770
Epoch: 44, Loss: 4.2538, Train: 0.6857, Val: 0.6660, Test: 0.6700
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
/root/code/DIR/DIR-GNN/train/cora.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(f"./pkl/{save_name}.pkl"))
Epoch: 45, Loss: 4.1513, Train: 0.6786, Val: 0.6660, Test: 0.6690
Epoch: 46, Loss: 4.3002, Train: 0.6786, Val: 0.6660, Test: 0.6680
Epoch: 47, Loss: 4.1319, Train: 0.6786, Val: 0.6660, Test: 0.6660
Epoch: 48, Loss: 4.2419, Train: 0.6857, Val: 0.6640, Test: 0.6680
Epoch: 49, Loss: 4.2945, Train: 0.6857, Val: 0.6580, Test: 0.6670
Epoch: 50, Loss: 4.1231, Train: 0.6857, Val: 0.6560, Test: 0.6610
MAD:  0.1144
Best Test Accuracy: 0.6820, Val Accuracy: 0.6620, Train Accuracy: 0.6714
Training completed.
Seed:  9
PMPGNN(
  (convs): ModuleList(
    (0-1): 2 x ParallelGNNBlock(
      (conv1): GCNConv(128, 128)
      (conv2): GCNConv(128, 128)
    )
  )
  (proj): Linear(in_features=1433, out_features=128, bias=True)
  (mlp): Linear(in_features=128, out_features=7, bias=True)
)
Epoch: 1, Loss: 4.8550, Train: 0.1714, Val: 0.2020, Test: 0.2080
Epoch: 2, Loss: 4.8440, Train: 0.2500, Val: 0.3260, Test: 0.3470
Epoch: 3, Loss: 4.8321, Train: 0.3357, Val: 0.3480, Test: 0.3780
Epoch: 4, Loss: 4.8269, Train: 0.3857, Val: 0.3680, Test: 0.3850
Epoch: 5, Loss: 4.8023, Train: 0.4071, Val: 0.3740, Test: 0.3900
Epoch: 6, Loss: 4.7632, Train: 0.4071, Val: 0.3560, Test: 0.3820
Epoch: 7, Loss: 4.7701, Train: 0.4071, Val: 0.2900, Test: 0.3180
Epoch: 8, Loss: 4.6872, Train: 0.3786, Val: 0.2380, Test: 0.2670
Epoch: 9, Loss: 4.7065, Train: 0.3286, Val: 0.2040, Test: 0.2350
Epoch: 10, Loss: 4.6648, Train: 0.3286, Val: 0.1820, Test: 0.2100
Epoch: 11, Loss: 4.5595, Train: 0.3071, Val: 0.1740, Test: 0.1940
Epoch: 12, Loss: 4.5909, Train: 0.2714, Val: 0.1600, Test: 0.1780
Epoch: 13, Loss: 4.5434, Train: 0.2571, Val: 0.1520, Test: 0.1730
Epoch: 14, Loss: 4.3515, Train: 0.2714, Val: 0.1540, Test: 0.1780
Epoch: 15, Loss: 4.4138, Train: 0.2714, Val: 0.1700, Test: 0.2000
Epoch: 16, Loss: 4.3817, Train: 0.2857, Val: 0.1960, Test: 0.2190
Epoch: 17, Loss: 4.4141, Train: 0.3429, Val: 0.2660, Test: 0.2770
Epoch: 18, Loss: 4.4487, Train: 0.3714, Val: 0.2900, Test: 0.3110
Epoch: 19, Loss: 4.3424, Train: 0.4071, Val: 0.3020, Test: 0.3330
Epoch: 20, Loss: 4.2670, Train: 0.5429, Val: 0.4040, Test: 0.4360
Epoch: 21, Loss: 4.4308, Train: 0.6286, Val: 0.5040, Test: 0.5320
Epoch: 22, Loss: 4.2328, Train: 0.7286, Val: 0.6160, Test: 0.6420
Epoch: 23, Loss: 4.1144, Train: 0.8286, Val: 0.7380, Test: 0.7520
Epoch: 24, Loss: 4.3078, Train: 0.8929, Val: 0.7840, Test: 0.8130
Epoch: 25, Loss: 4.3623, Train: 0.9357, Val: 0.7740, Test: 0.8210
Epoch: 26, Loss: 4.1047, Train: 0.9071, Val: 0.7360, Test: 0.7810
Epoch: 27, Loss: 4.1710, Train: 0.8643, Val: 0.7040, Test: 0.7210
Epoch: 28, Loss: 3.9671, Train: 0.8643, Val: 0.7000, Test: 0.6960
Epoch: 29, Loss: 4.4179, Train: 0.8714, Val: 0.7040, Test: 0.6980
Epoch: 30, Loss: 4.1898, Train: 0.8786, Val: 0.7000, Test: 0.7010
Epoch: 31, Loss: 4.2320, Train: 0.8857, Val: 0.7120, Test: 0.7160
Epoch: 32, Loss: 4.3614, Train: 0.8929, Val: 0.7260, Test: 0.7450
Epoch: 33, Loss: 4.1331, Train: 0.8929, Val: 0.7460, Test: 0.7690
Epoch: 34, Loss: 4.3409, Train: 0.9000, Val: 0.7620, Test: 0.7930
Epoch: 35, Loss: 4.1815, Train: 0.9071, Val: 0.7800, Test: 0.8010
Epoch: 36, Loss: 4.0283, Train: 0.9214, Val: 0.7920, Test: 0.8190
Epoch: 37, Loss: 3.9832, Train: 0.9286, Val: 0.7920, Test: 0.8280
Epoch: 38, Loss: 4.1809, Train: 0.9357, Val: 0.7960, Test: 0.8270
Epoch: 39, Loss: 4.0614, Train: 0.9429, Val: 0.8000, Test: 0.8290
Epoch: 40, Loss: 3.7235, Train: 0.9500, Val: 0.7920, Test: 0.8270
Epoch: 41, Loss: 4.3311, Train: 0.9429, Val: 0.7900, Test: 0.8260
Epoch: 42, Loss: 4.2936, Train: 0.9429, Val: 0.7920, Test: 0.8260
Epoch: 43, Loss: 4.3469, Train: 0.9429, Val: 0.7920, Test: 0.8260
Epoch: 44, Loss: 3.8274, Train: 0.9429, Val: 0.7900, Test: 0.8270
Epoch: 45, Loss: 3.8034, Train: 0.9429, Val: 0.7920, Test: 0.8290
Epoch: 46, Loss: 4.1319, Train: 0.9429, Val: 0.7900, Test: 0.8330
Epoch: 47, Loss: 4.0392, Train: 0.9429, Val: 0.7940, Test: 0.8360
Epoch: 48, Loss: 3.7533, Train: 0.9429, Val: 0.7940, Test: 0.8350
Epoch: 49, Loss: 3.6750, Train: 0.9429, Val: 0.7940, Test: 0.8310
Epoch: 50, Loss: 3.9111, Train: 0.9429, Val: 0.7960, Test: 0.8260
MAD:  0.1687
Best Test Accuracy: 0.8360, Val Accuracy: 0.7940, Train Accuracy: 0.9429
Training completed.
Average Test Accuracy:  0.7944000000000001 ± 0.040485058972415965
Average MAD:  0.14750000000000002 ± 0.04583239029332858
